{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=riscv-port\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -373,1 +373,2 @@\n-       test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\"; then\n+       test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\" || \\\n+       test \"x$OPENJDK_TARGET_CPU\" = \"xriscv64\"; then\n","filename":"make\/autoconf\/hotspot.m4","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2018, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -133,0 +133,6 @@\n+  # Because RISC-V only has word-sized atomics, it requries libatomic where\n+  # other common architectures do not.  So link libatomic by default.\n+  if test \"x$OPENJDK_TARGET_OS\" = xlinux && test \"x$OPENJDK_TARGET_CPU\" = xriscv64; then\n+    BASIC_JVM_LIBS=\"$BASIC_JVM_LIBS -latomic\"\n+  fi\n+\n","filename":"make\/autoconf\/libraries.m4","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -557,0 +557,2 @@\n+  elif test \"x$OPENJDK_$1_CPU\" = xriscv64; then\n+    HOTSPOT_$1_CPU_DEFINE=RISCV64\n","filename":"make\/autoconf\/platform.m4","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -153,0 +153,6 @@\n+  ifeq ($(HOTSPOT_TARGET_CPU_ARCH), riscv)\n+    AD_SRC_FILES += $(call uniq, $(wildcard $(foreach d, $(AD_SRC_ROOTS), \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_b.ad \\\n+    )))\n+  endif\n+\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1596,1 +1596,3 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on aarch64\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1827,1 +1827,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on arm\");\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2019, SAP SE. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n@@ -1556,0 +1556,3 @@\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on ppc\");\n@@ -1557,1 +1560,0 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2003, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"oops\/constMethod.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+int AbstractInterpreter::BasicType_as_index(BasicType type) {\n+  int i = 0;\n+  switch (type) {\n+    case T_BOOLEAN: i = 0; break;\n+    case T_CHAR   : i = 1; break;\n+    case T_BYTE   : i = 2; break;\n+    case T_SHORT  : i = 3; break;\n+    case T_INT    : i = 4; break;\n+    case T_LONG   : i = 5; break;\n+    case T_VOID   : i = 6; break;\n+    case T_FLOAT  : i = 7; break;\n+    case T_DOUBLE : i = 8; break;\n+    case T_OBJECT : i = 9; break;\n+    case T_ARRAY  : i = 9; break;\n+    default       : ShouldNotReachHere();\n+  }\n+  assert(0 <= i && i < AbstractInterpreter::number_of_result_handlers,\n+         \"index out of bounds\");\n+  return i;\n+}\n+\n+\/\/ How much stack a method activation needs in words.\n+int AbstractInterpreter::size_top_interpreter_activation(Method* method) {\n+  const int entry_size = frame::interpreter_frame_monitor_size();\n+\n+  \/\/ total overhead size: entry_size + (saved fp thru expr stack\n+  \/\/ bottom).  be sure to change this if you add\/subtract anything\n+  \/\/ to\/from the overhead area\n+  const int overhead_size =\n+    -(frame::interpreter_frame_initial_sp_offset) + entry_size;\n+\n+  const int stub_code = frame::entry_frame_after_call_words;\n+  assert_cond(method != NULL);\n+  const int method_stack = (method->max_locals() + method->max_stack()) *\n+                           Interpreter::stackElementWords;\n+  return (overhead_size + method_stack + stub_code);\n+}\n+\n+\/\/ asm based interpreter deoptimization helpers\n+int AbstractInterpreter::size_activation(int max_stack,\n+                                         int temps,\n+                                         int extra_args,\n+                                         int monitors,\n+                                         int callee_params,\n+                                         int callee_locals,\n+                                         bool is_top_frame) {\n+  \/\/ Note: This calculation must exactly parallel the frame setup\n+  \/\/ in TemplateInterpreterGenerator::generate_method_entry.\n+\n+  \/\/ fixed size of an interpreter frame:\n+  int overhead = frame::sender_sp_offset -\n+                 frame::interpreter_frame_initial_sp_offset;\n+  \/\/ Our locals were accounted for by the caller (or last_frame_adjust\n+  \/\/ on the transistion) Since the callee parameters already account\n+  \/\/ for the callee's params we only need to account for the extra\n+  \/\/ locals.\n+  int size = overhead +\n+             (callee_locals - callee_params) +\n+             monitors * frame::interpreter_frame_monitor_size() +\n+             \/\/ On the top frame, at all times SP <= ESP, and SP is\n+             \/\/ 16-aligned.  We ensure this by adjusting SP on method\n+             \/\/ entry and re-entry to allow room for the maximum size of\n+             \/\/ the expression stack.  When we call another method we bump\n+             \/\/ SP so that no stack space is wasted.  So, only on the top\n+             \/\/ frame do we need to allow max_stack words.\n+             (is_top_frame ? max_stack : temps + extra_args);\n+\n+  \/\/ On riscv we always keep the stack pointer 16-aligned, so we\n+  \/\/ must round up here.\n+  size = align_up(size, 2);\n+\n+  return size;\n+}\n+\n+void AbstractInterpreter::layout_activation(Method* method,\n+                                            int tempcount,\n+                                            int popframe_extra_args,\n+                                            int moncount,\n+                                            int caller_actual_parameters,\n+                                            int callee_param_count,\n+                                            int callee_locals,\n+                                            frame* caller,\n+                                            frame* interpreter_frame,\n+                                            bool is_top_frame,\n+                                            bool is_bottom_frame) {\n+  \/\/ The frame interpreter_frame is guaranteed to be the right size,\n+  \/\/ as determined by a previous call to the size_activation() method.\n+  \/\/ It is also guaranteed to be walkable even though it is in a\n+  \/\/ skeletal state\n+  assert_cond(method != NULL && caller != NULL && interpreter_frame != NULL);\n+  int max_locals = method->max_locals() * Interpreter::stackElementWords;\n+  int extra_locals = (method->max_locals() - method->size_of_parameters()) *\n+    Interpreter::stackElementWords;\n+\n+#ifdef ASSERT\n+  assert(caller->sp() == interpreter_frame->sender_sp(), \"Frame not properly walkable\");\n+#endif\n+\n+  interpreter_frame->interpreter_frame_set_method(method);\n+  \/\/ NOTE the difference in using sender_sp and interpreter_frame_sender_sp\n+  \/\/ interpreter_frame_sender_sp is the original sp of the caller (the unextended_sp)\n+  \/\/ and sender_sp is fp\n+  intptr_t* locals = NULL;\n+  if (caller->is_interpreted_frame()) {\n+    locals = caller->interpreter_frame_last_sp() + caller_actual_parameters - 1;\n+  } else {\n+    locals = interpreter_frame->sender_sp() + max_locals - 1;\n+  }\n+\n+#ifdef ASSERT\n+  if (caller->is_interpreted_frame()) {\n+    assert(locals < caller->fp() + frame::interpreter_frame_initial_sp_offset, \"bad placement\");\n+  }\n+#endif\n+\n+  interpreter_frame->interpreter_frame_set_locals(locals);\n+  BasicObjectLock* montop = interpreter_frame->interpreter_frame_monitor_begin();\n+  BasicObjectLock* monbot = montop - moncount;\n+  interpreter_frame->interpreter_frame_set_monitor_end(monbot);\n+\n+  \/\/ Set last_sp\n+  intptr_t* last_sp = (intptr_t*) monbot -\n+    tempcount*Interpreter::stackElementWords -\n+    popframe_extra_args;\n+  interpreter_frame->interpreter_frame_set_last_sp(last_sp);\n+\n+  \/\/ All frames but the initial (oldest) interpreter frame we fill in have\n+  \/\/ a value for sender_sp that allows walking the stack but isn't\n+  \/\/ truly correct. Correct the value here.\n+  if (extra_locals != 0 &&\n+      interpreter_frame->sender_sp() ==\n+      interpreter_frame->interpreter_frame_sender_sp()) {\n+    interpreter_frame->set_interpreter_frame_sender_sp(caller->sp() +\n+                                                       extra_locals);\n+  }\n+\n+  *interpreter_frame->interpreter_frame_cache_addr() =\n+    method->constants()->cache();\n+  *interpreter_frame->interpreter_frame_mirror_addr() =\n+    method->method_holder()->java_mirror();\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/abstractInterpreter_riscv.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,372 @@\n+\/*\n+ * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include <stdio.h>\n+#include <sys\/types.h>\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"compiler\/disassembler.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+\n+int AbstractAssembler::code_fill_byte() {\n+  return 0;\n+}\n+\n+void Assembler::add(Register Rd, Register Rn, int64_t increment, Register temp) {\n+  if (is_imm_in_range(increment, 12, 0)) {\n+    addi(Rd, Rn, increment);\n+  } else {\n+    assert_different_registers(Rn, temp);\n+    li(temp, increment);\n+    add(Rd, Rn, temp);\n+  }\n+}\n+\n+void Assembler::addw(Register Rd, Register Rn, int64_t increment, Register temp) {\n+  if (is_imm_in_range(increment, 12, 0)) {\n+    addiw(Rd, Rn, increment);\n+  } else {\n+    assert_different_registers(Rn, temp);\n+    li(temp, increment);\n+    addw(Rd, Rn, temp);\n+  }\n+}\n+\n+void Assembler::sub(Register Rd, Register Rn, int64_t decrement, Register temp) {\n+  if (is_imm_in_range(-decrement, 12, 0)) {\n+    addi(Rd, Rn, -decrement);\n+  } else {\n+    assert_different_registers(Rn, temp);\n+    li(temp, decrement);\n+    sub(Rd, Rn, temp);\n+  }\n+}\n+\n+void Assembler::subw(Register Rd, Register Rn, int64_t decrement, Register temp) {\n+  if (is_imm_in_range(-decrement, 12, 0)) {\n+    addiw(Rd, Rn, -decrement);\n+  } else {\n+    assert_different_registers(Rn, temp);\n+    li(temp, decrement);\n+    subw(Rd, Rn, temp);\n+  }\n+}\n+\n+void Assembler::zext_w(Register Rd, Register Rs) {\n+  add_uw(Rd, Rs, zr);\n+}\n+\n+void Assembler::_li(Register Rd, int64_t imm) {\n+  \/\/ int64_t is in range 0x8000 0000 0000 0000 ~ 0x7fff ffff ffff ffff\n+  int shift = 12;\n+  int64_t upper = imm, lower = imm;\n+  \/\/ Split imm to a lower 12-bit sign-extended part and the remainder,\n+  \/\/ because addi will sign-extend the lower imm.\n+  lower = ((int32_t)imm << 20) >> 20;\n+  upper -= lower;\n+\n+  \/\/ Test whether imm is a 32-bit integer.\n+  if (!(((imm) & ~(int64_t)0x7fffffff) == 0 ||\n+        (((imm) & ~(int64_t)0x7fffffff) == ~(int64_t)0x7fffffff))) {\n+    while (((upper >> shift) & 1) == 0) { shift++; }\n+    upper >>= shift;\n+    li(Rd, upper);\n+    slli(Rd, Rd, shift);\n+    if (lower != 0) {\n+      addi(Rd, Rd, lower);\n+    }\n+  } else {\n+    \/\/ 32-bit integer\n+    Register hi_Rd = zr;\n+    if (upper != 0) {\n+      lui(Rd, (int32_t)upper);\n+      hi_Rd = Rd;\n+    }\n+    if (lower != 0 || hi_Rd == zr) {\n+      addiw(Rd, hi_Rd, lower);\n+    }\n+  }\n+}\n+\n+void Assembler::li64(Register Rd, int64_t imm) {\n+   \/\/ Load upper 32 bits. upper = imm[63:32], but if imm[31] == 1 or\n+   \/\/ (imm[31:28] == 0x7ff && imm[19] == 1), upper = imm[63:32] + 1.\n+   int64_t lower = imm & 0xffffffff;\n+   lower -= ((lower << 44) >> 44);\n+   int64_t tmp_imm = ((uint64_t)(imm & 0xffffffff00000000)) + (uint64_t)lower;\n+   int32_t upper = (tmp_imm - (int32_t)lower) >> 32;\n+\n+   \/\/ Load upper 32 bits\n+   int64_t up = upper, lo = upper;\n+   lo = (lo << 52) >> 52;\n+   up -= lo;\n+   up = (int32_t)up;\n+   lui(Rd, up);\n+   addi(Rd, Rd, lo);\n+\n+   \/\/ Load the rest 32 bits.\n+   slli(Rd, Rd, 12);\n+   addi(Rd, Rd, (int32_t)lower >> 20);\n+   slli(Rd, Rd, 12);\n+   lower = ((int32_t)imm << 12) >> 20;\n+   addi(Rd, Rd, lower);\n+   slli(Rd, Rd, 8);\n+   lower = imm & 0xff;\n+   addi(Rd, Rd, lower);\n+}\n+\n+void Assembler::li32(Register Rd, int32_t imm) {\n+  \/\/ int32_t is in range 0x8000 0000 ~ 0x7fff ffff, and imm[31] is the sign bit\n+  int64_t upper = imm, lower = imm;\n+  lower = (imm << 20) >> 20;\n+  upper -= lower;\n+  upper = (int32_t)upper;\n+  \/\/ lui Rd, imm[31:12] + imm[11]\n+  lui(Rd, upper);\n+  \/\/ use addiw to distinguish li32 to li64\n+  addiw(Rd, Rd, lower);\n+}\n+\n+#define INSN(NAME, REGISTER)                                       \\\n+  void Assembler::NAME(const address &dest, Register temp) {       \\\n+    assert_cond(dest != NULL);                                     \\\n+    int64_t distance = dest - pc();                                \\\n+    if (is_imm_in_range(distance, 20, 1)) {                        \\\n+      jal(REGISTER, distance);                                     \\\n+    } else {                                                       \\\n+      assert(temp != noreg, \"temp must not be empty register!\");   \\\n+      int32_t offset = 0;                                          \\\n+      movptr_with_offset(temp, dest, offset);                      \\\n+      jalr(REGISTER, temp, offset);                                \\\n+    }                                                              \\\n+  }                                                                \\\n+  void Assembler::NAME(Label &l, Register temp) {                  \\\n+    jal(REGISTER, l, temp);                                        \\\n+  }                                                                \\\n+\n+  INSN(j,   x0);\n+  INSN(jal, x1);\n+\n+#undef INSN\n+\n+#define INSN(NAME, REGISTER)                                       \\\n+  void Assembler::NAME(Register Rs) {                              \\\n+    jalr(REGISTER, Rs, 0);                                         \\\n+  }\n+\n+  INSN(jr,   x0);\n+  INSN(jalr, x1);\n+\n+#undef INSN\n+\n+void Assembler::ret() {\n+  jalr(x0, x1, 0);\n+}\n+\n+#define INSN(NAME, REGISTER)                                      \\\n+  void Assembler::NAME(const address &dest, Register temp) {      \\\n+    assert_cond(dest != NULL);                                    \\\n+    assert(temp != noreg, \"temp must not be empty register!\");    \\\n+    int64_t distance = dest - pc();                               \\\n+    if (is_offset_in_range(distance, 32)) {                       \\\n+      auipc(temp, distance + 0x800);                              \\\n+      jalr(REGISTER, temp, ((int32_t)distance << 20) >> 20);      \\\n+    } else {                                                      \\\n+      int32_t offset = 0;                                         \\\n+      movptr_with_offset(temp, dest, offset);                     \\\n+      jalr(REGISTER, temp, offset);                               \\\n+    }                                                             \\\n+  }\n+\n+  INSN(call, x1);\n+  INSN(tail, x0);\n+\n+#undef INSN\n+\n+#define INSN(NAME, REGISTER)                                   \\\n+  void Assembler::NAME(const Address &adr, Register temp) {    \\\n+    switch (adr.getMode()) {                                   \\\n+      case Address::literal: {                                 \\\n+        code_section()->relocate(pc(), adr.rspec());           \\\n+        NAME(adr.target(), temp);                              \\\n+        break;                                                 \\\n+      }                                                        \\\n+      case Address::base_plus_offset: {                        \\\n+        int32_t offset = 0;                                    \\\n+        baseOffset(temp, adr, offset);                         \\\n+        jalr(REGISTER, temp, offset);                          \\\n+        break;                                                 \\\n+      }                                                        \\\n+      default:                                                 \\\n+        ShouldNotReachHere();                                  \\\n+    }                                                          \\\n+  }\n+\n+  INSN(j,    x0);\n+  INSN(jal,  x1);\n+  INSN(call, x1);\n+  INSN(tail, x0);\n+\n+#undef INSN\n+\n+void Assembler::wrap_label(Register r1, Register r2, Label &L, compare_and_branch_insn insn,\n+                           compare_and_branch_label_insn neg_insn, bool is_far) {\n+  if (is_far) {\n+    Label done;\n+    (this->*neg_insn)(r1, r2, done, \/* is_far *\/ false);\n+    j(L);\n+    bind(done);\n+  } else {\n+    if (L.is_bound()) {\n+      (this->*insn)(r1, r2, target(L));\n+    } else {\n+      L.add_patch_at(code(), locator());\n+      (this->*insn)(r1, r2, pc());\n+    }\n+  }\n+}\n+\n+void Assembler::wrap_label(Register Rt, Label &L, Register tmp, load_insn_by_temp insn) {\n+  if (L.is_bound()) {\n+    (this->*insn)(Rt, target(L), tmp);\n+  } else {\n+    L.add_patch_at(code(), locator());\n+    (this->*insn)(Rt, pc(), tmp);\n+  }\n+}\n+\n+void Assembler::wrap_label(Register Rt, Label &L, jal_jalr_insn insn) {\n+  if (L.is_bound()) {\n+    (this->*insn)(Rt, target(L));\n+  } else {\n+    L.add_patch_at(code(), locator());\n+    (this->*insn)(Rt, pc());\n+  }\n+}\n+\n+void Assembler::movptr_with_offset(Register Rd, address addr, int32_t &offset) {\n+  uintptr_t imm64 = (uintptr_t)addr;\n+#ifndef PRODUCT\n+  {\n+    char buffer[64];\n+    snprintf(buffer, sizeof(buffer), \"0x%\" PRIx64, imm64);\n+    block_comment(buffer);\n+  }\n+#endif\n+  assert(is_unsigned_imm_in_range(imm64, 47, 0) || (imm64 == (uintptr_t)-1),\n+         \"bit 47 overflows in address constant\");\n+  \/\/ Load upper 31 bits\n+  int32_t imm = imm64 >> 17;\n+  int64_t upper = imm, lower = imm;\n+  lower = (lower << 52) >> 52;\n+  upper -= lower;\n+  upper = (int32_t)upper;\n+  lui(Rd, upper);\n+  addi(Rd, Rd, lower);\n+\n+  \/\/ Load the rest 17 bits.\n+  slli(Rd, Rd, 11);\n+  addi(Rd, Rd, (imm64 >> 6) & 0x7ff);\n+  slli(Rd, Rd, 6);\n+\n+  \/\/ This offset will be used by following jalr\/ld.\n+  offset = imm64 & 0x3f;\n+}\n+\n+void Assembler::movptr(Register Rd, uintptr_t imm64) {\n+  movptr(Rd, (address)imm64);\n+}\n+\n+void Assembler::movptr(Register Rd, address addr) {\n+  int offset = 0;\n+  movptr_with_offset(Rd, addr, offset);\n+  addi(Rd, Rd, offset);\n+}\n+\n+void Assembler::ifence() {\n+  fence_i();\n+  if (UseConservativeFence) {\n+    fence(ir, ir);\n+  }\n+}\n+\n+#define INSN(NAME, NEG_INSN)                                                         \\\n+  void Assembler::NAME(Register Rs, Register Rt, const address &dest) {              \\\n+    NEG_INSN(Rt, Rs, dest);                                                          \\\n+  }                                                                                  \\\n+  void Assembler::NAME(Register Rs, Register Rt, Label &l, bool is_far) {            \\\n+    NEG_INSN(Rt, Rs, l, is_far);                                                     \\\n+  }\n+\n+  INSN(bgt,  blt);\n+  INSN(ble,  bge);\n+  INSN(bgtu, bltu);\n+  INSN(bleu, bgeu);\n+#undef INSN\n+\n+#undef __\n+\n+Address::Address(address target, relocInfo::relocType rtype) : _base(noreg), _offset(0), _mode(literal) {\n+  _target = target;\n+  switch (rtype) {\n+    case relocInfo::oop_type:\n+    case relocInfo::metadata_type:\n+      \/\/ Oops are a special case. Normally they would be their own section\n+      \/\/ but in cases like icBuffer they are literals in the code stream that\n+      \/\/ we don't have a section for. We use none so that we get a literal address\n+      \/\/ which is always patchable.\n+      break;\n+    case relocInfo::external_word_type:\n+      _rspec = external_word_Relocation::spec(target);\n+      break;\n+    case relocInfo::internal_word_type:\n+      _rspec = internal_word_Relocation::spec(target);\n+      break;\n+    case relocInfo::opt_virtual_call_type:\n+      _rspec = opt_virtual_call_Relocation::spec();\n+      break;\n+    case relocInfo::static_call_type:\n+      _rspec = static_call_Relocation::spec();\n+      break;\n+    case relocInfo::runtime_call_type:\n+      _rspec = runtime_call_Relocation::spec();\n+      break;\n+    case relocInfo::poll_type:\n+    case relocInfo::poll_return_type:\n+      _rspec = Relocation::spec_simple(rtype);\n+      break;\n+    case relocInfo::none:\n+      _rspec = RelocationHolder::none;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.cpp","additions":372,"deletions":0,"binary":false,"changes":372,"status":"added"},{"patch":"@@ -0,0 +1,3057 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_ASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_ASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/register.hpp\"\n+#include \"assembler_riscv.inline.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+\n+#define XLEN 64\n+\n+\/\/ definitions of various symbolic names for machine registers\n+\n+\/\/ First intercalls between C and Java which use 8 general registers\n+\/\/ and 8 floating registers\n+\n+class Argument {\n+ public:\n+  enum {\n+    n_int_register_parameters_c   = 8, \/\/ x10, x11, ... x17 (c_rarg0, c_rarg1, ...)\n+    n_float_register_parameters_c = 8, \/\/ f10, f11, ... f17 (c_farg0, c_farg1, ... )\n+\n+    n_int_register_parameters_j   = 8, \/\/ x11, ... x17, x10 (j_rarg0, j_rarg1, ...)\n+    n_float_register_parameters_j = 8  \/\/ f10, f11, ... f17 (j_farg0, j_farg1, ...)\n+  };\n+};\n+\n+\/\/ function argument(caller-save registers)\n+REGISTER_DECLARATION(Register, c_rarg0, x10);\n+REGISTER_DECLARATION(Register, c_rarg1, x11);\n+REGISTER_DECLARATION(Register, c_rarg2, x12);\n+REGISTER_DECLARATION(Register, c_rarg3, x13);\n+REGISTER_DECLARATION(Register, c_rarg4, x14);\n+REGISTER_DECLARATION(Register, c_rarg5, x15);\n+REGISTER_DECLARATION(Register, c_rarg6, x16);\n+REGISTER_DECLARATION(Register, c_rarg7, x17);\n+\n+REGISTER_DECLARATION(FloatRegister, c_farg0, f10);\n+REGISTER_DECLARATION(FloatRegister, c_farg1, f11);\n+REGISTER_DECLARATION(FloatRegister, c_farg2, f12);\n+REGISTER_DECLARATION(FloatRegister, c_farg3, f13);\n+REGISTER_DECLARATION(FloatRegister, c_farg4, f14);\n+REGISTER_DECLARATION(FloatRegister, c_farg5, f15);\n+REGISTER_DECLARATION(FloatRegister, c_farg6, f16);\n+REGISTER_DECLARATION(FloatRegister, c_farg7, f17);\n+\n+\/\/ Symbolically name the register arguments used by the Java calling convention.\n+\/\/ We have control over the convention for java so we can do what we please.\n+\/\/ What pleases us is to offset the java calling convention so that when\n+\/\/ we call a suitable jni method the arguments are lined up and we don't\n+\/\/ have to do much shuffling. A suitable jni method is non-static and a\n+\/\/ small number of arguments.\n+\/\/\n+\/\/ |------------------------------------------------------------------------|\n+\/\/ | c_rarg0  c_rarg1  c_rarg2  c_rarg3  c_rarg4  c_rarg5  c_rarg6  c_rarg7 |\n+\/\/ |------------------------------------------------------------------------|\n+\/\/ | x10      x11      x12      x13      x14      x15      x16      x17     |\n+\/\/ |------------------------------------------------------------------------|\n+\/\/ | j_rarg7  j_rarg0  j_rarg1  j_rarg2  j_rarg3  j_rarg4  j_rarg5  j_rarg6 |\n+\/\/ |------------------------------------------------------------------------|\n+\n+REGISTER_DECLARATION(Register, j_rarg0, c_rarg1);\n+REGISTER_DECLARATION(Register, j_rarg1, c_rarg2);\n+REGISTER_DECLARATION(Register, j_rarg2, c_rarg3);\n+REGISTER_DECLARATION(Register, j_rarg3, c_rarg4);\n+REGISTER_DECLARATION(Register, j_rarg4, c_rarg5);\n+REGISTER_DECLARATION(Register, j_rarg5, c_rarg6);\n+REGISTER_DECLARATION(Register, j_rarg6, c_rarg7);\n+REGISTER_DECLARATION(Register, j_rarg7, c_rarg0);\n+\n+\/\/ Java floating args are passed as per C\n+\n+REGISTER_DECLARATION(FloatRegister, j_farg0, f10);\n+REGISTER_DECLARATION(FloatRegister, j_farg1, f11);\n+REGISTER_DECLARATION(FloatRegister, j_farg2, f12);\n+REGISTER_DECLARATION(FloatRegister, j_farg3, f13);\n+REGISTER_DECLARATION(FloatRegister, j_farg4, f14);\n+REGISTER_DECLARATION(FloatRegister, j_farg5, f15);\n+REGISTER_DECLARATION(FloatRegister, j_farg6, f16);\n+REGISTER_DECLARATION(FloatRegister, j_farg7, f17);\n+\n+\/\/ zero rigster\n+REGISTER_DECLARATION(Register, zr,        x0);\n+\/\/ global pointer\n+REGISTER_DECLARATION(Register, gp,        x3);\n+\/\/ thread pointer\n+REGISTER_DECLARATION(Register, tp,        x4);\n+\n+\/\/ registers used to hold VM data either temporarily within a method\n+\/\/ or across method calls\n+\n+\/\/ volatile (caller-save) registers\n+\n+\/\/ current method -- must be in a call-clobbered register\n+REGISTER_DECLARATION(Register, xmethod,   x31);\n+\/\/ return address\n+REGISTER_DECLARATION(Register, ra,        x1);\n+\n+\/\/ non-volatile (callee-save) registers\n+\n+\/\/ stack pointer\n+REGISTER_DECLARATION(Register, sp,        x2);\n+\/\/ frame pointer\n+REGISTER_DECLARATION(Register, fp,        x8);\n+\/\/ base of heap\n+REGISTER_DECLARATION(Register, xheapbase, x27);\n+\/\/ constant pool cache\n+REGISTER_DECLARATION(Register, xcpool,    x26);\n+\/\/ monitors allocated on stack\n+REGISTER_DECLARATION(Register, xmonitors, x25);\n+\/\/ locals on stack\n+REGISTER_DECLARATION(Register, xlocals,   x24);\n+\n+\/\/ java thread pointer\n+REGISTER_DECLARATION(Register, xthread,   x23);\n+\/\/ bytecode pointer\n+REGISTER_DECLARATION(Register, xbcp,      x22);\n+\/\/ Dispatch table base\n+REGISTER_DECLARATION(Register, xdispatch, x21);\n+\/\/ Java stack pointer\n+REGISTER_DECLARATION(Register, esp,       x20);\n+\n+\/\/ temporary register(caller-save registers)\n+REGISTER_DECLARATION(Register, t0, x5);\n+REGISTER_DECLARATION(Register, t1, x6);\n+REGISTER_DECLARATION(Register, t2, x7);\n+\n+const Register g_INTArgReg[Argument::n_int_register_parameters_c] = {\n+  c_rarg0, c_rarg1, c_rarg2, c_rarg3, c_rarg4, c_rarg5, c_rarg6, c_rarg7\n+};\n+\n+const FloatRegister g_FPArgReg[Argument::n_float_register_parameters_c] = {\n+  c_farg0, c_farg1, c_farg2, c_farg3, c_farg4, c_farg5, c_farg6, c_farg7\n+};\n+\n+#define assert_cond(ARG1) assert(ARG1, #ARG1)\n+\n+\/\/ Addressing modes\n+class Address {\n+ public:\n+\n+  enum mode { no_mode, base_plus_offset, pcrel, literal };\n+\n+ private:\n+  Register _base;\n+  Register _index;\n+  int64_t _offset;\n+  enum mode _mode;\n+\n+  RelocationHolder _rspec;\n+\n+  \/\/ If the target is far we'll need to load the ea of this to a\n+  \/\/ register to reach it. Otherwise if near we can do PC-relative\n+  \/\/ addressing.\n+  address          _target;\n+\n+ public:\n+  Address()\n+    : _base(noreg), _index(noreg), _offset(0), _mode(no_mode), _target(NULL) { }\n+  Address(Register r)\n+    : _base(r), _index(noreg), _offset(0), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, int o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, long o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, long long o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, unsigned int o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, unsigned long o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+  Address(Register r, unsigned long long o)\n+    : _base(r), _index(noreg), _offset(o), _mode(base_plus_offset), _target(NULL) { }\n+#ifdef ASSERT\n+  Address(Register r, ByteSize disp)\n+    : _base(r), _index(noreg), _offset(in_bytes(disp)), _mode(base_plus_offset), _target(0) { }\n+#endif\n+  Address(address target, RelocationHolder const& rspec)\n+    : _base(noreg),\n+      _index(noreg),\n+      _offset(0),\n+      _mode(literal),\n+      _rspec(rspec),\n+      _target(target) { }\n+  Address(address target, relocInfo::relocType rtype = relocInfo::external_word_type);\n+\n+  const Register base() const {\n+    guarantee((_mode == base_plus_offset | _mode == pcrel | _mode == literal), \"wrong mode\");\n+    return _base;\n+  }\n+  long offset() const {\n+    return _offset;\n+  }\n+  Register index() const {\n+    return _index;\n+  }\n+  mode getMode() const {\n+    return _mode;\n+  }\n+\n+  bool uses(Register reg) const { return _base == reg; }\n+  const address target() const { return _target; }\n+  const RelocationHolder& rspec() const { return _rspec; }\n+  ~Address() {\n+    _target = NULL;\n+    _base = NULL;\n+  }\n+};\n+\n+\/\/ Convience classes\n+class RuntimeAddress: public Address {\n+\n+  public:\n+\n+  RuntimeAddress(address target) : Address(target, relocInfo::runtime_call_type) {}\n+  ~RuntimeAddress() {}\n+};\n+\n+class OopAddress: public Address {\n+\n+  public:\n+\n+  OopAddress(address target) : Address(target, relocInfo::oop_type) {}\n+  ~OopAddress() {}\n+};\n+\n+class ExternalAddress: public Address {\n+ private:\n+  static relocInfo::relocType reloc_for_target(address target) {\n+    \/\/ Sometimes ExternalAddress is used for values which aren't\n+    \/\/ exactly addresses, like the card table base.\n+    \/\/ external_word_type can't be used for values in the first page\n+    \/\/ so just skip the reloc in that case.\n+    return external_word_Relocation::can_be_relocated(target) ? relocInfo::external_word_type : relocInfo::none;\n+  }\n+\n+ public:\n+\n+  ExternalAddress(address target) : Address(target, reloc_for_target(target)) {}\n+  ~ExternalAddress() {}\n+};\n+\n+class InternalAddress: public Address {\n+\n+  public:\n+\n+  InternalAddress(address target) : Address(target, relocInfo::internal_word_type) {}\n+  ~InternalAddress() {}\n+};\n+\n+class Assembler : public AbstractAssembler {\n+public:\n+\n+  enum { instruction_size = 4 };\n+\n+  enum RoundingMode {\n+    rne = 0b000,     \/\/ round to Nearest, ties to Even\n+    rtz = 0b001,     \/\/ round towards Zero\n+    rdn = 0b010,     \/\/ round Down (towards eegative infinity)\n+    rup = 0b011,     \/\/ round Up (towards infinity)\n+    rmm = 0b100,     \/\/ round to Nearest, ties to Max Magnitude\n+    rdy = 0b111,     \/\/ in instruction's rm field, selects dynamic rounding mode.In Rounding Mode register, Invalid.\n+  };\n+\n+  void baseOffset32(Register Rd, const Address &adr, int32_t &offset) {\n+    assert(Rd != noreg, \"Rd must not be empty register!\");\n+    guarantee(Rd != adr.base(), \"should use different registers!\");\n+    if (is_offset_in_range(adr.offset(), 32)) {\n+      int32_t imm = adr.offset();\n+      int32_t upper = imm, lower = imm;\n+      lower = (imm << 20) >> 20;\n+      upper -= lower;\n+      lui(Rd, upper);\n+      offset = lower;\n+    } else {\n+      movptr_with_offset(Rd, (address)(uintptr_t)adr.offset(), offset);\n+    }\n+    add(Rd, Rd, adr.base());\n+  }\n+\n+  void baseOffset(Register Rd, const Address &adr, int32_t &offset) {\n+    if (is_offset_in_range(adr.offset(), 12)) {\n+      assert(Rd != noreg, \"Rd must not be empty register!\");\n+      addi(Rd, adr.base(), adr.offset());\n+      offset = 0;\n+    } else {\n+      baseOffset32(Rd, adr, offset);\n+    }\n+  }\n+\n+  void _li(Register Rd, int64_t imm);  \/\/ optimized load immediate\n+  void li32(Register Rd, int32_t imm);\n+  void li64(Register Rd, int64_t imm);\n+  void movptr(Register Rd, address addr);\n+  void movptr_with_offset(Register Rd, address addr, int32_t &offset);\n+  void movptr(Register Rd, uintptr_t imm64);\n+  void ifence();\n+  void j(const address &dest, Register temp = t0);\n+  void j(const Address &adr, Register temp = t0);\n+  void j(Label &l, Register temp = t0);\n+  void jal(Label &l, Register temp = t0);\n+  void jal(const address &dest, Register temp = t0);\n+  void jal(const Address &adr, Register temp = t0);\n+  void jr(Register Rs);\n+  void jalr(Register Rs);\n+  void ret();\n+  void call(const address &dest, Register temp = t0);\n+  void call(const Address &adr, Register temp = t0);\n+  void tail(const address &dest, Register temp = t0);\n+  void tail(const Address &adr, Register temp = t0);\n+  void call(Label &l, Register temp) {\n+    call(target(l), temp);\n+  }\n+  void tail(Label &l, Register temp) {\n+    tail(target(l), temp);\n+  }\n+\n+  static inline uint32_t extract(uint32_t val, unsigned msb, unsigned lsb) {\n+    assert_cond(msb >= lsb && msb <= 31);\n+    unsigned nbits = msb - lsb + 1;\n+    uint32_t mask = (1U << nbits) - 1;\n+    uint32_t result = val >> lsb;\n+    result &= mask;\n+    return result;\n+  }\n+\n+  static inline int32_t sextract(uint32_t val, unsigned msb, unsigned lsb) {\n+    assert_cond(msb >= lsb && msb <= 31);\n+    int32_t result = val << (31 - msb);\n+    result >>= (31 - msb + lsb);\n+    return result;\n+  }\n+\n+  static void patch(address a, unsigned msb, unsigned lsb, unsigned val) {\n+    assert_cond(a != NULL);\n+    assert_cond(msb >= lsb && msb <= 31);\n+    unsigned nbits = msb - lsb + 1;\n+    guarantee(val < (1U << nbits), \"Field too big for insn\");\n+    unsigned mask = (1U << nbits) - 1;\n+    val <<= lsb;\n+    mask <<= lsb;\n+    unsigned target = *(unsigned *)a;\n+    target &= ~mask;\n+    target |= val;\n+    *(unsigned *)a = target;\n+  }\n+\n+  static void patch(address a, unsigned bit, unsigned val) {\n+    patch(a, bit, bit, val);\n+  }\n+\n+  static void patch_reg(address a, unsigned lsb, Register reg) {\n+    patch(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  static void patch_reg(address a, unsigned lsb, FloatRegister reg) {\n+    patch(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  static void patch_reg(address a, unsigned lsb, VectorRegister reg) {\n+    patch(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  void emit(unsigned insn) {\n+    emit_int32((jint)insn);\n+  }\n+\n+  void _halt() {\n+    emit_int32(0);\n+  }\n+\n+\/\/ Register Instruction\n+#define INSN(NAME, op, funct3, funct7)                          \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {          \\\n+    unsigned insn = 0;                                          \\\n+    patch((address)&insn, 6,  0, op);                           \\\n+    patch((address)&insn, 14, 12, funct3);                      \\\n+    patch((address)&insn, 31, 25, funct7);                      \\\n+    patch_reg((address)&insn, 7, Rd);                           \\\n+    patch_reg((address)&insn, 15, Rs1);                         \\\n+    patch_reg((address)&insn, 20, Rs2);                         \\\n+    emit(insn);                                                 \\\n+  }\n+\n+  INSN(_add,  0b0110011, 0b000, 0b0000000);\n+  INSN(_sub,  0b0110011, 0b000, 0b0100000);\n+  INSN(_andr, 0b0110011, 0b111, 0b0000000);\n+  INSN(_orr,  0b0110011, 0b110, 0b0000000);\n+  INSN(_xorr, 0b0110011, 0b100, 0b0000000);\n+  INSN(sll,   0b0110011, 0b001, 0b0000000);\n+  INSN(sra,   0b0110011, 0b101, 0b0100000);\n+  INSN(srl,   0b0110011, 0b101, 0b0000000);\n+  INSN(slt,   0b0110011, 0b010, 0b0000000);\n+  INSN(sltu,  0b0110011, 0b011, 0b0000000);\n+  INSN(_addw, 0b0111011, 0b000, 0b0000000);\n+  INSN(_subw, 0b0111011, 0b000, 0b0100000);\n+  INSN(sllw,  0b0111011, 0b001, 0b0000000);\n+  INSN(sraw,  0b0111011, 0b101, 0b0100000);\n+  INSN(srlw,  0b0111011, 0b101, 0b0000000);\n+  INSN(mul,   0b0110011, 0b000, 0b0000001);\n+  INSN(mulh,  0b0110011, 0b001, 0b0000001);\n+  INSN(mulhsu,0b0110011, 0b010, 0b0000001);\n+  INSN(mulhu, 0b0110011, 0b011, 0b0000001);\n+  INSN(mulw,  0b0111011, 0b000, 0b0000001);\n+  INSN(div,   0b0110011, 0b100, 0b0000001);\n+  INSN(divu,  0b0110011, 0b101, 0b0000001);\n+  INSN(divw,  0b0111011, 0b100, 0b0000001);\n+  INSN(divuw, 0b0111011, 0b101, 0b0000001);\n+  INSN(rem,   0b0110011, 0b110, 0b0000001);\n+  INSN(remu,  0b0110011, 0b111, 0b0000001);\n+  INSN(remw,  0b0111011, 0b110, 0b0000001);\n+  INSN(remuw, 0b0111011, 0b111, 0b0000001);\n+\n+#undef INSN\n+\n+#define INSN_ENTRY_RELOC(result_type, header)                               \\\n+  result_type header {                                                      \\\n+    InstructionMark im(this);                                               \\\n+    guarantee(rtype == relocInfo::internal_word_type,                       \\\n+              \"only internal_word_type relocs make sense here\");            \\\n+    code_section()->relocate(inst_mark(), InternalAddress(dest).rspec());\n+\n+  \/\/ Load\/store register (all modes)\n+#define INSN(NAME, op, funct3)                                                                     \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                      \\\n+    guarantee(is_offset_in_range(offset, 12), \"offset is invalid.\");                               \\\n+    unsigned insn = 0;                                                                             \\\n+    int32_t val = offset & 0xfff;                                                                  \\\n+    patch((address)&insn, 6, 0, op);                                                               \\\n+    patch((address)&insn, 14, 12, funct3);                                                         \\\n+    patch_reg((address)&insn, 15, Rs);                                                             \\\n+    patch_reg((address)&insn, 7, Rd);                                                              \\\n+    patch((address)&insn, 31, 20, val);                                                            \\\n+    emit(insn);                                                                                    \\\n+  }\n+\n+  INSN(lb,  0b0000011, 0b000);\n+  INSN(lbu, 0b0000011, 0b100);\n+  INSN(lh,  0b0000011, 0b001);\n+  INSN(lhu, 0b0000011, 0b101);\n+  INSN(_lw, 0b0000011, 0b010);\n+  INSN(lwu, 0b0000011, 0b110);\n+  INSN(_ld, 0b0000011, 0b011);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                                 \\\n+  void NAME(Register Rd, address dest) {                                                           \\\n+    assert_cond(dest != NULL);                                                                     \\\n+    int64_t distance = (dest - pc());                                                              \\\n+    if (is_offset_in_range(distance, 32)) {                                                        \\\n+      auipc(Rd, (int32_t)distance + 0x800);                                                        \\\n+      NAME(Rd, Rd, ((int32_t)distance << 20) >> 20);                                               \\\n+    } else {                                                                                       \\\n+      int32_t offset = 0;                                                                          \\\n+      movptr_with_offset(Rd, dest, offset);                                                        \\\n+      NAME(Rd, Rd, offset);                                                                        \\\n+    }                                                                                              \\\n+  }                                                                                                \\\n+  INSN_ENTRY_RELOC(void, NAME(Register Rd, address dest, relocInfo::relocType rtype))              \\\n+    NAME(Rd, dest);                                                                                \\\n+  }                                                                                                \\\n+  void NAME(Register Rd, const Address &adr, Register temp = t0) {                                 \\\n+    switch (adr.getMode()) {                                                                       \\\n+      case Address::literal: {                                                                     \\\n+        code_section()->relocate(pc(), adr.rspec());                                               \\\n+        NAME(Rd, adr.target());                                                                    \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      case Address::base_plus_offset: {                                                            \\\n+        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+          NAME(Rd, adr.base(), adr.offset());                                                      \\\n+        } else {                                                                                   \\\n+          int32_t offset = 0;                                                                      \\\n+          if (Rd == adr.base()) {                                                                  \\\n+            baseOffset32(temp, adr, offset);                                                       \\\n+            NAME(Rd, temp, offset);                                                                \\\n+          } else {                                                                                 \\\n+            baseOffset32(Rd, adr, offset);                                                         \\\n+            NAME(Rd, Rd, offset);                                                                  \\\n+          }                                                                                        \\\n+        }                                                                                          \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      default:                                                                                     \\\n+        ShouldNotReachHere();                                                                      \\\n+    }                                                                                              \\\n+  }                                                                                                \\\n+  void NAME(Register Rd, Label &L) {                                                               \\\n+    wrap_label(Rd, L, &Assembler::NAME);                                                           \\\n+  }\n+\n+  INSN(lb);\n+  INSN(lbu);\n+  INSN(lh);\n+  INSN(lhu);\n+  INSN(lw);\n+  INSN(lwu);\n+  INSN(ld);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                                                     \\\n+  void NAME(FloatRegister Rd, Register Rs, const int32_t offset) {                                 \\\n+    guarantee(is_offset_in_range(offset, 12), \"offset is invalid.\");                               \\\n+    unsigned insn = 0;                                                                             \\\n+    uint32_t val = offset & 0xfff;                                                                 \\\n+    patch((address)&insn, 6, 0, op);                                                               \\\n+    patch((address)&insn, 14, 12, funct3);                                                         \\\n+    patch_reg((address)&insn, 15, Rs);                                                             \\\n+    patch_reg((address)&insn, 7, Rd);                                                              \\\n+    patch((address)&insn, 31, 20, val);                                                            \\\n+    emit(insn);                                                                                    \\\n+  }\n+\n+  INSN(flw,  0b0000111, 0b010);\n+  INSN(_fld, 0b0000111, 0b011);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                                 \\\n+  void NAME(FloatRegister Rd, address dest, Register temp = t0) {                                  \\\n+    assert_cond(dest != NULL);                                                                     \\\n+    int64_t distance = (dest - pc());                                                              \\\n+    if (is_offset_in_range(distance, 32)) {                                                        \\\n+      auipc(temp, (int32_t)distance + 0x800);                                                      \\\n+      NAME(Rd, temp, ((int32_t)distance << 20) >> 20);                                             \\\n+    } else {                                                                                       \\\n+      int32_t offset = 0;                                                                          \\\n+      movptr_with_offset(temp, dest, offset);                                                      \\\n+      NAME(Rd, temp, offset);                                                                      \\\n+    }                                                                                              \\\n+  }                                                                                                \\\n+  INSN_ENTRY_RELOC(void, NAME(FloatRegister Rd, address dest, relocInfo::relocType rtype, Register temp = t0)) \\\n+    NAME(Rd, dest, temp);                                                                          \\\n+  }                                                                                                \\\n+  void NAME(FloatRegister Rd, const Address &adr, Register temp = t0) {                            \\\n+    switch (adr.getMode()) {                                                                       \\\n+      case Address::literal: {                                                                     \\\n+        code_section()->relocate(pc(), adr.rspec());                                               \\\n+        NAME(Rd, adr.target(), temp);                                                              \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      case Address::base_plus_offset: {                                                            \\\n+        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+          NAME(Rd, adr.base(), adr.offset());                                                      \\\n+        } else {                                                                                   \\\n+          int32_t offset = 0;                                                                      \\\n+          baseOffset32(temp, adr, offset);                                                         \\\n+          NAME(Rd, temp, offset);                                                                  \\\n+        }                                                                                          \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      default:                                                                                     \\\n+        ShouldNotReachHere();                                                                      \\\n+    }                                                                                              \\\n+  }\n+\n+  INSN(flw);\n+  INSN(fld);\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                                                           \\\n+  void NAME(Register Rs1, Register Rs2, const int64_t offset) {                                          \\\n+    guarantee(is_imm_in_range(offset, 12, 1), \"offset is invalid.\");                                     \\\n+    unsigned insn = 0;                                                                                   \\\n+    uint32_t val  = offset & 0x1fff;                                                                     \\\n+    uint32_t val11 = (val >> 11) & 0x1;                                                                  \\\n+    uint32_t val12 = (val >> 12) & 0x1;                                                                  \\\n+    uint32_t low  = (val >> 1) & 0xf;                                                                    \\\n+    uint32_t high = (val >> 5) & 0x3f;                                                                   \\\n+    patch((address)&insn, 6, 0, op);                                                                     \\\n+    patch((address)&insn, 14, 12, funct3);                                                               \\\n+    patch_reg((address)&insn, 15, Rs1);                                                                  \\\n+    patch_reg((address)&insn, 20, Rs2);                                                                  \\\n+    patch((address)&insn, 7, val11);                                                                     \\\n+    patch((address)&insn, 11, 8, low);                                                                   \\\n+    patch((address)&insn, 30, 25, high);                                                                 \\\n+    patch((address)&insn, 31, val12);                                                                    \\\n+    emit(insn);                                                                                          \\\n+  }\n+\n+  INSN(_beq, 0b1100011, 0b000);\n+  INSN(_bne, 0b1100011, 0b001);\n+  INSN(bge,  0b1100011, 0b101);\n+  INSN(bgeu, 0b1100011, 0b111);\n+  INSN(blt,  0b1100011, 0b100);\n+  INSN(bltu, 0b1100011, 0b110);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                                       \\\n+  void NAME(Register Rs1, Register Rs2, const address dest) {                                            \\\n+    assert_cond(dest != NULL);                                                                           \\\n+    int64_t offset = (dest - pc());                                                                      \\\n+    guarantee(is_imm_in_range(offset, 12, 1), \"offset is invalid.\");                                     \\\n+    NAME(Rs1, Rs2, offset);                                                                              \\\n+  }                                                                                                      \\\n+  INSN_ENTRY_RELOC(void, NAME(Register Rs1, Register Rs2, address dest, relocInfo::relocType rtype))     \\\n+    NAME(Rs1, Rs2, dest);                                                                                \\\n+  }\n+\n+  INSN(beq);\n+  INSN(bne);\n+  INSN(bge);\n+  INSN(bgeu);\n+  INSN(blt);\n+  INSN(bltu);\n+\n+#undef INSN\n+\n+#define INSN(NAME, NEG_INSN)                                                                \\\n+  void NAME(Register Rs1, Register Rs2, Label &L, bool is_far = false) {                    \\\n+    wrap_label(Rs1, Rs2, L, &Assembler::NAME, &Assembler::NEG_INSN, is_far);                \\\n+  }\n+\n+  INSN(beq,  bne);\n+  INSN(bne,  beq);\n+  INSN(blt,  bge);\n+  INSN(bge,  blt);\n+  INSN(bltu, bgeu);\n+  INSN(bgeu, bltu);\n+\n+#undef INSN\n+\n+#define INSN(NAME, REGISTER, op, funct3)                                                                    \\\n+  void NAME(REGISTER Rs1, Register Rs2, const int32_t offset) {                                             \\\n+    guarantee(is_offset_in_range(offset, 12), \"offset is invalid.\");                                        \\\n+    unsigned insn = 0;                                                                                      \\\n+    uint32_t val  = offset & 0xfff;                                                                         \\\n+    uint32_t low  = val & 0x1f;                                                                             \\\n+    uint32_t high = (val >> 5) & 0x7f;                                                                      \\\n+    patch((address)&insn, 6, 0, op);                                                                        \\\n+    patch((address)&insn, 14, 12, funct3);                                                                  \\\n+    patch_reg((address)&insn, 15, Rs2);                                                                     \\\n+    patch_reg((address)&insn, 20, Rs1);                                                                     \\\n+    patch((address)&insn, 11, 7, low);                                                                      \\\n+    patch((address)&insn, 31, 25, high);                                                                    \\\n+    emit(insn);                                                                                             \\\n+  }                                                                                                         \\\n+\n+  INSN(sb,   Register,      0b0100011, 0b000);\n+  INSN(sh,   Register,      0b0100011, 0b001);\n+  INSN(_sw,  Register,      0b0100011, 0b010);\n+  INSN(_sd,  Register,      0b0100011, 0b011);\n+  INSN(fsw,  FloatRegister, 0b0100111, 0b010);\n+  INSN(_fsd, FloatRegister, 0b0100111, 0b011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, REGISTER)                                                                                \\\n+  INSN_ENTRY_RELOC(void, NAME(REGISTER Rs, address dest, relocInfo::relocType rtype, Register temp = t0))   \\\n+    NAME(Rs, dest, temp);                                                                                   \\\n+  }\n+\n+  INSN(sb,  Register);\n+  INSN(sh,  Register);\n+  INSN(sw,  Register);\n+  INSN(sd,  Register);\n+  INSN(fsw, FloatRegister);\n+  INSN(fsd, FloatRegister);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                                 \\\n+  void NAME(Register Rs, address dest, Register temp = t0) {                                       \\\n+    assert_cond(dest != NULL);                                                                     \\\n+    assert_different_registers(Rs, temp);                                                          \\\n+    int64_t distance = (dest - pc());                                                              \\\n+    if (is_offset_in_range(distance, 32)) {                                                        \\\n+      auipc(temp, (int32_t)distance + 0x800);                                                      \\\n+      NAME(Rs, temp, ((int32_t)distance << 20) >> 20);                                             \\\n+    } else {                                                                                       \\\n+      int32_t offset = 0;                                                                          \\\n+      movptr_with_offset(temp, dest, offset);                                                      \\\n+      NAME(Rs, temp, offset);                                                                      \\\n+    }                                                                                              \\\n+  }                                                                                                \\\n+  void NAME(Register Rs, const Address &adr, Register temp = t0) {                                 \\\n+    switch (adr.getMode()) {                                                                       \\\n+      case Address::literal: {                                                                     \\\n+        assert_different_registers(Rs, temp);                                                      \\\n+        code_section()->relocate(pc(), adr.rspec());                                               \\\n+        NAME(Rs, adr.target(), temp);                                                              \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      case Address::base_plus_offset: {                                                            \\\n+        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+          NAME(Rs, adr.base(), adr.offset());                                                      \\\n+        } else {                                                                                   \\\n+          int32_t offset= 0;                                                                       \\\n+          assert_different_registers(Rs, temp);                                                    \\\n+          baseOffset32(temp, adr, offset);                                                         \\\n+          NAME(Rs, temp, offset);                                                                  \\\n+        }                                                                                          \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      default:                                                                                     \\\n+        ShouldNotReachHere();                                                                      \\\n+    }                                                                                              \\\n+  }\n+\n+  INSN(sb);\n+  INSN(sh);\n+  INSN(sw);\n+  INSN(sd);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                                 \\\n+  void NAME(FloatRegister Rs, address dest, Register temp = t0) {                                  \\\n+    assert_cond(dest != NULL);                                                                     \\\n+    int64_t distance = (dest - pc());                                                              \\\n+    if (is_offset_in_range(distance, 32)) {                                                        \\\n+      auipc(temp, (int32_t)distance + 0x800);                                                      \\\n+      NAME(Rs, temp, ((int32_t)distance << 20) >> 20);                                             \\\n+    } else {                                                                                       \\\n+      int32_t offset = 0;                                                                          \\\n+      movptr_with_offset(temp, dest, offset);                                                      \\\n+      NAME(Rs, temp, offset);                                                                      \\\n+    }                                                                                              \\\n+  }                                                                                                \\\n+  void NAME(FloatRegister Rs, const Address &adr, Register temp = t0) {                            \\\n+    switch (adr.getMode()) {                                                                       \\\n+      case Address::literal: {                                                                     \\\n+        code_section()->relocate(pc(), adr.rspec());                                               \\\n+        NAME(Rs, adr.target(), temp);                                                              \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      case Address::base_plus_offset: {                                                            \\\n+        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+          NAME(Rs, adr.base(), adr.offset());                                                      \\\n+        } else {                                                                                   \\\n+          int32_t offset = 0;                                                                      \\\n+          baseOffset32(temp, adr, offset);                                                         \\\n+          NAME(Rs, temp, offset);                                                                  \\\n+        }                                                                                          \\\n+        break;                                                                                     \\\n+      }                                                                                            \\\n+      default:                                                                                     \\\n+        ShouldNotReachHere();                                                                      \\\n+    }                                                                                              \\\n+  }\n+\n+  INSN(fsw);\n+  INSN(fsd);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                                        \\\n+  void NAME(Register Rd, const uint32_t csr, Register Rs1) {                          \\\n+    guarantee(is_unsigned_imm_in_range(csr, 12, 0), \"csr is invalid\");                \\\n+    unsigned insn = 0;                                                                \\\n+    patch((address)&insn, 6, 0, op);                                                  \\\n+    patch((address)&insn, 14, 12, funct3);                                            \\\n+    patch_reg((address)&insn, 7, Rd);                                                 \\\n+    patch_reg((address)&insn, 15, Rs1);                                               \\\n+    patch((address)&insn, 31, 20, csr);                                               \\\n+    emit(insn);                                                                       \\\n+  }\n+\n+  INSN(csrrw, 0b1110011, 0b001);\n+  INSN(csrrs, 0b1110011, 0b010);\n+  INSN(csrrc, 0b1110011, 0b011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                                        \\\n+  void NAME(Register Rd, const uint32_t csr, const uint32_t uimm) {                   \\\n+    guarantee(is_unsigned_imm_in_range(csr, 12, 0), \"csr is invalid\");                \\\n+    guarantee(is_unsigned_imm_in_range(uimm, 5, 0), \"uimm is invalid\");               \\\n+    unsigned insn = 0;                                                                \\\n+    uint32_t val  = uimm & 0x1f;                                                      \\\n+    patch((address)&insn, 6, 0, op);                                                  \\\n+    patch((address)&insn, 14, 12, funct3);                                            \\\n+    patch_reg((address)&insn, 7, Rd);                                                 \\\n+    patch((address)&insn, 19, 15, val);                                               \\\n+    patch((address)&insn, 31, 20, csr);                                               \\\n+    emit(insn);                                                                       \\\n+  }\n+\n+  INSN(csrrwi, 0b1110011, 0b101);\n+  INSN(csrrsi, 0b1110011, 0b110);\n+  INSN(csrrci, 0b1110011, 0b111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op)                                                                        \\\n+  void NAME(Register Rd, const int32_t offset) {                                              \\\n+    guarantee(is_imm_in_range(offset, 20, 1), \"offset is invalid.\");                          \\\n+    unsigned insn = 0;                                                                        \\\n+    patch((address)&insn, 6, 0, op);                                                          \\\n+    patch_reg((address)&insn, 7, Rd);                                                         \\\n+    patch((address)&insn, 19, 12, (uint32_t)((offset >> 12) & 0xff));                         \\\n+    patch((address)&insn, 20, (uint32_t)((offset >> 11) & 0x1));                              \\\n+    patch((address)&insn, 30, 21, (uint32_t)((offset >> 1) & 0x3ff));                         \\\n+    patch((address)&insn, 31, (uint32_t)((offset >> 20) & 0x1));                              \\\n+    emit(insn);                                                                               \\\n+  }\n+\n+  INSN(_jal, 0b1101111);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                                            \\\n+  void NAME(Register Rd, const address dest, Register temp = t0) {                            \\\n+    assert_cond(dest != NULL);                                                                \\\n+    int64_t offset = dest - pc();                                                             \\\n+    if (is_imm_in_range(offset, 20, 1)) {                                                     \\\n+      NAME(Rd, offset);                                                                       \\\n+    } else {                                                                                  \\\n+      assert_different_registers(Rd, temp);                                                   \\\n+      int32_t off = 0;                                                                        \\\n+      movptr_with_offset(temp, dest, off);                                                    \\\n+      jalr(Rd, temp, off);                                                                    \\\n+    }                                                                                         \\\n+  }                                                                                           \\\n+  void NAME(Register Rd, Label &L, Register temp = t0) {                                      \\\n+    assert_different_registers(Rd, temp);                                                     \\\n+    wrap_label(Rd, L, temp, &Assembler::NAME);                                                \\\n+  }\n+\n+  INSN(jal);\n+\n+#undef INSN\n+\n+#undef INSN_ENTRY_RELOC\n+\n+#define INSN(NAME, op, funct)                                                              \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                              \\\n+    guarantee(is_offset_in_range(offset, 12), \"offset is invalid.\");                       \\\n+    unsigned insn = 0;                                                                     \\\n+    patch((address)&insn, 6, 0, op);                                                       \\\n+    patch_reg((address)&insn, 7, Rd);                                                      \\\n+    patch((address)&insn, 14, 12, funct);                                                  \\\n+    patch_reg((address)&insn, 15, Rs);                                                     \\\n+    int32_t val = offset & 0xfff;                                                          \\\n+    patch((address)&insn, 31, 20, val);                                                    \\\n+    emit(insn);                                                                            \\\n+  }\n+\n+  INSN(_jalr, 0b1100111, 0b000);\n+\n+#undef INSN\n+\n+  enum barrier {\n+    i = 0b1000, o = 0b0100, r = 0b0010, w = 0b0001,\n+    ir = i | r, ow = o | w, iorw = i | o | r | w\n+  };\n+\n+  void fence(const uint32_t predecessor, const uint32_t successor) {\n+    unsigned insn = 0;\n+    guarantee(predecessor < 16, \"predecessor is invalid\");\n+    guarantee(successor < 16, \"successor is invalid\");\n+    patch((address)&insn, 6, 0, 0b001111);\n+    patch((address)&insn, 11, 7, 0b00000);\n+    patch((address)&insn, 14, 12, 0b000);\n+    patch((address)&insn, 19, 15, 0b00000);\n+    patch((address)&insn, 23, 20, successor);\n+    patch((address)&insn, 27, 24, predecessor);\n+    patch((address)&insn, 31, 28, 0b0000);\n+    emit(insn);\n+  }\n+\n+#define INSN(NAME, op, funct3, funct7)                      \\\n+  void NAME() {                                             \\\n+    unsigned insn = 0;                                      \\\n+    patch((address)&insn, 6, 0, op);                        \\\n+    patch((address)&insn, 11, 7, 0b00000);                  \\\n+    patch((address)&insn, 14, 12, funct3);                  \\\n+    patch((address)&insn, 19, 15, 0b00000);                 \\\n+    patch((address)&insn, 31, 20, funct7);                  \\\n+    emit(insn);                                             \\\n+  }\n+\n+  INSN(fence_i, 0b0001111, 0b001, 0b000000000000);\n+  INSN(ecall,   0b1110011, 0b000, 0b000000000000);\n+  INSN(_ebreak, 0b1110011, 0b000, 0b000000000001);\n+\n+#undef INSN\n+\n+enum Aqrl {relaxed = 0b00, rl = 0b01, aq = 0b10, aqrl = 0b11};\n+\n+#define INSN(NAME, op, funct3, funct7)                                                  \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2, Aqrl memory_order = aqrl) {        \\\n+    unsigned insn = 0;                                                                  \\\n+    patch((address)&insn, 6, 0, op);                                                    \\\n+    patch((address)&insn, 14, 12, funct3);                                              \\\n+    patch_reg((address)&insn, 7, Rd);                                                   \\\n+    patch_reg((address)&insn, 15, Rs1);                                                 \\\n+    patch_reg((address)&insn, 20, Rs2);                                                 \\\n+    patch((address)&insn, 31, 27, funct7);                                              \\\n+    patch((address)&insn, 26, 25, memory_order);                                        \\\n+    emit(insn);                                                                         \\\n+  }\n+\n+  INSN(amoswap_w, 0b0101111, 0b010, 0b00001);\n+  INSN(amoadd_w,  0b0101111, 0b010, 0b00000);\n+  INSN(amoxor_w,  0b0101111, 0b010, 0b00100);\n+  INSN(amoand_w,  0b0101111, 0b010, 0b01100);\n+  INSN(amoor_w,   0b0101111, 0b010, 0b01000);\n+  INSN(amomin_w,  0b0101111, 0b010, 0b10000);\n+  INSN(amomax_w,  0b0101111, 0b010, 0b10100);\n+  INSN(amominu_w, 0b0101111, 0b010, 0b11000);\n+  INSN(amomaxu_w, 0b0101111, 0b010, 0b11100);\n+  INSN(amoswap_d, 0b0101111, 0b011, 0b00001);\n+  INSN(amoadd_d,  0b0101111, 0b011, 0b00000);\n+  INSN(amoxor_d,  0b0101111, 0b011, 0b00100);\n+  INSN(amoand_d,  0b0101111, 0b011, 0b01100);\n+  INSN(amoor_d,   0b0101111, 0b011, 0b01000);\n+  INSN(amomin_d,  0b0101111, 0b011, 0b10000);\n+  INSN(amomax_d , 0b0101111, 0b011, 0b10100);\n+  INSN(amominu_d, 0b0101111, 0b011, 0b11000);\n+  INSN(amomaxu_d, 0b0101111, 0b011, 0b11100);\n+#undef INSN\n+\n+enum operand_size { int8, int16, int32, uint32, int64 };\n+\n+#define INSN(NAME, op, funct3, funct7)                                              \\\n+  void NAME(Register Rd, Register Rs1, Aqrl memory_order = relaxed) {               \\\n+    unsigned insn = 0;                                                              \\\n+    uint32_t val = memory_order & 0x3;                                              \\\n+    patch((address)&insn, 6, 0, op);                                                \\\n+    patch((address)&insn, 14, 12, funct3);                                          \\\n+    patch_reg((address)&insn, 7, Rd);                                               \\\n+    patch_reg((address)&insn, 15, Rs1);                                             \\\n+    patch((address)&insn, 25, 20, 0b00000);                                         \\\n+    patch((address)&insn, 31, 27, funct7);                                          \\\n+    patch((address)&insn, 26, 25, val);                                             \\\n+    emit(insn);                                                                     \\\n+  }\n+\n+  INSN(lr_w, 0b0101111, 0b010, 0b00010);\n+  INSN(lr_d, 0b0101111, 0b011, 0b00010);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct7)                                                      \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2, Aqrl memory_order = relaxed) {         \\\n+    unsigned insn = 0;                                                                      \\\n+    uint32_t val = memory_order & 0x3;                                                      \\\n+    patch((address)&insn, 6, 0, op);                                                        \\\n+    patch((address)&insn, 14, 12, funct3);                                                  \\\n+    patch_reg((address)&insn, 7, Rd);                                                       \\\n+    patch_reg((address)&insn, 15, Rs2);                                                     \\\n+    patch_reg((address)&insn, 20, Rs1);                                                     \\\n+    patch((address)&insn, 31, 27, funct7);                                                  \\\n+    patch((address)&insn, 26, 25, val);                                                     \\\n+    emit(insn);                                                                             \\\n+  }\n+\n+  INSN(sc_w, 0b0101111, 0b010, 0b00011);\n+  INSN(sc_d, 0b0101111, 0b011, 0b00011);\n+#undef INSN\n+\n+#define INSN(NAME, op, funct5, funct7)                                                      \\\n+  void NAME(FloatRegister Rd, FloatRegister Rs1, RoundingMode rm = rne) {                   \\\n+    unsigned insn = 0;                                                                      \\\n+    patch((address)&insn, 6, 0, op);                                                        \\\n+    patch((address)&insn, 14, 12, rm);                                                      \\\n+    patch((address)&insn, 24, 20, funct5);                                                  \\\n+    patch((address)&insn, 31, 25, funct7);                                                  \\\n+    patch_reg((address)&insn, 7, Rd);                                                       \\\n+    patch_reg((address)&insn, 15, Rs1);                                                     \\\n+    emit(insn);                                                                             \\\n+  }\n+\n+  INSN(fsqrt_s,   0b1010011, 0b00000, 0b0101100);\n+  INSN(fsqrt_d,   0b1010011, 0b00000, 0b0101101);\n+  INSN(fcvt_s_d,  0b1010011, 0b00001, 0b0100000);\n+  INSN(fcvt_d_s,  0b1010011, 0b00000, 0b0100001);\n+#undef INSN\n+\n+\/\/ Immediate Instruction\n+#define INSN(NAME, op, funct3)                                                              \\\n+  void NAME(Register Rd, Register Rs1, int32_t imm) {                                       \\\n+    guarantee(is_imm_in_range(imm, 12, 0), \"Immediate is out of validity\");                 \\\n+    unsigned insn = 0;                                                                      \\\n+    patch((address)&insn, 6, 0, op);                                                        \\\n+    patch((address)&insn, 14, 12, funct3);                                                  \\\n+    patch((address)&insn, 31, 20, imm & 0x00000fff);                                        \\\n+    patch_reg((address)&insn, 7, Rd);                                                       \\\n+    patch_reg((address)&insn, 15, Rs1);                                                     \\\n+    emit(insn);                                                                             \\\n+  }\n+\n+  INSN(_addi,      0b0010011, 0b000);\n+  INSN(slti,       0b0010011, 0b010);\n+  INSN(_addiw,     0b0011011, 0b000);\n+  INSN(_and_imm12, 0b0010011, 0b111);\n+  INSN(ori,        0b0010011, 0b110);\n+  INSN(xori,       0b0010011, 0b100);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                                              \\\n+  void NAME(Register Rd, Register Rs1, uint32_t imm) {                                      \\\n+    guarantee(is_unsigned_imm_in_range(imm, 12, 0), \"Immediate is out of validity\");        \\\n+    unsigned insn = 0;                                                                      \\\n+    patch((address)&insn,6, 0,  op);                                                        \\\n+    patch((address)&insn, 14, 12, funct3);                                                  \\\n+    patch((address)&insn, 31, 20, imm & 0x00000fff);                                        \\\n+    patch_reg((address)&insn, 7, Rd);                                                       \\\n+    patch_reg((address)&insn, 15, Rs1);                                                     \\\n+    emit(insn);                                                                             \\\n+  }\n+\n+  INSN(sltiu, 0b0010011, 0b011);\n+\n+#undef INSN\n+\n+\/\/ Shift Immediate Instruction\n+#define INSN(NAME, op, funct3, funct6)                                   \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {                 \\\n+    guarantee(shamt <= 0x3f, \"Shamt is invalid\");                        \\\n+    unsigned insn = 0;                                                   \\\n+    patch((address)&insn, 6, 0, op);                                     \\\n+    patch((address)&insn, 14, 12, funct3);                               \\\n+    patch((address)&insn, 25, 20, shamt);                                \\\n+    patch((address)&insn, 31, 26, funct6);                               \\\n+    patch_reg((address)&insn, 7, Rd);                                    \\\n+    patch_reg((address)&insn, 15, Rs1);                                  \\\n+    emit(insn);                                                          \\\n+  }\n+\n+  INSN(_slli, 0b0010011, 0b001, 0b000000);\n+  INSN(_srai, 0b0010011, 0b101, 0b010000);\n+  INSN(_srli, 0b0010011, 0b101, 0b000000);\n+\n+#undef INSN\n+\n+\/\/ Shift Word Immediate Instruction\n+#define INSN(NAME, op, funct3, funct7)                                  \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {                \\\n+    guarantee(shamt <= 0x1f, \"Shamt is invalid\");                       \\\n+    unsigned insn = 0;                                                  \\\n+    patch((address)&insn, 6, 0, op);                                    \\\n+    patch((address)&insn, 14, 12, funct3);                              \\\n+    patch((address)&insn, 24, 20, shamt);                               \\\n+    patch((address)&insn, 31, 25, funct7);                              \\\n+    patch_reg((address)&insn, 7, Rd);                                   \\\n+    patch_reg((address)&insn, 15, Rs1);                                 \\\n+    emit(insn);                                                         \\\n+  }\n+\n+  INSN(slliw, 0b0011011, 0b001, 0b0000000);\n+  INSN(sraiw, 0b0011011, 0b101, 0b0100000);\n+  INSN(srliw, 0b0011011, 0b101, 0b0000000);\n+\n+#undef INSN\n+\n+\/\/ Upper Immediate Instruction\n+#define INSN(NAME, op)                                                  \\\n+  void NAME(Register Rd, int32_t imm) {                                 \\\n+    int32_t upperImm = imm >> 12;                                       \\\n+    unsigned insn = 0;                                                  \\\n+    patch((address)&insn, 6, 0, op);                                    \\\n+    patch_reg((address)&insn, 7, Rd);                                   \\\n+    upperImm &= 0x000fffff;                                             \\\n+    patch((address)&insn, 31, 12, upperImm);                            \\\n+    emit(insn);                                                         \\\n+  }\n+\n+  INSN(_lui,  0b0110111);\n+  INSN(auipc, 0b0010111);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Rigster Instruction\n+#define INSN(NAME, op, funct2)                                                                                     \\\n+  void NAME(FloatRegister Rd, FloatRegister Rs1, FloatRegister Rs2, FloatRegister Rs3, RoundingMode rm = rne) {    \\\n+    unsigned insn = 0;                                                                                             \\\n+    patch((address)&insn, 6, 0, op);                                                                               \\\n+    patch((address)&insn, 14, 12, rm);                                                                             \\\n+    patch((address)&insn, 26, 25, funct2);                                                                         \\\n+    patch_reg((address)&insn, 7, Rd);                                                                              \\\n+    patch_reg((address)&insn, 15, Rs1);                                                                            \\\n+    patch_reg((address)&insn, 20, Rs2);                                                                            \\\n+    patch_reg((address)&insn, 27, Rs3);                                                                            \\\n+    emit(insn);                                                                                                    \\\n+  }\n+\n+  INSN(fmadd_s,   0b1000011,  0b00);\n+  INSN(fmsub_s,   0b1000111,  0b00);\n+  INSN(fnmsub_s,  0b1001011,  0b00);\n+  INSN(fnmadd_s,  0b1001111,  0b00);\n+  INSN(fmadd_d,   0b1000011,  0b01);\n+  INSN(fmsub_d,   0b1000111,  0b01);\n+  INSN(fnmsub_d,  0b1001011,  0b01);\n+  INSN(fnmadd_d,  0b1001111,  0b01);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Rigster Instruction\n+#define INSN(NAME, op, funct3, funct7)                                        \\\n+  void NAME(FloatRegister Rd, FloatRegister Rs1, FloatRegister Rs2) {         \\\n+    unsigned insn = 0;                                                        \\\n+    patch((address)&insn, 6, 0, op);                                          \\\n+    patch((address)&insn, 14, 12, funct3);                                    \\\n+    patch((address)&insn, 31, 25, funct7);                                    \\\n+    patch_reg((address)&insn, 7, Rd);                                         \\\n+    patch_reg((address)&insn, 15, Rs1);                                       \\\n+    patch_reg((address)&insn, 20, Rs2);                                       \\\n+    emit(insn);                                                               \\\n+  }\n+\n+  INSN(fsgnj_s,  0b1010011, 0b000, 0b0010000);\n+  INSN(fsgnjn_s, 0b1010011, 0b001, 0b0010000);\n+  INSN(fsgnjx_s, 0b1010011, 0b010, 0b0010000);\n+  INSN(fmin_s,   0b1010011, 0b000, 0b0010100);\n+  INSN(fmax_s,   0b1010011, 0b001, 0b0010100);\n+  INSN(fsgnj_d,  0b1010011, 0b000, 0b0010001);\n+  INSN(fsgnjn_d, 0b1010011, 0b001, 0b0010001);\n+  INSN(fsgnjx_d, 0b1010011, 0b010, 0b0010001);\n+  INSN(fmin_d,   0b1010011, 0b000, 0b0010101);\n+  INSN(fmax_d,   0b1010011, 0b001, 0b0010101);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Rigster Arith Instruction\n+#define INSN(NAME, op, funct3, funct7)                                    \\\n+  void NAME(Register Rd, FloatRegister Rs1, FloatRegister Rs2) {          \\\n+    unsigned insn = 0;                                                    \\\n+    patch((address)&insn, 6, 0, op);                                      \\\n+    patch((address)&insn, 14, 12, funct3);                                \\\n+    patch((address)&insn, 31, 25, funct7);                                \\\n+    patch_reg((address)&insn, 7, Rd);                                     \\\n+    patch_reg((address)&insn, 15, Rs1);                                   \\\n+    patch_reg((address)&insn, 20, Rs2);                                   \\\n+    emit(insn);                                                           \\\n+  }\n+\n+  INSN(feq_s,    0b1010011, 0b010, 0b1010000);\n+  INSN(flt_s,    0b1010011, 0b001, 0b1010000);\n+  INSN(fle_s,    0b1010011, 0b000, 0b1010000);\n+  INSN(feq_d,    0b1010011, 0b010, 0b1010001);\n+  INSN(fle_d,    0b1010011, 0b000, 0b1010001);\n+  INSN(flt_d,    0b1010011, 0b001, 0b1010001);\n+#undef INSN\n+\n+\/\/ Float and Double Arith Instruction\n+#define INSN(NAME, op, funct7)                                                                  \\\n+  void NAME(FloatRegister Rd, FloatRegister Rs1, FloatRegister Rs2, RoundingMode rm = rne) {    \\\n+    unsigned insn = 0;                                                                          \\\n+    patch((address)&insn, 6, 0, op);                                                            \\\n+    patch((address)&insn, 14, 12, rm);                                                          \\\n+    patch((address)&insn, 31, 25, funct7);                                                      \\\n+    patch_reg((address)&insn, 7, Rd);                                                           \\\n+    patch_reg((address)&insn, 15, Rs1);                                                         \\\n+    patch_reg((address)&insn, 20, Rs2);                                                         \\\n+    emit(insn);                                                                                 \\\n+  }\n+\n+  INSN(fadd_s,   0b1010011, 0b0000000);\n+  INSN(fsub_s,   0b1010011, 0b0000100);\n+  INSN(fmul_s,   0b1010011, 0b0001000);\n+  INSN(fdiv_s,   0b1010011, 0b0001100);\n+  INSN(fadd_d,   0b1010011, 0b0000001);\n+  INSN(fsub_d,   0b1010011, 0b0000101);\n+  INSN(fmul_d,   0b1010011, 0b0001001);\n+  INSN(fdiv_d,   0b1010011, 0b0001101);\n+\n+#undef INSN\n+\n+\/\/ Whole Float and Double Conversion Instruction\n+#define INSN(NAME, op, funct5, funct7)                                  \\\n+  void NAME(FloatRegister Rd, Register Rs1, RoundingMode rm = rne) {    \\\n+    unsigned insn = 0;                                                  \\\n+    patch((address)&insn, 6, 0, op);                                    \\\n+    patch((address)&insn, 14, 12, rm);                                  \\\n+    patch((address)&insn, 24, 20, funct5);                              \\\n+    patch((address)&insn, 31, 25, funct7);                              \\\n+    patch_reg((address)&insn, 7, Rd);                                   \\\n+    patch_reg((address)&insn, 15, Rs1);                                 \\\n+    emit(insn);                                                         \\\n+  }\n+\n+  INSN(fcvt_s_w,   0b1010011, 0b00000, 0b1101000);\n+  INSN(fcvt_s_wu,  0b1010011, 0b00001, 0b1101000);\n+  INSN(fcvt_s_l,   0b1010011, 0b00010, 0b1101000);\n+  INSN(fcvt_s_lu,  0b1010011, 0b00011, 0b1101000);\n+  INSN(fcvt_d_w,   0b1010011, 0b00000, 0b1101001);\n+  INSN(fcvt_d_wu,  0b1010011, 0b00001, 0b1101001);\n+  INSN(fcvt_d_l,   0b1010011, 0b00010, 0b1101001);\n+  INSN(fcvt_d_lu,  0b1010011, 0b00011, 0b1101001);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Conversion Instruction\n+#define INSN(NAME, op, funct5, funct7)                                  \\\n+  void NAME(Register Rd, FloatRegister Rs1, RoundingMode rm = rtz) {    \\\n+    unsigned insn = 0;                                                  \\\n+    patch((address)&insn, 6, 0, op);                                    \\\n+    patch((address)&insn, 14, 12, rm);                                  \\\n+    patch((address)&insn, 24, 20, funct5);                              \\\n+    patch((address)&insn, 31, 25, funct7);                              \\\n+    patch_reg((address)&insn, 7, Rd);                                   \\\n+    patch_reg((address)&insn, 15, Rs1);                                 \\\n+    emit(insn);                                                         \\\n+  }\n+\n+  INSN(fcvt_w_s,   0b1010011, 0b00000, 0b1100000);\n+  INSN(fcvt_l_s,   0b1010011, 0b00010, 0b1100000);\n+  INSN(fcvt_wu_s,  0b1010011, 0b00001, 0b1100000);\n+  INSN(fcvt_lu_s,  0b1010011, 0b00011, 0b1100000);\n+  INSN(fcvt_w_d,   0b1010011, 0b00000, 0b1100001);\n+  INSN(fcvt_wu_d,  0b1010011, 0b00001, 0b1100001);\n+  INSN(fcvt_l_d,   0b1010011, 0b00010, 0b1100001);\n+  INSN(fcvt_lu_d,  0b1010011, 0b00011, 0b1100001);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Move Instruction\n+#define INSN(NAME, op, funct3, funct5, funct7)       \\\n+  void NAME(FloatRegister Rd, Register Rs1) {        \\\n+    unsigned insn = 0;                               \\\n+    patch((address)&insn, 6, 0, op);                 \\\n+    patch((address)&insn, 14, 12, funct3);           \\\n+    patch((address)&insn, 20, funct5);               \\\n+    patch((address)&insn, 31, 25, funct7);           \\\n+    patch_reg((address)&insn, 7, Rd);                \\\n+    patch_reg((address)&insn, 15, Rs1);              \\\n+    emit(insn);                                      \\\n+  }\n+\n+  INSN(fmv_w_x,  0b1010011, 0b000, 0b00000, 0b1111000);\n+  INSN(fmv_d_x,  0b1010011, 0b000, 0b00000, 0b1111001);\n+\n+#undef INSN\n+\n+\/\/ Float and Double Conversion Instruction\n+#define INSN(NAME, op, funct3, funct5, funct7)            \\\n+  void NAME(Register Rd, FloatRegister Rs1) {             \\\n+    unsigned insn = 0;                                    \\\n+    patch((address)&insn, 6, 0, op);                      \\\n+    patch((address)&insn, 14, 12, funct3);                \\\n+    patch((address)&insn, 20, funct5);                    \\\n+    patch((address)&insn, 31, 25, funct7);                \\\n+    patch_reg((address)&insn, 7, Rd);                     \\\n+    patch_reg((address)&insn, 15, Rs1);                   \\\n+    emit(insn);                                           \\\n+  }\n+\n+  INSN(fclass_s, 0b1010011, 0b001, 0b00000, 0b1110000);\n+  INSN(fclass_d, 0b1010011, 0b001, 0b00000, 0b1110001);\n+  INSN(fmv_x_w,  0b1010011, 0b000, 0b00000, 0b1110000);\n+  INSN(fmv_x_d,  0b1010011, 0b000, 0b00000, 0b1110001);\n+\n+#undef INSN\n+\n+\/\/ ==========================\n+\/\/ RISC-V Vector Extension\n+\/\/ ==========================\n+enum SEW {\n+  e8,\n+  e16,\n+  e32,\n+  e64,\n+  RESERVED,\n+};\n+\n+enum LMUL {\n+  mf8 = 0b101,\n+  mf4 = 0b110,\n+  mf2 = 0b111,\n+  m1  = 0b000,\n+  m2  = 0b001,\n+  m4  = 0b010,\n+  m8  = 0b011,\n+};\n+\n+enum VMA {\n+  mu, \/\/ undisturbed\n+  ma, \/\/ agnostic\n+};\n+\n+enum VTA {\n+  tu, \/\/ undisturbed\n+  ta, \/\/ agnostic\n+};\n+\n+static Assembler::SEW elembytes_to_sew(int ebytes) {\n+  assert(ebytes > 0 && ebytes <= 8, \"unsupported element size\");\n+  return (Assembler::SEW) exact_log2(ebytes);\n+}\n+\n+static Assembler::SEW elemtype_to_sew(BasicType etype) {\n+  return Assembler::elembytes_to_sew(type2aelembytes(etype));\n+}\n+\n+#define patch_vtype(hsb, lsb, vlmul, vsew, vta, vma, vill)   \\\n+    if (vill == 1) {                                         \\\n+      guarantee((vlmul | vsew | vta | vma == 0),             \\\n+                \"the other bits in vtype shall be zero\");    \\\n+    }                                                        \\\n+    patch((address)&insn, lsb + 2, lsb, vlmul);              \\\n+    patch((address)&insn, lsb + 5, lsb + 3, vsew);           \\\n+    patch((address)&insn, lsb + 6, vta);                     \\\n+    patch((address)&insn, lsb + 7, vma);                     \\\n+    patch((address)&insn, hsb - 1, lsb + 8, 0);              \\\n+    patch((address)&insn, hsb, vill)\n+\n+#define INSN(NAME, op, funct3)                                            \\\n+  void NAME(Register Rd, Register Rs1, SEW sew, LMUL lmul = m1,           \\\n+            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n+    unsigned insn = 0;                                                    \\\n+    patch((address)&insn, 6, 0, op);                                      \\\n+    patch((address)&insn, 14, 12, funct3);                                \\\n+    patch_vtype(30, 20, lmul, sew, vta, vma, vill);                       \\\n+    patch((address)&insn, 31, 0);                                         \\\n+    patch_reg((address)&insn, 7, Rd);                                     \\\n+    patch_reg((address)&insn, 15, Rs1);                                   \\\n+    emit(insn);                                                           \\\n+  }\n+\n+  INSN(vsetvli, 0b1010111, 0b111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                            \\\n+  void NAME(Register Rd, uint32_t imm, SEW sew, LMUL lmul = m1,           \\\n+            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n+    unsigned insn = 0;                                                    \\\n+    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");     \\\n+    patch((address)&insn, 6, 0, op);                                      \\\n+    patch((address)&insn, 14, 12, funct3);                                \\\n+    patch((address)&insn, 19, 15, imm);                                   \\\n+    patch_vtype(29, 20, lmul, sew, vta, vma, vill);                       \\\n+    patch((address)&insn, 31, 30, 0b11);                                  \\\n+    patch_reg((address)&insn, 7, Rd);                                     \\\n+    emit(insn);                                                           \\\n+  }\n+\n+  INSN(vsetivli, 0b1010111, 0b111);\n+\n+#undef INSN\n+\n+#undef patch_vtype\n+\n+#define INSN(NAME, op, funct3, funct7)                          \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {          \\\n+    unsigned insn = 0;                                          \\\n+    patch((address)&insn, 6,  0, op);                           \\\n+    patch((address)&insn, 14, 12, funct3);                      \\\n+    patch((address)&insn, 31, 25, funct7);                      \\\n+    patch_reg((address)&insn, 7, Rd);                           \\\n+    patch_reg((address)&insn, 15, Rs1);                         \\\n+    patch_reg((address)&insn, 20, Rs2);                         \\\n+    emit(insn);                                                 \\\n+  }\n+\n+  \/\/ Vector Configuration Instruction\n+  INSN(vsetvl, 0b1010111, 0b111, 0b1000000);\n+\n+#undef INSN\n+\n+enum VectorMask {\n+  v0_t = 0b0,\n+  unmasked = 0b1\n+};\n+\n+#define patch_VArith(op, Reg, funct3, Reg_or_Imm5, Vs2, vm, funct6)            \\\n+    unsigned insn = 0;                                                         \\\n+    patch((address)&insn, 6, 0, op);                                           \\\n+    patch((address)&insn, 14, 12, funct3);                                     \\\n+    patch((address)&insn, 19, 15, Reg_or_Imm5);                                \\\n+    patch((address)&insn, 25, vm);                                             \\\n+    patch((address)&insn, 31, 26, funct6);                                     \\\n+    patch_reg((address)&insn, 7, Reg);                                         \\\n+    patch_reg((address)&insn, 20, Vs2);                                        \\\n+    emit(insn)\n+\n+\/\/ r2_vm\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(Register Rd, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Mask\n+  INSN(vpopc_m,  0b1010111, 0b010, 0b10000, 0b010000);\n+  INSN(vfirst_m, 0b1010111, 0b010, 0b10001, 0b010000);\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Integer Extension\n+  INSN(vzext_vf2, 0b1010111, 0b010, 0b00110, 0b010010);\n+  INSN(vzext_vf4, 0b1010111, 0b010, 0b00100, 0b010010);\n+  INSN(vzext_vf8, 0b1010111, 0b010, 0b00010, 0b010010);\n+  INSN(vsext_vf2, 0b1010111, 0b010, 0b00111, 0b010010);\n+  INSN(vsext_vf4, 0b1010111, 0b010, 0b00101, 0b010010);\n+  INSN(vsext_vf8, 0b1010111, 0b010, 0b00011, 0b010010);\n+\n+  \/\/ Vector Mask\n+  INSN(vmsbf_m,   0b1010111, 0b010, 0b00001, 0b010100);\n+  INSN(vmsif_m,   0b1010111, 0b010, 0b00011, 0b010100);\n+  INSN(vmsof_m,   0b1010111, 0b010, 0b00010, 0b010100);\n+  INSN(viota_m,   0b1010111, 0b010, 0b10000, 0b010100);\n+\n+  \/\/ Vector Single-Width Floating-Point\/Integer Type-Convert Instructions\n+  INSN(vfcvt_xu_f_v, 0b1010111, 0b001, 0b00000, 0b010010);\n+  INSN(vfcvt_x_f_v,  0b1010111, 0b001, 0b00001, 0b010010);\n+  INSN(vfcvt_f_xu_v, 0b1010111, 0b001, 0b00010, 0b010010);\n+  INSN(vfcvt_f_x_v,  0b1010111, 0b001, 0b00011, 0b010010);\n+  INSN(vfcvt_rtz_xu_f_v, 0b1010111, 0b001, 0b00110, 0b010010);\n+  INSN(vfcvt_rtz_x_f_v,  0b1010111, 0b001, 0b00111, 0b010010);\n+\n+  \/\/ Vector Floating-Point Instruction\n+  INSN(vfsqrt_v,  0b1010111, 0b001, 0b00000, 0b010011);\n+  INSN(vfclass_v, 0b1010111, 0b001, 0b10000, 0b010011);\n+\n+#undef INSN\n+\n+\/\/ r2rd\n+#define INSN(NAME, op, funct3, simm5, vm, funct6)         \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2) {      \\\n+    patch_VArith(op, Vd, funct3, simm5, Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Whole Vector Register Move\n+  INSN(vmv1r_v, 0b1010111, 0b011, 0b00000, 0b1, 0b100111);\n+  INSN(vmv2r_v, 0b1010111, 0b011, 0b00001, 0b1, 0b100111);\n+  INSN(vmv4r_v, 0b1010111, 0b011, 0b00011, 0b1, 0b100111);\n+  INSN(vmv8r_v, 0b1010111, 0b011, 0b00111, 0b1, 0b100111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, vm, funct6)           \\\n+  void NAME(FloatRegister Rd, VectorRegister Vs2) {       \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);   \\\n+  }\n+\n+  \/\/ Vector Floating-Point Move Instruction\n+  INSN(vfmv_f_s, 0b1010111, 0b001, 0b00000, 0b1, 0b010000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, vm, funct6)          \\\n+  void NAME(Register Rd, VectorRegister Vs2) {           \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);  \\\n+  }\n+\n+  \/\/ Vector Integer Scalar Move Instructions\n+  INSN(vmv_x_s, 0b1010111, 0b010, 0b00000, 0b1, 0b010000);\n+\n+#undef INSN\n+\n+\/\/ r_vm\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, uint32_t imm, VectorMask vm = unmasked) {       \\\n+    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");                              \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n+  }\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vi,    0b1010111, 0b011, 0b101001);\n+  INSN(vsrl_vi,    0b1010111, 0b011, 0b101000);\n+  INSN(vsll_vi,    0b1010111, 0b011, 0b100101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs1, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n+  INSN(vfnmsub_vv, 0b1010111, 0b001, 0b101011);\n+  INSN(vfmsub_vv,  0b1010111, 0b001, 0b101010);\n+  INSN(vfnmadd_vv, 0b1010111, 0b001, 0b101001);\n+  INSN(vfmadd_vv,  0b1010111, 0b001, 0b101000);\n+  INSN(vfnmsac_vv, 0b1010111, 0b001, 0b101111);\n+  INSN(vfmsac_vv,  0b1010111, 0b001, 0b101110);\n+  INSN(vfmacc_vv,  0b1010111, 0b001, 0b101100);\n+  INSN(vfnmacc_vv, 0b1010111, 0b001, 0b101101);\n+\n+  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n+  INSN(vnmsub_vv, 0b1010111, 0b010, 0b101011);\n+  INSN(vmadd_vv,  0b1010111, 0b010, 0b101001);\n+  INSN(vnmsac_vv, 0b1010111, 0b010, 0b101111);\n+  INSN(vmacc_vv,  0b1010111, 0b010, 0b101101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n+  INSN(vnmsub_vx, 0b1010111, 0b110, 0b101011);\n+  INSN(vmadd_vx,  0b1010111, 0b110, 0b101001);\n+  INSN(vnmsac_vx, 0b1010111, 0b110, 0b101111);\n+  INSN(vmacc_vx,  0b1010111, 0b110, 0b101101);\n+\n+  INSN(vrsub_vx,  0b1010111, 0b100, 0b000011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, FloatRegister Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {  \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n+  INSN(vfnmsub_vf, 0b1010111, 0b101, 0b101011);\n+  INSN(vfmsub_vf,  0b1010111, 0b101, 0b101010);\n+  INSN(vfnmadd_vf, 0b1010111, 0b101, 0b101001);\n+  INSN(vfmadd_vf,  0b1010111, 0b101, 0b101000);\n+  INSN(vfnmsac_vf, 0b1010111, 0b101, 0b101111);\n+  INSN(vfmsac_vf,  0b1010111, 0b101, 0b101110);\n+  INSN(vfmacc_vf,  0b1010111, 0b101, 0b101100);\n+  INSN(vfnmacc_vf, 0b1010111, 0b101, 0b101101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Reduction Instructions\n+  INSN(vfredsum_vs,   0b1010111, 0b001, 0b000001);\n+  INSN(vfredosum_vs,  0b1010111, 0b001, 0b000011);\n+  INSN(vfredmin_vs,   0b1010111, 0b001, 0b000101);\n+  INSN(vfredmax_vs,   0b1010111, 0b001, 0b000111);\n+\n+  \/\/ Vector Single-Width Integer Reduction Instructions\n+  INSN(vredsum_vs,    0b1010111, 0b010, 0b000000);\n+  INSN(vredand_vs,    0b1010111, 0b010, 0b000001);\n+  INSN(vredor_vs,     0b1010111, 0b010, 0b000010);\n+  INSN(vredxor_vs,    0b1010111, 0b010, 0b000011);\n+  INSN(vredminu_vs,   0b1010111, 0b010, 0b000100);\n+  INSN(vredmin_vs,    0b1010111, 0b010, 0b000101);\n+  INSN(vredmaxu_vs,   0b1010111, 0b010, 0b000110);\n+  INSN(vredmax_vs,    0b1010111, 0b010, 0b000111);\n+\n+  \/\/ Vector Floating-Point Compare Instructions\n+  INSN(vmfle_vv, 0b1010111, 0b001, 0b011001);\n+  INSN(vmflt_vv, 0b1010111, 0b001, 0b011011);\n+  INSN(vmfne_vv, 0b1010111, 0b001, 0b011100);\n+  INSN(vmfeq_vv, 0b1010111, 0b001, 0b011000);\n+\n+  \/\/ Vector Floating-Point Sign-Injection Instructions\n+  INSN(vfsgnjx_vv, 0b1010111, 0b001, 0b001010);\n+  INSN(vfsgnjn_vv, 0b1010111, 0b001, 0b001001);\n+  INSN(vfsgnj_vv,  0b1010111, 0b001, 0b001000);\n+\n+  \/\/ Vector Floating-Point MIN\/MAX Instructions\n+  INSN(vfmax_vv,   0b1010111, 0b001, 0b000110);\n+  INSN(vfmin_vv,   0b1010111, 0b001, 0b000100);\n+\n+  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n+  INSN(vfdiv_vv,   0b1010111, 0b001, 0b100000);\n+  INSN(vfmul_vv,   0b1010111, 0b001, 0b100100);\n+\n+  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n+  INSN(vfsub_vv, 0b1010111, 0b001, 0b000010);\n+  INSN(vfadd_vv, 0b1010111, 0b001, 0b000000);\n+\n+  \/\/ Vector Single-Width Fractional Multiply with Rounding and Saturation\n+  INSN(vsmul_vv, 0b1010111, 0b000, 0b100111);\n+\n+  \/\/ Vector Integer Divide Instructions\n+  INSN(vrem_vv,  0b1010111, 0b010, 0b100011);\n+  INSN(vremu_vv, 0b1010111, 0b010, 0b100010);\n+  INSN(vdiv_vv,  0b1010111, 0b010, 0b100001);\n+  INSN(vdivu_vv, 0b1010111, 0b010, 0b100000);\n+\n+  \/\/ Vector Single-Width Integer Multiply Instructions\n+  INSN(vmulhsu_vv, 0b1010111, 0b010, 0b100110);\n+  INSN(vmulhu_vv,  0b1010111, 0b010, 0b100100);\n+  INSN(vmulh_vv,   0b1010111, 0b010, 0b100111);\n+  INSN(vmul_vv,    0b1010111, 0b010, 0b100101);\n+\n+  \/\/ Vector Integer Min\/Max Instructions\n+  INSN(vmax_vv,  0b1010111, 0b000, 0b000111);\n+  INSN(vmaxu_vv, 0b1010111, 0b000, 0b000110);\n+  INSN(vmin_vv,  0b1010111, 0b000, 0b000101);\n+  INSN(vminu_vv, 0b1010111, 0b000, 0b000100);\n+\n+  \/\/ Vector Integer Comparison Instructions\n+  INSN(vmsle_vv,  0b1010111, 0b000, 0b011101);\n+  INSN(vmsleu_vv, 0b1010111, 0b000, 0b011100);\n+  INSN(vmslt_vv,  0b1010111, 0b000, 0b011011);\n+  INSN(vmsltu_vv, 0b1010111, 0b000, 0b011010);\n+  INSN(vmsne_vv,  0b1010111, 0b000, 0b011001);\n+  INSN(vmseq_vv,  0b1010111, 0b000, 0b011000);\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vv, 0b1010111, 0b000, 0b101001);\n+  INSN(vsrl_vv, 0b1010111, 0b000, 0b101000);\n+  INSN(vsll_vv, 0b1010111, 0b000, 0b100101);\n+\n+  \/\/ Vector Bitwise Logical Instructions\n+  INSN(vxor_vv, 0b1010111, 0b000, 0b001011);\n+  INSN(vor_vv,  0b1010111, 0b000, 0b001010);\n+  INSN(vand_vv, 0b1010111, 0b000, 0b001001);\n+\n+  \/\/ Vector Single-Width Integer Add and Subtract\n+  INSN(vsub_vv, 0b1010111, 0b000, 0b000010);\n+  INSN(vadd_vv, 0b1010111, 0b000, 0b000000);\n+\n+#undef INSN\n+\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, Register Rs1, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Integer Divide Instructions\n+  INSN(vrem_vx,  0b1010111, 0b110, 0b100011);\n+  INSN(vremu_vx, 0b1010111, 0b110, 0b100010);\n+  INSN(vdiv_vx,  0b1010111, 0b110, 0b100001);\n+  INSN(vdivu_vx, 0b1010111, 0b110, 0b100000);\n+\n+  \/\/ Vector Single-Width Integer Multiply Instructions\n+  INSN(vmulhsu_vx, 0b1010111, 0b110, 0b100110);\n+  INSN(vmulhu_vx,  0b1010111, 0b110, 0b100100);\n+  INSN(vmulh_vx,   0b1010111, 0b110, 0b100111);\n+  INSN(vmul_vx,    0b1010111, 0b110, 0b100101);\n+\n+  \/\/ Vector Integer Min\/Max Instructions\n+  INSN(vmax_vx,  0b1010111, 0b100, 0b000111);\n+  INSN(vmaxu_vx, 0b1010111, 0b100, 0b000110);\n+  INSN(vmin_vx,  0b1010111, 0b100, 0b000101);\n+  INSN(vminu_vx, 0b1010111, 0b100, 0b000100);\n+\n+  \/\/ Vector Integer Comparison Instructions\n+  INSN(vmsgt_vx,  0b1010111, 0b100, 0b011111);\n+  INSN(vmsgtu_vx, 0b1010111, 0b100, 0b011110);\n+  INSN(vmsle_vx,  0b1010111, 0b100, 0b011101);\n+  INSN(vmsleu_vx, 0b1010111, 0b100, 0b011100);\n+  INSN(vmslt_vx,  0b1010111, 0b100, 0b011011);\n+  INSN(vmsltu_vx, 0b1010111, 0b100, 0b011010);\n+  INSN(vmsne_vx,  0b1010111, 0b100, 0b011001);\n+  INSN(vmseq_vx,  0b1010111, 0b100, 0b011000);\n+\n+  \/\/ Vector Narrowing Integer Right Shift Instructions\n+  INSN(vnsra_wx, 0b1010111, 0b100, 0b101101);\n+  INSN(vnsrl_wx, 0b1010111, 0b100, 0b101100);\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vx, 0b1010111, 0b100, 0b101001);\n+  INSN(vsrl_vx, 0b1010111, 0b100, 0b101000);\n+  INSN(vsll_vx, 0b1010111, 0b100, 0b100101);\n+\n+  \/\/ Vector Bitwise Logical Instructions\n+  INSN(vxor_vx, 0b1010111, 0b100, 0b001011);\n+  INSN(vor_vx,  0b1010111, 0b100, 0b001010);\n+  INSN(vand_vx, 0b1010111, 0b100, 0b001001);\n+\n+  \/\/ Vector Single-Width Integer Add and Subtract\n+  INSN(vsub_vx, 0b1010111, 0b100, 0b000010);\n+  INSN(vadd_vx, 0b1010111, 0b100, 0b000000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, FloatRegister Rs1, VectorMask vm = unmasked) {  \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Floating-Point Compare Instructions\n+  INSN(vmfge_vf, 0b1010111, 0b101, 0b011111);\n+  INSN(vmfgt_vf, 0b1010111, 0b101, 0b011101);\n+  INSN(vmfle_vf, 0b1010111, 0b101, 0b011001);\n+  INSN(vmflt_vf, 0b1010111, 0b101, 0b011011);\n+  INSN(vmfne_vf, 0b1010111, 0b101, 0b011100);\n+  INSN(vmfeq_vf, 0b1010111, 0b101, 0b011000);\n+\n+  \/\/ Vector Floating-Point Sign-Injection Instructions\n+  INSN(vfsgnjx_vf, 0b1010111, 0b101, 0b001010);\n+  INSN(vfsgnjn_vf, 0b1010111, 0b101, 0b001001);\n+  INSN(vfsgnj_vf,  0b1010111, 0b101, 0b001000);\n+\n+  \/\/ Vector Floating-Point MIN\/MAX Instructions\n+  INSN(vfmax_vf, 0b1010111, 0b101, 0b000110);\n+  INSN(vfmin_vf, 0b1010111, 0b101, 0b000100);\n+\n+  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n+  INSN(vfdiv_vf,  0b1010111, 0b101, 0b100000);\n+  INSN(vfmul_vf,  0b1010111, 0b101, 0b100100);\n+  INSN(vfrdiv_vf, 0b1010111, 0b101, 0b100001);\n+\n+  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n+  INSN(vfsub_vf,  0b1010111, 0b101, 0b000010);\n+  INSN(vfadd_vf,  0b1010111, 0b101, 0b000000);\n+  INSN(vfrsub_vf, 0b1010111, 0b101, 0b100111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {        \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)imm & 0x1f, Vs2, vm, funct6);                           \\\n+  }\n+\n+  INSN(vmsgt_vi,  0b1010111, 0b011, 0b011111);\n+  INSN(vmsgtu_vi, 0b1010111, 0b011, 0b011110);\n+  INSN(vmsle_vi,  0b1010111, 0b011, 0b011101);\n+  INSN(vmsleu_vi, 0b1010111, 0b011, 0b011100);\n+  INSN(vmsne_vi,  0b1010111, 0b011, 0b011001);\n+  INSN(vmseq_vi,  0b1010111, 0b011, 0b011000);\n+  INSN(vxor_vi,   0b1010111, 0b011, 0b001011);\n+  INSN(vor_vi,    0b1010111, 0b011, 0b001010);\n+  INSN(vand_vi,   0b1010111, 0b011, 0b001001);\n+  INSN(vadd_vi,   0b1010111, 0b011, 0b000000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, int32_t imm, VectorRegister Vs2, VectorMask vm = unmasked) {        \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n+  }\n+\n+  INSN(vrsub_vi, 0b1010111, 0b011, 0b000011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);  \\\n+  }\n+\n+  \/\/ Vector Compress Instruction\n+  INSN(vcompress_vm, 0b1010111, 0b010, 0b1, 0b010111);\n+\n+  \/\/ Vector Mask-Register Logical Instructions\n+  INSN(vmxnor_mm,   0b1010111, 0b010, 0b1, 0b011111);\n+  INSN(vmornot_mm,  0b1010111, 0b010, 0b1, 0b011100);\n+  INSN(vmnor_mm,    0b1010111, 0b010, 0b1, 0b011110);\n+  INSN(vmor_mm,     0b1010111, 0b010, 0b1, 0b011010);\n+  INSN(vmxor_mm,    0b1010111, 0b010, 0b1, 0b011011);\n+  INSN(vmandnot_mm, 0b1010111, 0b010, 0b1, 0b011000);\n+  INSN(vmnand_mm,   0b1010111, 0b010, 0b1, 0b011101);\n+  INSN(vmand_mm,    0b1010111, 0b010, 0b1, 0b011001);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                            \\\n+  void NAME(VectorRegister Vd, int32_t imm) {                              \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");               \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_i, 0b1010111, 0b011, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+  void NAME(VectorRegister Vd, FloatRegister Rs1) {                         \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Floating-Point Scalar Move Instructions\n+  INSN(vfmv_s_f, 0b1010111, 0b101, v0, 0b1, 0b010000);\n+  \/\/ Vector Floating-Point Move Instruction\n+  INSN(vfmv_v_f, 0b1010111, 0b101, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs1) {                        \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_v, 0b1010111, 0b000, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+   void NAME(VectorRegister Vd, Register Rs1) {                             \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+   }\n+\n+  \/\/ Integer Scalar Move Instructions\n+  INSN(vmv_s_x, 0b1010111, 0b110, v0, 0b1, 0b010000);\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_x, 0b1010111, 0b100, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+#undef patch_VArith\n+\n+#define INSN(NAME, op, funct13, funct6)                    \\\n+  void NAME(VectorRegister Vd, VectorMask vm = unmasked) { \\\n+    unsigned insn = 0;                                     \\\n+    patch((address)&insn, 6, 0, op);                       \\\n+    patch((address)&insn, 24, 12, funct13);                \\\n+    patch((address)&insn, 25, vm);                         \\\n+    patch((address)&insn, 31, 26, funct6);                 \\\n+    patch_reg((address)&insn, 7, Vd);                      \\\n+    emit(insn);                                            \\\n+  }\n+\n+  \/\/ Vector Element Index Instruction\n+  INSN(vid_v, 0b1010111, 0b0000010001010, 0b010100);\n+\n+#undef INSN\n+\n+enum Nf {\n+  g1 = 0b000,\n+  g2 = 0b001,\n+  g3 = 0b010,\n+  g4 = 0b011,\n+  g5 = 0b100,\n+  g6 = 0b101,\n+  g7 = 0b110,\n+  g8 = 0b111\n+};\n+\n+#define patch_VLdSt(op, VReg, width, Rs1, Reg_or_umop, vm, mop, mew, nf) \\\n+    unsigned insn = 0;                                                   \\\n+    patch((address)&insn, 6, 0, op);                                     \\\n+    patch((address)&insn, 14, 12, width);                                \\\n+    patch((address)&insn, 24, 20, Reg_or_umop);                          \\\n+    patch((address)&insn, 25, vm);                                       \\\n+    patch((address)&insn, 27, 26, mop);                                  \\\n+    patch((address)&insn, 28, mew);                                      \\\n+    patch((address)&insn, 31, 29, nf);                                   \\\n+    patch_reg((address)&insn, 7, VReg);                                  \\\n+    patch_reg((address)&insn, 15, Rs1);                                  \\\n+    emit(insn)\n+\n+#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n+  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n+    guarantee(is_unsigned_imm_in_range(width, 3, 0), \"width is invalid\");            \\\n+    patch_VLdSt(op, Vd, width, Rs1, lumop, vm, mop, mew, nf);                        \\\n+  }\n+\n+  \/\/ Vector Load\/Store Instructions\n+  INSN(vl1r_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, sumop, vm, mop, mew, nf)           \\\n+  void NAME(VectorRegister Vs3, Register Rs1) {                  \\\n+    patch_VLdSt(op, Vs3, width, Rs1, sumop, vm, mop, mew, nf);   \\\n+  }\n+\n+  \/\/ Vector Load\/Store Instructions\n+  INSN(vs1r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n+\n+#undef INSN\n+\n+\/\/ r2_nfvm\n+#define INSN(NAME, op, width, umop, mop, mew)                         \\\n+  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, Nf nf = g1) {     \\\n+    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, 1, mop, mew, nf);    \\\n+  }\n+\n+  \/\/ Vector Unit-Stride Instructions\n+  INSN(vle1_v, 0b0000111, 0b000, 0b01011, 0b00, 0b0);\n+  INSN(vse1_v, 0b0100111, 0b000, 0b01011, 0b00, 0b0);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, umop, mop, mew)                                               \\\n+  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, VectorMask vm = unmasked, Nf nf = g1) { \\\n+    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, vm, mop, mew, nf);                         \\\n+  }\n+\n+  \/\/ Vector Unit-Stride Instructions\n+  INSN(vle8_v,    0b0000111, 0b000, 0b00000, 0b00, 0b0);\n+  INSN(vle16_v,   0b0000111, 0b101, 0b00000, 0b00, 0b0);\n+  INSN(vle32_v,   0b0000111, 0b110, 0b00000, 0b00, 0b0);\n+  INSN(vle64_v,   0b0000111, 0b111, 0b00000, 0b00, 0b0);\n+\n+  \/\/ Vector unit-stride fault-only-first Instructions\n+  INSN(vle8ff_v,  0b0000111, 0b000, 0b10000, 0b00, 0b0);\n+  INSN(vle16ff_v, 0b0000111, 0b101, 0b10000, 0b00, 0b0);\n+  INSN(vle32ff_v, 0b0000111, 0b110, 0b10000, 0b00, 0b0);\n+  INSN(vle64ff_v, 0b0000111, 0b111, 0b10000, 0b00, 0b0);\n+\n+  INSN(vse8_v,  0b0100111, 0b000, 0b00000, 0b00, 0b0);\n+  INSN(vse16_v, 0b0100111, 0b101, 0b00000, 0b00, 0b0);\n+  INSN(vse32_v, 0b0100111, 0b110, 0b00000, 0b00, 0b0);\n+  INSN(vse64_v, 0b0100111, 0b111, 0b00000, 0b00, 0b0);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, mop, mew)                                                                  \\\n+  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked, Nf nf = g1) { \\\n+    patch_VLdSt(op, Vd, width, Rs1, Vs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n+  }\n+\n+  \/\/ Vector unordered indexed load instructions\n+  INSN(vluxei8_v,  0b0000111, 0b000, 0b01, 0b0);\n+  INSN(vluxei16_v, 0b0000111, 0b101, 0b01, 0b0);\n+  INSN(vluxei32_v, 0b0000111, 0b110, 0b01, 0b0);\n+  INSN(vluxei64_v, 0b0000111, 0b111, 0b01, 0b0);\n+\n+  \/\/ Vector ordered indexed load instructions\n+  INSN(vloxei8_v,  0b0000111, 0b000, 0b11, 0b0);\n+  INSN(vloxei16_v, 0b0000111, 0b101, 0b11, 0b0);\n+  INSN(vloxei32_v, 0b0000111, 0b110, 0b11, 0b0);\n+  INSN(vloxei64_v, 0b0000111, 0b111, 0b11, 0b0);\n+#undef INSN\n+\n+#define INSN(NAME, op, width, mop, mew)                                                                  \\\n+  void NAME(VectorRegister Vd, Register Rs1, Register Rs2, VectorMask vm = unmasked, Nf nf = g1) {       \\\n+    patch_VLdSt(op, Vd, width, Rs1, Rs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n+  }\n+\n+  \/\/ Vector Strided Instructions\n+  INSN(vlse8_v,  0b0000111, 0b000, 0b10, 0b0);\n+  INSN(vlse16_v, 0b0000111, 0b101, 0b10, 0b0);\n+  INSN(vlse32_v, 0b0000111, 0b110, 0b10, 0b0);\n+  INSN(vlse64_v, 0b0000111, 0b111, 0b10, 0b0);\n+\n+#undef INSN\n+#undef patch_VLdSt\n+\n+\/\/ ====================================\n+\/\/ RISC-V Bit-Manipulation Extension\n+\/\/ ====================================\n+#define INSN(NAME, op, funct3, funct7)                  \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {  \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6,  0, op);                   \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 31, 25, funct7);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    patch_reg((address)&insn, 20, Rs2);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(add_uw,    0b0111011, 0b000, 0b0000100);\n+  INSN(rol,       0b0110011, 0b001, 0b0110000);\n+  INSN(rolw,      0b0111011, 0b001, 0b0110000);\n+  INSN(ror,       0b0110011, 0b101, 0b0110000);\n+  INSN(rorw,      0b0111011, 0b101, 0b0110000);\n+  INSN(sh1add,    0b0110011, 0b010, 0b0010000);\n+  INSN(sh2add,    0b0110011, 0b100, 0b0010000);\n+  INSN(sh3add,    0b0110011, 0b110, 0b0010000);\n+  INSN(sh1add_uw, 0b0111011, 0b010, 0b0010000);\n+  INSN(sh2add_uw, 0b0111011, 0b100, 0b0010000);\n+  INSN(sh3add_uw, 0b0111011, 0b110, 0b0010000);\n+  INSN(andn,      0b0110011, 0b111, 0b0100000);\n+  INSN(orn,       0b0110011, 0b110, 0b0100000);\n+  INSN(xnor,      0b0110011, 0b100, 0b0100000);\n+  INSN(max,       0b0110011, 0b110, 0b0000101);\n+  INSN(maxu,      0b0110011, 0b111, 0b0000101);\n+  INSN(min,       0b0110011, 0b100, 0b0000101);\n+  INSN(minu,      0b0110011, 0b101, 0b0000101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct12)                 \\\n+  void NAME(Register Rd, Register Rs1) {                \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 31, 20, funct12);             \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(rev8,   0b0010011, 0b101, 0b011010111000);\n+  INSN(sext_b, 0b0010011, 0b001, 0b011000000100);\n+  INSN(sext_h, 0b0010011, 0b001, 0b011000000101);\n+  INSN(zext_h, 0b0111011, 0b100, 0b000010000000);\n+  INSN(clz,    0b0010011, 0b001, 0b011000000000);\n+  INSN(clzw,   0b0011011, 0b001, 0b011000000000);\n+  INSN(ctz,    0b0010011, 0b001, 0b011000000001);\n+  INSN(ctzw,   0b0011011, 0b001, 0b011000000001);\n+  INSN(cpop,   0b0010011, 0b001, 0b011000000010);\n+  INSN(cpopw,  0b0011011, 0b001, 0b011000000010);\n+  INSN(orc_b,  0b0010011, 0b101, 0b001010000111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                  \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {\\\n+    guarantee(shamt <= 0x3f, \"Shamt is invalid\");       \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 25, 20, shamt);               \\\n+    patch((address)&insn, 31, 26, funct6);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(rori,    0b0010011, 0b101, 0b011000);\n+  INSN(slli_uw, 0b0011011, 0b001, 0b000010);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct7)                  \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {\\\n+    guarantee(shamt <= 0x1f, \"Shamt is invalid\");       \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 24, 20, shamt);               \\\n+    patch((address)&insn, 31, 25, funct7);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(roriw, 0b0011011, 0b101, 0b0110000);\n+\n+#undef INSN\n+\n+\/\/ ========================================\n+\/\/ RISC-V Compressed Instructions Extension\n+\/\/ ========================================\n+\/\/ Note:\n+\/\/ 1. When UseRVC is enabled, 32-bit instructions under 'CompressibleRegion's will be\n+\/\/    transformed to 16-bit instructions if compressible.\n+\/\/ 2. RVC instructions in Assembler always begin with 'c_' prefix, as 'c_li',\n+\/\/    but most of time we have no need to explicitly use these instructions.\n+\/\/ 3. 'CompressibleRegion' is introduced to hint instructions in this Region's RTTI range\n+\/\/    are qualified to be compressed with their 2-byte versions.\n+\/\/    An example:\n+\/\/\n+\/\/      CompressibleRegion cr(_masm);\n+\/\/      __ andr(...);      \/\/ this instruction could change to c.and if able to\n+\/\/\n+\/\/ 4. Using -XX:PrintAssemblyOptions=no-aliases could distinguish RVC instructions from\n+\/\/    normal ones.\n+\/\/\n+\n+private:\n+  bool _in_compressible_region;\n+public:\n+  bool in_compressible_region() const { return _in_compressible_region; }\n+  void set_in_compressible_region(bool b) { _in_compressible_region = b; }\n+public:\n+\n+  \/\/ a compressible region\n+  class CompressibleRegion : public StackObj {\n+  protected:\n+    Assembler *_masm;\n+    bool _saved_in_compressible_region;\n+  public:\n+    CompressibleRegion(Assembler *_masm)\n+    : _masm(_masm)\n+    , _saved_in_compressible_region(_masm->in_compressible_region()) {\n+      _masm->set_in_compressible_region(true);\n+    }\n+    ~CompressibleRegion() {\n+      _masm->set_in_compressible_region(_saved_in_compressible_region);\n+    }\n+  };\n+\n+  \/\/ patch a 16-bit instruction.\n+  static void c_patch(address a, unsigned msb, unsigned lsb, uint16_t val) {\n+    assert_cond(a != NULL);\n+    assert_cond(msb >= lsb && msb <= 15);\n+    unsigned nbits = msb - lsb + 1;\n+    guarantee(val < (1U << nbits), \"Field too big for insn\");\n+    uint16_t mask = (1U << nbits) - 1;\n+    val <<= lsb;\n+    mask <<= lsb;\n+    uint16_t target = *(uint16_t *)a;\n+    target &= ~mask;\n+    target |= val;\n+    *(uint16_t *)a = target;\n+  }\n+\n+  static void c_patch(address a, unsigned bit, uint16_t val) {\n+    c_patch(a, bit, bit, val);\n+  }\n+\n+  \/\/ patch a 16-bit instruction with a general purpose register ranging [0, 31] (5 bits)\n+  static void c_patch_reg(address a, unsigned lsb, Register reg) {\n+    c_patch(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  \/\/ patch a 16-bit instruction with a general purpose register ranging [8, 15] (3 bits)\n+  static void c_patch_compressed_reg(address a, unsigned lsb, Register reg) {\n+    c_patch(a, lsb + 2, lsb, reg->compressed_encoding_nocheck());\n+  }\n+\n+  \/\/ patch a 16-bit instruction with a float register ranging [0, 31] (5 bits)\n+  static void c_patch_reg(address a, unsigned lsb, FloatRegister reg) {\n+    c_patch(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  \/\/ patch a 16-bit instruction with a float register ranging [8, 15] (3 bits)\n+  static void c_patch_compressed_reg(address a, unsigned lsb, FloatRegister reg) {\n+    c_patch(a, lsb + 2, lsb, reg->compressed_encoding_nocheck());\n+  }\n+\n+\/\/ --------------  RVC Instruction Definitions  --------------\n+\n+  void c_nop() {\n+    c_addi(x0, 0);\n+  }\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs1, int32_t imm) {                                                  \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    c_patch_reg((address)&insn, 7, Rd_Rs1);                                                  \\\n+    c_patch((address)&insn, 12, 12, (imm & nth_bit(5)) >> 5);                                \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_addi,   0b000, 0b01);\n+  INSN(c_addiw,  0b001, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(int32_t imm) {                                                                   \\\n+    assert_cond(is_imm_in_range(imm, 10, 0));                                                \\\n+    assert_cond((imm & 0b1111) == 0);                                                        \\\n+    assert_cond(imm != 0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 2, 2, (imm & nth_bit(5)) >> 5);                                  \\\n+    c_patch((address)&insn, 4, 3, (imm & right_n_bits(9)) >> 7);                             \\\n+    c_patch((address)&insn, 5, 5, (imm & nth_bit(6)) >> 6);                                  \\\n+    c_patch((address)&insn, 6, 6, (imm & nth_bit(4)) >> 4);                                  \\\n+    c_patch_reg((address)&insn, 7, sp);                                                      \\\n+    c_patch((address)&insn, 12, 12, (imm & nth_bit(9)) >> 9);                                \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_addi16sp, 0b011, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, uint32_t uimm) {                                                    \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 10, 0));                                      \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    assert_cond(uimm != 0);                                                                  \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_compressed_reg((address)&insn, 2, Rd);                                           \\\n+    c_patch((address)&insn, 5, 5, (uimm & nth_bit(3)) >> 3);                                 \\\n+    c_patch((address)&insn, 6, 6, (uimm & nth_bit(2)) >> 2);                                 \\\n+    c_patch((address)&insn, 10, 7, (uimm & right_n_bits(10)) >> 6);                          \\\n+    c_patch((address)&insn, 12, 11, (uimm & right_n_bits(6)) >> 4);                          \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_addi4spn, 0b000, 0b00);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs1, uint32_t shamt) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(shamt, 6, 0));                                      \\\n+    assert_cond(shamt != 0);                                                                 \\\n+    assert_cond(Rd_Rs1 != x0);                                                               \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (shamt & right_n_bits(5)));                                \\\n+    c_patch_reg((address)&insn, 7, Rd_Rs1);                                                  \\\n+    c_patch((address)&insn, 12, 12, (shamt & nth_bit(5)) >> 5);                              \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_slli, 0b000, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, uint32_t shamt) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(shamt, 6, 0));                                      \\\n+    assert_cond(shamt != 0);                                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (shamt & right_n_bits(5)));                                \\\n+    c_patch_compressed_reg((address)&insn, 7, Rd_Rs1);                                       \\\n+    c_patch((address)&insn, 11, 10, funct2);                                                 \\\n+    c_patch((address)&insn, 12, 12, (shamt & nth_bit(5)) >> 5);                              \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_srli, 0b100, 0b00, 0b01);\n+  INSN(c_srai, 0b100, 0b01, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, int32_t imm) {                                                  \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    c_patch_compressed_reg((address)&insn, 7, Rd_Rs1);                                       \\\n+    c_patch((address)&insn, 11, 10, funct2);                                                 \\\n+    c_patch((address)&insn, 12, 12, (imm & nth_bit(5)) >> 5);                                \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_andi, 0b100, 0b10, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct6, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, Register Rs2) {                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_compressed_reg((address)&insn, 2, Rs2);                                          \\\n+    c_patch((address)&insn, 6, 5, funct2);                                                   \\\n+    c_patch_compressed_reg((address)&insn, 7, Rd_Rs1);                                       \\\n+    c_patch((address)&insn, 15, 10, funct6);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_sub,  0b100011, 0b00, 0b01);\n+  INSN(c_xor,  0b100011, 0b01, 0b01);\n+  INSN(c_or,   0b100011, 0b10, 0b01);\n+  INSN(c_and,  0b100011, 0b11, 0b01);\n+  INSN(c_subw, 0b100111, 0b00, 0b01);\n+  INSN(c_addw, 0b100111, 0b01, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct4, op)                                                               \\\n+  void NAME(Register Rd_Rs1, Register Rs2) {                                                 \\\n+    assert_cond(Rd_Rs1 != x0);                                                               \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_reg((address)&insn, 2, Rs2);                                                     \\\n+    c_patch_reg((address)&insn, 7, Rd_Rs1);                                                  \\\n+    c_patch((address)&insn, 15, 12, funct4);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_mv,  0b1000, 0b10);\n+  INSN(c_add, 0b1001, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct4, op)                                                               \\\n+  void NAME(Register Rs1) {                                                                  \\\n+    assert_cond(Rs1 != x0);                                                                  \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_reg((address)&insn, 2, x0);                                                      \\\n+    c_patch_reg((address)&insn, 7, Rs1);                                                     \\\n+    c_patch((address)&insn, 15, 12, funct4);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_jr,   0b1000, 0b10);\n+  INSN(c_jalr, 0b1001, 0b10);\n+\n+#undef INSN\n+\n+  typedef void (Assembler::* j_c_insn)(address dest);\n+  typedef void (Assembler::* compare_and_branch_c_insn)(Register Rs1, address dest);\n+\n+  void wrap_label(Label &L, j_c_insn insn) {\n+    if (L.is_bound()) {\n+      (this->*insn)(target(L));\n+    } else {\n+      L.add_patch_at(code(), locator());\n+      (this->*insn)(pc());\n+    }\n+  }\n+\n+  void wrap_label(Label &L, Register r, compare_and_branch_c_insn insn) {\n+    if (L.is_bound()) {\n+      (this->*insn)(r, target(L));\n+    } else {\n+      L.add_patch_at(code(), locator());\n+      (this->*insn)(r, pc());\n+    }\n+  }\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(int32_t offset) {                                                                \\\n+    assert_cond(is_imm_in_range(offset, 11, 1));                                             \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 2, 2, (offset & nth_bit(5)) >> 5);                               \\\n+    c_patch((address)&insn, 5, 3, (offset & right_n_bits(4)) >> 1);                          \\\n+    c_patch((address)&insn, 6, 6, (offset & nth_bit(7)) >> 7);                               \\\n+    c_patch((address)&insn, 7, 7, (offset & nth_bit(6)) >> 6);                               \\\n+    c_patch((address)&insn, 8, 8, (offset & nth_bit(10)) >> 10);                             \\\n+    c_patch((address)&insn, 10, 9, (offset & right_n_bits(10)) >> 8);                        \\\n+    c_patch((address)&insn, 11, 11, (offset & nth_bit(4)) >> 4);                             \\\n+    c_patch((address)&insn, 12, 12, (offset & nth_bit(11)) >> 11);                           \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }                                                                                          \\\n+  void NAME(address dest) {                                                                  \\\n+    assert_cond(dest != NULL);                                                               \\\n+    int64_t distance = dest - pc();                                                          \\\n+    assert_cond(is_imm_in_range(distance, 11, 1));                                           \\\n+    c_j(distance);                                                                           \\\n+  }                                                                                          \\\n+  void NAME(Label &L) {                                                                      \\\n+    wrap_label(L, &Assembler::NAME);                                                         \\\n+  }\n+\n+  INSN(c_j, 0b101, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rs1, int32_t imm) {                                                     \\\n+    assert_cond(is_imm_in_range(imm, 8, 1));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 2, 2, (imm & nth_bit(5)) >> 5);                                  \\\n+    c_patch((address)&insn, 4, 3, (imm & right_n_bits(3)) >> 1);                             \\\n+    c_patch((address)&insn, 6, 5, (imm & right_n_bits(8)) >> 6);                             \\\n+    c_patch_compressed_reg((address)&insn, 7, Rs1);                                          \\\n+    c_patch((address)&insn, 11, 10, (imm & right_n_bits(5)) >> 3);                           \\\n+    c_patch((address)&insn, 12, 12, (imm & nth_bit(8)) >> 8);                                \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }                                                                                          \\\n+  void NAME(Register Rs1, address dest) {                                                    \\\n+    assert_cond(dest != NULL);                                                               \\\n+    int64_t distance = dest - pc();                                                          \\\n+    assert_cond(is_imm_in_range(distance, 8, 1));                                            \\\n+    NAME(Rs1, distance);                                                                     \\\n+  }                                                                                          \\\n+  void NAME(Register Rs1, Label &L) {                                                        \\\n+    wrap_label(L, Rs1, &Assembler::NAME);                                                    \\\n+  }\n+\n+  INSN(c_beqz, 0b110, 0b01);\n+  INSN(c_bnez, 0b111, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, int32_t imm) {                                                      \\\n+    assert_cond(is_imm_in_range(imm, 18, 0));                                                \\\n+    assert_cond((imm & 0xfff) == 0);                                                         \\\n+    assert_cond(imm != 0);                                                                   \\\n+    assert_cond(Rd != x0 && Rd != x2);                                                       \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (imm & right_n_bits(17)) >> 12);                           \\\n+    c_patch_reg((address)&insn, 7, Rd);                                                      \\\n+    c_patch((address)&insn, 12, 12, (imm & nth_bit(17)) >> 17);                              \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_lui, 0b011, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, int32_t imm) {                                                      \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    assert_cond(Rd != x0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    c_patch_reg((address)&insn, 7, Rd);                                                      \\\n+    c_patch((address)&insn, 12, 12, (imm & right_n_bits(6)) >> 5);                           \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_li, 0b010, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, uint32_t uimm) {                                                    \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 9, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    assert_cond(Rd != x0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 4, 2, (uimm & right_n_bits(9)) >> 6);                            \\\n+    c_patch((address)&insn, 6, 5, (uimm & right_n_bits(5)) >> 3);                            \\\n+    c_patch_reg((address)&insn, 7, Rd);                                                      \\\n+    c_patch((address)&insn, 12, 12, (uimm & nth_bit(5)) >> 5);                               \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_ldsp,  0b011, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(FloatRegister Rd, uint32_t uimm) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 9, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 4, 2, (uimm & right_n_bits(9)) >> 6);                            \\\n+    c_patch((address)&insn, 6, 5, (uimm & right_n_bits(5)) >> 3);                            \\\n+    c_patch_reg((address)&insn, 7, Rd);                                                      \\\n+    c_patch((address)&insn, 12, 12, (uimm & nth_bit(5)) >> 5);                               \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_fldsp, 0b001, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op, REGISTER_TYPE)                                                \\\n+  void NAME(REGISTER_TYPE Rd_Rs2, Register Rs1, uint32_t uimm) {                             \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_compressed_reg((address)&insn, 2, Rd_Rs2);                                       \\\n+    c_patch((address)&insn, 6, 5, (uimm & right_n_bits(8)) >> 6);                            \\\n+    c_patch_compressed_reg((address)&insn, 7, Rs1);                                          \\\n+    c_patch((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_ld,  0b011, 0b00, Register);\n+  INSN(c_sd,  0b111, 0b00, Register);\n+  INSN(c_fld, 0b001, 0b00, FloatRegister);\n+  INSN(c_fsd, 0b101, 0b00, FloatRegister);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op, REGISTER_TYPE)                                                \\\n+  void NAME(REGISTER_TYPE Rs2, uint32_t uimm) {                                              \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 9, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_reg((address)&insn, 2, Rs2);                                                     \\\n+    c_patch((address)&insn, 9, 7, (uimm & right_n_bits(9)) >> 6);                            \\\n+    c_patch((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_sdsp,  0b111, 0b10, Register);\n+  INSN(c_fsdsp, 0b101, 0b10, FloatRegister);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rs2, uint32_t uimm) {                                                   \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_reg((address)&insn, 2, Rs2);                                                     \\\n+    c_patch((address)&insn, 8, 7, (uimm & right_n_bits(8)) >> 6);                            \\\n+    c_patch((address)&insn, 12, 9, (uimm & right_n_bits(6)) >> 2);                           \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_swsp, 0b110, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, uint32_t uimm) {                                                    \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    assert_cond(Rd != x0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 3, 2, (uimm & right_n_bits(8)) >> 6);                            \\\n+    c_patch((address)&insn, 6, 4, (uimm & right_n_bits(5)) >> 2);                            \\\n+    c_patch_reg((address)&insn, 7, Rd);                                                      \\\n+    c_patch((address)&insn, 12, 12, (uimm & nth_bit(5)) >> 5);                               \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_lwsp, 0b010, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs2, Register Rs1, uint32_t uimm) {                                  \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 7, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch_compressed_reg((address)&insn, 2, Rd_Rs2);                                       \\\n+    c_patch((address)&insn, 5, 5, (uimm & nth_bit(6)) >> 6);                                 \\\n+    c_patch((address)&insn, 6, 6, (uimm & nth_bit(2)) >> 2);                                 \\\n+    c_patch_compressed_reg((address)&insn, 7, Rs1);                                          \\\n+    c_patch((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_lw, 0b010, 0b00);\n+  INSN(c_sw, 0b110, 0b00);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME() {                                                                              \\\n+    uint16_t insn = 0;                                                                       \\\n+    c_patch((address)&insn, 1, 0, op);                                                       \\\n+    c_patch((address)&insn, 11, 2, 0x0);                                                     \\\n+    c_patch((address)&insn, 12, 12, 0b1);                                                    \\\n+    c_patch((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(c_ebreak, 0b100, 0b10);\n+\n+#undef INSN\n+\n+\/\/ --------------  RVC Transformation Functions  --------------\n+\n+\/\/ --------------------------\n+\/\/ Register instructions\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                             \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {                                         \\\n+    \/* add -> c.add *\/                                                                         \\\n+    if (do_compress()) {                                                                       \\\n+      Register src = noreg;                                                                    \\\n+      if (Rs1 != x0 && Rs2 != x0 && ((src = Rs1, Rs2 == Rd) || (src = Rs2, Rs1 == Rd))) {      \\\n+        c_add(Rd, src);                                                                        \\\n+        return;                                                                                \\\n+      }                                                                                        \\\n+    }                                                                                          \\\n+    _add(Rd, Rs1, Rs2);                                                                        \\\n+  }\n+\n+  INSN(add);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME, C_NAME, NORMAL_NAME)                                                      \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {                                       \\\n+    \/* sub\/subw -> c.sub\/c.subw *\/                                                           \\\n+    if (do_compress() &&                                                                     \\\n+        (Rd == Rs1 && Rd->is_compressed_valid() && Rs2->is_compressed_valid())) {            \\\n+      C_NAME(Rd, Rs2);                                                                       \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    NORMAL_NAME(Rd, Rs1, Rs2);                                                               \\\n+  }\n+\n+  INSN(sub,  c_sub,  _sub);\n+  INSN(subw, c_subw, _subw);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME, C_NAME, NORMAL_NAME)                                                      \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {                                       \\\n+    \/* and\/or\/xor\/addw -> c.and\/c.or\/c.xor\/c.addw *\/                                         \\\n+    if (do_compress()) {                                                                     \\\n+      Register src = noreg;                                                                  \\\n+      if (Rs1->is_compressed_valid() && Rs2->is_compressed_valid() &&                        \\\n+        ((src = Rs1, Rs2 == Rd) || (src = Rs2, Rs1 == Rd))) {                                \\\n+        C_NAME(Rd, src);                                                                     \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    NORMAL_NAME(Rd, Rs1, Rs2);                                                               \\\n+  }\n+\n+  INSN(andr, c_and,  _andr);\n+  INSN(orr,  c_or,   _orr);\n+  INSN(xorr, c_xor,  _xorr);\n+  INSN(addw, c_addw, _addw);\n+\n+#undef INSN\n+\n+private:\n+\/\/ some helper functions\n+  bool do_compress() const {\n+    return UseRVC && in_compressible_region();\n+  }\n+\n+#define FUNC(NAME, funct3, bits)                                                             \\\n+  bool NAME(Register rs1, Register rd_rs2, int32_t imm12, bool ld) {                         \\\n+    return rs1 == sp &&                                                                      \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                            \\\n+      (intx(imm12) & funct3) == 0x0 &&                                                       \\\n+      (!ld || rd_rs2 != x0);                                                                 \\\n+  }                                                                                          \\\n+\n+  FUNC(is_c_ldsdsp,  0b111, 9);\n+  FUNC(is_c_lwswsp,  0b011, 8);\n+\n+#undef FUNC\n+\n+#define FUNC(NAME, funct3, bits)                                                             \\\n+  bool NAME(Register rs1, int32_t imm12) {                                                   \\\n+    return rs1 == sp &&                                                                      \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                            \\\n+      (intx(imm12) & funct3) == 0x0;                                                         \\\n+  }                                                                                          \\\n+\n+  FUNC(is_c_fldsdsp, 0b111, 9);\n+\n+#undef FUNC\n+\n+#define FUNC(NAME, REG_TYPE, funct3, bits)                                                   \\\n+  bool NAME(Register rs1, REG_TYPE rd_rs2, int32_t imm12) {                                  \\\n+    return rs1->is_compressed_valid() &&                                                     \\\n+      rd_rs2->is_compressed_valid() &&                                                       \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                            \\\n+      (intx(imm12) & funct3) == 0x0;                                                         \\\n+  }                                                                                          \\\n+\n+  FUNC(is_c_ldsd,  Register,      0b111, 8);\n+  FUNC(is_c_lwsw,  Register,      0b011, 7);\n+  FUNC(is_c_fldsd, FloatRegister, 0b111, 8);\n+\n+#undef FUNC\n+\n+public:\n+\/\/ --------------------------\n+\/\/ Load\/store register\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                \\\n+    \/* lw -> c.lwsp\/c.lw *\/                                                                  \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_lwswsp(Rs, Rd, offset, true)) {                                               \\\n+        c_lwsp(Rd, offset);                                                                  \\\n+        return;                                                                              \\\n+      } else if (is_c_lwsw(Rs, Rd, offset)) {                                                \\\n+        c_lw(Rd, Rs, offset);                                                                \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _lw(Rd, Rs, offset);                                                                     \\\n+  }\n+\n+  INSN(lw);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                \\\n+    \/* ld -> c.ldsp\/c.ld *\/                                                                  \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_ldsdsp(Rs, Rd, offset, true)) {                                               \\\n+        c_ldsp(Rd, offset);                                                                  \\\n+        return;                                                                              \\\n+      } else if (is_c_ldsd(Rs, Rd, offset)) {                                                \\\n+        c_ld(Rd, Rs, offset);                                                                \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _ld(Rd, Rs, offset);                                                                     \\\n+  }\n+\n+  INSN(ld);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(FloatRegister Rd, Register Rs, const int32_t offset) {                           \\\n+    \/* fld -> c.fldsp\/c.fld *\/                                                               \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_fldsdsp(Rs, offset)) {                                                        \\\n+        c_fldsp(Rd, offset);                                                                 \\\n+        return;                                                                              \\\n+      } else if (is_c_fldsd(Rs, Rd, offset)) {                                               \\\n+        c_fld(Rd, Rs, offset);                                                               \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _fld(Rd, Rs, offset);                                                                    \\\n+  }\n+\n+  INSN(fld);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                \\\n+    \/* sd -> c.sdsp\/c.sd *\/                                                                  \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_ldsdsp(Rs, Rd, offset, false)) {                                              \\\n+        c_sdsp(Rd, offset);                                                                  \\\n+        return;                                                                              \\\n+      } else if (is_c_ldsd(Rs, Rd, offset)) {                                                \\\n+        c_sd(Rd, Rs, offset);                                                                \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _sd(Rd, Rs, offset);                                                                     \\\n+  }\n+\n+  INSN(sd);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                \\\n+    \/* sw -> c.swsp\/c.sw *\/                                                                  \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_lwswsp(Rs, Rd, offset, false)) {                                              \\\n+        c_swsp(Rd, offset);                                                                  \\\n+        return;                                                                              \\\n+      } else if (is_c_lwsw(Rs, Rd, offset)) {                                                \\\n+        c_sw(Rd, Rs, offset);                                                                \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _sw(Rd, Rs, offset);                                                                     \\\n+  }\n+\n+  INSN(sw);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(FloatRegister Rd, Register Rs, const int32_t offset) {                           \\\n+    \/* fsd -> c.fsdsp\/c.fsd *\/                                                               \\\n+    if (do_compress()) {                                                                     \\\n+      if (is_c_fldsdsp(Rs, offset)) {                                                        \\\n+        c_fsdsp(Rd, offset);                                                                 \\\n+        return;                                                                              \\\n+      } else if (is_c_fldsd(Rs, Rd, offset)) {                                               \\\n+        c_fsd(Rd, Rs, offset);                                                               \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _fsd(Rd, Rs, offset);                                                                    \\\n+  }\n+\n+  INSN(fsd);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Conditional branch instructions\n+\/\/ --------------------------\n+#define INSN(NAME, C_NAME, NORMAL_NAME)                                                      \\\n+  void NAME(Register Rs1, Register Rs2, const int64_t offset) {                              \\\n+    \/* beq\/bne -> c.beqz\/c.bnez *\/                                                           \\\n+    if (do_compress() &&                                                                     \\\n+        (offset != 0 && Rs2 == x0 && Rs1->is_compressed_valid() &&                           \\\n+        is_imm_in_range(offset, 8, 1))) {                                                    \\\n+      C_NAME(Rs1, offset);                                                                   \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    NORMAL_NAME(Rs1, Rs2, offset);                                                           \\\n+  }\n+\n+  INSN(beq, c_beqz, _beq);\n+  INSN(bne, c_beqz, _bne);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Unconditional branch instructions\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, const int32_t offset) {                                             \\\n+    \/* jal -> c.j *\/                                                                         \\\n+    if (do_compress() && offset != 0 && Rd == x0 && is_imm_in_range(offset, 11, 1)) {        \\\n+      c_j(offset);                                                                           \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _jal(Rd, offset);                                                                        \\\n+  }\n+\n+  INSN(jal);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs, const int32_t offset) {                                \\\n+    \/* jalr -> c.jr\/c.jalr *\/                                                                \\\n+    if (do_compress() && (offset == 0 && Rs != x0)) {                                        \\\n+      if (Rd == x1) {                                                                        \\\n+        c_jalr(Rs);                                                                          \\\n+        return;                                                                              \\\n+      } else if (Rd == x0) {                                                                 \\\n+        c_jr(Rs);                                                                            \\\n+        return;                                                                              \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _jalr(Rd, Rs, offset);                                                                   \\\n+  }\n+\n+  INSN(jalr);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Miscellaneous Instructions\n+\/\/ --------------------------\n+#define INSN(NAME)                                                     \\\n+  void NAME() {                                                        \\\n+    \/* ebreak -> c.ebreak *\/                                           \\\n+    if (do_compress()) {                                               \\\n+      c_ebreak();                                                      \\\n+      return;                                                          \\\n+    }                                                                  \\\n+    _ebreak();                                                         \\\n+  }\n+\n+  INSN(ebreak);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                                                      \\\n+  void NAME() {                                                         \\\n+    \/* The illegal instruction in RVC is presented by a 16-bit 0. *\/    \\\n+    if (do_compress()) {                                                \\\n+      emit_int16(0);                                                    \\\n+      return;                                                           \\\n+    }                                                                   \\\n+    _halt();                                                            \\\n+  }\n+\n+  INSN(halt);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Immediate Instructions\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, int64_t imm) {                                                      \\\n+    \/* li -> c.li *\/                                                                         \\\n+    if (do_compress() && (is_imm_in_range(imm, 6, 0) && Rd != x0)) {                         \\\n+      c_li(Rd, imm);                                                                         \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _li(Rd, imm);                                                                            \\\n+  }\n+\n+  INSN(li);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs1, int32_t imm) {                                        \\\n+    \/* addi -> c.addi\/c.nop\/c.mv\/c.addi16sp\/c.addi4spn *\/                                    \\\n+    if (do_compress()) {                                                                     \\\n+      if (Rd == Rs1 && is_imm_in_range(imm, 6, 0)) {                                         \\\n+        c_addi(Rd, imm);                                                                     \\\n+        return;                                                                              \\\n+      } else if (imm == 0 && Rd != x0 && Rs1 != x0) {                                        \\\n+        c_mv(Rd, Rs1);                                                                       \\\n+        return;                                                                              \\\n+      } else if (Rs1 == sp && imm != 0) {                                                    \\\n+        if (Rd == Rs1 && (imm & 0b1111) == 0x0 && is_imm_in_range(imm, 10, 0)) {             \\\n+          c_addi16sp(imm);                                                                   \\\n+          return;                                                                            \\\n+        } else if (Rd->is_compressed_valid() && (imm & 0b11) == 0x0 && is_unsigned_imm_in_range(imm, 10, 0)) { \\\n+          c_addi4spn(Rd, imm);                                                               \\\n+          return;                                                                            \\\n+        }                                                                                    \\\n+      }                                                                                      \\\n+    }                                                                                        \\\n+    _addi(Rd, Rs1, imm);                                                                     \\\n+  }\n+\n+  INSN(addi);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs1, int32_t imm) {                                        \\\n+    \/* addiw -> c.addiw *\/                                                                   \\\n+    if (do_compress() && (Rd == Rs1 && Rd != x0 && is_imm_in_range(imm, 6, 0))) {            \\\n+      c_addiw(Rd, imm);                                                                      \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _addiw(Rd, Rs1, imm);                                                                    \\\n+  }\n+\n+  INSN(addiw);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs1, int32_t imm) {                                        \\\n+    \/* and_imm12 -> c.andi *\/                                                                \\\n+    if (do_compress() &&                                                                     \\\n+        (Rd == Rs1 && Rd->is_compressed_valid() && is_imm_in_range(imm, 6, 0))) {            \\\n+      c_andi(Rd, imm);                                                                       \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _and_imm12(Rd, Rs1, imm);                                                                \\\n+  }\n+\n+  INSN(and_imm12);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Shift Immediate Instructions\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {                                     \\\n+    \/* slli -> c.slli *\/                                                                     \\\n+    if (do_compress() && (Rd == Rs1 && Rd != x0 && shamt != 0)) {                            \\\n+      c_slli(Rd, shamt);                                                                     \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _slli(Rd, Rs1, shamt);                                                                   \\\n+  }\n+\n+  INSN(slli);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+#define INSN(NAME, C_NAME, NORMAL_NAME)                                                      \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {                                     \\\n+    \/* srai\/srli -> c.srai\/c.srli *\/                                                         \\\n+    if (do_compress() && (Rd == Rs1 && Rd->is_compressed_valid() && shamt != 0)) {           \\\n+      C_NAME(Rd, shamt);                                                                     \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    NORMAL_NAME(Rd, Rs1, shamt);                                                             \\\n+  }\n+\n+  INSN(srai, c_srai, _srai);\n+  INSN(srli, c_srli, _srli);\n+\n+#undef INSN\n+\n+\/\/ --------------------------\n+\/\/ Upper Immediate Instruction\n+\/\/ --------------------------\n+#define INSN(NAME)                                                                           \\\n+  void NAME(Register Rd, int32_t imm) {                                                      \\\n+    \/* lui -> c.lui *\/                                                                       \\\n+    if (do_compress() && (Rd != x0 && Rd != x2 && imm != 0 && is_imm_in_range(imm, 18, 0))) { \\\n+      c_lui(Rd, imm);                                                                        \\\n+      return;                                                                                \\\n+    }                                                                                        \\\n+    _lui(Rd, imm);                                                                           \\\n+  }\n+\n+  INSN(lui);\n+\n+#undef INSN\n+\n+\/\/ ---------------------------------------------------------------------------------------\n+\n+  void bgt(Register Rs, Register Rt, const address &dest);\n+  void ble(Register Rs, Register Rt, const address &dest);\n+  void bgtu(Register Rs, Register Rt, const address &dest);\n+  void bleu(Register Rs, Register Rt, const address &dest);\n+  void bgt(Register Rs, Register Rt, Label &l, bool is_far = false);\n+  void ble(Register Rs, Register Rt, Label &l, bool is_far = false);\n+  void bgtu(Register Rs, Register Rt, Label &l, bool is_far = false);\n+  void bleu(Register Rs, Register Rt, Label &l, bool is_far = false);\n+\n+  typedef void (Assembler::* jal_jalr_insn)(Register Rt, address dest);\n+  typedef void (Assembler::* load_insn_by_temp)(Register Rt, address dest, Register temp);\n+  typedef void (Assembler::* compare_and_branch_insn)(Register Rs1, Register Rs2, const address dest);\n+  typedef void (Assembler::* compare_and_branch_label_insn)(Register Rs1, Register Rs2, Label &L, bool is_far);\n+\n+  void wrap_label(Register r1, Register r2, Label &L, compare_and_branch_insn insn,\n+                  compare_and_branch_label_insn neg_insn, bool is_far);\n+  void wrap_label(Register r, Label &L, Register t, load_insn_by_temp insn);\n+  void wrap_label(Register r, Label &L, jal_jalr_insn insn);\n+\n+  \/\/ calculate pseudoinstruction\n+  void add(Register Rd, Register Rn, int64_t increment, Register temp = t0);\n+  void addw(Register Rd, Register Rn, int64_t increment, Register temp = t0);\n+  void sub(Register Rd, Register Rn, int64_t decrement, Register temp = t0);\n+  void subw(Register Rd, Register Rn, int64_t decrement, Register temp = t0);\n+\n+  \/\/ RVB pseudo instructions\n+  \/\/ zero extend word\n+  void zext_w(Register Rd, Register Rs);\n+\n+  Assembler(CodeBuffer* code) : AbstractAssembler(code), _in_compressible_region(false) {\n+  }\n+\n+  virtual RegisterOrConstant delayed_value_impl(intptr_t* delayed_value_addr,\n+                                                Register tmp,\n+                                                int offset) {\n+    ShouldNotCallThis();\n+    return RegisterOrConstant();\n+  }\n+\n+  \/\/ Stack overflow checking\n+  virtual void bang_stack_with_offset(int offset) { Unimplemented(); }\n+\n+  static bool operand_valid_for_add_immediate(long imm) {\n+    return is_imm_in_range(imm, 12, 0);\n+  }\n+\n+  \/\/ The maximum range of a branch is fixed for the RISCV architecture.\n+  static const unsigned long branch_range = 1 * M;\n+\n+  static bool reachable_from_branch_at(address branch, address target) {\n+    return uabs(target - branch) < branch_range;\n+  }\n+\n+  virtual ~Assembler() {}\n+};\n+\n+class BiasedLockingCounters;\n+\n+#endif \/\/ CPU_RISCV_ASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":3057,"deletions":0,"binary":false,"changes":3057,"status":"added"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_ASSEMBLER_RISCV_INLINE_HPP\n+#define CPU_RISCV_ASSEMBLER_RISCV_INLINE_HPP\n+\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"code\/codeCache.hpp\"\n+\n+inline bool is_imm_in_range(long value, unsigned bits, unsigned align_bits) {\n+  intx sign_bits = (value >> (bits + align_bits - 1));\n+  return ((value & right_n_bits(align_bits)) == 0) && ((sign_bits == 0) || (sign_bits == -1));\n+}\n+\n+inline bool is_unsigned_imm_in_range(intx value, unsigned bits, unsigned align_bits) {\n+  return (value >= 0) && ((value & right_n_bits(align_bits)) == 0) && ((value >> (align_bits + bits)) == 0);\n+}\n+\n+inline bool is_offset_in_range(intx offset, unsigned bits) {\n+  return is_imm_in_range(offset, bits, 0);\n+}\n+\n+#endif \/\/ CPU_RISCV_ASSEMBLER_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.inline.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2016 SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_BYTES_RISCV_HPP\n+#define CPU_RISCV_BYTES_RISCV_HPP\n+\n+class Bytes: AllStatic {\n+ public:\n+  \/\/ Efficient reading and writing of unaligned unsigned data in platform-specific byte ordering\n+  \/\/ RISCV needs to check for alignment.\n+\n+  \/\/ Forward declarations of the compiler-dependent implementation\n+  static inline u2 swap_u2(u2 x);\n+  static inline u4 swap_u4(u4 x);\n+  static inline u8 swap_u8(u8 x);\n+\n+  static inline u2 get_native_u2(address p) {\n+    if ((intptr_t(p) & 1) == 0) {\n+      return *(u2*)p;\n+    } else {\n+      return ((u2)(p[1]) << 8) |\n+             ((u2)(p[0]));\n+    }\n+  }\n+\n+  static inline u4 get_native_u4(address p) {\n+    switch (intptr_t(p) & 3) {\n+      case 0:\n+        return *(u4*)p;\n+\n+      case 2:\n+        return ((u4)(((u2*)p)[1]) << 16) |\n+               ((u4)(((u2*)p)[0]));\n+\n+      default:\n+        return ((u4)(p[3]) << 24) |\n+               ((u4)(p[2]) << 16) |\n+               ((u4)(p[1]) <<  8) |\n+               ((u4)(p[0]));\n+    }\n+  }\n+\n+  static inline u8 get_native_u8(address p) {\n+    switch (intptr_t(p) & 7) {\n+      case 0:\n+        return *(u8*)p;\n+\n+      case 4:\n+        return ((u8)(((u4*)p)[1]) << 32) |\n+               ((u8)(((u4*)p)[0]));\n+\n+      case 2:\n+        return ((u8)(((u2*)p)[3]) << 48) |\n+               ((u8)(((u2*)p)[2]) << 32) |\n+               ((u8)(((u2*)p)[1]) << 16) |\n+               ((u8)(((u2*)p)[0]));\n+\n+      default:\n+        return ((u8)(p[7]) << 56) |\n+               ((u8)(p[6]) << 48) |\n+               ((u8)(p[5]) << 40) |\n+               ((u8)(p[4]) << 32) |\n+               ((u8)(p[3]) << 24) |\n+               ((u8)(p[2]) << 16) |\n+               ((u8)(p[1]) <<  8) |\n+               ((u8)(p[0]));\n+    }\n+  }\n+\n+  static inline void put_native_u2(address p, u2 x) {\n+    if ((intptr_t(p) & 1) == 0) {\n+      *(u2*)p = x;\n+    } else {\n+      p[1] = x >> 8;\n+      p[0] = x;\n+    }\n+  }\n+\n+  static inline void put_native_u4(address p, u4 x) {\n+    switch (intptr_t(p) & 3) {\n+      case 0:\n+        *(u4*)p = x;\n+        break;\n+\n+      case 2:\n+        ((u2*)p)[1] = x >> 16;\n+        ((u2*)p)[0] = x;\n+        break;\n+\n+      default:\n+        ((u1*)p)[3] = x >> 24;\n+        ((u1*)p)[2] = x >> 16;\n+        ((u1*)p)[1] = x >>  8;\n+        ((u1*)p)[0] = x;\n+        break;\n+    }\n+  }\n+\n+  static inline void put_native_u8(address p, u8 x) {\n+    switch (intptr_t(p) & 7) {\n+      case 0:\n+        *(u8*)p = x;\n+        break;\n+\n+      case 4:\n+        ((u4*)p)[1] = x >> 32;\n+        ((u4*)p)[0] = x;\n+        break;\n+\n+      case 2:\n+        ((u2*)p)[3] = x >> 48;\n+        ((u2*)p)[2] = x >> 32;\n+        ((u2*)p)[1] = x >> 16;\n+        ((u2*)p)[0] = x;\n+        break;\n+\n+      default:\n+        ((u1*)p)[7] = x >> 56;\n+        ((u1*)p)[6] = x >> 48;\n+        ((u1*)p)[5] = x >> 40;\n+        ((u1*)p)[4] = x >> 32;\n+        ((u1*)p)[3] = x >> 24;\n+        ((u1*)p)[2] = x >> 16;\n+        ((u1*)p)[1] = x >>  8;\n+        ((u1*)p)[0] = x;\n+        break;\n+    }\n+  }\n+\n+  \/\/ Efficient reading and writing of unaligned unsigned data in Java byte ordering (i.e. big-endian ordering)\n+  static inline u2 get_Java_u2(address p) { return swap_u2(get_native_u2(p)); }\n+  static inline u4 get_Java_u4(address p) { return swap_u4(get_native_u4(p)); }\n+  static inline u8 get_Java_u8(address p) { return swap_u8(get_native_u8(p)); }\n+\n+  static inline void put_Java_u2(address p, u2 x) { put_native_u2(p, swap_u2(x)); }\n+  static inline void put_Java_u4(address p, u4 x) { put_native_u4(p, swap_u4(x)); }\n+  static inline void put_Java_u8(address p, u8 x) { put_native_u8(p, swap_u8(x)); }\n+};\n+\n+#include OS_CPU_HEADER(bytes)\n+\n+#endif \/\/ CPU_RISCV_BYTES_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/bytes_riscv.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -0,0 +1,339 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"c1\/c1_CodeStubs.hpp\"\n+#include \"c1\/c1_FrameMap.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+\n+#define __ ce->masm()->\n+\n+void CounterOverflowStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  Metadata *m = _method->as_constant_ptr()->as_metadata();\n+  __ mov_metadata(t0, m);\n+  ce->store_parameter(t0, 1);\n+  ce->store_parameter(_bci, 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::counter_overflow_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ j(_continuation);\n+}\n+\n+RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n+  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n+  assert(info != NULL, \"must have info\");\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n+  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  assert(info != NULL, \"must have info\");\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void RangeCheckStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  if (_info->deoptimize_on_exception()) {\n+    address a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);\n+    __ far_call(RuntimeAddress(a));\n+    ce->add_call_info_here(_info);\n+    ce->verify_oop_map(_info);\n+    debug_only(__ should_not_reach_here());\n+    return;\n+  }\n+\n+  if (_index->is_cpu_register()) {\n+    __ mv(t0, _index->as_register());\n+  } else {\n+    __ mv(t0, _index->as_jint());\n+  }\n+  Runtime1::StubID stub_id;\n+  if (_throw_index_out_of_bounds_exception) {\n+    stub_id = Runtime1::throw_index_exception_id;\n+  } else {\n+    assert(_array != NULL, \"sanity\");\n+    __ mv(t1, _array->as_pointer_register());\n+    stub_id = Runtime1::throw_range_check_failed_id;\n+  }\n+  int32_t off = 0;\n+  __ la_patchable(ra, RuntimeAddress(Runtime1::entry_for(stub_id)), off);\n+  __ jalr(ra, ra, off);\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  debug_only(__ should_not_reach_here());\n+}\n+\n+PredicateFailedStub::PredicateFailedStub(CodeEmitInfo* info) {\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void PredicateFailedStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  address a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);\n+  __ far_call(RuntimeAddress(a));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  debug_only(__ should_not_reach_here());\n+}\n+\n+void DivByZeroStub::emit_code(LIR_Assembler* ce) {\n+  if (_offset != -1) {\n+    ce->compilation()->implicit_exception_table()->append(_offset, __ offset());\n+  }\n+  __ bind(_entry);\n+  __ far_call(Address(Runtime1::entry_for(Runtime1::throw_div0_exception_id), relocInfo::runtime_call_type));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+#ifdef ASSERT\n+  __ should_not_reach_here();\n+#endif\n+}\n+\n+\/\/ Implementation of NewInstanceStub\n+NewInstanceStub::NewInstanceStub(LIR_Opr klass_reg, LIR_Opr result, ciInstanceKlass* klass, CodeEmitInfo* info, Runtime1::StubID stub_id) {\n+  _result = result;\n+  _klass = klass;\n+  _klass_reg = klass_reg;\n+  _info = new CodeEmitInfo(info);\n+  assert(stub_id == Runtime1::new_instance_id                 ||\n+         stub_id == Runtime1::fast_new_instance_id            ||\n+         stub_id == Runtime1::fast_new_instance_init_check_id,\n+         \"need new_instance id\");\n+  _stub_id = stub_id;\n+}\n+\n+void NewInstanceStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  __ mv(x13, _klass_reg->as_register());\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(_stub_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  assert(_result->as_register() == x10, \"result must in x10\");\n+  __ j(_continuation);\n+}\n+\n+\/\/ Implementation of NewTypeArrayStub\n+NewTypeArrayStub::NewTypeArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+  _klass_reg = klass_reg;\n+  _length = length;\n+  _result = result;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void NewTypeArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  assert(_length->as_register() == x9, \"length must in x9\");\n+  assert(_klass_reg->as_register() == x13, \"klass_reg must in x13\");\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_type_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  assert(_result->as_register() == x10, \"result must in x10\");\n+  __ j(_continuation);\n+}\n+\n+\/\/ Implementation of NewObjectArrayStub\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+  _klass_reg = klass_reg;\n+  _result = result;\n+  _length = length;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void NewObjectArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  assert(_length->as_register() == x9, \"length must in x9\");\n+  assert(_klass_reg->as_register() == x13, \"klass_reg must in x13\");\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  assert(_result->as_register() == x10, \"result must in x10\");\n+  __ j(_continuation);\n+}\n+\n+\/\/ Implementation of MonitorAccessStubs\n+MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+: MonitorAccessStub(obj_reg, lock_reg) {\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void MonitorEnterStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_obj_reg->as_register(),  1);\n+  ce->store_parameter(_lock_reg->as_register(), 0);\n+  Runtime1::StubID enter_id;\n+  if (ce->compilation()->has_fpu_code()) {\n+    enter_id = Runtime1::monitorenter_id;\n+  } else {\n+    enter_id = Runtime1::monitorenter_nofpu_id;\n+  }\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(enter_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ j(_continuation);\n+}\n+\n+void MonitorExitStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  if (_compute_lock) {\n+    \/\/ lock_reg was destroyed by fast unlocking attempt => recompute it\n+    ce->monitor_address(_monitor_ix, _lock_reg);\n+  }\n+  ce->store_parameter(_lock_reg->as_register(), 0);\n+  \/\/ note: non-blocking leaf routine => no call info needed\n+  Runtime1::StubID exit_id;\n+  if (ce->compilation()->has_fpu_code()) {\n+    exit_id = Runtime1::monitorexit_id;\n+  } else {\n+    exit_id = Runtime1::monitorexit_nofpu_id;\n+  }\n+  __ la(ra, _continuation);\n+  __ far_jump(RuntimeAddress(Runtime1::entry_for(exit_id)));\n+}\n+\n+\/\/ Implementation of patching:\n+\/\/ - Copy the code at given offset to an inlined buffer (first the bytes, then the number of bytes)\n+\/\/ - Replace original code with a call to the stub\n+\/\/ At Runtime:\n+\/\/ - call to stub, jump to runtime\n+\/\/ - in runtime: preserve all registers (rspecially objects, i.e., source and destination object)\n+\/\/ - in runtime: after initializing class, restore original code, reexecute instruction\n+\n+int PatchingStub::_patch_info_offset = -NativeGeneralJump::instruction_size;\n+\n+void PatchingStub::align_patch_site(MacroAssembler* masm) {}\n+\n+void PatchingStub::emit_code(LIR_Assembler* ce) {\n+  assert(false, \"RISCV should not use C1 runtime patching\");\n+}\n+\n+void DeoptimizeStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  ce->store_parameter(_trap_request, 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::deoptimize_id)));\n+  ce->add_call_info_here(_info);\n+  DEBUG_ONLY(__ should_not_reach_here());\n+}\n+\n+void ImplicitNullCheckStub::emit_code(LIR_Assembler* ce) {\n+  address a = NULL;\n+  if (_info->deoptimize_on_exception()) {\n+    \/\/ Deoptimize, do not throw the exception, because it is probably wrong to do it here.\n+    a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);\n+  } else {\n+    a = Runtime1::entry_for(Runtime1::throw_null_pointer_exception_id);\n+  }\n+\n+  ce->compilation()->implicit_exception_table()->append(_offset, __ offset());\n+  __ bind(_entry);\n+  __ far_call(RuntimeAddress(a));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  debug_only(__ should_not_reach_here());\n+}\n+\n+void SimpleExceptionStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+\n+  __ bind(_entry);\n+  \/\/ pass the object in a tmp register because all other registers\n+  \/\/ must be preserved\n+  if (_obj->is_cpu_register()) {\n+    __ mv(t0, _obj->as_register());\n+  }\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(_stub)), NULL, t1);\n+  ce->add_call_info_here(_info);\n+  debug_only(__ should_not_reach_here());\n+}\n+\n+void ArrayCopyStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ ---------------slow case: call to native-----------------\n+  __ bind(_entry);\n+  \/\/ Figure out where the args should go\n+  \/\/ This should really convert the IntrinsicID to the Method* and signature\n+  \/\/ but I don't know how to do that.\n+  const int args_num = 5;\n+  VMRegPair args[args_num];\n+  BasicType signature[args_num] = { T_OBJECT, T_INT, T_OBJECT, T_INT, T_INT };\n+  SharedRuntime::java_calling_convention(signature, args, args_num, true);\n+\n+  \/\/ push parameters\n+  Register r[args_num];\n+  r[0] = src()->as_register();\n+  r[1] = src_pos()->as_register();\n+  r[2] = dst()->as_register();\n+  r[3] = dst_pos()->as_register();\n+  r[4] = length()->as_register();\n+\n+  \/\/ next registers will get stored on the stack\n+  for (int j = 0; j < args_num; j++) {\n+    VMReg r_1 = args[j].first();\n+    if (r_1->is_stack()) {\n+      int st_off = r_1->reg2stack() * wordSize;\n+      __ sd(r[j], Address(sp, st_off));\n+    } else {\n+      assert(r[j] == args[j].first()->as_Register(), \"Wrong register for arg\");\n+    }\n+  }\n+\n+  ce->align_call(lir_static_call);\n+\n+  ce->emit_static_call_stub();\n+  if (ce->compilation()->bailed_out()) {\n+    return; \/\/ CodeCache is full\n+  }\n+  Address resolve(SharedRuntime::get_resolve_static_call_stub(),\n+                  relocInfo::static_call_type);\n+  address call = __ trampoline_call(resolve);\n+  if (call == NULL) {\n+    ce->bailout(\"trampoline stub overflow\");\n+    return;\n+  }\n+  ce->add_call_info_here(info());\n+\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    __ la(t1, ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n+    __ add_memory_int32(Address(t1), 1);\n+  }\n+#endif\n+\n+  __ j(_continuation);\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/c1_CodeStubs_riscv.cpp","additions":339,"deletions":0,"binary":false,"changes":339,"status":"added"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_DEFS_RISCV_HPP\n+#define CPU_RISCV_C1_DEFS_RISCV_HPP\n+\n+\/\/ native word offsets from memory address (little endian)\n+enum {\n+  pd_lo_word_offset_in_bytes = 0,\n+  pd_hi_word_offset_in_bytes = BytesPerWord\n+};\n+\n+\/\/ explicit rounding operations are required to implement the strictFP mode\n+enum {\n+  pd_strict_fp_requires_explicit_rounding = false\n+};\n+\n+\/\/ registers\n+enum {\n+  pd_nof_cpu_regs_frame_map = RegisterImpl::number_of_registers,       \/\/ number of registers used during code emission\n+  pd_nof_fpu_regs_frame_map = FloatRegisterImpl::number_of_registers,  \/\/ number of float registers used during code emission\n+\n+  \/\/ caller saved\n+  pd_nof_caller_save_cpu_regs_frame_map = 13, \/\/ number of registers killed by calls\n+  pd_nof_caller_save_fpu_regs_frame_map = 32, \/\/ number of float registers killed by calls\n+\n+  pd_first_callee_saved_reg = pd_nof_caller_save_cpu_regs_frame_map,\n+  pd_last_callee_saved_reg = 21,\n+\n+  pd_last_allocatable_cpu_reg = pd_nof_caller_save_cpu_regs_frame_map - 1,\n+\n+  pd_nof_cpu_regs_reg_alloc\n+    = pd_nof_caller_save_cpu_regs_frame_map,  \/\/ number of registers that are visible to register allocator\n+  pd_nof_fpu_regs_reg_alloc = 32,  \/\/ number of float registers that are visible to register allocator\n+\n+  pd_nof_cpu_regs_linearscan = 32, \/\/ number of registers visible to linear scan\n+  pd_nof_fpu_regs_linearscan = pd_nof_fpu_regs_frame_map, \/\/ number of float registers visible to linear scan\n+  pd_nof_xmm_regs_linearscan = 0, \/\/ don't have vector registers\n+\n+  pd_first_cpu_reg  = 0,\n+  pd_last_cpu_reg   = pd_nof_cpu_regs_reg_alloc - 1,\n+  pd_first_byte_reg = 0,\n+  pd_last_byte_reg  = pd_nof_cpu_regs_reg_alloc - 1,\n+\n+  pd_first_fpu_reg  = pd_nof_cpu_regs_frame_map,\n+  pd_last_fpu_reg   = pd_first_fpu_reg + 31,\n+\n+  pd_first_callee_saved_fpu_reg_1 = 8 + pd_first_fpu_reg,\n+  pd_last_callee_saved_fpu_reg_1  = 9 + pd_first_fpu_reg,\n+  pd_first_callee_saved_fpu_reg_2 = 18 + pd_first_fpu_reg,\n+  pd_last_callee_saved_fpu_reg_2  = 27 + pd_first_fpu_reg\n+};\n+\n+\n+\/\/ Encoding of float value in debug info.  This is true on x86 where\n+\/\/ floats are extended to doubles when stored in the stack, false for\n+\/\/ RISCV where floats and doubles are stored in their native form.\n+enum {\n+  pd_float_saved_as_double = false\n+};\n+\n+#endif \/\/ CPU_RISCV_C1_DEFS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Defs_riscv.hpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2005, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/\/--------------------------------------------------------\n+\/\/               FpuStackSim\n+\/\/--------------------------------------------------------\n+\n+\/\/ No FPU stack on RISCV\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FpuStackSim_riscv.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,32 @@\n+\/*\n+ * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n+#define CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n+\n+\/\/ No FPU stack on RISCV\n+class FpuStackSim;\n+\n+#endif \/\/ CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FpuStackSim_riscv.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"added"},{"patch":"@@ -0,0 +1,388 @@\n+\/*\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"c1\/c1_FrameMap.hpp\"\n+#include \"c1\/c1_LIR.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+LIR_Opr FrameMap::map_to_opr(BasicType type, VMRegPair* reg, bool) {\n+  LIR_Opr opr = LIR_OprFact::illegalOpr;\n+  VMReg r_1 = reg->first();\n+  VMReg r_2 = reg->second();\n+  if (r_1->is_stack()) {\n+    \/\/ Convert stack slot to an SP offset\n+    \/\/ The calling convention does not count the SharedRuntime::out_preserve_stack_slots() value\n+    \/\/ so we must add it in here.\n+    int st_off = (r_1->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+    opr = LIR_OprFact::address(new LIR_Address(sp_opr, st_off, type));\n+  } else if (r_1->is_Register()) {\n+    Register reg1 = r_1->as_Register();\n+    if (r_2->is_Register() && (type == T_LONG || type == T_DOUBLE)) {\n+      Register reg2 = r_2->as_Register();\n+      assert(reg2 == reg1, \"must be same register\");\n+      opr = as_long_opr(reg1);\n+    } else if (is_reference_type(type)) {\n+      opr = as_oop_opr(reg1);\n+    } else if (type == T_METADATA) {\n+      opr = as_metadata_opr(reg1);\n+    } else if (type == T_ADDRESS) {\n+      opr = as_address_opr(reg1);\n+    } else {\n+      opr = as_opr(reg1);\n+    }\n+  } else if (r_1->is_FloatRegister()) {\n+    assert(type == T_DOUBLE || type == T_FLOAT, \"wrong type\");\n+    int num = r_1->as_FloatRegister()->encoding();\n+    if (type == T_FLOAT) {\n+      opr = LIR_OprFact::single_fpu(num);\n+    } else {\n+      opr = LIR_OprFact::double_fpu(num);\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return opr;\n+}\n+\n+LIR_Opr FrameMap::zr_opr;\n+LIR_Opr FrameMap::r1_opr;\n+LIR_Opr FrameMap::r2_opr;\n+LIR_Opr FrameMap::r3_opr;\n+LIR_Opr FrameMap::r4_opr;\n+LIR_Opr FrameMap::r5_opr;\n+LIR_Opr FrameMap::r6_opr;\n+LIR_Opr FrameMap::r7_opr;\n+LIR_Opr FrameMap::r8_opr;\n+LIR_Opr FrameMap::r9_opr;\n+LIR_Opr FrameMap::r10_opr;\n+LIR_Opr FrameMap::r11_opr;\n+LIR_Opr FrameMap::r12_opr;\n+LIR_Opr FrameMap::r13_opr;\n+LIR_Opr FrameMap::r14_opr;\n+LIR_Opr FrameMap::r15_opr;\n+LIR_Opr FrameMap::r16_opr;\n+LIR_Opr FrameMap::r17_opr;\n+LIR_Opr FrameMap::r18_opr;\n+LIR_Opr FrameMap::r19_opr;\n+LIR_Opr FrameMap::r20_opr;\n+LIR_Opr FrameMap::r21_opr;\n+LIR_Opr FrameMap::r22_opr;\n+LIR_Opr FrameMap::r23_opr;\n+LIR_Opr FrameMap::r24_opr;\n+LIR_Opr FrameMap::r25_opr;\n+LIR_Opr FrameMap::r26_opr;\n+LIR_Opr FrameMap::r27_opr;\n+LIR_Opr FrameMap::r28_opr;\n+LIR_Opr FrameMap::r29_opr;\n+LIR_Opr FrameMap::r30_opr;\n+LIR_Opr FrameMap::r31_opr;\n+\n+LIR_Opr FrameMap::fp_opr;\n+LIR_Opr FrameMap::sp_opr;\n+\n+LIR_Opr FrameMap::receiver_opr;\n+\n+LIR_Opr FrameMap::zr_oop_opr;\n+LIR_Opr FrameMap::r1_oop_opr;\n+LIR_Opr FrameMap::r2_oop_opr;\n+LIR_Opr FrameMap::r3_oop_opr;\n+LIR_Opr FrameMap::r4_oop_opr;\n+LIR_Opr FrameMap::r5_oop_opr;\n+LIR_Opr FrameMap::r6_oop_opr;\n+LIR_Opr FrameMap::r7_oop_opr;\n+LIR_Opr FrameMap::r8_oop_opr;\n+LIR_Opr FrameMap::r9_oop_opr;\n+LIR_Opr FrameMap::r10_oop_opr;\n+LIR_Opr FrameMap::r11_oop_opr;\n+LIR_Opr FrameMap::r12_oop_opr;\n+LIR_Opr FrameMap::r13_oop_opr;\n+LIR_Opr FrameMap::r14_oop_opr;\n+LIR_Opr FrameMap::r15_oop_opr;\n+LIR_Opr FrameMap::r16_oop_opr;\n+LIR_Opr FrameMap::r17_oop_opr;\n+LIR_Opr FrameMap::r18_oop_opr;\n+LIR_Opr FrameMap::r19_oop_opr;\n+LIR_Opr FrameMap::r20_oop_opr;\n+LIR_Opr FrameMap::r21_oop_opr;\n+LIR_Opr FrameMap::r22_oop_opr;\n+LIR_Opr FrameMap::r23_oop_opr;\n+LIR_Opr FrameMap::r24_oop_opr;\n+LIR_Opr FrameMap::r25_oop_opr;\n+LIR_Opr FrameMap::r26_oop_opr;\n+LIR_Opr FrameMap::r27_oop_opr;\n+LIR_Opr FrameMap::r28_oop_opr;\n+LIR_Opr FrameMap::r29_oop_opr;\n+LIR_Opr FrameMap::r30_oop_opr;\n+LIR_Opr FrameMap::r31_oop_opr;\n+\n+LIR_Opr FrameMap::t0_opr;\n+LIR_Opr FrameMap::t1_opr;\n+LIR_Opr FrameMap::t0_long_opr;\n+LIR_Opr FrameMap::t1_long_opr;\n+\n+LIR_Opr FrameMap::r10_metadata_opr;\n+LIR_Opr FrameMap::r11_metadata_opr;\n+LIR_Opr FrameMap::r12_metadata_opr;\n+LIR_Opr FrameMap::r13_metadata_opr;\n+LIR_Opr FrameMap::r14_metadata_opr;\n+LIR_Opr FrameMap::r15_metadata_opr;\n+\n+LIR_Opr FrameMap::long10_opr;\n+LIR_Opr FrameMap::long11_opr;\n+LIR_Opr FrameMap::fpu10_float_opr;\n+LIR_Opr FrameMap::fpu10_double_opr;\n+\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0, };\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = { 0, };\n+\n+\/\/--------------------------------------------------------\n+\/\/               FrameMap\n+\/\/--------------------------------------------------------\n+\/\/ |---f31--|\n+\/\/ |---..---|\n+\/\/ |---f28--|\n+\/\/ |---f27--|<---pd_last_callee_saved_fpu_reg_2\n+\/\/ |---..---|\n+\/\/ |---f18--|<---pd_first_callee_saved_fpu_reg_2\n+\/\/ |---f17--|\n+\/\/ |---..---|\n+\/\/ |---f10--|\n+\/\/ |---f9---|<---pd_last_callee_saved_fpu_reg_1\n+\/\/ |---f8---|<---pd_first_callee_saved_fpu_reg_1\n+\/\/ |---f7---|\n+\/\/ |---..---|\n+\/\/ |---f0---|\n+\/\/ |---x27--|\n+\/\/ |---x23--|\n+\/\/ |---x8---|\n+\/\/ |---x4---|\n+\/\/ |---x3---|\n+\/\/ |---x2---|\n+\/\/ |---x1---|\n+\/\/ |---x0---|\n+\/\/ |---x26--|<---pd_last_callee_saved_reg\n+\/\/ |---..---|\n+\/\/ |---x18--|\n+\/\/ |---x9---|<---pd_first_callee_saved_reg\n+\/\/ |---x31--|\n+\/\/ |---..---|\n+\/\/ |---x28--|\n+\/\/ |---x17--|\n+\/\/ |---..---|\n+\/\/ |---x10--|\n+\/\/ |---x7---|\n+\n+void FrameMap::initialize() {\n+  assert(!_init_done, \"once\");\n+\n+  int i = 0;\n+\n+  \/\/ caller save register\n+  map_register(i, x7);  r7_opr  = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x10); r10_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x11); r11_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x12); r12_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x13); r13_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x14); r14_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x15); r15_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x16); r16_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x17); r17_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x28); r28_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x29); r29_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x30); r30_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x31); r31_opr = LIR_OprFact::single_cpu(i); i++;\n+\n+  \/\/ callee save register\n+  map_register(i, x9);  r9_opr  = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x18); r18_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x19); r19_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x20); r20_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x21); r21_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x22); r22_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x24); r24_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x25); r25_opr = LIR_OprFact::single_cpu(i); i++;\n+  map_register(i, x26); r26_opr = LIR_OprFact::single_cpu(i); i++;\n+\n+  \/\/ special register\n+  map_register(i, x0);  zr_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ zr\n+  map_register(i, x1);  r1_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ ra\n+  map_register(i, x2);  r2_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ sp\n+  map_register(i, x3);  r3_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ gp\n+  map_register(i, x4);  r4_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ thread\n+  map_register(i, x8);  r8_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ fp\n+  map_register(i, x23); r23_opr = LIR_OprFact::single_cpu(i); i++;  \/\/ java thread\n+  map_register(i, x27); r27_opr = LIR_OprFact::single_cpu(i); i++;  \/\/ heapbase\n+\n+  \/\/ tmp register\n+  map_register(i, x5);  r5_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ t0\n+  map_register(i, x6);  r6_opr  = LIR_OprFact::single_cpu(i); i++;  \/\/ t1\n+\n+  t0_opr = r5_opr;\n+  t1_opr = r6_opr;\n+  t0_long_opr = LIR_OprFact::double_cpu(r5_opr->cpu_regnr(), r5_opr->cpu_regnr());\n+  t1_long_opr = LIR_OprFact::double_cpu(r6_opr->cpu_regnr(), r6_opr->cpu_regnr());\n+\n+  long10_opr  = LIR_OprFact::double_cpu(r10_opr->cpu_regnr(), r10_opr->cpu_regnr());\n+  long11_opr  = LIR_OprFact::double_cpu(r11_opr->cpu_regnr(), r11_opr->cpu_regnr());\n+\n+  fpu10_float_opr   = LIR_OprFact::single_fpu(10);\n+  fpu10_double_opr  = LIR_OprFact::double_fpu(10);\n+\n+  i = 0;\n+  _caller_save_cpu_regs[i++]  = r7_opr;\n+  _caller_save_cpu_regs[i++]  = r10_opr;\n+  _caller_save_cpu_regs[i++]  = r11_opr;\n+  _caller_save_cpu_regs[i++]  = r12_opr;\n+  _caller_save_cpu_regs[i++]  = r13_opr;\n+  _caller_save_cpu_regs[i++]  = r14_opr;\n+  _caller_save_cpu_regs[i++]  = r15_opr;\n+  _caller_save_cpu_regs[i++]  = r16_opr;\n+  _caller_save_cpu_regs[i++]  = r17_opr;\n+  _caller_save_cpu_regs[i++]  = r28_opr;\n+  _caller_save_cpu_regs[i++]  = r29_opr;\n+  _caller_save_cpu_regs[i++]  = r30_opr;\n+  _caller_save_cpu_regs[i++]  = r31_opr;\n+\n+  _init_done = true;\n+\n+  zr_oop_opr  = as_oop_opr(x0);\n+  r1_oop_opr  = as_oop_opr(x1);\n+  r2_oop_opr  = as_oop_opr(x2);\n+  r3_oop_opr  = as_oop_opr(x3);\n+  r4_oop_opr  = as_oop_opr(x4);\n+  r5_oop_opr  = as_oop_opr(x5);\n+  r6_oop_opr  = as_oop_opr(x6);\n+  r7_oop_opr  = as_oop_opr(x7);\n+  r8_oop_opr  = as_oop_opr(x8);\n+  r9_oop_opr  = as_oop_opr(x9);\n+  r10_oop_opr = as_oop_opr(x10);\n+  r11_oop_opr = as_oop_opr(x11);\n+  r12_oop_opr = as_oop_opr(x12);\n+  r13_oop_opr = as_oop_opr(x13);\n+  r14_oop_opr = as_oop_opr(x14);\n+  r15_oop_opr = as_oop_opr(x15);\n+  r16_oop_opr = as_oop_opr(x16);\n+  r17_oop_opr = as_oop_opr(x17);\n+  r18_oop_opr = as_oop_opr(x18);\n+  r19_oop_opr = as_oop_opr(x19);\n+  r20_oop_opr = as_oop_opr(x20);\n+  r21_oop_opr = as_oop_opr(x21);\n+  r22_oop_opr = as_oop_opr(x22);\n+  r23_oop_opr = as_oop_opr(x23);\n+  r24_oop_opr = as_oop_opr(x24);\n+  r25_oop_opr = as_oop_opr(x25);\n+  r26_oop_opr = as_oop_opr(x26);\n+  r27_oop_opr = as_oop_opr(x27);\n+  r28_oop_opr = as_oop_opr(x28);\n+  r29_oop_opr = as_oop_opr(x29);\n+  r30_oop_opr = as_oop_opr(x30);\n+  r31_oop_opr = as_oop_opr(x31);\n+\n+  r10_metadata_opr = as_metadata_opr(x10);\n+  r11_metadata_opr = as_metadata_opr(x11);\n+  r12_metadata_opr = as_metadata_opr(x12);\n+  r13_metadata_opr = as_metadata_opr(x13);\n+  r14_metadata_opr = as_metadata_opr(x14);\n+  r15_metadata_opr = as_metadata_opr(x15);\n+\n+  sp_opr = as_pointer_opr(sp);\n+  fp_opr = as_pointer_opr(fp);\n+\n+  VMRegPair regs;\n+  BasicType sig_bt = T_OBJECT;\n+  SharedRuntime::java_calling_convention(&sig_bt, &regs, 1, true);\n+  receiver_opr = as_oop_opr(regs.first()->as_Register());\n+\n+  for (i = 0; i < nof_caller_save_fpu_regs; i++) {\n+    _caller_save_fpu_regs[i] = LIR_OprFact::single_fpu(i);\n+  }\n+}\n+\n+\n+Address FrameMap::make_new_address(ByteSize sp_offset) const {\n+  return Address(sp, in_bytes(sp_offset));\n+}\n+\n+\n+\/\/ ----------------mapping-----------------------\n+\/\/ all mapping is based on fp addressing, except for simple leaf methods where we access\n+\/\/ the locals sp based (and no frame is built)\n+\n+\n+\/\/ Frame for simple leaf methods (quick entries)\n+\/\/\n+\/\/   +----------+\n+\/\/   | ret addr |   <- TOS\n+\/\/   +----------+\n+\/\/   | args     |\n+\/\/   | ......   |\n+\n+\/\/ Frame for standard methods\n+\/\/\n+\/\/   | .........|  <- TOS\n+\/\/   | locals   |\n+\/\/   +----------+\n+\/\/   |  old fp, |\n+\/\/   +----------+\n+\/\/   | ret addr |\n+\/\/   +----------+\n+\/\/   |  args    |  <- FP\n+\/\/   | .........|\n+\n+\n+\/\/ For OopMaps, map a local variable or spill index to an VMRegImpl name.\n+\/\/ This is the offset from sp() in the frame of the slot for the index,\n+\/\/ skewed by VMRegImpl::stack0 to indicate a stack location (vs.a register.)\n+\/\/\n+\/\/           framesize +\n+\/\/           stack0         stack0          0  <- VMReg\n+\/\/             |              | <registers> |\n+\/\/  ...........|..............|.............|\n+\/\/      0 1 2 3 x x 4 5 6 ... |                <- local indices\n+\/\/      ^           ^        sp()                 ( x x indicate link\n+\/\/      |           |                               and return addr)\n+\/\/  arguments   non-argument locals\n+\n+\n+VMReg FrameMap::fpu_regname (int n) {\n+  \/\/ Return the OptoReg name for the fpu stack slot \"n\"\n+  \/\/ A spilled fpu stack slot comprises to two single-word OptoReg's.\n+  return as_FloatRegister(n)->as_VMReg();\n+}\n+\n+LIR_Opr FrameMap::stack_pointer() {\n+  return FrameMap::sp_opr;\n+}\n+\n+\/\/ JSR 292\n+LIR_Opr FrameMap::method_handle_invoke_SP_save_opr() {\n+  return LIR_OprFact::illegalOpr;  \/\/ Not needed on riscv\n+}\n+\n+bool FrameMap::validate_frame() {\n+  return true;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FrameMap_riscv.cpp","additions":388,"deletions":0,"binary":false,"changes":388,"status":"added"},{"patch":"@@ -0,0 +1,148 @@\n+\/*\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_FRAMEMAP_RISCV_HPP\n+#define CPU_RISCV_C1_FRAMEMAP_RISCV_HPP\n+\n+\/\/  On RISCV the frame looks as follows:\n+\/\/\n+\/\/  +-----------------------------+---------+----------------------------------------+----------------+-----------\n+\/\/  | size_arguments-nof_reg_args | 2 words | size_locals-size_arguments+numreg_args | _size_monitors | spilling .\n+\/\/  +-----------------------------+---------+----------------------------------------+----------------+-----------\n+\n+ public:\n+  static const int pd_c_runtime_reserved_arg_size;\n+\n+  enum {\n+    first_available_sp_in_frame = 0,\n+    frame_pad_in_bytes = 16,\n+    nof_reg_args = 8\n+  };\n+\n+ public:\n+  static LIR_Opr receiver_opr;\n+\n+  static LIR_Opr zr_opr;\n+  static LIR_Opr r1_opr;\n+  static LIR_Opr r2_opr;\n+  static LIR_Opr r3_opr;\n+  static LIR_Opr r4_opr;\n+  static LIR_Opr r5_opr;\n+  static LIR_Opr r6_opr;\n+  static LIR_Opr r7_opr;\n+  static LIR_Opr r8_opr;\n+  static LIR_Opr r9_opr;\n+  static LIR_Opr r10_opr;\n+  static LIR_Opr r11_opr;\n+  static LIR_Opr r12_opr;\n+  static LIR_Opr r13_opr;\n+  static LIR_Opr r14_opr;\n+  static LIR_Opr r15_opr;\n+  static LIR_Opr r16_opr;\n+  static LIR_Opr r17_opr;\n+  static LIR_Opr r18_opr;\n+  static LIR_Opr r19_opr;\n+  static LIR_Opr r20_opr;\n+  static LIR_Opr r21_opr;\n+  static LIR_Opr r22_opr;\n+  static LIR_Opr r23_opr;\n+  static LIR_Opr r24_opr;\n+  static LIR_Opr r25_opr;\n+  static LIR_Opr r26_opr;\n+  static LIR_Opr r27_opr;\n+  static LIR_Opr r28_opr;\n+  static LIR_Opr r29_opr;\n+  static LIR_Opr r30_opr;\n+  static LIR_Opr r31_opr;\n+  static LIR_Opr fp_opr;\n+  static LIR_Opr sp_opr;\n+\n+  static LIR_Opr zr_oop_opr;\n+  static LIR_Opr r1_oop_opr;\n+  static LIR_Opr r2_oop_opr;\n+  static LIR_Opr r3_oop_opr;\n+  static LIR_Opr r4_oop_opr;\n+  static LIR_Opr r5_oop_opr;\n+  static LIR_Opr r6_oop_opr;\n+  static LIR_Opr r7_oop_opr;\n+  static LIR_Opr r8_oop_opr;\n+  static LIR_Opr r9_oop_opr;\n+  static LIR_Opr r10_oop_opr;\n+  static LIR_Opr r11_oop_opr;\n+  static LIR_Opr r12_oop_opr;\n+  static LIR_Opr r13_oop_opr;\n+  static LIR_Opr r14_oop_opr;\n+  static LIR_Opr r15_oop_opr;\n+  static LIR_Opr r16_oop_opr;\n+  static LIR_Opr r17_oop_opr;\n+  static LIR_Opr r18_oop_opr;\n+  static LIR_Opr r19_oop_opr;\n+  static LIR_Opr r20_oop_opr;\n+  static LIR_Opr r21_oop_opr;\n+  static LIR_Opr r22_oop_opr;\n+  static LIR_Opr r23_oop_opr;\n+  static LIR_Opr r24_oop_opr;\n+  static LIR_Opr r25_oop_opr;\n+  static LIR_Opr r26_oop_opr;\n+  static LIR_Opr r27_oop_opr;\n+  static LIR_Opr r28_oop_opr;\n+  static LIR_Opr r29_oop_opr;\n+  static LIR_Opr r30_oop_opr;\n+  static LIR_Opr r31_oop_opr;\n+\n+  static LIR_Opr t0_opr;\n+  static LIR_Opr t1_opr;\n+  static LIR_Opr t0_long_opr;\n+  static LIR_Opr t1_long_opr;\n+\n+  static LIR_Opr r10_metadata_opr;\n+  static LIR_Opr r11_metadata_opr;\n+  static LIR_Opr r12_metadata_opr;\n+  static LIR_Opr r13_metadata_opr;\n+  static LIR_Opr r14_metadata_opr;\n+  static LIR_Opr r15_metadata_opr;\n+\n+  static LIR_Opr long10_opr;\n+  static LIR_Opr long11_opr;\n+  static LIR_Opr fpu10_float_opr;\n+  static LIR_Opr fpu10_double_opr;\n+\n+  static LIR_Opr as_long_opr(Register r) {\n+    return LIR_OprFact::double_cpu(cpu_reg2rnr(r), cpu_reg2rnr(r));\n+  }\n+  static LIR_Opr as_pointer_opr(Register r) {\n+    return LIR_OprFact::double_cpu(cpu_reg2rnr(r), cpu_reg2rnr(r));\n+  }\n+\n+  \/\/ VMReg name for spilled physical FPU stack slot n\n+  static VMReg fpu_regname(int n);\n+\n+  static bool is_caller_save_register(LIR_Opr opr) { return true; }\n+  static bool is_caller_save_register(Register r)  { return true; }\n+\n+  static int nof_caller_save_cpu_regs() { return pd_nof_caller_save_cpu_regs_frame_map; }\n+  static int last_cpu_reg()             { return pd_last_cpu_reg; }\n+\n+#endif \/\/ CPU_RISCV_C1_FRAMEMAP_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FrameMap_riscv.hpp","additions":148,"deletions":0,"binary":false,"changes":148,"status":"added"},{"patch":"@@ -0,0 +1,285 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+\n+#ifndef PRODUCT\n+#define COMMENT(x)   do { __ block_comment(x); } while (0)\n+#else\n+#define COMMENT(x)\n+#endif\n+\n+#define __ _masm->\n+\n+void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr illegal,\n+                                    LIR_Opr result, CodeEmitInfo* info) {\n+  \/\/ opcode check\n+  assert((code == lir_idiv) || (code == lir_irem), \"opcode must be idiv or irem\");\n+  bool is_irem = (code == lir_irem);\n+  \/\/ opreand check\n+  assert(left->is_single_cpu(), \"left must be a register\");\n+  assert(right->is_single_cpu() || right->is_constant(), \"right must be a register or constant\");\n+  assert(result->is_single_cpu(), \"result must be a register\");\n+  Register lreg = left->as_register();\n+  Register dreg = result->as_register();\n+\n+  \/\/ power-of-2 constant check and codegen\n+  if (right->is_constant()) {\n+    int c = right->as_constant_ptr()->as_jint();\n+    assert(c > 0 && is_power_of_2(c), \"divisor must be power-of-2 constant\");\n+    if (is_irem) {\n+      if (c == 1) {\n+        \/\/ move 0 to dreg if divisor is 1\n+        __ mv(dreg, zr);\n+      } else {\n+        unsigned int shift = exact_log2(c);\n+        __ sraiw(t0, lreg, 0x1f);\n+        __ srliw(t0, t0, BitsPerInt - shift);\n+        __ addw(t1, lreg, t0);\n+        if (is_imm_in_range(c - 1, 12, 0)) {\n+          __ andi(t1, t1, c - 1);\n+        } else {\n+          __ zero_extend(t1, t1, shift);\n+        }\n+        __ subw(dreg, t1, t0);\n+      }\n+    } else {\n+      if (c == 1) {\n+        \/\/ move lreg to dreg if divisor is 1\n+        __ mv(dreg, lreg);\n+      } else {\n+        unsigned int shift = exact_log2(c);\n+        __ sraiw(t0, lreg, 0x1f);\n+        if (is_imm_in_range(c - 1, 12, 0)) {\n+          __ andi(t0, t0, c - 1);\n+        } else {\n+          __ zero_extend(t0, t0, shift);\n+        }\n+        __ addw(dreg, t0, lreg);\n+        __ sraiw(dreg, dreg, shift);\n+      }\n+    }\n+  } else {\n+    Register rreg = right->as_register();\n+    __ corrected_idivl(dreg, lreg, rreg, is_irem);\n+  }\n+}\n+\n+void LIR_Assembler::arith_op_single_cpu_right_constant(LIR_Code code, LIR_Opr left, LIR_Opr right,\n+                                                       Register lreg, Register dreg) {\n+  \/\/ cpu register - constant\n+  jlong c;\n+\n+  switch (right->type()) {\n+    case T_LONG:\n+      c = right->as_constant_ptr()->as_jlong(); break;\n+    case T_INT:     \/\/ fall through\n+    case T_ADDRESS:\n+      c = right->as_constant_ptr()->as_jint(); break;\n+    default:\n+      ShouldNotReachHere();\n+      c = 0;   \/\/ unreachable\n+  }\n+\n+  assert(code == lir_add || code == lir_sub, \"mismatched arithmetic op\");\n+  if (c == 0 && dreg == lreg) {\n+    COMMENT(\"effective nop elided\");\n+    return;\n+  }\n+  switch (left->type()) {\n+    case T_INT:\n+      switch (code) {\n+        case lir_add: __ addw(dreg, lreg, c); break;\n+        case lir_sub: __ subw(dreg, lreg, c); break;\n+        default:      ShouldNotReachHere();\n+      }\n+    break;\n+    case T_OBJECT:  \/\/ fall through\n+    case T_ADDRESS:\n+      switch (code) {\n+        case lir_add: __ add(dreg, lreg, c); break;\n+        case lir_sub: __ sub(dreg, lreg, c); break;\n+        default:      ShouldNotReachHere();\n+      }\n+    break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::arith_op_single_cpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {\n+  Register lreg = left->as_register();\n+  Register dreg = as_reg(dest);\n+\n+  if (right->is_single_cpu()) {\n+    \/\/ cpu register - cpu register\n+    assert(left->type() == T_INT && right->type() == T_INT && dest->type() == T_INT, \"should be\");\n+    Register rreg = right->as_register();\n+    switch (code) {\n+      case lir_add: __ addw(dest->as_register(), lreg, rreg); break;\n+      case lir_sub: __ subw(dest->as_register(), lreg, rreg); break;\n+      case lir_mul: __ mulw(dest->as_register(), lreg, rreg); break;\n+      default:      ShouldNotReachHere();\n+    }\n+  } else if (right->is_double_cpu()) {\n+    Register rreg = right->as_register_lo();\n+    \/\/ sigle_cpu + double_cpu; can happen with obj_long\n+    assert(code == lir_add || code == lir_sub, \"mismatched arithmetic op\");\n+    switch (code) {\n+      case lir_add: __ add(dreg, lreg, rreg); break;\n+      case lir_sub: __ sub(dreg, lreg, rreg); break;\n+      default:      ShouldNotReachHere();\n+    }\n+  } else if (right->is_constant()) {\n+    arith_op_single_cpu_right_constant(code, left, right, lreg, dreg);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::arith_op_double_cpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {\n+  Register lreg_lo = left->as_register_lo();\n+\n+  if (right->is_double_cpu()) {\n+    \/\/ cpu register - cpu register\n+    Register rreg_lo = right->as_register_lo();\n+    switch (code) {\n+      case lir_add: __ add(dest->as_register_lo(), lreg_lo, rreg_lo); break;\n+      case lir_sub: __ sub(dest->as_register_lo(), lreg_lo, rreg_lo); break;\n+      case lir_mul: __ mul(dest->as_register_lo(), lreg_lo, rreg_lo); break;\n+      case lir_div: __ corrected_idivq(dest->as_register_lo(), lreg_lo, rreg_lo, false); break;\n+      case lir_rem: __ corrected_idivq(dest->as_register_lo(), lreg_lo, rreg_lo, true); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else if (right->is_constant()) {\n+    jlong c = right->as_constant_ptr()->as_jlong();\n+    Register dreg = as_reg(dest);\n+    switch (code) {\n+      case lir_add: \/\/ fall through\n+      case lir_sub:\n+        if (c == 0 && dreg == lreg_lo) {\n+          COMMENT(\"effective nop elided\");\n+          return;\n+        }\n+        code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);\n+        break;\n+      case lir_div:\n+        assert(c > 0 && is_power_of_2_long(c), \"divisor must be power-of-2 constant\");\n+        if (c == 1) {\n+          \/\/ move lreg_lo to dreg if divisor is 1\n+          __ mv(dreg, lreg_lo);\n+        } else {\n+          unsigned int shift = exact_log2_long(c);\n+          \/\/ use t0 as intermediate result register\n+          __ srai(t0, lreg_lo, 0x3f);\n+          if (is_imm_in_range(c - 1, 12, 0)) {\n+            __ andi(t0, t0, c - 1);\n+          } else {\n+            __ zero_extend(t0, t0, shift);\n+          }\n+          __ add(dreg, t0, lreg_lo);\n+          __ srai(dreg, dreg, shift);\n+        }\n+        break;\n+      case lir_rem:\n+        assert(c > 0 && is_power_of_2_long(c), \"divisor must be power-of-2 constant\");\n+        if (c == 1) {\n+          \/\/ move 0 to dreg if divisor is 1\n+          __ mv(dreg, zr);\n+        } else {\n+          unsigned int shift = exact_log2_long(c);\n+          __ srai(t0, lreg_lo, 0x3f);\n+          __ srli(t0, t0, BitsPerLong - shift);\n+          __ add(t1, lreg_lo, t0);\n+          if (is_imm_in_range(c - 1, 12, 0)) {\n+            __ andi(t1, t1, c - 1);\n+          } else {\n+            __ zero_extend(t1, t1, shift);\n+          }\n+          __ sub(dreg, t1, t0);\n+        }\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::arith_op_single_fpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {\n+  assert(right->is_single_fpu(), \"right hand side of float arithmetics needs to be float register\");\n+  switch (code) {\n+    case lir_add: __ fadd_s(dest->as_float_reg(), left->as_float_reg(), right->as_float_reg()); break;\n+    case lir_sub: __ fsub_s(dest->as_float_reg(), left->as_float_reg(), right->as_float_reg()); break;\n+    case lir_mul_strictfp: \/\/ fall through\n+    case lir_mul: __ fmul_s(dest->as_float_reg(), left->as_float_reg(), right->as_float_reg()); break;\n+    case lir_div_strictfp: \/\/ fall through\n+    case lir_div: __ fdiv_s(dest->as_float_reg(), left->as_float_reg(), right->as_float_reg()); break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::arith_op_double_fpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {\n+  if (right->is_double_fpu()) {\n+    \/\/ fpu register - fpu register\n+    switch (code) {\n+      case lir_add: __ fadd_d(dest->as_double_reg(), left->as_double_reg(), right->as_double_reg()); break;\n+      case lir_sub: __ fsub_d(dest->as_double_reg(), left->as_double_reg(), right->as_double_reg()); break;\n+      case lir_mul_strictfp: \/\/ fall through\n+      case lir_mul: __ fmul_d(dest->as_double_reg(), left->as_double_reg(), right->as_double_reg()); break;\n+      case lir_div_strictfp: \/\/ fall through\n+      case lir_div: __ fdiv_d(dest->as_double_reg(), left->as_double_reg(), right->as_double_reg()); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest,\n+                             CodeEmitInfo* info, bool pop_fpu_stack) {\n+  assert(info == NULL, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n+\n+  if (left->is_single_cpu()) {\n+    arith_op_single_cpu(code, left, right, dest);\n+  } else if (left->is_double_cpu()) {\n+    arith_op_double_cpu(code, left, right, dest);\n+  } else if (left->is_single_fpu()) {\n+    arith_op_single_fpu(code, left, right, dest);\n+  } else if (left->is_double_fpu()) {\n+    arith_op_double_fpu(code, left, right, dest);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_arith_riscv.cpp","additions":285,"deletions":0,"binary":false,"changes":285,"status":"added"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_LIRASSEMBLER_ARITH_RISCV_HPP\n+#define CPU_RISCV_C1_LIRASSEMBLER_ARITH_RISCV_HPP\n+\n+  \/\/ arith_op sub functions\n+  void arith_op_single_cpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest);\n+  void arith_op_double_cpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest);\n+  void arith_op_single_fpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest);\n+  void arith_op_double_fpu(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest);\n+  void arith_op_single_cpu_right_constant(LIR_Code code, LIR_Opr left, LIR_Opr right, Register lreg, Register dreg);\n+  void arithmetic_idiv(LIR_Op3* op, bool is_irem);\n+\n+#endif \/\/ CPU_RISCV_C1_LIRASSEMBLER_ARITH_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_arith_riscv.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -0,0 +1,388 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ _masm->\n+\n+\n+void LIR_Assembler::generic_arraycopy(Register src, Register src_pos, Register length,\n+                                      Register dst, Register dst_pos, CodeStub *stub) {\n+  assert(src == x11 && src_pos == x12, \"mismatch in calling convention\");\n+  \/\/ Save the arguments in case the generic arraycopy fails and we\n+  \/\/ have to fall back to the JNI stub\n+  arraycopy_store_args(src, src_pos, length, dst, dst_pos);\n+\n+  address copyfunc_addr = StubRoutines::generic_arraycopy();\n+  assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+\n+  \/\/ The arguments are in java calling convention so we shift them\n+  \/\/ to C convention\n+  assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);\n+  __ mv(c_rarg0, j_rarg0);\n+  assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);\n+  __ mv(c_rarg1, j_rarg1);\n+  assert_different_registers(c_rarg2, j_rarg3, j_rarg4);\n+  __ mv(c_rarg2, j_rarg2);\n+  assert_different_registers(c_rarg3, j_rarg4);\n+  __ mv(c_rarg3, j_rarg3);\n+  __ mv(c_rarg4, j_rarg4);\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    __ add_memory_int32(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt), 1);\n+  }\n+#endif\n+  __ far_call(RuntimeAddress(copyfunc_addr));\n+  __ beqz(x10, *stub->continuation());\n+  \/\/ Reload values from the stack so they are where the stub\n+  \/\/ expects them.\n+  arraycopy_load_args(src, src_pos, length, dst, dst_pos);\n+\n+  \/\/ x10 is -1^K where K == partial copied count\n+  __ xori(t0, x10, -1);\n+  \/\/ adjust length down and src\/end pos up by partial copied count\n+  __ subw(length, length, t0);\n+  __ addw(src_pos, src_pos, t0);\n+  __ addw(dst_pos, dst_pos, t0);\n+  __ j(*stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void LIR_Assembler::arraycopy_simple_check(Register src, Register src_pos, Register length,\n+                                           Register dst, Register dst_pos, Register tmp,\n+                                           CodeStub *stub, int flags) {\n+  \/\/ test for NULL\n+  if (flags & LIR_OpArrayCopy::src_null_check) {\n+    __ beqz(src, *stub->entry(), \/* is_far *\/ true);\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_null_check) {\n+    __ beqz(dst, *stub->entry(), \/* is_far *\/ true);\n+  }\n+\n+  \/\/ If the compiler was not able to prove that exact type of the source or the destination\n+  \/\/ of the arraycopy is an array type, check at runtime if the source or the destination is\n+  \/\/ an instance type.\n+  if (flags & LIR_OpArrayCopy::type_check) {\n+    if (!(flags & LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {\n+      __ load_klass(tmp, dst);\n+      __ lw(t0, Address(tmp, in_bytes(Klass::layout_helper_offset())));\n+      __ li(t1, Klass::_lh_neutral_value);\n+      __ bge(t0, t1, *stub->entry(), \/* is_far *\/ true);\n+    }\n+\n+    if (!(flags & LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {\n+      __ load_klass(tmp, src);\n+      __ lw(t0, Address(tmp, in_bytes(Klass::layout_helper_offset())));\n+      __ li(t1, Klass::_lh_neutral_value);\n+      __ bge(t0, t1, *stub->entry(), \/* is_far *\/ true);\n+    }\n+  }\n+\n+  \/\/ check if negative\n+  if (flags & LIR_OpArrayCopy::src_pos_positive_check) {\n+    __ bltz(src_pos, *stub->entry(), \/* is_far *\/ true);\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_pos_positive_check) {\n+    __ bltz(dst_pos, *stub->entry(), \/* is_far *\/ true);\n+  }\n+  if (flags & LIR_OpArrayCopy::length_positive_check) {\n+    __ bltz(length, *stub->entry(), \/* is_far *\/ true);\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::src_range_check) {\n+    __ addw(tmp, src_pos, length);\n+    __ lwu(t0, Address(src, arrayOopDesc::length_offset_in_bytes()));\n+    __ bgtu(tmp, t0, *stub->entry(), \/* is_far *\/ true);\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_range_check) {\n+    __ addw(tmp, dst_pos, length);\n+    __ lwu(t0, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n+    __ bgtu(tmp, t0, *stub->entry(), \/* is_far *\/ true);\n+  }\n+}\n+\n+void LIR_Assembler::arraycopy_checkcast(Register src, Register src_pos, Register length,\n+                                        Register dst, Register dst_pos, Register tmp,\n+                                        CodeStub *stub, BasicType basic_type,\n+                                        address copyfunc_addr, int flags) {\n+  \/\/ src is not a sub class of dst so we have to do a\n+  \/\/ per-element check.\n+  int mask = LIR_OpArrayCopy::src_objarray | LIR_OpArrayCopy::dst_objarray;\n+  if ((flags & mask) != mask) {\n+    \/\/ Check that at least both of them object arrays.\n+    assert(flags & mask, \"one of the two should be known to be an object array\");\n+\n+    if (!(flags & LIR_OpArrayCopy::src_objarray)) {\n+      __ load_klass(tmp, src);\n+    } else if (!(flags & LIR_OpArrayCopy::dst_objarray)) {\n+      __ load_klass(tmp, dst);\n+    }\n+    int lh_offset = in_bytes(Klass::layout_helper_offset());\n+    Address klass_lh_addr(tmp, lh_offset);\n+    jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n+    __ lw(t0, klass_lh_addr);\n+    __ mvw(t1, objArray_lh);\n+    __ bne(t0, t1, *stub->entry(), \/* is_far *\/ true);\n+  }\n+\n+  \/\/ Spill because stubs can use any register they like and it's\n+  \/\/ easier to restore just those that we care about.\n+  arraycopy_store_args(src, src_pos, length, dst, dst_pos);\n+  arraycopy_checkcast_prepare_params(src, src_pos, length, dst, dst_pos, basic_type);\n+  __ far_call(RuntimeAddress(copyfunc_addr));\n+\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    Label failed;\n+    __ bnez(x10, failed);\n+    __ add_memory_int32(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_cnt), 1);\n+    __ bind(failed);\n+  }\n+#endif\n+\n+  __ beqz(x10, *stub->continuation());\n+\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    __ add_memory_int32(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_attempt_cnt), 1);\n+  }\n+#endif\n+  assert_different_registers(dst, dst_pos, length, src_pos, src, x10, t0);\n+\n+  \/\/ Restore previously spilled arguments\n+  arraycopy_load_args(src, src_pos, length, dst, dst_pos);\n+\n+  \/\/ return value is -1^K where K is partial copied count\n+  __ xori(t0, x10, -1);\n+  \/\/ adjust length down and src\/end pos up by partial copied count\n+  __ subw(length, length, t0);\n+  __ addw(src_pos, src_pos, t0);\n+  __ addw(dst_pos, dst_pos, t0);\n+}\n+\n+void LIR_Assembler::arraycopy_type_check(Register src, Register src_pos, Register length,\n+                                         Register dst, Register dst_pos, Register tmp,\n+                                         CodeStub *stub, BasicType basic_type, int flags) {\n+  \/\/ We don't know the array types are compatible\n+  if (basic_type != T_OBJECT) {\n+    \/\/ Simple test for basic type arrays\n+    if (UseCompressedClassPointers) {\n+      __ lwu(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+      __ lwu(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      __ ld(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+      __ ld(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    }\n+    __ bne(tmp, t0, *stub->entry(), \/* is_far *\/ true);\n+  } else {\n+    \/\/ For object arrays, if src is a sub class of dst then we can\n+    \/\/ safely do the copy.\n+    Label cont, slow;\n+\n+#define PUSH(r1, r2)                                     \\\n+    __ addi(sp, sp, -2 * wordSize);                      \\\n+    __ sd(r1, Address(sp, 1 * wordSize));                \\\n+    __ sd(r2, Address(sp, 0));\n+\n+#define POP(r1, r2)                                      \\\n+    __ ld(r1, Address(sp, 1 * wordSize));                \\\n+    __ ld(r2, Address(sp, 0));                           \\\n+    __ addi(sp, sp, 2 * wordSize);\n+\n+    PUSH(src, dst);\n+    __ load_klass(src, src);\n+    __ load_klass(dst, dst);\n+    __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, NULL);\n+\n+    PUSH(src, dst);\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));\n+    POP(src, dst);\n+    __ bnez(dst, cont);\n+\n+    __ bind(slow);\n+    POP(src, dst);\n+\n+    address copyfunc_addr = StubRoutines::checkcast_arraycopy();\n+    if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      arraycopy_checkcast(src, src_pos, length, dst, dst_pos, tmp, stub, basic_type, copyfunc_addr, flags);\n+    }\n+\n+    __ j(*stub->entry());\n+    __ bind(cont);\n+    POP(src, dst);\n+  }\n+}\n+\n+void LIR_Assembler::arraycopy_assert(Register src, Register dst, Register tmp, ciArrayKlass *default_type, int flags) {\n+  assert(default_type != NULL, \"NULL default_type!\");\n+  BasicType basic_type = default_type->element_type()->basic_type();\n+\n+  if (basic_type == T_ARRAY) { basic_type = T_OBJECT; }\n+  if (basic_type != T_OBJECT || !(flags & LIR_OpArrayCopy::type_check)) {\n+    \/\/ Sanity check the known type with the incoming class.  For the\n+    \/\/ primitive case the types must match exactly with src.klass and\n+    \/\/ dst.klass each exactly matching the default type.  For the\n+    \/\/ object array case, if no type check is needed then either the\n+    \/\/ dst type is exactly the expected type and the src type is a\n+    \/\/ subtype which we can't check or src is the same array as dst\n+    \/\/ but not necessarily exactly of type default_type.\n+    Label known_ok, halt;\n+    __ mov_metadata(tmp, default_type->constant_encoding());\n+    if (UseCompressedClassPointers) {\n+      __ encode_klass_not_null(tmp);\n+    }\n+\n+    if (basic_type != T_OBJECT) {\n+      if (UseCompressedClassPointers) {\n+        __ lwu(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+      } else {\n+        __ ld(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+      }\n+      __ bne(tmp, t0, halt);\n+      if (UseCompressedClassPointers) {\n+        __ lwu(t0, Address(src, oopDesc::klass_offset_in_bytes()));\n+      } else {\n+        __ ld(t0, Address(src, oopDesc::klass_offset_in_bytes()));\n+      }\n+      __ beq(tmp, t0, known_ok);\n+    } else {\n+      if (UseCompressedClassPointers) {\n+        __ lwu(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+      } else {\n+        __ ld(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n+      }\n+      __ beq(tmp, t0, known_ok);\n+      __ beq(src, dst, known_ok);\n+    }\n+    __ bind(halt);\n+    __ stop(\"incorrect type information in arraycopy\");\n+    __ bind(known_ok);\n+  }\n+}\n+\n+void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {\n+  ciArrayKlass *default_type = op->expected_type();\n+  Register src = op->src()->as_register();\n+  Register dst = op->dst()->as_register();\n+  Register src_pos = op->src_pos()->as_register();\n+  Register dst_pos = op->dst_pos()->as_register();\n+  Register length = op->length()->as_register();\n+  Register tmp = op->tmp()->as_register();\n+\n+  CodeStub* stub = op->stub();\n+  int flags = op->flags();\n+  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  if (is_reference_type(basic_type)) { basic_type = T_OBJECT; }\n+\n+  \/\/ if we don't know anything, just go through the generic arraycopy\n+  if (default_type == NULL) {\n+    generic_arraycopy(src, src_pos, length, dst, dst_pos, stub);\n+    return;\n+  }\n+\n+  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(),\n+         \"must be true at this point\");\n+\n+  arraycopy_simple_check(src, src_pos, length, dst, dst_pos, tmp, stub, flags);\n+\n+  if (flags & LIR_OpArrayCopy::type_check) {\n+    arraycopy_type_check(src, src_pos, length, dst, dst_pos, tmp, stub, basic_type, flags);\n+  }\n+\n+#ifdef ASSERT\n+  arraycopy_assert(src, dst, tmp, default_type, flags);\n+#endif\n+\n+#ifndef PRODUCT\n+  if (PrintC1Statistics) {\n+    __ add_memory_int32(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)), 1);\n+  }\n+#endif\n+  arraycopy_prepare_params(src, src_pos, length, dst, dst_pos, basic_type);\n+\n+  bool disjoint = (flags & LIR_OpArrayCopy::overlapping) == 0;\n+  bool aligned = (flags & LIR_OpArrayCopy::unaligned) == 0;\n+  const char *name = NULL;\n+  address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);\n+\n+  CodeBlob *cb = CodeCache::find_blob(entry);\n+  if (cb != NULL) {\n+    __ far_call(RuntimeAddress(entry));\n+  } else {\n+    const int args_num = 3;\n+    __ call_VM_leaf(entry, args_num);\n+  }\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+\n+void LIR_Assembler::arraycopy_prepare_params(Register src, Register src_pos, Register length,\n+                                             Register dst, Register dst_pos, BasicType basic_type) {\n+  int scale = array_element_size(basic_type);\n+  __ shadd(c_rarg0, src_pos, src, t0, scale);\n+  __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));\n+  assert_different_registers(c_rarg0, dst, dst_pos, length);\n+  __ shadd(c_rarg1, dst_pos, dst, t0, scale);\n+  __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));\n+  assert_different_registers(c_rarg1, dst, length);\n+  __ mv(c_rarg2, length);\n+  assert_different_registers(c_rarg2, dst);\n+}\n+\n+void LIR_Assembler::arraycopy_checkcast_prepare_params(Register src, Register src_pos, Register length,\n+                                                       Register dst, Register dst_pos, BasicType basic_type) {\n+  arraycopy_prepare_params(src, src_pos, length, dst, dst_pos, basic_type);\n+  __ load_klass(c_rarg4, dst);\n+  __ ld(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));\n+  __ lwu(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));\n+}\n+\n+void LIR_Assembler::arraycopy_store_args(Register src, Register src_pos, Register length,\n+                                         Register dst, Register dst_pos) {\n+  __ sd(dst_pos, Address(sp, 0));                \/\/ 0: dst_pos sp offset\n+  __ sd(dst, Address(sp, 1 * BytesPerWord));     \/\/ 1: dst sp offset\n+  __ sd(length, Address(sp, 2 * BytesPerWord));  \/\/ 2: length sp offset\n+  __ sd(src_pos, Address(sp, 3 * BytesPerWord)); \/\/ 3: src_pos sp offset\n+  __ sd(src, Address(sp, 4 * BytesPerWord));     \/\/ 4: src sp offset\n+}\n+\n+void LIR_Assembler::arraycopy_load_args(Register src, Register src_pos, Register length,\n+                                        Register dst, Register dst_pos) {\n+  __ ld(dst_pos, Address(sp, 0));                \/\/ 0: dst_pos sp offset\n+  __ ld(dst, Address(sp, 1 * BytesPerWord));     \/\/ 1: dst sp offset\n+  __ ld(length, Address(sp, 2 * BytesPerWord));  \/\/ 2: length sp offset\n+  __ ld(src_pos, Address(sp, 3 * BytesPerWord)); \/\/ 3: src_pos sp offset\n+  __ ld(src, Address(sp, 4 * BytesPerWord));     \/\/ 4: src sp offset\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_arraycopy_riscv.cpp","additions":388,"deletions":0,"binary":false,"changes":388,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_LIRASSEMBLER_ARRAYCOPY_RISCV_HPP\n+#define CPU_RISCV_C1_LIRASSEMBLER_ARRAYCOPY_RISCV_HPP\n+\n+  \/\/ arraycopy sub functions\n+  void generic_arraycopy(Register src, Register src_pos, Register length,\n+                         Register dst, Register dst_pos, CodeStub *stub);\n+  void arraycopy_simple_check(Register src, Register src_pos, Register length,\n+                              Register dst, Register dst_pos, Register tmp,\n+                              CodeStub *stub, int flags);\n+  void arraycopy_checkcast(Register src, Register src_pos, Register length,\n+                           Register dst, Register dst_pos, Register tmp,\n+                           CodeStub *stub, BasicType basic_type,\n+                           address copyfunc_addr, int flags);\n+  void arraycopy_type_check(Register src, Register src_pos, Register length,\n+                            Register dst, Register dst_pos, Register tmp,\n+                            CodeStub *stub, BasicType basic_type, int flags);\n+  void arraycopy_assert(Register src, Register dst, Register tmp, ciArrayKlass *default_type, int flags);\n+  void arraycopy_prepare_params(Register src, Register src_pos, Register length,\n+                                Register dst, Register dst_pos, BasicType basic_type);\n+  void arraycopy_checkcast_prepare_params(Register src, Register src_pos, Register length,\n+                                          Register dst, Register dst_pos, BasicType basic_type);\n+  void arraycopy_store_args(Register src, Register src_pos, Register length,\n+                            Register dst, Register dst_pos);\n+  void arraycopy_load_args(Register src, Register src_pos, Register length,\n+                           Register dst, Register dst_pos);\n+\n+#endif \/\/ CPU_RISCV_C1_LIRASSEMBLER_ARRAYCOPY_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_arraycopy_riscv.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,2268 @@\n+\/*\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"c1\/c1_CodeStubs.hpp\"\n+#include \"c1\/c1_Compilation.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"c1\/c1_ValueStack.hpp\"\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciInstance.hpp\"\n+#include \"code\/compiledIC.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+#ifndef PRODUCT\n+#define COMMENT(x)   do { __ block_comment(x); } while (0)\n+#else\n+#define COMMENT(x)\n+#endif\n+\n+NEEDS_CLEANUP \/\/ remove this definitions ?\n+const Register IC_Klass    = t1;    \/\/ where the IC klass is cached\n+const Register SYNC_header = x10;   \/\/ synchronization header\n+const Register SHIFT_count = x10;   \/\/ where count for shift operations must be\n+\n+#define __ _masm->\n+\n+static void select_different_registers(Register preserve,\n+                                       Register extra,\n+                                       Register &tmp1,\n+                                       Register &tmp2) {\n+  if (tmp1 == preserve) {\n+    assert_different_registers(tmp1, tmp2, extra);\n+    tmp1 = extra;\n+  } else if (tmp2 == preserve) {\n+    assert_different_registers(tmp1, tmp2, extra);\n+    tmp2 = extra;\n+  }\n+  assert_different_registers(preserve, tmp1, tmp2);\n+}\n+\n+static void select_different_registers(Register preserve,\n+                                       Register extra,\n+                                       Register &tmp1,\n+                                       Register &tmp2,\n+                                       Register &tmp3) {\n+  if (tmp1 == preserve) {\n+    assert_different_registers(tmp1, tmp2, tmp3, extra);\n+    tmp1 = extra;\n+  } else if (tmp2 == preserve) {\n+    assert_different_registers(tmp1, tmp2, tmp3, extra);\n+    tmp2 = extra;\n+  } else if (tmp3 == preserve) {\n+    assert_different_registers(tmp1, tmp2, tmp3, extra);\n+    tmp3 = extra;\n+  }\n+  assert_different_registers(preserve, tmp1, tmp2, tmp3);\n+}\n+\n+bool LIR_Assembler::is_small_constant(LIR_Opr opr) { Unimplemented(); return false; }\n+\n+LIR_Opr LIR_Assembler::receiverOpr() {\n+  return FrameMap::receiver_opr;\n+}\n+\n+LIR_Opr LIR_Assembler::osrBufferPointer() {\n+  return FrameMap::as_pointer_opr(receiverOpr()->as_register());\n+}\n+\n+void LIR_Assembler::breakpoint() { Unimplemented(); }\n+\n+void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }\n+\n+void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }\n+\n+static jlong as_long(LIR_Opr data) {\n+  jlong result;\n+  switch (data->type()) {\n+    case T_INT:\n+      result = (data->as_jint());\n+      break;\n+    case T_LONG:\n+      result = (data->as_jlong());\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      result = 0;  \/\/ unreachable\n+  }\n+  return result;\n+}\n+\n+Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {\n+  if (addr->base()->is_illegal()) {\n+    assert(addr->index()->is_illegal(), \"must be illegal too\");\n+    __ movptr(tmp, addr->disp());\n+    return Address(tmp, 0);\n+  }\n+\n+  Register base = addr->base()->as_pointer_register();\n+  LIR_Opr index_opr = addr->index();\n+\n+  if (index_opr->is_illegal()) {\n+    return Address(base, addr->disp());\n+  }\n+\n+  int scale = addr->scale();\n+  if (index_opr->is_cpu_register()) {\n+    Register index;\n+    if (index_opr->is_single_cpu()) {\n+      index = index_opr->as_register();\n+    } else {\n+      index = index_opr->as_register_lo();\n+    }\n+    if (scale != 0) {\n+      __ shadd(tmp, index, base, tmp, scale);\n+    } else {\n+      __ add(tmp, base, index);\n+    }\n+    return Address(tmp, addr->disp());\n+  } else if (index_opr->is_constant()) {\n+    intptr_t addr_offset = (((intptr_t)index_opr->as_constant_ptr()->as_jint()) << scale) + addr->disp();\n+    return Address(base, addr_offset);\n+  }\n+\n+  Unimplemented();\n+  return Address();\n+}\n+\n+Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {\n+  ShouldNotReachHere();\n+  return Address();\n+}\n+\n+Address LIR_Assembler::as_Address(LIR_Address* addr) {\n+  return as_Address(addr, t0);\n+}\n+\n+Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {\n+  return as_Address(addr);\n+}\n+\n+\/\/ Ensure a valid Address (base + offset) to a stack-slot. If stack access is\n+\/\/ not encodable as a base + (immediate) offset, generate an explicit address\n+\/\/ calculation to hold the address in t0.\n+Address LIR_Assembler::stack_slot_address(int index, uint size, int adjust) {\n+  precond(size == 4 || size == 8);\n+  Address addr = frame_map()->address_for_slot(index, adjust);\n+  precond(addr.getMode() == Address::base_plus_offset);\n+  precond(addr.base() == sp);\n+  precond(addr.offset() > 0);\n+  uint mask = size - 1;\n+  assert((addr.offset() & mask) == 0, \"scaled offsets only\");\n+\n+  return addr;\n+}\n+\n+void LIR_Assembler::osr_entry() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, code_offset());\n+  BlockBegin* osr_entry = compilation()->hir()->osr_entry();\n+  guarantee(osr_entry != NULL, \"NULL osr_entry!\");\n+  ValueStack* entry_state = osr_entry->state();\n+  int number_of_locks = entry_state->locks_size();\n+\n+  \/\/ we jump here if osr happens with the interpreter\n+  \/\/ state set up to continue at the beginning of the\n+  \/\/ loop that triggered osr - in particular, we have\n+  \/\/ the following registers setup:\n+  \/\/\n+  \/\/ x12: osr buffer\n+  \/\/\n+\n+  \/\/build frame\n+  ciMethod* m = compilation()->method();\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+\n+  \/\/ OSR buffer is\n+  \/\/\n+  \/\/ locals[nlocals-1..0]\n+  \/\/ monitors[0..number_of_locks]\n+  \/\/\n+  \/\/ locals is a direct copy of the interpreter frame so in the osr buffer\n+  \/\/ so first slot in the local array is the last local from the interpreter\n+  \/\/ and last slot is local[0] (receiver) from the interpreter\n+  \/\/\n+  \/\/ Similarly with locks. The first lock slot in the osr buffer is the nth lock\n+  \/\/ from the interpreter frame, the nth lock slot in the osr buffer is 0th lock\n+  \/\/ in the interpreter frame (the method lock if a sync method)\n+\n+  \/\/ Initialize monitors in the compiled activation.\n+  \/\/   x12: pointer to osr buffer\n+  \/\/ All other registers are dead at this point and the locals will be\n+  \/\/ copied into place by code emitted in the IR.\n+\n+  Register OSR_buf = osrBufferPointer()->as_pointer_register();\n+  {\n+    assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), \"adjust code below\");\n+    int monitor_offset = BytesPerWord * method()->max_locals() +\n+      (2 * BytesPerWord) * (number_of_locks - 1);\n+    \/\/ SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in\n+    \/\/ the OSR buffer using 2 word entries: first the lock and then\n+    \/\/ the oop.\n+    for (int i = 0; i < number_of_locks; i++) {\n+      int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);\n+#ifdef ASSERT\n+      \/\/ verify the interpreter's monitor has a non-null object\n+      {\n+        Label L;\n+        __ ld(t0, Address(OSR_buf, slot_offset + 1 * BytesPerWord));\n+        __ bnez(t0, L);\n+        __ stop(\"locked object is NULL\");\n+        __ bind(L);\n+      }\n+#endif \/\/ ASSERT\n+      __ ld(x9, Address(OSR_buf, slot_offset + 0));\n+      __ sd(x9, frame_map()->address_for_monitor_lock(i));\n+      __ ld(x9, Address(OSR_buf, slot_offset + 1 * BytesPerWord));\n+      __ sd(x9, frame_map()->address_for_monitor_object(i));\n+    }\n+  }\n+}\n+\n+\/\/ inline cache check; done before the frame is built.\n+int LIR_Assembler::check_icache() {\n+  Register receiver = FrameMap::receiver_opr->as_register();\n+  Register ic_klass = IC_Klass;\n+  int start_offset = __ offset();\n+  Label dont;\n+  __ inline_cache_check(receiver, ic_klass, dont);\n+\n+  \/\/ if icache check fails, then jump to runtime routine\n+  \/\/ Note: RECEIVER must still contain the receiver!\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  \/\/ We align the verified entry point unless the method body\n+  \/\/ (including its inline cache check) will fit in a single 64-byte\n+  \/\/ icache line.\n+  if (!method()->is_accessor() || __ offset() - start_offset > 4 * 4) {\n+    \/\/ force alignment after the cache check.\n+    __ align(CodeEntryAlignment);\n+  }\n+\n+  __ bind(dont);\n+  return start_offset;\n+}\n+\n+void LIR_Assembler::jobject2reg(jobject o, Register reg) {\n+  if (o == NULL) {\n+    __ mv(reg, zr);\n+  } else {\n+    __ movoop(reg, o, \/* immediate *\/ true);\n+  }\n+}\n+\n+void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {\n+  deoptimize_trap(info);\n+}\n+\n+\/\/ This specifies the rsp decrement needed to build the frame\n+int LIR_Assembler::initial_frame_size_in_bytes() const {\n+  \/\/ if rounding, must let FrameMap know!\n+\n+  return in_bytes(frame_map()->framesize_in_bytes());\n+}\n+\n+int LIR_Assembler::emit_exception_handler() {\n+  \/\/ if the last instruction is a call (typically to do a throw which\n+  \/\/ is coming at the end after block reordering) the return address\n+  \/\/ must still point into the code area in order to avoid assertion\n+  \/\/ failures when searching for the corresponding bci ==> add a nop\n+  \/\/ (was bug 5\/14\/1999 -gri)\n+  __ nop();\n+\n+  \/\/ generate code for exception handler\n+  address handler_base = __ start_a_stub(exception_handler_size());\n+  if (handler_base == NULL) {\n+    \/\/ not enough space left for the handler\n+    bailout(\"exception handler overflow\");\n+    return -1;\n+  }\n+\n+  int offset = code_offset();\n+\n+  \/\/ the exception oop and pc are in x10, and x13\n+  \/\/ no other registers need to be preserved, so invalidate them\n+  __ invalidate_registers(false, true, true, false, true, true);\n+\n+  \/\/ check that there is really an exception\n+  __ verify_not_null_oop(x10);\n+\n+  \/\/ search an exception handler (x10: exception oop, x13: throwing pc)\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));\n+  __ should_not_reach_here();\n+  guarantee(code_offset() - offset <= exception_handler_size(), \"overflow\");\n+  __ end_a_stub();\n+\n+  return offset;\n+}\n+\n+\/\/ Emit the code to remove the frame from the stack in the exception\n+\/\/ unwind path.\n+int LIR_Assembler::emit_unwind_handler() {\n+#ifndef PRODUCT\n+  if (CommentedAssembly) {\n+    _masm->block_comment(\"Unwind handler\");\n+  }\n+#endif \/\/ PRODUCT\n+\n+  int offset = code_offset();\n+\n+  \/\/ Fetch the exception from TLS and clear out exception related thread state\n+  __ ld(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ sd(zr, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ sd(zr, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+  __ bind(_unwind_handler_entry);\n+  __ verify_not_null_oop(x10);\n+  if (method()->is_synchronized() || compilation()->env()->dtrace_method_probes()) {\n+    __ mv(x9, x10);   \/\/ Perserve the exception\n+  }\n+\n+  \/\/ Preform needed unlocking\n+  MonitorExitStub* stub = NULL;\n+  if (method()->is_synchronized()) {\n+    monitor_address(0, FrameMap::r10_opr);\n+    stub = new MonitorExitStub(FrameMap::r10_opr, true, 0);\n+    __ unlock_object(x15, x14, x10, *stub->entry());\n+    __ bind(*stub->continuation());\n+  }\n+\n+  if (compilation()->env()->dtrace_method_probes()) {\n+    __ mv(c_rarg0, xthread);\n+    __ mov_metadata(c_rarg1, method()->constant_encoding());\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), c_rarg0, c_rarg1);\n+  }\n+\n+  if (method()->is_synchronized() || compilation()->env()->dtrace_method_probes()) {\n+    __ mv(x10, x9);   \/\/ Restore the exception\n+  }\n+\n+  \/\/ remove the activation and dispatch to the unwind handler\n+  __ block_comment(\"remove_frame and dispatch to the unwind handler\");\n+  __ remove_frame(initial_frame_size_in_bytes());\n+  __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));\n+\n+  \/\/ Emit the slow path assembly\n+  if (stub != NULL) {\n+    stub->emit_code(this);\n+  }\n+\n+  return offset;\n+}\n+\n+int LIR_Assembler::emit_deopt_handler() {\n+  \/\/ if the last instruciton is a call (typically to do a throw which\n+  \/\/ is coming at the end after block reordering) the return address\n+  \/\/ must still point into the code area in order to avoid assertion\n+  \/\/ failures when searching for the corresponding bck => add a nop\n+  \/\/ (was bug 5\/14\/1999 - gri)\n+  __ nop();\n+\n+  \/\/ generate code for exception handler\n+  address handler_base = __ start_a_stub(deopt_handler_size());\n+  if (handler_base == NULL) {\n+    \/\/ not enough space left for the handler\n+    bailout(\"deopt handler overflow\");\n+    return -1;\n+  }\n+\n+  int offset = code_offset();\n+\n+  __ auipc(ra, 0);\n+  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  guarantee(code_offset() - offset <= deopt_handler_size(), \"overflow\");\n+  __ end_a_stub();\n+\n+  return offset;\n+}\n+\n+void LIR_Assembler::return_op(LIR_Opr result) {\n+  assert(result->is_illegal() || !result->is_single_cpu() || result->as_register() == x10, \"word returns are in x10\");\n+\n+  \/\/ Pop the stack before the safepoint code\n+  __ remove_frame(initial_frame_size_in_bytes());\n+\n+  if (StackReservedPages > 0 && compilation()->has_reserved_stack_access()) {\n+    __ reserved_stack_check();\n+  }\n+\n+  address polling_page(os::get_polling_page());\n+  __ read_polling_page(t0, polling_page, relocInfo::poll_return_type);\n+  __ ret();\n+}\n+\n+int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {\n+  address polling_page(os::get_polling_page());\n+  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  assert(os::is_poll_address(polling_page), \"should be\");\n+  int32_t offset = 0;\n+  __ get_polling_page(t0, polling_page, offset, relocInfo::poll_type);\n+  add_debug_info_for_branch(info);  \/\/ This isn't just debug info:\n+                                    \/\/ it's the oop map\n+  __ read_polling_page(t0, offset, relocInfo::poll_type);\n+  return __ offset();\n+}\n+\n+void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {\n+  __ mv(to_reg, from_reg);\n+}\n+\n+void LIR_Assembler::swap_reg(Register a, Register b) { Unimplemented(); }\n+\n+void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {\n+  assert(src->is_constant(), \"should not call otherwise\");\n+  assert(dest->is_register(), \"should not call otherwise\");\n+  LIR_Const* c = src->as_constant_ptr();\n+  address const_addr = NULL;\n+\n+  switch (c->type()) {\n+    case T_INT:\n+      assert(patch_code == lir_patch_none, \"no patching handled here\");\n+      __ mvw(dest->as_register(), c->as_jint());\n+      break;\n+\n+    case T_ADDRESS:\n+      assert(patch_code == lir_patch_none, \"no patching handled here\");\n+      __ mv(dest->as_register(), c->as_jint());\n+      break;\n+\n+    case T_LONG:\n+      assert(patch_code == lir_patch_none, \"no patching handled here\");\n+      __ mv(dest->as_register_lo(), (intptr_t)c->as_jlong());\n+      break;\n+\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (patch_code == lir_patch_none) {\n+        jobject2reg(c->as_jobject(), dest->as_register());\n+      } else {\n+        jobject2reg_with_patching(dest->as_register(), info);\n+      }\n+      break;\n+\n+    case T_METADATA:\n+      if (patch_code != lir_patch_none) {\n+        klass2reg_with_patching(dest->as_register(), info);\n+      } else {\n+        __ mov_metadata(dest->as_register(), c->as_metadata());\n+      }\n+      break;\n+\n+    case T_FLOAT:\n+      const_addr = float_constant(c->as_jfloat());\n+      assert(const_addr != NULL, \"must create float constant in the constant table\");\n+      __ flw(dest->as_float_reg(), InternalAddress(const_addr));\n+      break;\n+\n+    case T_DOUBLE:\n+      const_addr = double_constant(c->as_jdouble());\n+      assert(const_addr != NULL, \"must create double constant in the constant table\");\n+      __ fld(dest->as_double_reg(), InternalAddress(const_addr));\n+      break;\n+\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {\n+  assert(src->is_constant(), \"should not call otherwise\");\n+  assert(dest->is_stack(), \"should not call otherwise\");\n+  LIR_Const* c = src->as_constant_ptr();\n+  switch (c->type()) {\n+    case T_OBJECT:\n+      if (c->as_jobject() == NULL) {\n+        __ sd(zr, frame_map()->address_for_slot(dest->single_stack_ix()));\n+      } else {\n+        const2reg(src, FrameMap::t1_opr, lir_patch_none, NULL);\n+        reg2stack(FrameMap::t1_opr, dest, c->type(), false);\n+      }\n+      break;\n+    case T_ADDRESS:   \/\/ fall through\n+      const2reg(src, FrameMap::t1_opr, lir_patch_none, NULL);\n+      reg2stack(FrameMap::t1_opr, dest, c->type(), false);\n+    case T_INT:       \/\/ fall through\n+    case T_FLOAT:\n+      if (c->as_jint_bits() == 0) {\n+        __ sw(zr, frame_map()->address_for_slot(dest->single_stack_ix()));\n+      } else {\n+        __ mvw(t1, c->as_jint_bits());\n+        __ sw(t1, frame_map()->address_for_slot(dest->single_stack_ix()));\n+      }\n+      break;\n+    case T_LONG:      \/\/ fall through\n+    case T_DOUBLE:\n+      if (c->as_jlong_bits() == 0) {\n+        __ sd(zr, frame_map()->address_for_slot(dest->double_stack_ix(),\n+                                                lo_word_offset_in_bytes));\n+      } else {\n+        __ mv(t1, (intptr_t)c->as_jlong_bits());\n+        __ sd(t1, frame_map()->address_for_slot(dest->double_stack_ix(),\n+                                                lo_word_offset_in_bytes));\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {\n+  assert(src->is_constant(), \"should not call otherwise\");\n+  assert(dest->is_address(), \"should not call otherwise\");\n+  LIR_Const* c = src->as_constant_ptr();\n+  LIR_Address* to_addr = dest->as_address_ptr();\n+  void (Assembler::* insn)(Register Rt, const Address &adr, Register temp);\n+  switch (type) {\n+    case T_ADDRESS:\n+      assert(c->as_jint() == 0, \"should be\");\n+      insn = &Assembler::sd; break;\n+    case T_LONG:\n+      assert(c->as_jlong() == 0, \"should be\");\n+      insn = &Assembler::sd; break;\n+    case T_DOUBLE:\n+      assert(c->as_jdouble() == 0.0, \"should be\");\n+      insn = &Assembler::sd; break;\n+    case T_INT:\n+      assert(c->as_jint() == 0, \"should be\");\n+      insn = &Assembler::sw; break;\n+    case T_FLOAT:\n+      assert(c->as_jfloat() == 0.0f, \"should be\");\n+      insn = &Assembler::sw; break;\n+    case T_OBJECT:    \/\/ fall through\n+    case T_ARRAY:\n+      assert(c->as_jobject() == 0, \"should be\");\n+      if (UseCompressedOops && !wide) {\n+        insn = &Assembler::sw;\n+      } else {\n+        insn = &Assembler::sd;\n+      }\n+      break;\n+    case T_CHAR:      \/\/ fall through\n+    case T_SHORT:\n+      assert(c->as_jint() == 0, \"should be\");\n+      insn = &Assembler::sh;\n+      break;\n+    case T_BOOLEAN:   \/\/ fall through\n+    case T_BYTE:\n+      assert(c->as_jint() == 0, \"should be\");\n+      insn = &Assembler::sb; break;\n+    default:\n+      ShouldNotReachHere();\n+      insn = &Assembler::sd;  \/\/ unreachable\n+  }\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+  (_masm->*insn)(zr, as_Address(to_addr), t0);\n+}\n+\n+void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {\n+  assert(src->is_register(), \"should not call otherwise\");\n+  assert(dest->is_register(), \"should not call otherwise\");\n+\n+  \/\/ move between cpu-registers\n+  if (dest->is_single_cpu()) {\n+    if (src->type() == T_LONG) {\n+      \/\/ Can do LONG -> OBJECT\n+      move_regs(src->as_register_lo(), dest->as_register());\n+      return;\n+    }\n+    assert(src->is_single_cpu(), \"must match\");\n+    if (src->type() == T_OBJECT) {\n+      __ verify_oop(src->as_register());\n+    }\n+    move_regs(src->as_register(), dest->as_register());\n+  } else if (dest->is_double_cpu()) {\n+    if (is_reference_type(src->type())) {\n+      __ verify_oop(src->as_register());\n+      move_regs(src->as_register(), dest->as_register_lo());\n+      return;\n+    }\n+    assert(src->is_double_cpu(), \"must match\");\n+    Register f_lo = src->as_register_lo();\n+    Register f_hi = src->as_register_hi();\n+    Register t_lo = dest->as_register_lo();\n+    Register t_hi = dest->as_register_hi();\n+    assert(f_hi == f_lo, \"must be same\");\n+    assert(t_hi == t_lo, \"must be same\");\n+    move_regs(f_lo, t_lo);\n+  } else if (dest->is_single_fpu()) {\n+    assert(src->is_single_fpu(), \"expect single fpu\");\n+    __ fmv_s(dest->as_float_reg(), src->as_float_reg());\n+  } else if (dest->is_double_fpu()) {\n+    assert(src->is_double_fpu(), \"expect double fpu\");\n+    __ fmv_d(dest->as_double_reg(), src->as_double_reg());\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+  precond(src->is_register() && dest->is_stack());\n+\n+  uint const c_sz32 = sizeof(uint32_t);\n+  uint const c_sz64 = sizeof(uint64_t);\n+\n+  assert(src->is_register(), \"should not call otherwise\");\n+  assert(dest->is_stack(), \"should not call otherwise\");\n+  if (src->is_single_cpu()) {\n+    int index = dest->single_stack_ix();\n+    if (is_reference_type(type)) {\n+      __ sd(src->as_register(), stack_slot_address(index, c_sz64));\n+      __ verify_oop(src->as_register());\n+    } else if (type == T_METADATA || type == T_DOUBLE || type == T_ADDRESS) {\n+      __ sd(src->as_register(), stack_slot_address(index, c_sz64));\n+    } else {\n+      __ sw(src->as_register(), stack_slot_address(index, c_sz32));\n+    }\n+  } else if (src->is_double_cpu()) {\n+    int index = dest->double_stack_ix();\n+    Address dest_addr_LO = stack_slot_address(index, c_sz64, lo_word_offset_in_bytes);\n+    __ sd(src->as_register_lo(), dest_addr_LO);\n+  } else if (src->is_single_fpu()) {\n+    int index = dest->single_stack_ix();\n+    __ fsw(src->as_float_reg(), stack_slot_address(index, c_sz32));\n+  } else if (src->is_double_fpu()) {\n+    int index = dest->double_stack_ix();\n+    __ fsd(src->as_double_reg(), stack_slot_address(index, c_sz64));\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool \/* unaligned *\/) {\n+  LIR_Address* to_addr = dest->as_address_ptr();\n+  \/\/ t0 was used as tmp reg in as_Address, so we use t1 as compressed_src\n+  Register compressed_src = t1;\n+\n+  if (patch_code != lir_patch_none) {\n+    deoptimize_trap(info);\n+    return;\n+  }\n+\n+  if (is_reference_type(type)) {\n+    __ verify_oop(src->as_register());\n+\n+    if (UseCompressedOops && !wide) {\n+      __ encode_heap_oop(compressed_src, src->as_register());\n+    } else {\n+      compressed_src = src->as_register();\n+    }\n+  }\n+\n+  int null_check_here = code_offset();\n+\n+  switch (type) {\n+    case T_FLOAT:\n+      __ fsw(src->as_float_reg(), as_Address(to_addr));\n+      break;\n+\n+    case T_DOUBLE:\n+      __ fsd(src->as_double_reg(), as_Address(to_addr));\n+      break;\n+\n+    case T_ARRAY:      \/\/ fall through\n+    case T_OBJECT:\n+      if (UseCompressedOops && !wide) {\n+        __ sw(compressed_src, as_Address(to_addr));\n+      } else {\n+        __ sd(compressed_src, as_Address(to_addr));\n+      }\n+      break;\n+    case T_METADATA:\n+      \/\/ We get here to store a method pointer to the stack to pass to\n+      \/\/ a dtrace runtime call. This can't work on 64 bit with\n+      \/\/ compressed klass ptrs: T_METADATA can be compressed klass\n+      \/\/ ptr or a 64 bit method pointer.\n+      ShouldNotReachHere();\n+      __ sd(src->as_register(), as_Address(to_addr));\n+      break;\n+    case T_ADDRESS:\n+      __ sd(src->as_register(), as_Address(to_addr));\n+      break;\n+    case T_INT:\n+      __ sw(src->as_register(), as_Address(to_addr));\n+      break;\n+    case T_LONG:\n+      __ sd(src->as_register_lo(), as_Address(to_addr));\n+      break;\n+    case T_BYTE:    \/\/ fall through\n+    case T_BOOLEAN:\n+      __ sb(src->as_register(), as_Address(to_addr));\n+      break;\n+    case T_CHAR:    \/\/ fall through\n+    case T_SHORT:\n+      __ sh(src->as_register(), as_Address(to_addr));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  if (info != NULL) {\n+    add_debug_info_for_null_check(null_check_here, info);\n+  }\n+}\n+\n+void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {\n+  precond(src->is_stack() && dest->is_register());\n+\n+  uint const c_sz32 = sizeof(uint32_t);\n+  uint const c_sz64 = sizeof(uint64_t);\n+\n+  if (dest->is_single_cpu()) {\n+    int index = src->single_stack_ix();\n+    if (type == T_INT) {\n+      __ lw(dest->as_register(), stack_slot_address(index, c_sz32));\n+    } else if (is_reference_type(type)) {\n+      __ ld(dest->as_register(), stack_slot_address(index, c_sz64));\n+      __ verify_oop(dest->as_register());\n+    } else if (type == T_METADATA || type == T_ADDRESS) {\n+      __ ld(dest->as_register(), stack_slot_address(index, c_sz64));\n+    } else {\n+      __ lwu(dest->as_register(), stack_slot_address(index, c_sz32));\n+    }\n+  } else if (dest->is_double_cpu()) {\n+    int index = src->double_stack_ix();\n+    Address src_addr_LO = stack_slot_address(index, c_sz64, lo_word_offset_in_bytes);\n+    __ ld(dest->as_register_lo(), src_addr_LO);\n+  } else if (dest->is_single_fpu()) {\n+    int index = src->single_stack_ix();\n+    __ flw(dest->as_float_reg(), stack_slot_address(index, c_sz32));\n+  } else if (dest->is_double_fpu()) {\n+    int index = src->double_stack_ix();\n+    __ fld(dest->as_double_reg(), stack_slot_address(index, c_sz64));\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {\n+  deoptimize_trap(info);\n+}\n+\n+void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {\n+  LIR_Opr temp;\n+  if (type == T_LONG || type == T_DOUBLE) {\n+    temp = FrameMap::t1_long_opr;\n+  } else {\n+    temp = FrameMap::t1_opr;\n+  }\n+\n+  stack2reg(src, temp, src->type());\n+  reg2stack(temp, dest, dest->type(), false);\n+}\n+\n+void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool \/* unaligned *\/) {\n+  assert(src->is_address(), \"should not call otherwise\");\n+  assert(dest->is_register(), \"should not call otherwise\");\n+\n+  LIR_Address* addr = src->as_address_ptr();\n+  LIR_Address* from_addr = src->as_address_ptr();\n+\n+  if (addr->base()->type() == T_OBJECT) {\n+    __ verify_oop(addr->base()->as_pointer_register());\n+  }\n+\n+  if (patch_code != lir_patch_none) {\n+    deoptimize_trap(info);\n+    return;\n+  }\n+\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+\n+  int null_check_here = code_offset();\n+  switch (type) {\n+    case T_FLOAT:\n+      __ flw(dest->as_float_reg(), as_Address(from_addr));\n+      break;\n+    case T_DOUBLE:\n+      __ fld(dest->as_double_reg(), as_Address(from_addr));\n+      break;\n+    case T_ARRAY:     \/\/ fall through\n+    case T_OBJECT:\n+      if (UseCompressedOops && !wide) {\n+        __ lwu(dest->as_register(), as_Address(from_addr));\n+      } else {\n+        __ ld(dest->as_register(), as_Address(from_addr));\n+      }\n+      break;\n+    case T_METADATA:\n+      \/\/ We get here to store a method pointer to the stack to pass to\n+      \/\/ a dtrace runtime call. This can't work on 64 bit with\n+      \/\/ compressed klass ptrs: T_METADATA can be a compressed klass\n+      \/\/ ptr or a 64 bit method pointer.\n+      ShouldNotReachHere();\n+      __ ld(dest->as_register(), as_Address(from_addr));\n+      break;\n+    case T_ADDRESS:\n+      \/\/ FIXME: OMG this is a horrible kludge.  Any offset from an\n+      \/\/ address that matches klass_offset_in_bytes() will be loaded\n+      \/\/ as a word, not a long.\n+      if (UseCompressedClassPointers && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n+        __ lwu(dest->as_register(), as_Address(from_addr));\n+      } else {\n+        __ ld(dest->as_register(), as_Address(from_addr));\n+      }\n+      break;\n+    case T_INT:\n+      __ lw(dest->as_register(), as_Address(from_addr));\n+      break;\n+    case T_LONG:\n+      __ ld(dest->as_register_lo(), as_Address_lo(from_addr));\n+      break;\n+    case T_BYTE:\n+      __ lb(dest->as_register(), as_Address(from_addr));\n+      break;\n+    case T_BOOLEAN:\n+      __ lbu(dest->as_register(), as_Address(from_addr));\n+      break;\n+    case T_CHAR:\n+      __ lhu(dest->as_register(), as_Address(from_addr));\n+      break;\n+    case T_SHORT:\n+      __ lh(dest->as_register(), as_Address(from_addr));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  if (is_reference_type(type)) {\n+    if (UseCompressedOops && !wide) {\n+      __ decode_heap_oop(dest->as_register());\n+    }\n+    __ verify_oop(dest->as_register());\n+  } else if (type == T_ADDRESS && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n+    if (UseCompressedClassPointers) {\n+      __ decode_klass_not_null(dest->as_register());\n+    }\n+  }\n+}\n+\n+void LIR_Assembler::emit_op3(LIR_Op3* op) {\n+  switch (op->code()) {\n+    case lir_idiv: \/\/ fall through\n+    case lir_irem:\n+      arithmetic_idiv(op->code(),\n+                      op->in_opr1(),\n+                      op->in_opr2(),\n+                      op->in_opr3(),\n+                      op->result_opr(),\n+                      op->info());\n+      break;\n+    case lir_fmad:\n+      __ fmadd_d(op->result_opr()->as_double_reg(),\n+                 op->in_opr1()->as_double_reg(),\n+                 op->in_opr2()->as_double_reg(),\n+                 op->in_opr3()->as_double_reg());\n+      break;\n+    case lir_fmaf:\n+      __ fmadd_s(op->result_opr()->as_float_reg(),\n+                 op->in_opr1()->as_float_reg(),\n+                 op->in_opr2()->as_float_reg(),\n+                 op->in_opr3()->as_float_reg());\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  Label label;\n+\n+  emit_branch(condition, cmp_opr1, cmp_opr2, label, \/* is_far *\/ false,\n+              \/* is_unordered *\/ (condition == lir_cond_greaterEqual || condition == lir_cond_greater) ? false : true);\n+\n+  Label done;\n+  move_op(opr2, result, type, lir_patch_none, NULL,\n+          false,   \/\/ pop_fpu_stack\n+          false,   \/\/ unaligned\n+          false);  \/\/ wide\n+  __ j(done);\n+  __ bind(label);\n+  move_op(opr1, result, type, lir_patch_none, NULL,\n+          false,   \/\/ pop_fpu_stack\n+          false,   \/\/ unaligned\n+          false);  \/\/ wide\n+  __ bind(done);\n+}\n+\n+void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {\n+  LIR_Condition condition = op->cond();\n+  if (condition == lir_cond_always) {\n+    if (op->info() != NULL) {\n+      add_debug_info_for_branch(op->info());\n+    }\n+  } else {\n+    assert(op->in_opr1() != LIR_OprFact::illegalOpr && op->in_opr2() != LIR_OprFact::illegalOpr, \"conditional branches must have legal operands\");\n+  }\n+  bool is_unordered = (op->ublock() == op->block());\n+  emit_branch(condition, op->in_opr1(), op->in_opr2(), *op->label(), \/* is_far *\/ true, is_unordered);\n+}\n+\n+void LIR_Assembler::emit_branch(LIR_Condition cmp_flag, LIR_Opr cmp1, LIR_Opr cmp2, Label& label,\n+                                bool is_far, bool is_unordered) {\n+\n+  if (cmp_flag == lir_cond_always) {\n+    __ j(label);\n+    return;\n+  }\n+\n+  if (cmp1->is_cpu_register()) {\n+    Register reg1 = as_reg(cmp1);\n+    if (cmp2->is_cpu_register()) {\n+      Register reg2 = as_reg(cmp2);\n+      __ c1_cmp_branch(cmp_flag, reg1, reg2, label, cmp1->type(), is_far);\n+    } else if (cmp2->is_constant()) {\n+      const2reg_helper(cmp2);\n+      __ c1_cmp_branch(cmp_flag, reg1, t0, label, cmp2->type(), is_far);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (cmp1->is_single_fpu()) {\n+    assert(cmp2->is_single_fpu(), \"expect single float register\");\n+    __ c1_float_cmp_branch(cmp_flag, cmp1->as_float_reg(), cmp2->as_float_reg(), label, is_far, is_unordered);\n+  } else if (cmp1->is_double_fpu()) {\n+    assert(cmp2->is_double_fpu(), \"expect double float register\");\n+    __ c1_float_cmp_branch(cmp_flag | C1_MacroAssembler::c1_double_branch_mask,\n+                           cmp1->as_double_reg(), cmp2->as_double_reg(), label, is_far, is_unordered);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {\n+  LIR_Opr src  = op->in_opr();\n+  LIR_Opr dest = op->result_opr();\n+\n+  switch (op->bytecode()) {\n+    case Bytecodes::_i2f:\n+      __ fcvt_s_w(dest->as_float_reg(), src->as_register()); break;\n+    case Bytecodes::_i2d:\n+      __ fcvt_d_w(dest->as_double_reg(), src->as_register()); break;\n+    case Bytecodes::_l2d:\n+      __ fcvt_d_l(dest->as_double_reg(), src->as_register_lo()); break;\n+    case Bytecodes::_l2f:\n+      __ fcvt_s_l(dest->as_float_reg(), src->as_register_lo()); break;\n+    case Bytecodes::_f2d:\n+      __ fcvt_d_s(dest->as_double_reg(), src->as_float_reg()); break;\n+    case Bytecodes::_d2f:\n+      __ fcvt_s_d(dest->as_float_reg(), src->as_double_reg()); break;\n+    case Bytecodes::_i2c:\n+      __ zero_extend(dest->as_register(), src->as_register(), 16); break;\n+    case Bytecodes::_i2l:\n+      __ addw(dest->as_register_lo(), src->as_register(), zr); break;\n+    case Bytecodes::_i2s:\n+      __ sign_extend(dest->as_register(), src->as_register(), 16); break;\n+    case Bytecodes::_i2b:\n+      __ sign_extend(dest->as_register(), src->as_register(), 8); break;\n+    case Bytecodes::_l2i:\n+      _masm->block_comment(\"FIXME: This coulde be no-op\");\n+      __ addw(dest->as_register(), src->as_register_lo(), zr); break;\n+    case Bytecodes::_d2l:\n+      __ fcvt_l_d_safe(dest->as_register_lo(), src->as_double_reg()); break;\n+    case Bytecodes::_f2i:\n+      __ fcvt_w_s_safe(dest->as_register(), src->as_float_reg()); break;\n+    case Bytecodes::_f2l:\n+      __ fcvt_l_s_safe(dest->as_register_lo(), src->as_float_reg()); break;\n+    case Bytecodes::_d2i:\n+      __ fcvt_w_d_safe(dest->as_register(), src->as_double_reg()); break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {\n+  if (op->init_check()) {\n+    __ lbu(t0, Address(op->klass()->as_register(),\n+                       InstanceKlass::init_state_offset()));\n+    __ mvw(t1, InstanceKlass::fully_initialized);\n+    add_debug_info_for_null_check_here(op->stub()->info());\n+    __ bne(t0, t1, *op->stub()->entry(), \/* is_far *\/ true);\n+  }\n+\n+  __ allocate_object(op->obj()->as_register(),\n+                     op->tmp1()->as_register(),\n+                     op->tmp2()->as_register(),\n+                     op->header_size(),\n+                     op->object_size(),\n+                     op->klass()->as_register(),\n+                     *op->stub()->entry());\n+\n+  __ bind(*op->stub()->continuation());\n+}\n+\n+void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {\n+  Register len = op->len()->as_register();\n+\n+  if (UseSlowPath ||\n+      (!UseFastNewObjectArray && is_reference_type(op->type())) ||\n+      (!UseFastNewTypeArray   && !is_reference_type(op->type()))) {\n+    __ j(*op->stub()->entry());\n+  } else {\n+    Register tmp1 = op->tmp1()->as_register();\n+    Register tmp2 = op->tmp2()->as_register();\n+    Register tmp3 = op->tmp3()->as_register();\n+    if (len == tmp1) {\n+      tmp1 = tmp3;\n+    } else if (len == tmp2) {\n+      tmp2 = tmp3;\n+    } else if (len == tmp3) {\n+      \/\/ everything is ok\n+    } else {\n+      __ mv(tmp3, len);\n+    }\n+    __ allocate_array(op->obj()->as_register(),\n+                      len,\n+                      tmp1,\n+                      tmp2,\n+                      arrayOopDesc::header_size(op->type()),\n+                      array_element_size(op->type()),\n+                      op->klass()->as_register(),\n+                      *op->stub()->entry());\n+  }\n+  __ bind(*op->stub()->continuation());\n+}\n+\n+void LIR_Assembler::type_profile_helper(Register mdo, ciMethodData *md, ciProfileData *data,\n+                                        Register recv, Label* update_done) {\n+  for (uint i = 0; i < ReceiverTypeData::row_limit(); i++) {\n+    Label next_test;\n+    \/\/ See if the receiver is receiver[n].\n+    __ ld(t1, Address(mdo, md->byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));\n+    __ bne(recv, t1, next_test);\n+    Address data_addr(mdo, md->byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));\n+    __ add_memory_int64(data_addr, DataLayout::counter_increment);\n+    __ j(*update_done);\n+    __ bind(next_test);\n+  }\n+\n+  \/\/ Didn't find receiver; find next empty slot and fill it in\n+  for (uint i = 0; i < ReceiverTypeData::row_limit(); i++) {\n+    Label next_test;\n+    Address recv_addr(mdo, md->byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)));\n+    __ ld(t1, recv_addr);\n+    __ bnez(t1, next_test);\n+    __ sd(recv, recv_addr);\n+    __ li(t1, DataLayout::counter_increment);\n+    __ sd(t1, Address(mdo, md->byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))));\n+    __ j(*update_done);\n+    __ bind(next_test);\n+  }\n+}\n+\n+void LIR_Assembler::data_check(LIR_OpTypeCheck *op, ciMethodData **md, ciProfileData **data) {\n+  ciMethod* method = op->profiled_method();\n+  assert(method != NULL, \"Should have method\");\n+  int bci = op->profiled_bci();\n+  *md = method->method_data_or_null();\n+  guarantee(*md != NULL, \"Sanity\");\n+  *data = ((*md)->bci_to_data(bci));\n+  assert(*data != NULL, \"need data for type check\");\n+  assert((*data)->is_ReceiverTypeData(), \"need ReceiverTypeData for type check\");\n+}\n+\n+void LIR_Assembler::typecheck_helper_slowcheck(ciKlass *k, Register obj, Register Rtmp1,\n+                                               Register k_RInfo, Register klass_RInfo,\n+                                               Label *failure_target, Label *success_target) {\n+  \/\/ get object class\n+  \/\/ not a safepoint as obj null check happens earlier\n+  __ load_klass(klass_RInfo, obj);\n+  if (k->is_loaded()) {\n+    \/\/ See if we get an immediate positive hit\n+    __ ld(t0, Address(klass_RInfo, int64_t(k->super_check_offset())));\n+    if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k->super_check_offset()) {\n+      __ bne(k_RInfo, t0, *failure_target, \/* is_far *\/ true);\n+      \/\/ successful cast, fall through to profile or jump\n+    } else {\n+      \/\/ See if we get an immediate positive hit\n+      __ beq(k_RInfo, t0, *success_target);\n+      \/\/ check for self\n+      __ beq(klass_RInfo, k_RInfo, *success_target);\n+\n+      __ addi(sp, sp, -2 * wordSize); \/\/ 2: store k_RInfo and klass_RInfo\n+      __ sd(k_RInfo, Address(sp, 0));             \/\/ sub klass\n+      __ sd(klass_RInfo, Address(sp, wordSize));  \/\/ super klass\n+      __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));\n+      \/\/ load result to k_RInfo\n+      __ ld(k_RInfo, Address(sp, 0));\n+      __ addi(sp, sp, 2 * wordSize); \/\/ 2: pop out k_RInfo and klass_RInfo\n+      \/\/ result is a boolean\n+      __ beqz(k_RInfo, *failure_target, \/* is_far *\/ true);\n+      \/\/ successful cast, fall through to profile or jump\n+    }\n+  } else {\n+    \/\/ perform the fast part of the checking logic\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    \/\/ call out-of-line instance of __ check_klass_subtytpe_slow_path(...)\n+    __ addi(sp, sp, -2 * wordSize); \/\/ 2: store k_RInfo and klass_RInfo\n+    __ sd(klass_RInfo, Address(sp, wordSize));  \/\/ sub klass\n+    __ sd(k_RInfo, Address(sp, 0));             \/\/ super klass\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));\n+    \/\/ load result to k_RInfo\n+    __ ld(k_RInfo, Address(sp, 0));\n+    __ addi(sp, sp, 2 * wordSize); \/\/ 2: pop out k_RInfo and klass_RInfo\n+    \/\/ result is a boolean\n+    __ beqz(k_RInfo, *failure_target, \/* is_far *\/ true);\n+    \/\/ successful cast, fall thriugh to profile or jump\n+  }\n+}\n+\n+void LIR_Assembler::profile_object(ciMethodData* md, ciProfileData* data, Register obj,\n+                                   Register klass_RInfo, Label* obj_is_null) {\n+  Label not_null;\n+  __ bnez(obj, not_null);\n+  \/\/ Object is null, update MDO and exit\n+  Register mdo = klass_RInfo;\n+  __ mov_metadata(mdo, md->constant_encoding());\n+  Address data_addr = __ form_address(t1, mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+  __ lbu(t0, data_addr);\n+  __ ori(t0, t0, BitData::null_seen_byte_constant());\n+  __ sb(t0, data_addr);\n+  __ j(*obj_is_null);\n+  __ bind(not_null);\n+}\n+\n+void LIR_Assembler::typecheck_loaded(LIR_OpTypeCheck *op, ciKlass* k, Register k_RInfo) {\n+  if (!k->is_loaded()) {\n+    klass2reg_with_patching(k_RInfo, op->info_for_patch());\n+  } else {\n+    __ mov_metadata(k_RInfo, k->constant_encoding());\n+  }\n+}\n+\n+void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {\n+  Register obj = op->object()->as_register();\n+  Register k_RInfo = op->tmp1()->as_register();\n+  Register klass_RInfo = op->tmp2()->as_register();\n+  Register dst = op->result_opr()->as_register();\n+  ciKlass* k = op->klass();\n+  Register Rtmp1 = noreg;\n+\n+  \/\/ check if it needs to be profiled\n+  ciMethodData* md = NULL;\n+  ciProfileData* data = NULL;\n+\n+  const bool should_profile = op->should_profile();\n+  if (should_profile) {\n+    data_check(op, &md, &data);\n+  }\n+  Label profile_cast_success, profile_cast_failure;\n+  Label *success_target = should_profile ? &profile_cast_success : success;\n+  Label *failure_target = should_profile ? &profile_cast_failure : failure;\n+\n+  if (obj == k_RInfo) {\n+    k_RInfo = dst;\n+  } else if (obj == klass_RInfo) {\n+    klass_RInfo = dst;\n+  }\n+  if (k->is_loaded() && !UseCompressedClassPointers) {\n+    select_different_registers(obj, dst, k_RInfo, klass_RInfo);\n+  } else {\n+    Rtmp1 = op->tmp3()->as_register();\n+    select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);\n+  }\n+\n+  assert_different_registers(obj, k_RInfo, klass_RInfo);\n+\n+  if (should_profile) {\n+    profile_object(md, data, obj, klass_RInfo, obj_is_null);\n+  } else {\n+    __ beqz(obj, *obj_is_null);\n+  }\n+\n+  typecheck_loaded(op, k, k_RInfo);\n+  __ verify_oop(obj);\n+\n+  if (op->fast_check()) {\n+    \/\/ get object class\n+    \/\/ not a safepoint as obj null check happens earlier\n+    __ load_klass(t0, obj);\n+    __ bne(t0, k_RInfo, *failure_target, \/* is_far *\/ true);\n+    \/\/ successful cast, fall through to profile or jump\n+  } else {\n+    typecheck_helper_slowcheck(k, obj, Rtmp1, k_RInfo, klass_RInfo, failure_target, success_target);\n+  }\n+  if (should_profile) {\n+    type_profile(obj, md, klass_RInfo, k_RInfo, data, success, failure, profile_cast_success, profile_cast_failure);\n+  }\n+  __ j(*success);\n+}\n+\n+void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {\n+  const bool should_profile = op->should_profile();\n+\n+  LIR_Code code = op->code();\n+  if (code == lir_store_check) {\n+    typecheck_lir_store(op, should_profile);\n+  } else if (code == lir_checkcast) {\n+    Register obj = op->object()->as_register();\n+    Register dst = op->result_opr()->as_register();\n+    Label success;\n+    emit_typecheck_helper(op, &success, op->stub()->entry(), &success);\n+    __ bind(success);\n+    if (dst != obj) {\n+      __ mv(dst, obj);\n+    }\n+  } else if (code == lir_instanceof) {\n+    Register obj = op->object()->as_register();\n+    Register dst = op->result_opr()->as_register();\n+    Label success, failure, done;\n+    emit_typecheck_helper(op, &success, &failure, &failure);\n+    __ bind(failure);\n+    __ mv(dst, zr);\n+    __ j(done);\n+    __ bind(success);\n+    __ mv(dst, 1);\n+    __ bind(done);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {\n+  assert(VM_Version::supports_cx8(), \"wrong machine\");\n+  Register addr;\n+  if (op->addr()->is_register()) {\n+    addr = as_reg(op->addr());\n+  } else {\n+    assert(op->addr()->is_address(), \"what else?\");\n+    LIR_Address* addr_ptr = op->addr()->as_address_ptr();\n+    assert(addr_ptr->disp() == 0, \"need 0 disp\");\n+    assert(addr_ptr->index() == LIR_OprDesc::illegalOpr(), \"need 0 index\");\n+    addr = as_reg(addr_ptr->base());\n+  }\n+  Register newval = as_reg(op->new_value());\n+  Register cmpval = as_reg(op->cmp_value());\n+\n+  if (op->code() == lir_cas_obj) {\n+    if (UseCompressedOops) {\n+      Register tmp1 = op->tmp1()->as_register();\n+      assert(op->tmp1()->is_valid(), \"must be\");\n+      __ encode_heap_oop(tmp1, cmpval);\n+      cmpval = tmp1;\n+      __ encode_heap_oop(t1, newval);\n+      newval = t1;\n+      caswu(addr, newval, cmpval);\n+    } else {\n+      casl(addr, newval, cmpval);\n+    }\n+  } else if (op->code() == lir_cas_int) {\n+    casw(addr, newval, cmpval);\n+  } else {\n+    casl(addr, newval, cmpval);\n+  }\n+}\n+\n+void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {\n+  switch (code) {\n+    case lir_abs:  __ fabs_d(dest->as_double_reg(), value->as_double_reg()); break;\n+    case lir_sqrt: __ fsqrt_d(dest->as_double_reg(), value->as_double_reg()); break;\n+    default:       ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {\n+  assert(left->is_single_cpu() || left->is_double_cpu(), \"expect single or double register\");\n+  Register Rleft = left->is_single_cpu() ? left->as_register() : left->as_register_lo();\n+  if (dst->is_single_cpu()) {\n+    Register Rdst = dst->as_register();\n+    if (right->is_constant()) {\n+      int right_const = right->as_jint();\n+      if (Assembler::operand_valid_for_add_immediate(right_const)) {\n+        logic_op_imm(Rdst, Rleft, right_const, code);\n+        __ addw(Rdst, Rdst, zr);\n+     } else {\n+        __ mv(t0, right_const);\n+        logic_op_reg32(Rdst, Rleft, t0, code);\n+     }\n+    } else {\n+      Register Rright = right->is_single_cpu() ? right->as_register() : right->as_register_lo();\n+      logic_op_reg32(Rdst, Rleft, Rright, code);\n+    }\n+  } else {\n+    Register Rdst = dst->as_register_lo();\n+    if (right->is_constant()) {\n+      long right_const = right->as_jlong();\n+      if (Assembler::operand_valid_for_add_immediate(right_const)) {\n+        logic_op_imm(Rdst, Rleft, right_const, code);\n+      } else {\n+        __ mv(t0, right_const);\n+        logic_op_reg(Rdst, Rleft, t0, code);\n+      }\n+    } else {\n+      Register Rright = right->is_single_cpu() ? right->as_register() : right->as_register_lo();\n+      logic_op_reg(Rdst, Rleft, Rright, code);\n+    }\n+  }\n+}\n+\n+void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr src, LIR_Opr result, LIR_Op2* op) {\n+  ShouldNotCallThis();\n+}\n+\n+void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {\n+  if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {\n+    bool is_unordered_less = (code == lir_ucmp_fd2i);\n+    if (left->is_single_fpu()) {\n+      __ float_cmp(true, is_unordered_less ? -1 : 1,\n+                   left->as_float_reg(), right->as_float_reg(), dst->as_register());\n+    } else if (left->is_double_fpu()) {\n+      __ float_cmp(false, is_unordered_less ? -1 : 1,\n+                   left->as_double_reg(), right->as_double_reg(), dst->as_register());\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (code == lir_cmp_l2i) {\n+    __ cmp_l2i(dst->as_register(), left->as_register_lo(), right->as_register_lo());\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::align_call(LIR_Code code) {\n+  \/\/ With RVC a call instruction may get 2-byte aligned.\n+  \/\/ The address of the call instruction needs to be 4-byte aligned to\n+  \/\/ ensure that it does not span a cache line so that it can be patched.\n+  __ align(4);\n+}\n+\n+void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {\n+  address call = __ trampoline_call(Address(op->addr(), rtype));\n+  if (call == NULL) {\n+    bailout(\"trampoline stub overflow\");\n+    return;\n+  }\n+  add_call_info(code_offset(), op->info());\n+}\n+\n+void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {\n+  address call = __ ic_call(op->addr());\n+  if (call == NULL) {\n+    bailout(\"trampoline stub overflow\");\n+    return;\n+  }\n+  add_call_info(code_offset(), op->info());\n+}\n+\n+\/* Currently, vtable-dispatch is only enabled for sparc platforms *\/\n+void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {\n+  ShouldNotReachHere();\n+}\n+\n+void LIR_Assembler::emit_static_call_stub() {\n+  address call_pc = __ pc();\n+  assert((__ offset() % 4) == 0, \"bad alignment\");\n+  address stub = __ start_a_stub(call_stub_size());\n+  if (stub == NULL) {\n+    bailout(\"static call stub overflow\");\n+    return;\n+  }\n+\n+  int start = __ offset();\n+\n+  __ relocate(static_stub_Relocation::spec(call_pc));\n+  __ emit_static_call_stub();\n+\n+  assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()\n+         <= call_stub_size(), \"stub too big\");\n+  __ end_a_stub();\n+}\n+\n+void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {\n+  assert(exceptionOop->as_register() == x10, \"must match\");\n+  assert(exceptionPC->as_register() == x13, \"must match\");\n+\n+  \/\/ exception object is not added to oop map by LinearScan\n+  \/\/ (LinearScan assumes that no oops are in fixed registers)\n+  info->add_register_oop(exceptionOop);\n+  Runtime1::StubID unwind_id;\n+\n+  \/\/ get current pc information\n+  \/\/ pc is only needed if the method has an exception handler, the unwind code does not need it.\n+  if (compilation()->debug_info_recorder()->last_pc_offset() == __ offset()) {\n+    \/\/ As no instructions have been generated yet for this LIR node it's\n+    \/\/ possible that an oop map already exists for the current offset.\n+    \/\/ In that case insert an dummy NOP here to ensure all oop map PCs\n+    \/\/ are unique. See JDK-8237483.\n+    __ nop();\n+  }\n+  int pc_for_athrow_offset = __ offset();\n+  InternalAddress pc_for_athrow(__ pc());\n+  int32_t off = 0;\n+  __ la_patchable(exceptionPC->as_register(), pc_for_athrow, off);\n+  __ addi(exceptionPC->as_register(), exceptionPC->as_register(), off);\n+  add_call_info(pc_for_athrow_offset, info); \/\/ for exception handler\n+\n+  __ verify_not_null_oop(x10);\n+  \/\/ search an exception handler (x10: exception oop, x13: throwing pc)\n+  if (compilation()->has_fpu_code()) {\n+    unwind_id = Runtime1::handle_exception_id;\n+  } else {\n+    unwind_id = Runtime1::handle_exception_nofpu_id;\n+  }\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(unwind_id)));\n+  __ nop();\n+}\n+\n+void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {\n+  assert(exceptionOop->as_register() == x10, \"must match\");\n+  __ j(_unwind_handler_entry);\n+}\n+\n+void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {\n+  Register left_reg = left->is_single_cpu() ? left->as_register() : left->as_register_lo();\n+  Register dest_reg = dest->is_single_cpu() ? dest->as_register() : dest->as_register_lo();\n+  Register count_reg = count->as_register();\n+  if (dest->is_single_cpu()) {\n+    assert (dest->type() == T_INT, \"unexpected result type\");\n+    assert (left->type() == T_INT, \"unexpected left type\");\n+    __ andi(t0, count_reg, 31); \/\/ should not shift more than 31 bits\n+    switch (code) {\n+      case lir_shl:  __ sllw(dest_reg, left_reg, t0); break;\n+      case lir_shr:  __ sraw(dest_reg, left_reg, t0); break;\n+      case lir_ushr: __ srlw(dest_reg, left_reg, t0); break;\n+      default: ShouldNotReachHere();\n+    }\n+  } else if (dest->is_double_cpu()) {\n+    __ andi(t0, count_reg, 63); \/\/ should not shift more than 63 bits\n+    switch (code) {\n+      case lir_shl:  __ sll(dest_reg, left_reg, t0); break;\n+      case lir_shr:  __ sra(dest_reg, left_reg, t0); break;\n+      case lir_ushr: __ srl(dest_reg, left_reg, t0); break;\n+      default: ShouldNotReachHere();\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {\n+  Register left_reg = left->is_single_cpu() ? left->as_register() : left->as_register_lo();\n+  Register dest_reg = dest->is_single_cpu() ? dest->as_register() : dest->as_register_lo();\n+  if (dest->is_single_cpu()) {\n+    assert (dest->type() == T_INT, \"unexpected result type\");\n+    assert (left->type() == T_INT, \"unexpected left type\");\n+    count &= 0x1f;\n+    if (count != 0) {\n+      switch (code) {\n+        case lir_shl:  __ slliw(dest_reg, left_reg, count); break;\n+        case lir_shr:  __ sraiw(dest_reg, left_reg, count); break;\n+        case lir_ushr: __ srliw(dest_reg, left_reg, count); break;\n+        default: ShouldNotReachHere();\n+      }\n+    } else {\n+      move_regs(left_reg, dest_reg);\n+    }\n+  } else if (dest->is_double_cpu()) {\n+    count &= 0x3f;\n+    if (count != 0) {\n+      switch (code) {\n+        case lir_shl:  __ slli(dest_reg, left_reg, count); break;\n+        case lir_shr:  __ srai(dest_reg, left_reg, count); break;\n+        case lir_ushr: __ srli(dest_reg, left_reg, count); break;\n+        default: ShouldNotReachHere();\n+      }\n+    } else {\n+      move_regs(left->as_register_lo(), dest->as_register_lo());\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::emit_lock(LIR_OpLock* op) {\n+  Register obj = op->obj_opr()->as_register();  \/\/ may not be an oop\n+  Register hdr = op->hdr_opr()->as_register();\n+  Register lock = op->lock_opr()->as_register();\n+  if (!UseFastLocking) {\n+    __ j(*op->stub()->entry());\n+  } else if (op->code() == lir_lock) {\n+    Register scratch = noreg;\n+    if (UseBiasedLocking) {\n+      scratch = op->scratch_opr()->as_register();\n+    }\n+    assert(BasicLock::displaced_header_offset_in_bytes() == 0, \"lock_reg must point to the displaced header\");\n+    \/\/ add debug info for NullPointerException only if one is possible\n+    int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op->stub()->entry());\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check(null_check_offset, op->info());\n+    }\n+  } else if (op->code() == lir_unlock) {\n+    assert(BasicLock::displaced_header_offset_in_bytes() == 0, \"lock_reg must point to the displaced header\");\n+    __ unlock_object(hdr, obj, lock, *op->stub()->entry());\n+  } else {\n+    Unimplemented();\n+  }\n+  __ bind(*op->stub()->continuation());\n+}\n+\n+void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {\n+  ciMethod* method = op->profiled_method();\n+  int bci          = op->profiled_bci();\n+\n+  \/\/ Update counter for all call types\n+  ciMethodData* md = method->method_data_or_null();\n+  guarantee(md != NULL, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(op->mdo()->is_single_cpu(),  \"mdo must be allocated\");\n+  Register mdo  = op->mdo()->as_register();\n+  __ mov_metadata(mdo, md->constant_encoding());\n+  Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+  \/\/ Perform additional virtual call profiling for invokevirtual and\n+  \/\/ invokeinterface bytecodes\n+  if (op->should_profile_receiver_type()) {\n+    assert(op->recv()->is_single_cpu(), \"recv must be allocated\");\n+    Register recv = op->recv()->as_register();\n+    assert_different_registers(mdo, recv);\n+    assert(data->is_VirtualCallData(), \"need VirtualCallData for virtual calls\");\n+    ciKlass* known_klass = op->known_holder();\n+    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+      \/\/ We know the type that will be seen at this call site; we can\n+      \/\/ statically update the MethodData* rather than needing to do\n+      \/\/ dynamic tests on the receiver type\n+      \/\/ NOTE: we should probably put a lock around this search to\n+      \/\/ avoid collisions by concurrent compilations\n+      ciVirtualCallData* vc_data = (ciVirtualCallData*) data;\n+      uint i;\n+      for (i = 0; i < VirtualCallData::row_limit(); i++) {\n+        ciKlass* receiver = vc_data->receiver(i);\n+        if (known_klass->equals(receiver)) {\n+          Address data_addr(mdo, md->byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));\n+          __ add_memory_int64(data_addr, DataLayout::counter_increment);\n+          return;\n+        }\n+      }\n+\n+      \/\/ Receiver type not found in profile data; select an empty slot\n+      \/\/ Note that this is less efficient than it should be because it\n+      \/\/ always does a write to the receiver part of the\n+      \/\/ VirtualCallData rather than just the first time\n+      for (i = 0; i < VirtualCallData::row_limit(); i++) {\n+        ciKlass* receiver = vc_data->receiver(i);\n+        if (receiver == NULL) {\n+          Address recv_addr(mdo, md->byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));\n+          __ mov_metadata(t1, known_klass->constant_encoding());\n+          __ sd(t1, recv_addr);\n+          Address data_addr(mdo, md->byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));\n+          __ add_memory_int64(data_addr, DataLayout::counter_increment);\n+          return;\n+        }\n+      }\n+    } else {\n+      __ load_klass(recv, recv);\n+      Label update_done;\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      \/\/ Receiver did not match any saved receiver and there is no empty row for it.\n+      \/\/ Increment total counter to indicate polymorphic case.\n+      __ add_memory_int64(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    }\n+  } else {\n+    \/\/ Static call\n+    __ add_memory_int64(counter_addr, DataLayout::counter_increment);\n+  }\n+}\n+\n+void LIR_Assembler::emit_delay(LIR_OpDelay*) { Unimplemented(); }\n+\n+void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {\n+  __ la(dst->as_register(), frame_map()->address_for_monitor_lock(monitor_no));\n+}\n+\n+void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) { Unimplemented(); }\n+\n+void LIR_Assembler::check_conflict(ciKlass* exact_klass, intptr_t current_klass,\n+                                   Register tmp, Label &next, Label &none,\n+                                   Address mdo_addr) {\n+  if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n+    if (exact_klass != NULL) {\n+      __ mov_metadata(tmp, exact_klass->constant_encoding());\n+    } else {\n+      __ load_klass(tmp, tmp);\n+    }\n+\n+    __ ld(t1, mdo_addr);\n+    __ xorr(tmp, tmp, t1);\n+    __ andi(t0, tmp, TypeEntries::type_klass_mask);\n+    \/\/ klass seen before, nothing to do. The unknown bit may have been\n+    \/\/ set already but no need to check.\n+    __ beqz(t0, next);\n+\n+    \/\/ already unknown. Nothing to do anymore.\n+    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ bnez(t0, next);\n+\n+    if (TypeEntries::is_type_none(current_klass)) {\n+      __ beqz(t1, none);\n+      __ li(t0, (u1)TypeEntries::null_seen);\n+      __ beq(t0, t1, none);\n+      \/\/ There is a chance that the checks above (re-reading profiling\n+      \/\/ data from memory) fail if another thread has just set the\n+      \/\/ profiling to this obj's klass\n+      __ membar(MacroAssembler::LoadLoad);\n+      __ ld(t1, mdo_addr);\n+      __ xorr(tmp, tmp, t1);\n+      __ andi(t0, tmp, TypeEntries::type_klass_mask);\n+      __ beqz(t0, next);\n+    }\n+  } else {\n+    assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+           ciTypeEntries::valid_ciklass(current_klass) != exact_klass, \"conflict only\");\n+\n+    __ ld(tmp, mdo_addr);\n+    \/\/ already unknown. Nothing to do anymore.\n+    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ bnez(t0, next);\n+  }\n+\n+  \/\/ different than before. Cannot keep accurate profile.\n+  __ ld(t1, mdo_addr);\n+  __ ori(t1, t1, TypeEntries::type_unknown);\n+  __ sd(t1, mdo_addr);\n+\n+  if (TypeEntries::is_type_none(current_klass)) {\n+    __ j(next);\n+\n+    __ bind(none);\n+    \/\/ first time here. Set profile type.\n+    __ sd(tmp, mdo_addr);\n+  }\n+}\n+\n+void LIR_Assembler::check_no_conflict(ciKlass* exact_klass, intptr_t current_klass, Register tmp,\n+                                      Address mdo_addr, Label &next) {\n+  \/\/ There's a single possible klass at this profile point\n+  assert(exact_klass != NULL, \"should be\");\n+  if (TypeEntries::is_type_none(current_klass)) {\n+    __ mov_metadata(tmp, exact_klass->constant_encoding());\n+    __ ld(t1, mdo_addr);\n+    __ xorr(tmp, tmp, t1);\n+    __ andi(t0, tmp, TypeEntries::type_klass_mask);\n+    __ beqz(t0, next);\n+#ifdef ASSERT\n+  {\n+    Label ok;\n+    __ ld(t0, mdo_addr);\n+    __ beqz(t0, ok);\n+    __ li(t1, (u1)TypeEntries::null_seen);\n+    __ beq(t0, t1, ok);\n+    \/\/ may have been set by another thread\n+    __ membar(MacroAssembler::LoadLoad);\n+    __ mov_metadata(t0, exact_klass->constant_encoding());\n+    __ ld(t1, mdo_addr);\n+    __ xorr(t1, t0, t1);\n+    __ andi(t1, t1, TypeEntries::type_mask);\n+    __ beqz(t1, ok);\n+\n+    __ stop(\"unexpected profiling mismatch\");\n+    __ bind(ok);\n+  }\n+#endif\n+    \/\/ first time here. Set profile type.\n+    __ sd(tmp, mdo_addr);\n+  } else {\n+    assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+           ciTypeEntries::valid_ciklass(current_klass) != exact_klass, \"inconsistent\");\n+\n+    __ ld(tmp, mdo_addr);\n+    \/\/ already unknown. Nothing to do anymore.\n+    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ bnez(t0, next);\n+\n+    __ ori(tmp, tmp, TypeEntries::type_unknown);\n+    __ sd(tmp, mdo_addr);\n+  }\n+}\n+\n+void LIR_Assembler::check_null(Register tmp, Label &update, intptr_t current_klass,\n+                               Address mdo_addr, bool do_update, Label &next) {\n+  __ bnez(tmp, update);\n+  if (!TypeEntries::was_null_seen(current_klass)) {\n+    __ ld(t1, mdo_addr);\n+    __ ori(t1, t1, TypeEntries::null_seen);\n+    __ sd(t1, mdo_addr);\n+  }\n+  if (do_update) {\n+    __ j(next);\n+  }\n+}\n+\n+void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {\n+  COMMENT(\"emit_profile_type {\");\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  ciKlass* exact_klass = op->exact_klass();\n+  intptr_t current_klass = op->current_klass();\n+  bool not_null = op->not_null();\n+  bool no_conflict = op->no_conflict();\n+\n+  Label update, next, none;\n+\n+  bool do_null = !not_null;\n+  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool do_update = !TypeEntries::is_type_unknown(current_klass) && !exact_klass_set;\n+\n+  assert(do_null || do_update, \"why are we here?\");\n+  assert(!TypeEntries::was_null_seen(current_klass) || do_update, \"why are we here?\");\n+  assert_different_registers(tmp, t0, t1, mdo_addr.base());\n+\n+  __ verify_oop(obj);\n+\n+  if (tmp != obj) {\n+    __ mv(tmp, obj);\n+  }\n+  if (do_null) {\n+    check_null(tmp, update, current_klass, mdo_addr, do_update, next);\n+#ifdef ASSERT\n+  } else {\n+    __ bnez(tmp, update);\n+    __ stop(\"unexpected null obj\");\n+#endif\n+  }\n+\n+  __ bind(update);\n+\n+  if (do_update) {\n+#ifdef ASSERT\n+    if (exact_klass != NULL) {\n+      check_exact_klass(tmp, exact_klass);\n+    }\n+#endif\n+    if (!no_conflict) {\n+      check_conflict(exact_klass, current_klass, tmp, next, none, mdo_addr);\n+    } else {\n+      check_no_conflict(exact_klass, current_klass, tmp, mdo_addr, next);\n+    }\n+\n+    __ bind(next);\n+  }\n+  COMMENT(\"} emit_profile_type\");\n+}\n+\n+void LIR_Assembler::align_backward_branch_target() { }\n+\n+void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {\n+  \/\/ tmp must be unused\n+  assert(tmp->is_illegal(), \"wasting a register if tmp is allocated\");\n+\n+  if (left->is_single_cpu()) {\n+    assert(dest->is_single_cpu(), \"expect single result reg\");\n+    __ negw(dest->as_register(), left->as_register());\n+  } else if (left->is_double_cpu()) {\n+    assert(dest->is_double_cpu(), \"expect double result reg\");\n+    __ neg(dest->as_register_lo(), left->as_register_lo());\n+  } else if (left->is_single_fpu()) {\n+    assert(dest->is_single_fpu(), \"expect single float result reg\");\n+    __ fneg_s(dest->as_float_reg(), left->as_float_reg());\n+  } else {\n+    assert(left->is_double_fpu(), \"expect double float operand reg\");\n+    assert(dest->is_double_fpu(), \"expect double float result reg\");\n+    __ fneg_d(dest->as_double_reg(), left->as_double_reg());\n+  }\n+}\n+\n+\n+void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {\n+#if INCLUDE_SHENANDOAHGC\n+  if (UseShenandoahGC && patch_code != lir_patch_none) {\n+    deoptimize_trap(info);\n+    return;\n+  }\n+#endif\n+\n+  assert(patch_code == lir_patch_none, \"Patch code not supported\");\n+  LIR_Address* adr = addr->as_address_ptr();\n+  Register dst = dest->as_register_lo();\n+\n+  assert_different_registers(dst, t0);\n+  if (adr->base()->is_valid() && dst == adr->base()->as_pointer_register() && (!adr->index()->is_cpu_register())) {\n+    int scale = adr->scale();\n+    intptr_t offset = adr->disp();\n+    LIR_Opr index_op = adr->index();\n+    if (index_op->is_constant()) {\n+      offset += ((intptr_t)index_op->as_constant_ptr()->as_jint()) << scale;\n+    }\n+\n+    if (!is_imm_in_range(offset, 12, 0)) {\n+      __ la(t0, as_Address(adr));\n+      __ mv(dst, t0);\n+      return;\n+    }\n+  }\n+\n+  __ la(dst, as_Address(adr));\n+}\n+\n+\n+void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {\n+  assert(!tmp->is_valid(), \"don't need temporary\");\n+\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb != NULL) {\n+    __ far_call(RuntimeAddress(dest));\n+  } else {\n+    int32_t offset = 0;\n+    __ la_patchable(t0, RuntimeAddress(dest), offset);\n+    __ jalr(x1, t0, offset);\n+  }\n+\n+  if (info != NULL) {\n+    add_call_info_here(info);\n+  }\n+}\n+\n+void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {\n+  if (dest->is_address() || src->is_address()) {\n+    move_op(src, dest, type, lir_patch_none, info, \/* pop_fpu_stack *\/ false, \/*unaligned*\/ false, \/* wide *\/ false);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ emit run-time assertion\n+void LIR_Assembler::emit_assert(LIR_OpAssert* op) {\n+  assert(op->code() == lir_assert, \"must be\");\n+\n+  Label ok;\n+  if (op->in_opr1()->is_valid()) {\n+    assert(op->in_opr2()->is_valid(), \"both operands must be valid\");\n+    bool is_unordered = false;\n+    LIR_Condition cond = op->condition();\n+    emit_branch(cond, op->in_opr1(), op->in_opr2(), ok, \/* is_far *\/ false,\n+                \/* is_unordered *\/(cond == lir_cond_greaterEqual || cond == lir_cond_greater) ? false : true);\n+  } else {\n+    assert(op->in_opr2()->is_illegal(), \"both operands must be illegal\");\n+    assert(op->condition() == lir_cond_always, \"no other conditions allowed\");\n+  }\n+\n+  if (op->halt()) {\n+    const char* str = __ code_string(op->msg());\n+    __ stop(str);\n+  } else {\n+    breakpoint();\n+  }\n+  __ bind(ok);\n+}\n+#endif\n+\n+#ifndef PRODUCT\n+#define COMMENT(x)   do { __ block_comment(x); } while (0)\n+#else\n+#define COMMENT(x)\n+#endif\n+\n+void LIR_Assembler::membar() {\n+  COMMENT(\"membar\");\n+  __ membar(MacroAssembler::AnyAny);\n+}\n+\n+void LIR_Assembler::membar_acquire() {\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+}\n+\n+void LIR_Assembler::membar_release() {\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+}\n+\n+void LIR_Assembler::membar_loadload() {\n+  __ membar(MacroAssembler::LoadLoad);\n+}\n+\n+void LIR_Assembler::membar_storestore() {\n+  __ membar(MacroAssembler::StoreStore);\n+}\n+\n+void LIR_Assembler::membar_loadstore() { __ membar(MacroAssembler::LoadStore); }\n+\n+void LIR_Assembler::membar_storeload() { __ membar(MacroAssembler::StoreLoad); }\n+\n+void LIR_Assembler::on_spin_wait() {\n+  Unimplemented();\n+}\n+\n+void LIR_Assembler::get_thread(LIR_Opr result_reg) {\n+  __ mv(result_reg->as_register(), xthread);\n+}\n+\n+void LIR_Assembler::peephole(LIR_List *lir) {}\n+\n+void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {\n+  Address addr = as_Address(src->as_address_ptr());\n+  BasicType type = src->type();\n+  bool is_oop = is_reference_type(type);\n+\n+  get_op(type);\n+\n+  switch (code) {\n+    case lir_xadd:\n+      {\n+        RegisterOrConstant inc;\n+        Register tmp = as_reg(tmp_op);\n+        Register dst = as_reg(dest);\n+        if (data->is_constant()) {\n+          inc = RegisterOrConstant(as_long(data));\n+          assert_different_registers(dst, addr.base(), tmp);\n+          assert_different_registers(tmp, t0);\n+        } else {\n+          inc = RegisterOrConstant(as_reg(data));\n+          assert_different_registers(inc.as_register(), dst, addr.base(), tmp);\n+        }\n+        __ la(tmp, addr);\n+        (_masm->*add)(dst, inc, tmp);\n+        break;\n+      }\n+    case lir_xchg:\n+      {\n+        Register tmp = tmp_op->as_register();\n+        Register obj = as_reg(data);\n+        Register dst = as_reg(dest);\n+        if (is_oop && UseCompressedOops) {\n+          __ encode_heap_oop(t0, obj);\n+          obj = t0;\n+        }\n+        assert_different_registers(obj, addr.base(), tmp, dst);\n+        __ la(tmp, addr);\n+        (_masm->*xchg)(dst, obj, tmp);\n+        if (is_oop && UseCompressedOops) {\n+          __ decode_heap_oop(dst);\n+        }\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  __ membar(MacroAssembler::AnyAny);\n+}\n+\n+int LIR_Assembler::array_element_size(BasicType type) const {\n+  int elem_size = type2aelembytes(type);\n+  return exact_log2(elem_size);\n+}\n+\n+\/\/ helper functions which checks for overflow and sets bailout if it\n+\/\/ occurs.  Always returns a valid embeddable pointer but in the\n+\/\/ bailout case the pointer won't be to unique storage.\n+address LIR_Assembler::float_constant(float f) {\n+  address const_addr = __ float_constant(f);\n+  if (const_addr == NULL) {\n+    bailout(\"const section overflow\");\n+    return __ code()->consts()->start();\n+  } else {\n+    return const_addr;\n+  }\n+}\n+\n+address LIR_Assembler::double_constant(double d) {\n+  address const_addr = __ double_constant(d);\n+  if (const_addr == NULL) {\n+    bailout(\"const section overflow\");\n+    return __ code()->consts()->start();\n+  } else {\n+    return const_addr;\n+  }\n+}\n+\n+address LIR_Assembler::int_constant(jlong n) {\n+  address const_addr = __ long_constant(n);\n+  if (const_addr == NULL) {\n+    bailout(\"const section overflow\");\n+    return __ code()->consts()->start();\n+  } else {\n+    return const_addr;\n+  }\n+}\n+\n+void LIR_Assembler::set_24bit_FPU() { Unimplemented(); }\n+\n+void LIR_Assembler::reset_FPU() { Unimplemented(); }\n+\n+void LIR_Assembler::fpop() { Unimplemented(); }\n+\n+void LIR_Assembler::fxch(int i) { Unimplemented(); }\n+\n+void LIR_Assembler::fld(int i) { Unimplemented(); }\n+\n+void LIR_Assembler::ffree(int i) { Unimplemented(); }\n+\n+void LIR_Assembler::casw(Register addr, Register newval, Register cmpval) {\n+  __ cmpxchg(addr, cmpval, newval, Assembler::int32, Assembler::aq \/* acquire *\/,\n+             Assembler::rl \/* release *\/, t0, true \/* result as bool *\/);\n+  __ seqz(t0, t0); \/\/ cmpxchg not equal, set t0 to 1\n+  __ membar(MacroAssembler::AnyAny);\n+}\n+\n+void LIR_Assembler::caswu(Register addr, Register newval, Register cmpval) {\n+  __ cmpxchg(addr, cmpval, newval, Assembler::uint32, Assembler::aq \/* acquire *\/,\n+             Assembler::rl \/* release *\/, t0, true \/* result as bool *\/);\n+  __ seqz(t0, t0); \/\/ cmpxchg not equal, set t0 to 1\n+  __ membar(MacroAssembler::AnyAny);\n+}\n+\n+void LIR_Assembler::casl(Register addr, Register newval, Register cmpval) {\n+  __ cmpxchg(addr, cmpval, newval, Assembler::int64, Assembler::aq \/* acquire *\/,\n+             Assembler::rl \/* release *\/, t0, true \/* result as bool *\/);\n+  __ seqz(t0, t0); \/\/ cmpxchg not equal, set t0 to 1\n+  __ membar(MacroAssembler::AnyAny);\n+}\n+\n+void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {\n+  address target = NULL;\n+\n+  switch (patching_id(info)) {\n+    case PatchingStub::access_field_id:\n+      target = Runtime1::entry_for(Runtime1::access_field_patching_id);\n+      break;\n+    case PatchingStub::load_klass_id:\n+      target = Runtime1::entry_for(Runtime1::load_klass_patching_id);\n+      break;\n+    case PatchingStub::load_mirror_id:\n+      target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);\n+      break;\n+    case PatchingStub::load_appendix_id:\n+      target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);\n+      break;\n+    default: ShouldNotReachHere();\n+  }\n+\n+  __ far_call(RuntimeAddress(target));\n+  add_call_info_here(info);\n+}\n+\n+void LIR_Assembler::check_exact_klass(Register tmp, ciKlass* exact_klass) {\n+  Label ok;\n+  __ load_klass(tmp, tmp);\n+  __ mov_metadata(t0, exact_klass->constant_encoding());\n+  __ beq(tmp, t0, ok);\n+  __ stop(\"exact klass and actual klass differ\");\n+  __ bind(ok);\n+}\n+\n+void LIR_Assembler::get_op(BasicType type) {\n+  switch (type) {\n+    case T_INT:\n+      xchg = &MacroAssembler::atomic_xchgalw;\n+      add = &MacroAssembler::atomic_addalw;\n+      break;\n+    case T_LONG:\n+      xchg = &MacroAssembler::atomic_xchgal;\n+      add = &MacroAssembler::atomic_addal;\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+      if (UseCompressedOops) {\n+        xchg = &MacroAssembler::atomic_xchgalwu;\n+        add = &MacroAssembler::atomic_addalw;\n+      } else {\n+        xchg = &MacroAssembler::atomic_xchgal;\n+        add = &MacroAssembler::atomic_addal;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ emit_opTypeCheck sub functions\n+void LIR_Assembler::typecheck_lir_store(LIR_OpTypeCheck* op, bool should_profile) {\n+  Register value = op->object()->as_register();\n+  Register array = op->array()->as_register();\n+  Register k_RInfo = op->tmp1()->as_register();\n+  Register klass_RInfo = op->tmp2()->as_register();\n+  Register Rtmp1 = op->tmp3()->as_register();\n+\n+  CodeStub* stub = op->stub();\n+\n+  \/\/ check if it needs to be profiled\n+  ciMethodData* md = NULL;\n+  ciProfileData* data = NULL;\n+\n+  if (should_profile) {\n+    data_check(op, &md, &data);\n+  }\n+  Label profile_cast_success, profile_cast_failure, done;\n+  Label *success_target = should_profile ? &profile_cast_success : &done;\n+  Label *failure_target = should_profile ? &profile_cast_failure : stub->entry();\n+\n+  if (should_profile) {\n+    profile_object(md, data, value, klass_RInfo, &done);\n+  } else {\n+    __ beqz(value, done);\n+  }\n+\n+  add_debug_info_for_null_check_here(op->info_for_exception());\n+  __ load_klass(k_RInfo, array);\n+  __ load_klass(klass_RInfo, value);\n+\n+  lir_store_slowcheck(k_RInfo, klass_RInfo, Rtmp1, success_target, failure_target);\n+\n+  \/\/ fall through to the success case\n+  if (should_profile) {\n+    Register mdo = klass_RInfo;\n+    Register recv = k_RInfo;\n+    __ bind(profile_cast_success);\n+    __ mov_metadata(mdo, md->constant_encoding());\n+    __ load_klass(recv, value);\n+    type_profile_helper(mdo, md, data, recv, &done);\n+    __ j(done);\n+\n+    __ bind(profile_cast_failure);\n+    __ mov_metadata(mdo, md->constant_encoding());\n+    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+    __ ld(t1, counter_addr);\n+    __ addi(t1, t1, -DataLayout::counter_increment);\n+    __ sd(t1, counter_addr);\n+    __ j(*stub->entry());\n+  }\n+\n+  __ bind(done);\n+}\n+\n+void LIR_Assembler::add_debug_info_for_branch(address adr, CodeEmitInfo* info) {\n+  _masm->code_section()->relocate(adr, relocInfo::poll_type);\n+  int pc_offset = code_offset();\n+  flush_debug_info(pc_offset);\n+  info->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  if (info->exception_handlers() != NULL) {\n+    compilation()->add_exception_handlers_for_pco(pc_offset, info->exception_handlers());\n+  }\n+}\n+\n+void LIR_Assembler::type_profile(Register obj, ciMethodData* md, Register klass_RInfo, Register k_RInfo,\n+                                 ciProfileData* data, Label* success, Label* failure,\n+                                 Label& profile_cast_success, Label& profile_cast_failure) {\n+  Register mdo = klass_RInfo;\n+  Register recv = k_RInfo;\n+  __ bind(profile_cast_success);\n+  __ mov_metadata(mdo, md->constant_encoding());\n+  __ load_klass(recv, obj);\n+  Label update_done;\n+  type_profile_helper(mdo, md, data, recv, success);\n+  __ j(*success);\n+\n+  __ bind(profile_cast_failure);\n+  __ mov_metadata(mdo, md->constant_encoding());\n+  Address counter_addr = __ form_address(t1, mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+  __ ld(t0, counter_addr);\n+  __ addi(t0, t0, -DataLayout::counter_increment);\n+  __ sd(t0, counter_addr);\n+  __ j(*failure);\n+}\n+\n+void LIR_Assembler::lir_store_slowcheck(Register k_RInfo, Register klass_RInfo, Register Rtmp1,\n+                                        Label* success_target, Label* failure_target) {\n+  \/\/ get instance klass (it's already uncompressed)\n+  __ ld(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));\n+  \/\/ perform the fast part of the checking logic\n+  __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+  \/\/ call out-of-line instance of __ check_klass_subtype_slow_path(...)\n+  __ addi(sp, sp, -2 * wordSize); \/\/ 2: store k_RInfo and klass_RInfo\n+  __ sd(klass_RInfo, Address(sp, wordSize));  \/\/ sub klass\n+  __ sd(k_RInfo, Address(sp, 0));             \/\/ super klass\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));\n+  \/\/ load result to k_RInfo\n+  __ ld(k_RInfo, Address(sp, 0));\n+  __ addi(sp, sp, 2 * wordSize); \/\/ 2: pop out k_RInfo and klass_RInfo\n+  \/\/ result is a boolean\n+  __ beqz(k_RInfo, *failure_target, \/* is_far *\/ true);\n+}\n+\n+void LIR_Assembler::const2reg_helper(LIR_Opr src) {\n+  switch (src->as_constant_ptr()->type()) {\n+    case T_INT:\n+    case T_ADDRESS:\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_METADATA:\n+        const2reg(src, FrameMap::t0_opr, lir_patch_none, NULL);\n+        break;\n+    case T_LONG:\n+        const2reg(src, FrameMap::t0_long_opr, lir_patch_none, NULL);\n+        break;\n+    case T_FLOAT:\n+    case T_DOUBLE:\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::logic_op_reg32(Register dst, Register left, Register right, LIR_Code code) {\n+  switch (code) {\n+    case lir_logic_and: __ andrw(dst, left, right); break;\n+    case lir_logic_or:  __ orrw (dst, left, right); break;\n+    case lir_logic_xor: __ xorrw(dst, left, right); break;\n+    default:            ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::logic_op_reg(Register dst, Register left, Register right, LIR_Code code) {\n+  switch (code) {\n+    case lir_logic_and: __ andr(dst, left, right); break;\n+    case lir_logic_or:  __ orr (dst, left, right); break;\n+    case lir_logic_xor: __ xorr(dst, left, right); break;\n+    default:            ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::logic_op_imm(Register dst, Register left, int right, LIR_Code code) {\n+  switch (code) {\n+    case lir_logic_and: __ andi(dst, left, right); break;\n+    case lir_logic_or:  __ ori (dst, left, right); break;\n+    case lir_logic_xor: __ xori(dst, left, right); break;\n+    default:            ShouldNotReachHere();\n+  }\n+}\n+\n+void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {\n+  assert(offset_from_rsp_in_words >= 0, \"invalid offset from rsp\");\n+  int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;\n+  assert(offset_from_rsp_in_bytes < frame_map()->reserved_argument_area_size(), \"invalid offset\");\n+  __ sd(r, Address(sp, offset_from_rsp_in_bytes));\n+}\n+\n+void LIR_Assembler::store_parameter(jint c, int offset_from_rsp_in_words) {\n+  assert(offset_from_rsp_in_words >= 0, \"invalid offset from rsp\");\n+  int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;\n+  assert(offset_from_rsp_in_bytes < frame_map()->reserved_argument_area_size(), \"invalid offset\");\n+  __ li(t0, c);\n+  __ sd(t0, Address(sp, offset_from_rsp_in_bytes));\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":2268,"deletions":0,"binary":false,"changes":2268,"status":"added"},{"patch":"@@ -0,0 +1,133 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_LIRASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_C1_LIRASSEMBLER_RISCV_HPP\n+\n+\/\/ ArrayCopyStub needs access to bailout\n+friend class ArrayCopyStub;\n+\n+private:\n+\n+#include \"c1_LIRAssembler_arith_riscv.hpp\"\n+#include \"c1_LIRAssembler_arraycopy_riscv.hpp\"\n+\n+  int array_element_size(BasicType type) const;\n+\n+  static Register as_reg(LIR_Opr op) {\n+    return op->is_double_cpu() ? op->as_register_lo() : op->as_register();\n+  }\n+\n+  Address as_Address(LIR_Address* addr, Register tmp);\n+\n+  \/\/ helper functions which checks for overflow and sets bailout if it\n+  \/\/ occurs.  Always returns a valid embeddable pointer but in the\n+  \/\/ bailout case the pointer won't be to unique storage.\n+  address float_constant(float f);\n+  address double_constant(double d);\n+  address int_constant(jlong n);\n+\n+  \/\/ Ensure we have a valid Address (base + offset) to a stack-slot.\n+  Address stack_slot_address(int index, uint shift, int adjust = 0);\n+\n+  \/\/ Record the type of the receiver in ReceiverTypeData\n+  void type_profile_helper(Register mdo,\n+                           ciMethodData *md, ciProfileData *data,\n+                           Register recv, Label* update_done);\n+\n+  void add_debug_info_for_branch(address adr, CodeEmitInfo* info);\n+\n+  void casw(Register addr, Register newval, Register cmpval);\n+  void caswu(Register addr, Register newval, Register cmpval);\n+  void casl(Register addr, Register newval, Register cmpval);\n+\n+  void poll_for_safepoint(relocInfo::relocType rtype, CodeEmitInfo* info = NULL);\n+\n+  void deoptimize_trap(CodeEmitInfo *info);\n+\n+  enum {\n+    \/\/ See emit_static_call_stub for detail\n+    \/\/ CompiledStaticCall::to_interp_stub_size() (14) + CompiledStaticCall::to_trampoline_stub_size() (1 + 3 + address)\n+    _call_stub_size = 14 * NativeInstruction::instruction_size +\n+                      (NativeInstruction::instruction_size + NativeCallTrampolineStub::instruction_size),\n+    _call_aot_stub_size = 0,\n+    \/\/ See emit_exception_handler for detail\n+    \/\/ verify_not_null_oop + far_call + should_not_reach_here + invalidate_registers(DEBUG_ONLY)\n+    _exception_handler_size = DEBUG_ONLY(584) NOT_DEBUG(548), \/\/ or smaller\n+    \/\/ See emit_deopt_handler for detail\n+    \/\/ auipc (1) + far_jump (6 or 2)\n+    _deopt_handler_size = 1 * NativeInstruction::instruction_size +\n+                          6 * NativeInstruction::instruction_size \/\/ or smaller\n+  };\n+\n+  void check_conflict(ciKlass* exact_klass, intptr_t current_klass, Register tmp,\n+                      Label &next, Label &none, Address mdo_addr);\n+  void check_no_conflict(ciKlass* exact_klass, intptr_t current_klass, Register tmp, Address mdo_addr, Label &next);\n+\n+  void check_exact_klass(Register tmp, ciKlass* exact_klass);\n+\n+  void check_null(Register tmp, Label &update, intptr_t current_klass, Address mdo_addr, bool do_update, Label &next);\n+\n+  void (MacroAssembler::*add)(Register prev, RegisterOrConstant incr, Register addr);\n+  void (MacroAssembler::*xchg)(Register prev, Register newv, Register addr);\n+\n+  void get_op(BasicType type);\n+\n+  \/\/ emit_typecheck_helper sub functions\n+  void data_check(LIR_OpTypeCheck *op, ciMethodData **md, ciProfileData **data);\n+  void typecheck_helper_slowcheck(ciKlass* k, Register obj, Register Rtmp1,\n+                                  Register k_RInfo, Register klass_RInfo,\n+                                  Label* failure_target, Label* success_target);\n+  void profile_object(ciMethodData* md, ciProfileData* data, Register obj,\n+                      Register klass_RInfo, Label* obj_is_null);\n+  void typecheck_loaded(LIR_OpTypeCheck* op, ciKlass* k, Register k_RInfo);\n+\n+  \/\/ emit_opTypeCheck sub functions\n+  void typecheck_lir_store(LIR_OpTypeCheck* op, bool should_profile);\n+\n+  void type_profile(Register obj, ciMethodData* md, Register klass_RInfo, Register k_RInfo,\n+                    ciProfileData* data, Label* success, Label* failure,\n+                    Label& profile_cast_success, Label& profile_cast_failure);\n+\n+  void lir_store_slowcheck(Register k_RInfo, Register klass_RInfo, Register Rtmp1,\n+                           Label* success_target, Label* failure_target);\n+\n+  void const2reg_helper(LIR_Opr src);\n+\n+  void emit_branch(LIR_Condition cmp_flag, LIR_Opr cmp1, LIR_Opr cmp2, Label& label, bool is_far, bool is_unordered);\n+\n+  void logic_op_reg32(Register dst, Register left, Register right, LIR_Code code);\n+  void logic_op_reg(Register dst, Register left, Register right, LIR_Code code);\n+  void logic_op_imm(Register dst, Register left, int right, LIR_Code code);\n+\n+public:\n+\n+  void emit_cmove(LIR_Op4* op);\n+\n+  void store_parameter(Register r, int offset_from_rsp_in_words);\n+  void store_parameter(jint c, int offset_from_rsp_in_words);\n+\n+#endif \/\/ CPU_RISCV_C1_LIRASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.hpp","additions":133,"deletions":0,"binary":false,"changes":133,"status":"added"},{"patch":"@@ -0,0 +1,1094 @@\n+\/*\n+ * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"c1\/c1_Compilation.hpp\"\n+#include \"c1\/c1_FrameMap.hpp\"\n+#include \"c1\/c1_Instruction.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_LIRGenerator.hpp\"\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"c1\/c1_ValueStack.hpp\"\n+#include \"ci\/ciArray.hpp\"\n+#include \"ci\/ciObjArrayKlass.hpp\"\n+#include \"ci\/ciTypeArrayKlass.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+#ifdef ASSERT\n+#define __ gen()->lir(__FILE__, __LINE__)->\n+#else\n+#define __ gen()->lir()->\n+#endif\n+\n+\/\/ Item will be loaded into a byte register; Intel only\n+void LIRItem::load_byte_item() {\n+  load_item();\n+}\n+\n+\n+void LIRItem::load_nonconstant() {\n+  LIR_Opr r = value()->operand();\n+  if (r->is_constant()) {\n+    _result = r;\n+  } else {\n+    load_item();\n+  }\n+}\n+\n+\/\/--------------------------------------------------------------\n+\/\/               LIRGenerator\n+\/\/--------------------------------------------------------------\n+\n+\n+LIR_Opr LIRGenerator::exceptionOopOpr() { return FrameMap::r10_oop_opr; }\n+LIR_Opr LIRGenerator::exceptionPcOpr()  { return FrameMap::r13_opr; }\n+LIR_Opr LIRGenerator::divInOpr()        { Unimplemented(); return LIR_OprFact::illegalOpr; }\n+LIR_Opr LIRGenerator::divOutOpr()       { Unimplemented(); return LIR_OprFact::illegalOpr; }\n+LIR_Opr LIRGenerator::remOutOpr()       { Unimplemented(); return LIR_OprFact::illegalOpr; }\n+LIR_Opr LIRGenerator::shiftCountOpr()   { Unimplemented(); return LIR_OprFact::illegalOpr; }\n+LIR_Opr LIRGenerator::syncLockOpr()     { return new_register(T_INT); }\n+LIR_Opr LIRGenerator::syncTempOpr()     { return FrameMap::r10_opr; }\n+LIR_Opr LIRGenerator::getThreadTemp()   { return LIR_OprFact::illegalOpr; }\n+\n+\n+LIR_Opr LIRGenerator::result_register_for(ValueType* type, bool callee) {\n+  LIR_Opr opr;\n+  switch (type->tag()) {\n+    case intTag:     opr = FrameMap::r10_opr;          break;\n+    case objectTag:  opr = FrameMap::r10_oop_opr;      break;\n+    case longTag:    opr = FrameMap::long10_opr;       break;\n+    case floatTag:   opr = FrameMap::fpu10_float_opr;  break;\n+    case doubleTag:  opr = FrameMap::fpu10_double_opr; break;\n+\n+    case addressTag: \/\/ fall through\n+    default:\n+      ShouldNotReachHere();\n+      return LIR_OprFact::illegalOpr;\n+  }\n+\n+  assert(opr->type_field() == as_OprType(as_BasicType(type)), \"type mismatch\");\n+  return opr;\n+}\n+\n+\n+LIR_Opr LIRGenerator::rlock_byte(BasicType type) {\n+  LIR_Opr reg = new_register(T_INT);\n+  set_vreg_flag(reg, LIRGenerator::byte_reg);\n+  return reg;\n+}\n+\n+\/\/--------- loading items into registers --------------------------------\n+\n+\n+bool LIRGenerator::can_store_as_constant(Value v, BasicType type) const {\n+  if (v->type()->as_IntConstant() != NULL) {\n+    return v->type()->as_IntConstant()->value() == 0;\n+  } else if (v->type()->as_LongConstant() != NULL) {\n+    return v->type()->as_LongConstant()->value() == 0;\n+  } else if (v->type()->as_ObjectConstant() != NULL) {\n+    return v->type()->as_ObjectConstant()->value()->is_null_object();\n+  } else if (v->type()->as_FloatConstant() != NULL) {\n+    return jint_cast(v->type()->as_FloatConstant()->value()) == 0.0f;\n+  } else if (v->type()->as_DoubleConstant() != NULL) {\n+    return jlong_cast(v->type()->as_DoubleConstant()->value()) == 0.0;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::can_inline_as_constant(Value v) const {\n+  if (v->type()->as_IntConstant() != NULL) {\n+    int value = v->type()->as_IntConstant()->value();\n+    \/\/ \"-value\" must be defined for value may be used for sub\n+    return Assembler::operand_valid_for_add_immediate(value) &&\n+           Assembler::operand_valid_for_add_immediate(- value);\n+  } else if (v->type()->as_ObjectConstant() != NULL) {\n+    return v->type()->as_ObjectConstant()->value()->is_null_object();\n+  } else if (v->type()->as_LongConstant() != NULL) {\n+    long value = v->type()->as_LongConstant()->value();\n+    \/\/ \"-value\" must be defined for value may be used for sub\n+    return Assembler::operand_valid_for_add_immediate(value) &&\n+           Assembler::operand_valid_for_add_immediate(- value);\n+  } else if (v->type()->as_FloatConstant() != NULL) {\n+    return v->type()->as_FloatConstant()->value() == 0.0f;\n+  } else if (v->type()->as_DoubleConstant() != NULL) {\n+    return v->type()->as_DoubleConstant()->value() == 0.0;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::can_inline_as_constant(LIR_Const* c) const {\n+  if (c->as_constant() != NULL) {\n+    long constant = 0;\n+    switch (c->type()) {\n+      case T_INT:  constant = c->as_jint();   break;\n+      case T_LONG: constant = c->as_jlong();  break;\n+      default:     return false;\n+    }\n+    \/\/ \"-constant\" must be defined for c may be used for sub\n+    return Assembler::operand_valid_for_add_immediate(constant) &&\n+           Assembler::operand_valid_for_add_immediate(- constant);\n+  }\n+  return false;\n+}\n+\n+LIR_Opr LIRGenerator::safepoint_poll_register() {\n+  return LIR_OprFact::illegalOpr;\n+}\n+\n+LIR_Address* LIRGenerator::generate_address(LIR_Opr base, LIR_Opr index,\n+                                            int shift, int disp, BasicType type) {\n+  assert(base->is_register(), \"must be\");\n+\n+  if (index->is_constant()) {\n+    LIR_Const *constant = index->as_constant_ptr();\n+    jlong c;\n+    if (constant->type() == T_INT) {\n+      c = (jlong(index->as_jint()) << shift) + disp;\n+    } else {\n+      assert(constant->type() == T_LONG, \"should be\");\n+      c = (index->as_jlong() << shift) + disp;\n+    }\n+    if ((jlong)((jint)c) == c) {\n+      return new LIR_Address(base, (jint)c, type);\n+    } else {\n+      LIR_Opr tmp = new_register(T_LONG);\n+      __ move(index, tmp);\n+      return new LIR_Address(base, tmp, type);\n+    }\n+  }\n+\n+  return new LIR_Address(base, index, (LIR_Address::Scale)shift, disp, type);\n+}\n+\n+LIR_Address* LIRGenerator::emit_array_address(LIR_Opr array_opr, LIR_Opr index_opr,\n+                                              BasicType type) {\n+  int offset_in_bytes = arrayOopDesc::base_offset_in_bytes(type);\n+  int elem_size = type2aelembytes(type);\n+  int shift = exact_log2(elem_size);\n+  return generate_address(array_opr, index_opr, shift, offset_in_bytes, type);\n+}\n+\n+LIR_Opr LIRGenerator::load_immediate(int x, BasicType type) {\n+  LIR_Opr r;\n+  switch (type) {\n+    case T_LONG:\n+      r = LIR_OprFact::longConst(x);\n+      break;\n+    case T_INT:\n+      r = LIR_OprFact::intConst(x);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      r = NULL;\n+  }\n+  return r;\n+}\n+\n+void LIRGenerator::increment_counter(address counter, BasicType type, int step) {\n+  LIR_Opr pointer = new_pointer_register();\n+  __ move(LIR_OprFact::intptrConst(counter), pointer);\n+  LIR_Address* addr = new LIR_Address(pointer, type);\n+  increment_counter(addr, step);\n+}\n+\n+void LIRGenerator::increment_counter(LIR_Address* addr, int step) {\n+  LIR_Opr reg = new_register(addr->type());\n+  __ load(addr, reg);\n+  __ add(reg, load_immediate(step, addr->type()), reg);\n+  __ store(reg, addr);\n+}\n+\n+void LIRGenerator::cmp_mem_int(LIR_Condition condition, LIR_Opr base, int disp, int c, CodeEmitInfo* info) {\n+  LIR_Opr reg = new_register(T_INT);\n+  __ load(generate_address(base, disp, T_INT), reg, info);\n+  __ cmp(condition, reg, LIR_OprFact::intConst(c));\n+}\n+\n+void LIRGenerator::cmp_reg_mem(LIR_Condition condition, LIR_Opr reg, LIR_Opr base, int disp, BasicType type, CodeEmitInfo* info) {\n+  LIR_Opr reg1 = new_register(T_INT);\n+  __ load(generate_address(base, disp, type), reg1, info);\n+  __ cmp(condition, reg, reg1);\n+}\n+\n+bool LIRGenerator::strength_reduce_multiply(LIR_Opr left, jint c, LIR_Opr result, LIR_Opr tmp) {\n+  if (tmp->is_valid() && c > 0 && c < max_jint) {\n+    if (is_power_of_2(c - 1)) {\n+      __ shift_left(left, exact_log2(c - 1), tmp);\n+      __ add(tmp, left, result);\n+      return true;\n+    } else if (is_power_of_2(c + 1)) {\n+      __ shift_left(left, exact_log2(c + 1), tmp);\n+      __ sub(tmp, left, result);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void LIRGenerator::store_stack_parameter (LIR_Opr item, ByteSize offset_from_sp) {\n+  BasicType type = item->type();\n+  __ store(item, new LIR_Address(FrameMap::sp_opr, in_bytes(offset_from_sp), type));\n+}\n+\n+void LIRGenerator::array_store_check(LIR_Opr value, LIR_Opr array, CodeEmitInfo* store_check_info,\n+                                     ciMethod* profiled_method, int profiled_bci) {\n+    LIR_Opr tmp1 = new_register(objectType);\n+    LIR_Opr tmp2 = new_register(objectType);\n+    LIR_Opr tmp3 = new_register(objectType);\n+    __ store_check(value, array, tmp1, tmp2, tmp3, store_check_info, profiled_method, profiled_bci);\n+}\n+\n+\/\/----------------------------------------------------------------------\n+\/\/             visitor functions\n+\/\/----------------------------------------------------------------------\n+\n+void LIRGenerator::do_MonitorEnter(MonitorEnter* x) {\n+  assert(x->is_pinned(), \"\");\n+  LIRItem obj(x->obj(), this);\n+  obj.load_item();\n+\n+  set_no_result(x);\n+\n+  \/\/ \"lock\" stores the address of the monitor stack slot, so this is not an oop\n+  LIR_Opr lock = new_register(T_INT);\n+  \/\/ Need a scratch register for biased locking\n+  LIR_Opr scratch = LIR_OprFact::illegalOpr;\n+  if (UseBiasedLocking) {\n+    scratch = new_register(T_INT);\n+  }\n+\n+  CodeEmitInfo* info_for_exception = NULL;\n+  if (x->needs_null_check()) {\n+    info_for_exception = state_for(x);\n+  }\n+  \/\/ this CodeEmitInfo must not have the xhandlers because here the\n+  \/\/ object is already locked (xhandlers expect object to be unlocked)\n+  CodeEmitInfo* info = state_for(x, x->state(), true);\n+  monitor_enter(obj.result(), lock, syncTempOpr(), scratch,\n+                x->monitor_no(), info_for_exception, info);\n+}\n+\n+void LIRGenerator::do_MonitorExit(MonitorExit* x) {\n+  assert(x->is_pinned(), \"\");\n+\n+  LIRItem obj(x->obj(), this);\n+  obj.dont_load_item();\n+\n+  LIR_Opr lock = new_register(T_INT);\n+  LIR_Opr obj_temp = new_register(T_INT);\n+  set_no_result(x);\n+  monitor_exit(obj_temp, lock, syncTempOpr(), LIR_OprFact::illegalOpr, x->monitor_no());\n+}\n+\n+\/\/ neg\n+void LIRGenerator::do_NegateOp(NegateOp* x) {\n+  LIRItem from(x->x(), this);\n+  from.load_item();\n+  LIR_Opr result = rlock_result(x);\n+  __ negate(from.result(), result);\n+}\n+\n+\/\/ for  _fadd, _fmul, _fsub, _fdiv, _frem\n+\/\/      _dadd, _dmul, _dsub, _ddiv, _drem\n+void LIRGenerator::do_ArithmeticOp_FPU(ArithmeticOp* x) {\n+  LIRItem left(x->x(), this);\n+  LIRItem right(x->y(), this);\n+\n+  if (x->op() == Bytecodes::_frem || x->op() == Bytecodes::_drem) {\n+\n+    \/\/ float remainder is implemented as a direct call into the runtime\n+    BasicTypeList signature(2);\n+    if (x->op() == Bytecodes::_frem) {\n+      signature.append(T_FLOAT);\n+      signature.append(T_FLOAT);\n+    } else {\n+      signature.append(T_DOUBLE);\n+      signature.append(T_DOUBLE);\n+    }\n+    CallingConvention* cc = frame_map()->c_calling_convention(&signature);\n+\n+    const LIR_Opr result_reg = result_register_for(x->type());\n+\n+    left.load_item();\n+    __ move(left.result(), cc->at(0));\n+    right.load_item_force(cc->at(1));\n+\n+    address entry;\n+    if (x->op() == Bytecodes::_frem) {\n+      entry = CAST_FROM_FN_PTR(address, SharedRuntime::frem);\n+    } else {\n+      entry = CAST_FROM_FN_PTR(address, SharedRuntime::drem);\n+    }\n+\n+    LIR_Opr result = rlock_result(x);\n+    __ call_runtime_leaf(entry, getThreadTemp(), result_reg, cc->args());\n+    __ move(result_reg, result);\n+\n+    return;\n+  }\n+\n+  if (!left.is_register()) {\n+    left.load_item();\n+  }\n+  \/\/ Always load right hand side.\n+  right.load_item();\n+\n+  LIR_Opr reg = rlock(x);\n+  LIR_Opr tmp = LIR_OprFact::illegalOpr;\n+  if (x->is_strictfp() && (x->op() == Bytecodes::_dmul || x->op() == Bytecodes::_ddiv)) {\n+    tmp = new_register(T_DOUBLE);\n+  }\n+\n+  arithmetic_op_fpu(x->op(), reg, left.result(), right.result(), x->is_strictfp());\n+\n+  set_result(x, round_item(reg));\n+}\n+\n+\/\/ for  _ladd, _lmul, _lsub, _ldiv, _lrem\n+void LIRGenerator::do_ArithmeticOp_Long(ArithmeticOp* x) {\n+\n+  \/\/ missing test if instr is commutative and if we should swap\n+  LIRItem left(x->x(), this);\n+  LIRItem right(x->y(), this);\n+\n+  if (x->op() == Bytecodes::_ldiv || x->op() == Bytecodes::_lrem) {\n+\n+    left.load_item();\n+\n+    bool need_zero_check = true;\n+    if (right.is_constant()) {\n+      jlong c = right.get_jlong_constant();\n+      \/\/ no need to do div-by-zero check if the divisor is a non-zero constant\n+      if (c != 0) { need_zero_check = false; }\n+      \/\/ do not load right if the divisor is a power-of-2 constant\n+      if (c > 0 && is_power_of_2_long(c)) {\n+        right.dont_load_item();\n+      } else {\n+        right.load_item();\n+      }\n+    } else {\n+      right.load_item();\n+    }\n+    if (need_zero_check) {\n+      CodeEmitInfo* info = state_for(x);\n+      __ cmp(lir_cond_equal, right.result(), LIR_OprFact::longConst(0));\n+      __ branch(lir_cond_equal, T_LONG, new DivByZeroStub(info));\n+    }\n+\n+    rlock_result(x);\n+    switch (x->op()) {\n+      case Bytecodes::_lrem:\n+        __ rem(left.result(), right.result(), x->operand());\n+        break;\n+      case Bytecodes::_ldiv:\n+        __ div(left.result(), right.result(), x->operand());\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(x->op() == Bytecodes::_lmul || x->op() == Bytecodes::_ladd || x->op() == Bytecodes::_lsub,\n+           \"expect lmul, ladd or lsub\");\n+    \/\/ add, sub, mul\n+    left.load_item();\n+    if (!right.is_register()) {\n+      if (x->op() == Bytecodes::_lmul ||\n+          !right.is_constant() ||\n+          (x->op() == Bytecodes::_ladd &&\n+          !Assembler::operand_valid_for_add_immediate(right.get_jlong_constant())) ||\n+          (x->op() == Bytecodes::_lsub &&\n+          !Assembler::operand_valid_for_add_immediate(-right.get_jlong_constant()))) {\n+            right.load_item();\n+      } else { \/\/ add, sub\n+        assert(x->op() == Bytecodes::_ladd || x->op() == Bytecodes::_lsub, \"expected ladd or lsub\");\n+        \/\/ don't load constants to save register\n+        right.load_nonconstant();\n+      }\n+    }\n+    rlock_result(x);\n+    arithmetic_op_long(x->op(), x->operand(), left.result(), right.result(), NULL);\n+  }\n+}\n+\n+\/\/ for: _iadd, _imul, _isub, _idiv, _irem\n+void LIRGenerator::do_ArithmeticOp_Int(ArithmeticOp* x) {\n+\n+  \/\/ Test if instr is commutative and if we should swap\n+  LIRItem left(x->x(),  this);\n+  LIRItem right(x->y(), this);\n+  LIRItem* left_arg = &left;\n+  LIRItem* right_arg = &right;\n+  if (x->is_commutative() && left.is_stack() && right.is_register()) {\n+    \/\/ swap them if left is real stack (or cached) and right is real register(not cached)\n+    left_arg = &right;\n+    right_arg = &left;\n+  }\n+  left_arg->load_item();\n+  \/\/ do not need to load right, as we can handle stack and constants\n+  if (x->op() == Bytecodes::_idiv || x->op() == Bytecodes::_irem) {\n+\n+    rlock_result(x);\n+\n+    bool need_zero_check = true;\n+    if (right.is_constant()) {\n+      jint c = right.get_jint_constant();\n+      \/\/ no need to do div-by-zero check if the divisor is a non-zero constant\n+      if (c != 0) { need_zero_check = false; }\n+      \/\/ do not load right if the divisor is a power-of-2 constant\n+      if (c > 0 && is_power_of_2(c)) {\n+        right_arg->dont_load_item();\n+      } else {\n+        right_arg->load_item();\n+      }\n+    } else {\n+      right_arg->load_item();\n+    }\n+    if (need_zero_check) {\n+      CodeEmitInfo* info = state_for(x);\n+      __ cmp(lir_cond_equal, right_arg->result(), LIR_OprFact::longConst(0));\n+      __ branch(lir_cond_equal, T_INT, new DivByZeroStub(info));\n+    }\n+\n+    LIR_Opr ill = LIR_OprFact::illegalOpr;\n+    if (x->op() == Bytecodes::_irem) {\n+      __ irem(left_arg->result(), right_arg->result(), x->operand(), ill, NULL);\n+    } else if (x->op() == Bytecodes::_idiv) {\n+      __ idiv(left_arg->result(), right_arg->result(), x->operand(), ill, NULL);\n+    }\n+\n+  } else if (x->op() == Bytecodes::_iadd || x->op() == Bytecodes::_isub) {\n+    if (right.is_constant() &&\n+        ((x->op() == Bytecodes::_iadd && !Assembler::operand_valid_for_add_immediate(right.get_jint_constant())) ||\n+         (x->op() == Bytecodes::_isub && !Assembler::operand_valid_for_add_immediate(-right.get_jint_constant())))) {\n+      right.load_nonconstant();\n+    } else {\n+      right.load_item();\n+    }\n+    rlock_result(x);\n+    arithmetic_op_int(x->op(), x->operand(), left_arg->result(), right_arg->result(), LIR_OprFact::illegalOpr);\n+  } else {\n+    assert (x->op() == Bytecodes::_imul, \"expect imul\");\n+    if (right.is_constant()) {\n+      jint c = right.get_jint_constant();\n+      if (c > 0 && c < max_jint && (is_power_of_2(c) || is_power_of_2(c - 1) || is_power_of_2(c + 1))) {\n+        right_arg->dont_load_item();\n+      } else {\n+        \/\/ Cannot use constant op.\n+        right_arg->load_item();\n+      }\n+    } else {\n+      right.load_item();\n+    }\n+    rlock_result(x);\n+    arithmetic_op_int(x->op(), x->operand(), left_arg->result(), right_arg->result(), new_register(T_INT));\n+  }\n+}\n+\n+void LIRGenerator::do_ArithmeticOp(ArithmeticOp* x) {\n+  \/\/ when an operand with use count 1 is the left operand, then it is\n+  \/\/ likely that no move for 2-operand-LIR-form is necessary\n+  if (x->is_commutative() && x->y()->as_Constant() == NULL && x->x()->use_count() > x->y()->use_count()) {\n+    x->swap_operands();\n+  }\n+\n+  ValueTag tag = x->type()->tag();\n+  assert(x->x()->type()->tag() == tag && x->y()->type()->tag() == tag, \"wrong parameters\");\n+  switch (tag) {\n+    case floatTag:\n+    case doubleTag:  do_ArithmeticOp_FPU(x);  return;\n+    case longTag:    do_ArithmeticOp_Long(x); return;\n+    case intTag:     do_ArithmeticOp_Int(x);  return;\n+    default:         ShouldNotReachHere();    return;\n+  }\n+}\n+\n+\/\/ _ishl, _lshl, _ishr, _lshr, _iushr, _lushr\n+void LIRGenerator::do_ShiftOp(ShiftOp* x) {\n+  LIRItem value(x->x(), this);\n+  LIRItem count(x->y(), this);\n+\n+  value.load_item();\n+  if (count.is_constant()) {\n+    assert(count.type()->as_IntConstant() != NULL || count.type()->as_LongConstant() != NULL , \"should be\");\n+    count.dont_load_item();\n+  } else {\n+    count.load_item();\n+  }\n+\n+  LIR_Opr res = rlock_result(x);\n+  shift_op(x->op(), res, value.result(), count.result(), LIR_OprFact::illegalOpr);\n+}\n+\n+\n+\/\/ _iand, _land, _ior, _lor, _ixor, _lxor\n+void LIRGenerator::do_LogicOp(LogicOp* x) {\n+\n+  LIRItem left(x->x(),  this);\n+  LIRItem right(x->y(), this);\n+\n+  left.load_item();\n+  rlock_result(x);\n+  ValueTag tag = right.type()->tag();\n+  if (right.is_constant() &&\n+     ((tag == longTag && Assembler::operand_valid_for_add_immediate(right.get_jlong_constant())) ||\n+      (tag == intTag && Assembler::operand_valid_for_add_immediate(right.get_jint_constant()))))  {\n+    right.dont_load_item();\n+  } else {\n+    right.load_item();\n+  }\n+\n+  switch (x->op()) {\n+    case Bytecodes::_iand:  \/\/ fall through\n+    case Bytecodes::_land:\n+      __ logical_and(left.result(), right.result(), x->operand()); break;\n+    case Bytecodes::_ior:   \/\/ fall through\n+    case Bytecodes::_lor:\n+      __ logical_or(left.result(), right.result(), x->operand()); break;\n+    case Bytecodes::_ixor:  \/\/ fall through\n+    case Bytecodes::_lxor:\n+      __ logical_xor(left.result(), right.result(), x->operand()); break;\n+    default: Unimplemented();\n+  }\n+}\n+\n+\/\/ _lcmp, _fcmpl, _fcmpg, _dcmpl, _dcmpg\n+void LIRGenerator::do_CompareOp(CompareOp* x) {\n+  LIRItem left(x->x(), this);\n+  LIRItem right(x->y(), this);\n+  ValueTag tag = x->x()->type()->tag();\n+  if (tag == longTag) {\n+    left.set_destroys_register();\n+  }\n+  left.load_item();\n+  right.load_item();\n+  LIR_Opr reg = rlock_result(x);\n+\n+  if (x->x()->type()->is_float_kind()) {\n+    Bytecodes::Code code = x->op();\n+    __ fcmp2int(left.result(), right.result(), reg, (code == Bytecodes::_fcmpl || code == Bytecodes::_dcmpl));\n+  } else if (x->x()->type()->tag() == longTag) {\n+    __ lcmp2int(left.result(), right.result(), reg);\n+  } else {\n+    Unimplemented();\n+  }\n+}\n+\n+LIR_Opr LIRGenerator::atomic_cmpxchg(BasicType type, LIR_Opr addr, LIRItem& cmp_value, LIRItem& new_value) {\n+  LIR_Opr ill = LIR_OprFact::illegalOpr;  \/\/ for convenience\n+  new_value.load_item();\n+  cmp_value.load_item();\n+  LIR_Opr result = new_register(T_INT);\n+  if (is_reference_type(type)) {\n+    __ cas_obj(addr, cmp_value.result(), new_value.result(), new_register(T_INT), new_register(T_INT), result);\n+  } else if (type == T_INT) {\n+    __ cas_int(addr->as_address_ptr()->base(), cmp_value.result(), new_value.result(), ill, ill);\n+  } else if (type == T_LONG) {\n+    __ cas_long(addr->as_address_ptr()->base(), cmp_value.result(), new_value.result(), ill, ill);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  __ logical_xor(FrameMap::r5_opr, LIR_OprFact::intConst(1), result);\n+  return result;\n+}\n+\n+LIR_Opr LIRGenerator::atomic_xchg(BasicType type, LIR_Opr addr, LIRItem& value) {\n+  bool is_oop = is_reference_type(type);\n+  LIR_Opr result = new_register(type);\n+  value.load_item();\n+  assert(type == T_INT || is_oop LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  LIR_Opr tmp = new_register(T_INT);\n+  __ xchg(addr, value.result(), result, tmp);\n+  return result;\n+}\n+\n+LIR_Opr LIRGenerator::atomic_add(BasicType type, LIR_Opr addr, LIRItem& value) {\n+  LIR_Opr result = new_register(type);\n+  value.load_item();\n+  assert(type == T_INT LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  LIR_Opr tmp = new_register(T_INT);\n+  __ xadd(addr, value.result(), result, tmp);\n+  return result;\n+}\n+\n+void LIRGenerator::do_MathIntrinsic(Intrinsic* x) {\n+  assert(x->number_of_arguments() == 1 || (x->number_of_arguments() == 2 && x->id() == vmIntrinsics::_dpow),\n+         \"wrong type\");\n+\n+  switch (x->id()) {\n+    case vmIntrinsics::_dexp: \/\/ fall through\n+    case vmIntrinsics::_dlog: \/\/ fall through\n+    case vmIntrinsics::_dpow: \/\/ fall through\n+    case vmIntrinsics::_dcos: \/\/ fall through\n+    case vmIntrinsics::_dsin: \/\/ fall through\n+    case vmIntrinsics::_dtan: \/\/ fall through\n+    case vmIntrinsics::_dlog10:\n+      do_LibmIntrinsic(x);\n+      break;\n+    case vmIntrinsics::_dabs: \/\/ fall through\n+    case vmIntrinsics::_dsqrt: {\n+      assert(x->number_of_arguments() == 1, \"wrong type\");\n+      LIRItem value(x->argument_at(0), this);\n+      value.load_item();\n+      LIR_Opr dst = rlock_result(x);\n+\n+      switch (x->id()) {\n+        case vmIntrinsics::_dsqrt: {\n+          __ sqrt(value.result(), dst, LIR_OprFact::illegalOpr);\n+          break;\n+        }\n+        case vmIntrinsics::_dabs: {\n+          __ abs(value.result(), dst, LIR_OprFact::illegalOpr);\n+          break;\n+        }\n+        default:\n+          ShouldNotReachHere();\n+      }\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void LIRGenerator::do_LibmIntrinsic(Intrinsic* x) {\n+  LIRItem value(x->argument_at(0), this);\n+  value.set_destroys_register();\n+\n+  LIR_Opr calc_result = rlock_result(x);\n+  LIR_Opr result_reg = result_register_for(x->type());\n+\n+  CallingConvention* cc = NULL;\n+\n+  if (x->id() == vmIntrinsics::_dpow) {\n+    LIRItem value1(x->argument_at(1), this);\n+\n+    value1.set_destroys_register();\n+\n+    BasicTypeList signature(2);\n+    signature.append(T_DOUBLE);\n+    signature.append(T_DOUBLE);\n+    cc = frame_map()->c_calling_convention(&signature);\n+    value.load_item_force(cc->at(0));\n+    value1.load_item_force(cc->at(1));\n+  } else {\n+    BasicTypeList signature(1);\n+    signature.append(T_DOUBLE);\n+    cc = frame_map()->c_calling_convention(&signature);\n+    value.load_item_force(cc->at(0));\n+  }\n+\n+  switch (x->id()) {\n+    case vmIntrinsics::_dexp:\n+      if (StubRoutines::dexp() != NULL) { __ call_runtime_leaf(StubRoutines::dexp(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dexp), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dlog:\n+      if (StubRoutines::dlog() != NULL) {  __ call_runtime_leaf(StubRoutines::dlog(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dlog10:\n+      if (StubRoutines::dlog10() != NULL) { __ call_runtime_leaf(StubRoutines::dlog10(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog10), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dsin:\n+      if (StubRoutines::dsin() != NULL) { __ call_runtime_leaf(StubRoutines::dsin(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dsin), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dcos:\n+      if (StubRoutines::dcos() != NULL) {  __ call_runtime_leaf(StubRoutines::dcos(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcos), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dtan:\n+      if (StubRoutines::dtan() != NULL) { __ call_runtime_leaf(StubRoutines::dtan(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtan), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    case vmIntrinsics::_dpow:\n+      if (StubRoutines::dpow() != NULL) { __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args()); }\n+      else { __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dpow), getThreadTemp(), result_reg, cc->args()); }\n+      break;\n+    default:  ShouldNotReachHere();\n+  }\n+  __ move(result_reg, calc_result);\n+}\n+\n+\n+void LIRGenerator::do_ArrayCopy(Intrinsic* x) {\n+  assert(x->number_of_arguments() == 5, \"wrong type\");\n+\n+  \/\/ Make all state_for calls early since they can emit code\n+  CodeEmitInfo* info = state_for(x, x->state());\n+\n+  LIRItem src(x->argument_at(0), this);\n+  LIRItem src_pos(x->argument_at(1), this);\n+  LIRItem dst(x->argument_at(2), this);\n+  LIRItem dst_pos(x->argument_at(3), this);\n+  LIRItem length(x->argument_at(4), this);\n+\n+  \/\/ operands for arraycopy must use fixed registers, otherwise\n+  \/\/ LinearScan will fail allocation (because arraycopy always needs a\n+  \/\/ call)\n+\n+  \/\/ The java calling convention will give us enough registers\n+  \/\/ so that on the stub side the args will be perfect already.\n+  \/\/ On the other slow\/special case side we call C and the arg\n+  \/\/ positions are not similar enough to pick one as the best.\n+  \/\/ Also because the java calling convention is a \"shifted\" version\n+  \/\/ of the C convention we can process the java args trivially into C\n+  \/\/ args without worry of overwriting during the xfer\n+\n+  src.load_item_force     (FrameMap::as_oop_opr(j_rarg0));\n+  src_pos.load_item_force (FrameMap::as_opr(j_rarg1));\n+  dst.load_item_force     (FrameMap::as_oop_opr(j_rarg2));\n+  dst_pos.load_item_force (FrameMap::as_opr(j_rarg3));\n+  length.load_item_force  (FrameMap::as_opr(j_rarg4));\n+\n+  LIR_Opr tmp = FrameMap::as_opr(j_rarg5);\n+\n+  set_no_result(x);\n+\n+  int flags;\n+  ciArrayKlass* expected_type = NULL;\n+  arraycopy_helper(x, &flags, &expected_type);\n+\n+  __ arraycopy(src.result(), src_pos.result(), dst.result(), dst_pos.result(), length.result(), tmp,\n+               expected_type, flags, info); \/\/ does add_safepoint\n+}\n+\n+void LIRGenerator::do_update_CRC32(Intrinsic* x) {\n+  ShouldNotReachHere();\n+}\n+\n+void LIRGenerator::do_update_CRC32C(Intrinsic* x) {\n+  ShouldNotReachHere();\n+}\n+\n+void LIRGenerator::do_FmaIntrinsic(Intrinsic* x) {\n+  assert(x->number_of_arguments() == 3, \"wrong type\");\n+  assert(UseFMA, \"Needs FMA instructions support.\");\n+  LIRItem value(x->argument_at(0), this);\n+  LIRItem value1(x->argument_at(1), this);\n+  LIRItem value2(x->argument_at(2), this);\n+\n+  value.load_item();\n+  value1.load_item();\n+  value2.load_item();\n+\n+  LIR_Opr calc_input = value.result();\n+  LIR_Opr calc_input1 = value1.result();\n+  LIR_Opr calc_input2 = value2.result();\n+  LIR_Opr calc_result = rlock_result(x);\n+\n+  switch (x->id()) {\n+    case vmIntrinsics::_fmaD:   __ fmad(calc_input, calc_input1, calc_input2, calc_result); break;\n+    case vmIntrinsics::_fmaF:   __ fmaf(calc_input, calc_input1, calc_input2, calc_result); break;\n+    default:                    ShouldNotReachHere();\n+  }\n+}\n+\n+void LIRGenerator::do_vectorizedMismatch(Intrinsic* x) {\n+  fatal(\"vectorizedMismatch intrinsic is not implemented on this platform\");\n+}\n+\n+\/\/ _i2l, _i2f, _i2d, _l2i, _l2f, _l2d, _f2i, _f2l, _f2d, _d2i, _d2l, _d2f\n+\/\/ _i2b, _i2c, _i2s\n+void LIRGenerator::do_Convert(Convert* x) {\n+  LIRItem value(x->value(), this);\n+  value.load_item();\n+  LIR_Opr input = value.result();\n+  LIR_Opr result = rlock(x);\n+\n+  \/\/ arguments of lir_convert\n+  LIR_Opr conv_input = input;\n+  LIR_Opr conv_result = result;\n+\n+  __ convert(x->op(), conv_input, conv_result);\n+\n+  assert(result->is_virtual(), \"result must be virtual register\");\n+  set_result(x, result);\n+}\n+\n+void LIRGenerator::do_NewInstance(NewInstance* x) {\n+#ifndef PRODUCT\n+  if (PrintNotLoaded && !x->klass()->is_loaded()) {\n+    tty->print_cr(\"   ###class not loaded at new bci %d\", x->printable_bci());\n+  }\n+#endif\n+  CodeEmitInfo* info = state_for(x, x->state());\n+  LIR_Opr reg = result_register_for(x->type());\n+  new_instance(reg, x->klass(), x->is_unresolved(),\n+               FrameMap::r12_oop_opr,\n+               FrameMap::r15_oop_opr,\n+               FrameMap::r14_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::r13_metadata_opr,\n+               info);\n+  LIR_Opr result = rlock_result(x);\n+  __ move(reg, result);\n+}\n+\n+void LIRGenerator::do_NewTypeArray(NewTypeArray* x) {\n+  CodeEmitInfo* info = state_for(x, x->state());\n+\n+  LIRItem length(x->length(), this);\n+  length.load_item_force(FrameMap::r9_opr);\n+\n+  LIR_Opr reg = result_register_for(x->type());\n+  LIR_Opr tmp1 = FrameMap::r12_oop_opr;\n+  LIR_Opr tmp2 = FrameMap::r14_oop_opr;\n+  LIR_Opr tmp3 = FrameMap::r15_oop_opr;\n+  LIR_Opr tmp4 = reg;\n+  LIR_Opr klass_reg = FrameMap::r13_metadata_opr;\n+  LIR_Opr len = length.result();\n+  BasicType elem_type = x->elt_type();\n+\n+  __ metadata2reg(ciTypeArrayKlass::make(elem_type)->constant_encoding(), klass_reg);\n+\n+  CodeStub* slow_path = new NewTypeArrayStub(klass_reg, len, reg, info);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, elem_type, klass_reg, slow_path);\n+\n+  LIR_Opr result = rlock_result(x);\n+  __ move(reg, result);\n+}\n+\n+void LIRGenerator::do_NewObjectArray(NewObjectArray* x) {\n+  LIRItem length(x->length(), this);\n+  \/\/ in case of patching (i.e., object class is not yet loaded), we need to reexecute the instruction\n+  \/\/ and therefore provide the state before the parameters have been consumed\n+  CodeEmitInfo* patching_info = NULL;\n+  if (!x->klass()->is_loaded() || PatchALot) {\n+    patching_info =  state_for(x, x->state_before());\n+  }\n+\n+  CodeEmitInfo* info = state_for(x, x->state());\n+\n+  LIR_Opr reg = result_register_for(x->type());\n+  LIR_Opr tmp1 = FrameMap::r12_oop_opr;\n+  LIR_Opr tmp2 = FrameMap::r14_oop_opr;\n+  LIR_Opr tmp3 = FrameMap::r15_oop_opr;\n+  LIR_Opr tmp4 = reg;\n+  LIR_Opr klass_reg = FrameMap::r13_metadata_opr;\n+\n+  length.load_item_force(FrameMap::r9_opr);\n+  LIR_Opr len = length.result();\n+\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n+  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  if (obj == ciEnv::unloaded_ciobjarrayklass()) {\n+    BAILOUT(\"encountered unloaded_ciobjarrayklass due to out of memory error\");\n+  }\n+  klass2reg_with_patching(klass_reg, obj, patching_info);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+\n+  LIR_Opr result = rlock_result(x);\n+  __ move(reg, result);\n+}\n+\n+\n+void LIRGenerator::do_NewMultiArray(NewMultiArray* x) {\n+  Values* dims = x->dims();\n+  int i = dims->length();\n+  LIRItemList* items = new LIRItemList(i, i, NULL);\n+  while (i-- > 0) {\n+    LIRItem* size = new LIRItem(dims->at(i), this);\n+    items->at_put(i, size);\n+  }\n+\n+  \/\/ Evaluate state_for early since it may emit code.\n+  CodeEmitInfo* patching_info = NULL;\n+  if (!x->klass()->is_loaded() || PatchALot) {\n+    patching_info = state_for(x, x->state_before());\n+\n+    \/\/ Cannot re-use same xhandlers for multiple CodeEmitInfos, so\n+    \/\/ clone all handlers (NOTE: Usually this is handled transparently\n+    \/\/ by the CodeEmitInfo cloning logic in CodeStub constructors but\n+    \/\/ is done explicitly here because a stub isn't being used).\n+    x->set_exception_handlers(new XHandlers(x->exception_handlers()));\n+  }\n+  CodeEmitInfo* info = state_for(x, x->state());\n+\n+  i = dims->length();\n+  while (i-- > 0) {\n+    LIRItem* size = items->at(i);\n+    size->load_item();\n+\n+    store_stack_parameter(size->result(), in_ByteSize(i * BytesPerInt));\n+  }\n+\n+  LIR_Opr klass_reg = FrameMap::r10_metadata_opr;\n+  klass2reg_with_patching(klass_reg, x->klass(), patching_info);\n+\n+  LIR_Opr rank = FrameMap::r9_opr;\n+  __ move(LIR_OprFact::intConst(x->rank()), rank);\n+  LIR_Opr varargs = FrameMap::r12_opr;\n+  __ move(FrameMap::sp_opr, varargs);\n+  LIR_OprList* args = new LIR_OprList(3);\n+  args->append(klass_reg);\n+  args->append(rank);\n+  args->append(varargs);\n+  LIR_Opr reg = result_register_for(x->type());\n+  __ call_runtime(Runtime1::entry_for(Runtime1::new_multi_array_id),\n+                  LIR_OprFact::illegalOpr,\n+                  reg, args, info);\n+\n+  LIR_Opr result = rlock_result(x);\n+  __ move(reg, result);\n+}\n+\n+void LIRGenerator::do_BlockBegin(BlockBegin* x) {\n+  \/\/ nothing to do for now\n+}\n+\n+void LIRGenerator::do_CheckCast(CheckCast* x) {\n+  LIRItem obj(x->obj(), this);\n+\n+  CodeEmitInfo* patching_info = NULL;\n+  if (!x->klass()->is_loaded() ||\n+      (PatchALot && !x->is_incompatible_class_change_check() && !x->is_invokespecial_receiver_check())) {\n+    \/\/ must do this before locking the destination register as an oop register,\n+    \/\/ and before the obj is loaded (the latter is for deoptimization)\n+    patching_info = state_for(x, x->state_before());\n+  }\n+  obj.load_item();\n+\n+  \/\/ info for exceptions\n+  CodeEmitInfo* info_for_exception =\n+      (x->needs_exception_state() ? state_for(x) :\n+                                    state_for(x, x->state_before(), true \/*ignore_xhandler*\/ ));\n+\n+  CodeStub* stub = NULL;\n+  if (x->is_incompatible_class_change_check()) {\n+    assert(patching_info == NULL, \"can't patch this\");\n+    stub = new SimpleExceptionStub(Runtime1::throw_incompatible_class_change_error_id, LIR_OprFact::illegalOpr,\n+                                   info_for_exception);\n+  } else if (x->is_invokespecial_receiver_check()) {\n+    assert(patching_info == NULL, \"can't patch this\");\n+    stub = new DeoptimizeStub(info_for_exception,\n+                              Deoptimization::Reason_class_check,\n+                              Deoptimization::Action_none);\n+  } else {\n+    stub = new SimpleExceptionStub(Runtime1::throw_class_cast_exception_id, obj.result(), info_for_exception);\n+  }\n+  LIR_Opr reg = rlock_result(x);\n+  LIR_Opr tmp3 = LIR_OprFact::illegalOpr;\n+  if (!x->klass()->is_loaded() || UseCompressedClassPointers) {\n+    tmp3 = new_register(objectType);\n+  }\n+  __ checkcast(reg, obj.result(), x->klass(),\n+               new_register(objectType), new_register(objectType), tmp3,\n+               x->direct_compare(), info_for_exception, patching_info, stub,\n+               x->profiled_method(), x->profiled_bci());\n+}\n+\n+void LIRGenerator::do_InstanceOf(InstanceOf* x) {\n+  LIRItem obj(x->obj(), this);\n+\n+  \/\/ result and test object may not be in same register\n+  LIR_Opr reg = rlock_result(x);\n+  CodeEmitInfo* patching_info = NULL;\n+  if ((!x->klass()->is_loaded() || PatchALot)) {\n+    \/\/ must do this before locking the destination register as an oop register\n+    patching_info = state_for(x, x->state_before());\n+  }\n+  obj.load_item();\n+  LIR_Opr tmp3 = LIR_OprFact::illegalOpr;\n+  if (!x->klass()->is_loaded() || UseCompressedClassPointers) {\n+    tmp3 = new_register(objectType);\n+  }\n+  __ instanceof(reg, obj.result(), x->klass(),\n+                new_register(objectType), new_register(objectType), tmp3,\n+                x->direct_compare(), patching_info, x->profiled_method(), x->profiled_bci());\n+}\n+\n+void LIRGenerator::do_If(If* x) {\n+  \/\/ If should have two successors\n+  assert(x->number_of_sux() == 2, \"inconsistency\");\n+  ValueTag tag = x->x()->type()->tag();\n+  bool is_safepoint = x->is_safepoint();\n+\n+  If::Condition cond = x->cond();\n+\n+  LIRItem xitem(x->x(), this);\n+  LIRItem yitem(x->y(), this);\n+  LIRItem* xin = &xitem;\n+  LIRItem* yin = &yitem;\n+\n+  if (tag == longTag) {\n+    \/\/ for longs, only conditions \"eql\", \"neq\", \"lss\", \"geq\" are valid;\n+    \/\/ mirror for other conditions\n+    if (cond == If::gtr || cond == If::leq) {\n+      cond = Instruction::mirror(cond);\n+      xin = &yitem;\n+      yin = &xitem;\n+    }\n+    xin->set_destroys_register();\n+  }\n+  xin->load_item();\n+  yin->load_item();\n+\n+  set_no_result(x);\n+\n+  LIR_Opr left = xin->result();\n+  LIR_Opr right = yin->result();\n+\n+  \/\/ add safepoint before generating condition code so it can be recomputed\n+  if (x->is_safepoint()) {\n+    \/\/ increment backedge counter if needed\n+    increment_backedge_counter_conditionally(lir_cond(cond), left, right, state_for(x, x->state_before()),\n+                                             x->tsux()->bci(), x->fsux()->bci(), x->profiled_bci());\n+    __ safepoint(LIR_OprFact::illegalOpr, state_for(x, x->state_before()));\n+  }\n+\n+  \/\/ Generate branch profiling. Profiling code doesn't kill flags.\n+  __ cmp(lir_cond(cond), left, right);\n+  profile_branch(x, cond);\n+  move_to_phi(x->state());\n+  if (x->x()->type()->is_float_kind()) {\n+    __ branch(lir_cond(cond), right->type(), x->tsux(), x->usux());\n+  } else {\n+    __ branch(lir_cond(cond), right->type(), x->tsux());\n+  }\n+  assert(x->default_sux() == x->fsux(), \"wrong destination above\");\n+  __ jump(x->default_sux());\n+}\n+\n+LIR_Opr LIRGenerator::getThreadPointer() {\n+   return FrameMap::as_pointer_opr(xthread);\n+}\n+\n+void LIRGenerator::trace_block_entry(BlockBegin* block) { Unimplemented(); }\n+\n+void LIRGenerator::volatile_field_store(LIR_Opr value, LIR_Address* address,\n+                                        CodeEmitInfo* info) {\n+  __ volatile_store_mem_reg(value, address, info);\n+}\n+\n+void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+                                       CodeEmitInfo* info) {\n+  __ volatile_load_mem_reg(address, result, info);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRGenerator_riscv.cpp","additions":1094,"deletions":0,"binary":false,"changes":1094,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/register.hpp\"\n+#include \"c1\/c1_LIR.hpp\"\n+\n+FloatRegister LIR_OprDesc::as_float_reg() const {\n+  return as_FloatRegister(fpu_regnr());\n+}\n+\n+FloatRegister LIR_OprDesc::as_double_reg() const {\n+  return as_FloatRegister(fpu_regnrLo());\n+}\n+\n+\/\/ Reg2 unused.\n+LIR_Opr LIR_OprFact::double_fpu(int reg1, int reg2) {\n+  assert(as_FloatRegister(reg2) == fnoreg, \"Not used on this platform\");\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n+                             (reg1 << LIR_OprDesc::reg2_shift) |\n+                             LIR_OprDesc::double_type          |\n+                             LIR_OprDesc::fpu_register         |\n+                             LIR_OprDesc::double_size);\n+}\n+\n+#ifndef PRODUCT\n+void LIR_Address::verify() const {\n+  assert(base()->is_cpu_register(), \"wrong base operand\");\n+  assert(index()->is_illegal() || index()->is_double_cpu() || index()->is_single_cpu(), \"wrong index operand\");\n+  assert(base()->type() == T_ADDRESS || base()->type() == T_OBJECT || base()->type() == T_LONG ||\n+         base()->type() == T_METADATA, \"wrong type for addresses\");\n+}\n+#endif \/\/ PRODUCT\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIR_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"c1\/c1_Instruction.hpp\"\n+#include \"c1\/c1_LinearScan.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+\n+void LinearScan::allocate_fpu_stack() {\n+  \/\/ No FPU stack on RISCV\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LinearScan_riscv.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_LINEARSCAN_RISCV_HPP\n+#define CPU_RISCV_C1_LINEARSCAN_RISCV_HPP\n+\n+inline bool LinearScan::is_processed_reg_num(int reg_num)\n+{\n+  return reg_num <= FrameMap::last_cpu_reg() || reg_num >= pd_nof_cpu_regs_frame_map;\n+}\n+\n+inline int LinearScan::num_physical_regs(BasicType type) {\n+  return 1;\n+}\n+\n+inline bool LinearScan::requires_adjacent_regs(BasicType type) {\n+  return false;\n+}\n+\n+inline bool LinearScan::is_caller_save(int assigned_reg) {\n+  assert(assigned_reg >= 0 && assigned_reg < nof_regs, \"should call this only for registers\");\n+  if (assigned_reg < pd_first_callee_saved_reg) {\n+    return true;\n+  }\n+  if (assigned_reg > pd_last_callee_saved_reg && assigned_reg < pd_first_callee_saved_fpu_reg_1) {\n+    return true;\n+  }\n+  if (assigned_reg > pd_last_callee_saved_fpu_reg_1 && assigned_reg < pd_first_callee_saved_fpu_reg_2) {\n+    return true;\n+  }\n+  if (assigned_reg > pd_last_callee_saved_fpu_reg_2 && assigned_reg < pd_last_fpu_reg) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+inline void LinearScan::pd_add_temps(LIR_Op* op) {\n+  \/\/ No special case behaviours yet\n+}\n+\n+\n+\/\/ Implementation of LinearScanWalker\n+\n+inline bool LinearScanWalker::pd_init_regs_for_alloc(Interval* cur)\n+{\n+  if (allocator()->gen()->is_vreg_flag_set(cur->reg_num(), LIRGenerator::callee_saved)) {\n+    assert(cur->type() != T_FLOAT && cur->type() != T_DOUBLE, \"cpu regs only\");\n+    _first_reg = pd_first_callee_saved_reg;\n+    _last_reg = pd_last_callee_saved_reg;\n+    return true;\n+  } else if (cur->type() == T_INT || cur->type() == T_LONG || cur->type() == T_OBJECT ||\n+             cur->type() == T_ADDRESS || cur->type() == T_METADATA) {\n+    _first_reg = pd_first_cpu_reg;\n+    _last_reg = pd_last_allocatable_cpu_reg;\n+    return true;\n+  }\n+  return false;\n+}\n+\n+#endif \/\/ CPU_RISCV_C1_LINEARSCAN_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LinearScan_riscv.hpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -0,0 +1,443 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"c1\/c1_LIR.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"runtime\/basicLock.hpp\"\n+#include \"runtime\/biasedLocking.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+void C1_MacroAssembler::float_cmp(bool is_float, int unordered_result,\n+                                  FloatRegister freg0, FloatRegister freg1,\n+                                  Register result)\n+{\n+  if (is_float) {\n+    float_compare(result, freg0, freg1, unordered_result);\n+  } else {\n+    double_compare(result, freg0, freg1, unordered_result);\n+  }\n+}\n+\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label& slow_case) {\n+  const int aligned_mask = BytesPerWord - 1;\n+  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n+  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n+  Label done;\n+  int null_check_offset = -1;\n+\n+  verify_oop(obj);\n+\n+  \/\/ save object being locked into the BasicObjectLock\n+  sd(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+\n+  if (UseBiasedLocking) {\n+    assert(scratch != noreg, \"should have scratch register at this point\");\n+    null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, false, done, &slow_case);\n+  } else {\n+    null_check_offset = offset();\n+  }\n+\n+  \/\/ Load object header\n+  ld(hdr, Address(obj, hdr_offset));\n+  \/\/ and mark it as unlocked\n+  ori(hdr, hdr, markOopDesc::unlocked_value);\n+  \/\/ save unlocked object header into the displaced header location on the stack\n+  sd(hdr, Address(disp_hdr, 0));\n+  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+  \/\/ displaced header address in the object header - if it is not the same, get the\n+  \/\/ object header instead\n+  la(t1, Address(obj, hdr_offset));\n+  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n+  \/\/ if the object header was the same, we're done\n+  \/\/ if the object header was not the same, it is now in the hdr register\n+  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+  \/\/\n+  \/\/ 1) (hdr & aligned_mask) == 0\n+  \/\/ 2) sp <= hdr\n+  \/\/ 3) hdr <= sp + page_size\n+  \/\/\n+  \/\/ these 3 tests can be done by evaluating the following expression:\n+  \/\/\n+  \/\/ (hdr -sp) & (aligned_mask - page_size)\n+  \/\/\n+  \/\/ assuming both the stack pointer and page_size have their least\n+  \/\/ significant 2 bits cleared and page_size is a power of 2\n+  sub(hdr, hdr, sp);\n+  li(t0, aligned_mask - os::vm_page_size());\n+  andr(hdr, hdr, t0);\n+  \/\/ for recursive locking, the result is zero => save it in the displaced header\n+  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+  sd(hdr, Address(disp_hdr, 0));\n+  \/\/ otherwise we don't care about the result and handle locking via runtime call\n+  bnez(hdr, slow_case, \/* is_far *\/ true);\n+  bind(done);\n+  if (PrintBiasedLockingStatistics) {\n+    la(t1, ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n+    add_memory_int32(Address(t1, 0), 1);\n+  }\n+  return null_check_offset;\n+}\n+\n+void C1_MacroAssembler::unlock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+  const int aligned_mask = BytesPerWord - 1;\n+  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n+  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n+  Label done;\n+\n+  if (UseBiasedLocking) {\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    biased_locking_exit(obj, hdr, done);\n+  }\n+\n+  \/\/ load displaced header\n+  ld(hdr, Address(disp_hdr, 0));\n+  \/\/ if the loaded hdr is NULL we had recursive locking\n+  \/\/ if we had recursive locking, we are done\n+  beqz(hdr, done);\n+  if (!UseBiasedLocking) {\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  }\n+  verify_oop(obj);\n+  \/\/ test if object header is pointing to the displaced header, and if so, restore\n+  \/\/ the displaced header in the object - if the object header is not pointing to\n+  \/\/ the displaced header, get the object header instead\n+  \/\/ if the object header was not pointing to the displaced header,\n+  \/\/ we do unlocking via runtime call\n+  if (hdr_offset) {\n+    la(t0, Address(obj, hdr_offset));\n+    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+  } else {\n+    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+  }\n+  bind(done);\n+}\n+\n+\/\/ Defines obj, preserves var_size_in_bytes\n+void C1_MacroAssembler::try_allocate(Register obj, Register var_size_in_bytes, int con_size_in_bytes, Register tmp1, Register tmp2, Label& slow_case) {\n+  if (UseTLAB) {\n+    tlab_allocate(obj, var_size_in_bytes, con_size_in_bytes, tmp1, tmp2, slow_case, \/* is_far *\/ true);\n+  } else {\n+    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, tmp1, slow_case, \/* is_far *\/ true);\n+  }\n+}\n+\n+void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register tmp1, Register tmp2) {\n+  assert_different_registers(obj, klass, len);\n+  if (UseBiasedLocking && !len->is_valid()) {\n+    assert_different_registers(obj, klass, len, tmp1, tmp2);\n+    ld(tmp1, Address(klass, Klass::prototype_header_offset()));\n+  } else {\n+    \/\/ This assumes that all prototype bits fitr in an int32_t\n+    mv(tmp1, (int32_t)(intptr_t)markOopDesc::prototype());\n+  }\n+  sd(tmp1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+    encode_klass_not_null(tmp1, klass);\n+    sw(tmp1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    sd(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+\n+  if (len->is_valid()) {\n+    sw(len, Address(obj, arrayOopDesc::length_offset_in_bytes()));\n+  } else if (UseCompressedClassPointers) {\n+    store_klass_gap(obj, zr);\n+  }\n+}\n+\n+\/\/ preserves obj, destroys len_in_bytes\n+void C1_MacroAssembler::initialize_body(Register obj, Register len_in_bytes, int hdr_size_in_bytes, Register tmp) {\n+  assert(hdr_size_in_bytes >= 0, \"header size must be positive or 0\");\n+  Label done;\n+\n+  \/\/ len_in_bytes is positive and ptr sized\n+  sub(len_in_bytes, len_in_bytes, hdr_size_in_bytes);\n+  beqz(len_in_bytes, done);\n+\n+  \/\/ Preserve obj\n+  if (hdr_size_in_bytes) {\n+    add(obj, obj, hdr_size_in_bytes);\n+  }\n+  zero_memory(obj, len_in_bytes, tmp);\n+  if (hdr_size_in_bytes) {\n+    sub(obj, obj, hdr_size_in_bytes);\n+  }\n+\n+  bind(done);\n+}\n+\n+void C1_MacroAssembler::allocate_object(Register obj, Register tmp1, Register tmp2, int header_size, int object_size, Register klass, Label& slow_case) {\n+  assert_different_registers(obj, tmp1, tmp2);\n+  assert(header_size >= 0 && object_size >= header_size, \"illegal sizes\");\n+\n+  try_allocate(obj, noreg, object_size * BytesPerWord, tmp1, tmp2, slow_case);\n+\n+  initialize_object(obj, klass, noreg, object_size * HeapWordSize, tmp1, tmp2, UseTLAB);\n+}\n+\n+void C1_MacroAssembler::initialize_object(Register obj, Register klass, Register var_size_in_bytes, int con_size_in_bytes, Register tmp1, Register tmp2, bool is_tlab_allocated) {\n+  assert((con_size_in_bytes & MinObjAlignmentInBytesMask) == 0,\n+         \"con_size_in_bytes is not multiple of alignment\");\n+  const int hdr_size_in_bytes = instanceOopDesc::header_size() * HeapWordSize;\n+\n+  initialize_header(obj, klass, noreg, tmp1, tmp2);\n+\n+  if (!(UseTLAB && ZeroTLAB && is_tlab_allocated)) {\n+    \/\/ clear rest of allocated space\n+    const Register index = tmp2;\n+    \/\/ 16: multipler for threshold\n+    const int threshold = 16 * BytesPerWord;    \/\/ approximate break even point for code size (see comments below)\n+    if (var_size_in_bytes != noreg) {\n+      mv(index, var_size_in_bytes);\n+      initialize_body(obj, index, hdr_size_in_bytes, tmp1);\n+    } else if (con_size_in_bytes <= threshold) {\n+      \/\/ use explicit null stores\n+      int i = hdr_size_in_bytes;\n+      if (i < con_size_in_bytes && (con_size_in_bytes % (2 * BytesPerWord))) { \/\/ 2: multipler for BytesPerWord\n+        sd(zr, Address(obj, i));\n+        i += BytesPerWord;\n+      }\n+      for (; i < con_size_in_bytes; i += BytesPerWord) {\n+        sd(zr, Address(obj, i));\n+      }\n+    } else if (con_size_in_bytes > hdr_size_in_bytes) {\n+      block_comment(\"zero memory\");\n+      \/\/ use loop to null out the fields\n+      int words = (con_size_in_bytes - hdr_size_in_bytes) \/ BytesPerWord;\n+      mv(index, words \/ 8); \/\/ 8: byte size\n+\n+      const int unroll = 8; \/\/ Number of sd(zr) instructions we'll unroll\n+      int remainder = words % unroll;\n+      la(t0, Address(obj, hdr_size_in_bytes + remainder * BytesPerWord));\n+\n+      Label entry_point, loop;\n+      j(entry_point);\n+\n+      bind(loop);\n+      sub(index, index, 1);\n+      for (int i = -unroll; i < 0; i++) {\n+        if (-i == remainder) {\n+          bind(entry_point);\n+        }\n+        sd(zr, Address(t0, i * wordSize));\n+      }\n+      if (remainder == 0) {\n+        bind(entry_point);\n+      }\n+      add(t0, t0, unroll * wordSize);\n+      bnez(index, loop);\n+    }\n+  }\n+\n+  membar(MacroAssembler::StoreStore);\n+\n+  if (CURRENT_ENV->dtrace_alloc_probes()) {\n+    assert(obj == x10, \"must be\");\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::dtrace_object_alloc_id)));\n+  }\n+\n+  verify_oop(obj);\n+}\n+\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register tmp1, Register tmp2, int header_size, int f, Register klass, Label& slow_case) {\n+  assert_different_registers(obj, len, tmp1, tmp2, klass);\n+\n+  \/\/ determine alignment mask\n+  assert(!(BytesPerWord & 1), \"must be multiple of 2 for masking code to work\");\n+\n+  \/\/ check for negative or excessive length\n+  mv(t0, (int32_t)max_array_allocation_length);\n+  bgeu(len, t0, slow_case, \/* is_far *\/ true);\n+\n+  const Register arr_size = tmp2; \/\/ okay to be the same\n+  \/\/ align object end\n+  mv(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  shadd(arr_size, len, arr_size, t0, f);\n+  andi(arr_size, arr_size, ~(uint)MinObjAlignmentInBytesMask);\n+\n+  try_allocate(obj, arr_size, 0, tmp1, tmp2, slow_case);\n+\n+  initialize_header(obj, klass, len, tmp1, tmp2);\n+\n+  \/\/ clear rest of allocated space\n+  const Register len_zero = len;\n+  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+\n+  membar(MacroAssembler::StoreStore);\n+\n+  if (CURRENT_ENV->dtrace_alloc_probes()) {\n+    assert(obj == x10, \"must be\");\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::dtrace_object_alloc_id)));\n+  }\n+\n+  verify_oop(obj);\n+}\n+\n+void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache, Label &L) {\n+  verify_oop(receiver);\n+  \/\/ explicit NULL check not needed since load from [klass_offset] causes a trap\n+  \/\/ check against inline cache\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  cmp_klass(receiver, iCache, t0, L);\n+}\n+\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+  \/\/ If we have to make this method not-entrant we'll overwrite its\n+  \/\/ first instruction with a jump. For this action to be legal we\n+  \/\/ must ensure that this first instruction is a J, JAL or NOP.\n+  \/\/ Make it a NOP.\n+  nop();\n+\n+  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  \/\/ Note that we do this before creating a frame.\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+  MacroAssembler::build_frame(framesize);\n+}\n+\n+void C1_MacroAssembler::remove_frame(int framesize) {\n+  MacroAssembler::remove_frame(framesize);\n+}\n+\n+\n+void C1_MacroAssembler::verified_entry() {\n+}\n+\n+void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {\n+  \/\/  fp + -2: link\n+  \/\/     + -1: return address\n+  \/\/     +  0: argument with offset 0\n+  \/\/     +  1: argument with offset 1\n+  \/\/     +  2: ...\n+  ld(reg, Address(fp, offset_in_words * BytesPerWord));\n+}\n+\n+#ifndef PRODUCT\n+\n+void C1_MacroAssembler::verify_stack_oop(int stack_offset) {\n+  if (!VerifyOops) {\n+    return;\n+  }\n+  verify_oop_addr(Address(sp, stack_offset), \"oop\");\n+}\n+\n+void C1_MacroAssembler::verify_not_null_oop(Register r) {\n+  if (!VerifyOops) return;\n+  Label not_null;\n+  bnez(r, not_null);\n+  stop(\"non-null oop required\");\n+  bind(not_null);\n+  verify_oop(r);\n+}\n+\n+void C1_MacroAssembler::invalidate_registers(bool inv_x10, bool inv_x9, bool inv_x12, bool inv_x13, bool inv_x14, bool inv_x15) {\n+#ifdef ASSERT\n+  static int nn;\n+  if (inv_x10) { mv(x10, 0xDEAD); }\n+  if (inv_x9)  { mv(x9, 0xDEAD);  }\n+  if (inv_x12) { mv(x12, nn++);   }\n+  if (inv_x13) { mv(x13, 0xDEAD); }\n+  if (inv_x14) { mv(x14, 0xDEAD); }\n+  if (inv_x15) { mv(x15, 0xDEAD); }\n+#endif \/\/ ASSERT\n+}\n+#endif \/\/ ifndef PRODUCT\n+\n+typedef void (C1_MacroAssembler::*c1_cond_branch_insn)(Register op1, Register op2, Label& label, bool is_far);\n+typedef void (C1_MacroAssembler::*c1_float_cond_branch_insn)(FloatRegister op1, FloatRegister op2,\n+              Label& label, bool is_far, bool is_unordered);\n+\n+static c1_cond_branch_insn c1_cond_branch[] =\n+{\n+  \/* SHORT branches *\/\n+  (c1_cond_branch_insn)&Assembler::beq,\n+  (c1_cond_branch_insn)&Assembler::bne,\n+  (c1_cond_branch_insn)&Assembler::blt,\n+  (c1_cond_branch_insn)&Assembler::ble,\n+  (c1_cond_branch_insn)&Assembler::bge,\n+  (c1_cond_branch_insn)&Assembler::bgt,\n+  (c1_cond_branch_insn)&Assembler::bleu, \/\/ lir_cond_belowEqual\n+  (c1_cond_branch_insn)&Assembler::bgeu  \/\/ lir_cond_aboveEqual\n+};\n+\n+static c1_float_cond_branch_insn c1_float_cond_branch[] =\n+{\n+  \/* FLOAT branches *\/\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_beq,\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_bne,\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_blt,\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_ble,\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_bge,\n+  (c1_float_cond_branch_insn)&MacroAssembler::float_bgt,\n+  NULL, \/\/ lir_cond_belowEqual\n+  NULL, \/\/ lir_cond_aboveEqual\n+\n+  \/* DOUBLE branches *\/\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_beq,\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_bne,\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_blt,\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_ble,\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_bge,\n+  (c1_float_cond_branch_insn)&MacroAssembler::double_bgt,\n+  NULL, \/\/ lir_cond_belowEqual\n+  NULL  \/\/ lir_cond_aboveEqual\n+};\n+\n+void C1_MacroAssembler::c1_cmp_branch(int cmpFlag, Register op1, Register op2, Label& label,\n+                                      BasicType type, bool is_far) {\n+  if (type == T_OBJECT || type == T_ARRAY) {\n+    assert(cmpFlag == lir_cond_equal || cmpFlag == lir_cond_notEqual, \"Should be equal or notEqual\");\n+    if (cmpFlag == lir_cond_equal) {\n+      beq(op1, op2, label, is_far);\n+    } else {\n+      bne(op1, op2, label, is_far);\n+    }\n+  } else {\n+    assert(cmpFlag >= 0 && cmpFlag < (int)(sizeof(c1_cond_branch) \/ sizeof(c1_cond_branch[0])),\n+           \"invalid c1 conditional branch index\");\n+    (this->*c1_cond_branch[cmpFlag])(op1, op2, label, is_far);\n+  }\n+}\n+\n+void C1_MacroAssembler::c1_float_cmp_branch(int cmpFlag, FloatRegister op1, FloatRegister op2, Label& label,\n+                                            bool is_far, bool is_unordered) {\n+  assert(cmpFlag >= 0 &&\n+         cmpFlag < (int)(sizeof(c1_float_cond_branch) \/ sizeof(c1_float_cond_branch[0])),\n+         \"invalid c1 float conditional branch index\");\n+  (this->*c1_float_cond_branch[cmpFlag])(op1, op2, label, is_far, is_unordered);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":443,"deletions":0,"binary":false,"changes":443,"status":"added"},{"patch":"@@ -0,0 +1,121 @@\n+\/*\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_MACROASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_C1_MACROASSEMBLER_RISCV_HPP\n+\n+using MacroAssembler::build_frame;\n+using MacroAssembler::null_check;\n+\n+\/\/ C1_MacroAssembler contains high-level macros for C1\n+\n+ private:\n+  int _rsp_offset;    \/\/ track rsp changes\n+  \/\/ initialization\n+  void pd_init() { _rsp_offset = 0; }\n+\n+\n+ public:\n+  void try_allocate(\n+    Register obj,                      \/\/ result: pointer to object after successful allocation\n+    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,        \/\/ object size in bytes if known at compile time\n+    Register tmp1,                     \/\/ temp register\n+    Register tmp2,                     \/\/ temp register\n+    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n+  );\n+\n+  void initialize_header(Register obj, Register klass, Register len, Register tmp1, Register tmp2);\n+  void initialize_body(Register obj, Register len_in_bytes, int hdr_size_in_bytes, Register tmp);\n+\n+  void float_cmp(bool is_float, int unordered_result,\n+                 FloatRegister f0, FloatRegister f1,\n+                 Register result);\n+\n+  \/\/ locking\n+  \/\/ hdr     : must be x10, contents destroyed\n+  \/\/ obj     : must point to the object to lock, contents preserved\n+  \/\/ disp_hdr: must point to the displaced header location, contents preserved\n+  \/\/ scratch : scratch register, contents destroyed\n+  \/\/ returns code offset at which to add null check debug information\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register scratch, Label& slow_case);\n+\n+  \/\/ unlocking\n+  \/\/ hdr     : contents destroyed\n+  \/\/ obj     : must point to the object to lock, contents preserved\n+  \/\/ disp_hdr: must be x10 & must point to the displaced header location, contents destroyed\n+  void unlock_object(Register swap, Register obj, Register lock, Label& slow_case);\n+\n+  void initialize_object(\n+    Register obj,                      \/\/ result: pointer to object after successful allocation\n+    Register klass,                    \/\/ object klass\n+    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n+    Register tmp1,                     \/\/ temp register\n+    Register tmp2,                     \/\/ temp register\n+    bool     is_tlab_allocated         \/\/ the object was allocated in a TLAB; relevant for the implementation of ZeroTLAB\n+  );\n+\n+  \/\/ allocation of fixed-size objects\n+  \/\/ (can also be used to allocate fixed-size arrays, by setting\n+  \/\/ hdr_size correctly and storing the array length afterwards)\n+  \/\/ obj        : will contain pointer to allocated object\n+  \/\/ t1, t2     : temp registers - contents destroyed\n+  \/\/ header_size: size of object header in words\n+  \/\/ object_size: total size of object in words\n+  \/\/ slow_case  : exit to slow case implementation if fast allocation fails\n+  void allocate_object(Register obj, Register tmp1, Register tmp2, int header_size, int object_size, Register klass, Label& slow_case);\n+\n+  enum {\n+    max_array_allocation_length = 0x00FFFFFF\n+  };\n+\n+  \/\/ allocation of arrays\n+  \/\/ obj        : will contain pointer to allocated object\n+  \/\/ len        : array length in number of elements\n+  \/\/ t          : temp register - contents destroyed\n+  \/\/ header_size: size of object header in words\n+  \/\/ f          : element scale factor\n+  \/\/ slow_case  : exit to slow case implementation if fast allocation fails\n+  void allocate_array(Register obj, Register len, Register tmp1, Register tmp2, int header_size, int f, Register klass, Label& slow_case);\n+\n+  int  rsp_offset() const { return _rsp_offset; }\n+\n+  void invalidate_registers(bool inv_r0, bool inv_r19, bool inv_r2, bool inv_r3, bool inv_r4, bool inv_r5) PRODUCT_RETURN;\n+\n+  \/\/ This platform only uses signal-based null checks. The Label is not needed.\n+  void null_check(Register r, Label *Lnull = NULL) { MacroAssembler::null_check(r); }\n+\n+  void load_parameter(int offset_in_words, Register reg);\n+\n+  void inline_cache_check(Register receiver, Register iCache, Label &L);\n+\n+  static const int c1_double_branch_mask = 1 << 3; \/\/ depend on c1_float_cond_branch\n+  void c1_cmp_branch(int cmpFlag, Register op1, Register op2, Label& label, BasicType type, bool is_far);\n+  void c1_float_cmp_branch(int cmpFlag, FloatRegister op1, FloatRegister op2, Label& label,\n+                           bool is_far, bool is_unordered = false);\n+\n+#endif \/\/ CPU_RISCV_C1_MACROASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.hpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"added"},{"patch":"@@ -0,0 +1,1210 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"c1\/c1_CodeStubs.hpp\"\n+#include \"c1\/c1_Defs.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"compiler\/disassembler.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/compiledICHolder.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"register_riscv.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/vframe.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+\n+\/\/ Implementation of StubAssembler\n+\n+int StubAssembler::call_RT(Register oop_result, Register metadata_result, address entry, int args_size) {\n+  \/\/ setup registers\n+  assert(!(oop_result->is_valid() || metadata_result->is_valid()) || oop_result != metadata_result,\n+         \"registers must be different\");\n+  assert(oop_result != xthread && metadata_result != xthread, \"registers must be different\");\n+  assert(args_size >= 0, \"illegal args_size\");\n+  bool align_stack = false;\n+\n+  mv(c_rarg0, xthread);\n+  set_num_rt_args(0); \/\/ Nothing on stack\n+\n+  Label retaddr;\n+  set_last_Java_frame(sp, fp, retaddr, t0);\n+\n+  \/\/ do the call\n+  int32_t off = 0;\n+  la_patchable(t0, RuntimeAddress(entry), off);\n+  jalr(x1, t0, off);\n+  bind(retaddr);\n+  int call_offset = offset();\n+  \/\/ verify callee-saved register\n+#ifdef ASSERT\n+  push_reg(x10, sp);\n+  { Label L;\n+    get_thread(x10);\n+    beq(xthread, x10, L);\n+    stop(\"StubAssembler::call_RT: xthread not callee saved?\");\n+    bind(L);\n+  }\n+  pop_reg(x10, sp);\n+#endif\n+  reset_last_Java_frame(true);\n+\n+  \/\/ check for pending exceptions\n+  { Label L;\n+    \/\/ check for pending exceptions (java_thread is set upon return)\n+    ld(t0, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+    beqz(t0, L);\n+    \/\/ exception pending => remove activation and forward to exception handler\n+    \/\/ make sure that the vm_results are cleared\n+    if (oop_result->is_valid()) {\n+      sd(zr, Address(xthread, JavaThread::vm_result_offset()));\n+    }\n+    if (metadata_result->is_valid()) {\n+      sd(zr, Address(xthread, JavaThread::vm_result_2_offset()));\n+    }\n+    if (frame_size() == no_frame_size) {\n+      leave();\n+      far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    } else if (_stub_id == Runtime1::forward_exception_id) {\n+      should_not_reach_here();\n+    } else {\n+      far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));\n+    }\n+    bind(L);\n+  }\n+  \/\/ get oop results if there are any and reset the values in the thread\n+  if (oop_result->is_valid()) {\n+    get_vm_result(oop_result, xthread);\n+  }\n+  if (metadata_result->is_valid()) {\n+    get_vm_result_2(metadata_result, xthread);\n+  }\n+  return call_offset;\n+}\n+\n+int StubAssembler::call_RT(Register oop_result, Register metadata_result, address entry, Register arg1) {\n+  mv(c_rarg1, arg1);\n+  return call_RT(oop_result, metadata_result, entry, 1);\n+}\n+\n+int StubAssembler::call_RT(Register oop_result, Register metadata_result, address entry, Register arg1, Register arg2) {\n+  const int arg_num = 2;\n+  if (c_rarg1 == arg2) {\n+    if (c_rarg2 == arg1) {\n+      xorr(arg1, arg1, arg2);\n+      xorr(arg2, arg1, arg2);\n+      xorr(arg1, arg1, arg2);\n+    } else {\n+      mv(c_rarg2, arg2);\n+      mv(c_rarg1, arg1);\n+    }\n+  } else {\n+    mv(c_rarg1, arg1);\n+    mv(c_rarg2, arg2);\n+  }\n+  return call_RT(oop_result, metadata_result, entry, arg_num);\n+}\n+\n+int StubAssembler::call_RT(Register oop_result, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {\n+  const int arg_num = 3;\n+  \/\/ if there is any conflict use the stack\n+  if (arg1 == c_rarg2 || arg1 == c_rarg3 ||\n+      arg2 == c_rarg1 || arg2 == c_rarg3 ||\n+      arg3 == c_rarg1 || arg3 == c_rarg2) {\n+    const int arg1_sp_offset = 0;\n+    const int arg2_sp_offset = 1;\n+    const int arg3_sp_offset = 2;\n+    addi(sp, sp, -(arg_num + 1) * wordSize);\n+    sd(arg1, Address(sp, arg1_sp_offset * wordSize));\n+    sd(arg2, Address(sp, arg2_sp_offset * wordSize));\n+    sd(arg3, Address(sp, arg3_sp_offset * wordSize));\n+\n+    ld(c_rarg1, Address(sp, arg1_sp_offset * wordSize));\n+    ld(c_rarg2, Address(sp, arg2_sp_offset * wordSize));\n+    ld(c_rarg3, Address(sp, arg3_sp_offset * wordSize));\n+    addi(sp, sp, (arg_num + 1) * wordSize);\n+  } else {\n+    mv(c_rarg1, arg1);\n+    mv(c_rarg2, arg2);\n+    mv(c_rarg3, arg3);\n+  }\n+  return call_RT(oop_result, metadata_result, entry, arg_num);\n+}\n+\n+\/\/ Implementation of StubFrame\n+\n+class StubFrame: public StackObj {\n+ private:\n+  StubAssembler* _sasm;\n+\n+ public:\n+  StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments);\n+  void load_argument(int offset_in_words, Register reg);\n+\n+  ~StubFrame();\n+};;\n+\n+void StubAssembler::prologue(const char* name, bool must_gc_arguments) {\n+  set_info(name, must_gc_arguments);\n+  enter();\n+}\n+\n+void StubAssembler::epilogue() {\n+  leave();\n+  ret();\n+}\n+\n+#define __ _sasm->\n+\n+StubFrame::StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments) {\n+  _sasm = sasm;\n+  __ prologue(name, must_gc_arguments);\n+}\n+\n+\/\/ load parameters that were stored with LIR_Assembler::store_parameter\n+\/\/ Note: offsets for store_parameter and load_argument must match\n+void StubFrame::load_argument(int offset_in_words, Register reg) {\n+  __ load_parameter(offset_in_words, reg);\n+}\n+\n+\n+StubFrame::~StubFrame() {\n+  __ epilogue();\n+  _sasm = NULL;\n+}\n+\n+#undef __\n+\n+\n+\/\/ Implementation of Runtime1\n+\n+#define __ sasm->\n+\n+const int float_regs_as_doubles_size_in_slots = pd_nof_fpu_regs_frame_map * 2;\n+\n+\/\/ Stack layout for saving\/restoring  all the registers needed during a runtime\n+\/\/ call (this includes deoptimization)\n+\/\/ Note: note that users of this frame may well have arguments to some runtime\n+\/\/ while these values are on the stack. These positions neglect those arguments\n+\/\/ but the code in save_live_registers will take the argument count into\n+\/\/ account.\n+\/\/\n+\n+enum reg_save_layout {\n+  reg_save_frame_size = 32 \/* float *\/ + 30 \/* integer excluding x3, x4 *\/\n+};\n+\n+\/\/ Save off registers which might be killed by calls into the runtime.\n+\/\/ Tries to smart of about FPU registers.  In particular we separate\n+\/\/ saving and describing the FPU registers for deoptimization since we\n+\/\/ have to save the FPU registers twice if we describe them.  The\n+\/\/ deopt blob is the only thing which needs to describe FPU registers.\n+\/\/ In all other cases it should be sufficient to simply save their\n+\/\/ current value.\n+\n+static int cpu_reg_save_offsets[FrameMap::nof_cpu_regs];\n+static int fpu_reg_save_offsets[FrameMap::nof_fpu_regs];\n+\n+static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers) {\n+  int frame_size_in_bytes = reg_save_frame_size * BytesPerWord;\n+  sasm->set_frame_size(frame_size_in_bytes \/ BytesPerWord);\n+  int frame_size_in_slots = frame_size_in_bytes \/ sizeof(jint);\n+  OopMap* oop_map = new OopMap(frame_size_in_slots, 0);\n+  assert_cond(oop_map != NULL);\n+\n+  \/\/ caller save registers only, see FrameMap::initialize\n+  \/\/ in c1_FrameMap_riscv.cpp for detail.\n+  const static Register caller_save_cpu_regs[FrameMap::max_nof_caller_save_cpu_regs] = {\n+    x7, x10, x11, x12, x13, x14, x15, x16, x17, x28, x29, x30, x31\n+  };\n+\n+  for (int i = 0; i < FrameMap::max_nof_caller_save_cpu_regs; i++) {\n+    Register r = caller_save_cpu_regs[i];\n+    int sp_offset = cpu_reg_save_offsets[r->encoding()];\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n+                              r->as_VMReg());\n+  }\n+\n+  \/\/ fpu_regs\n+  if (save_fpu_registers) {\n+    for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n+      FloatRegister r = as_FloatRegister(i);\n+      int sp_offset = fpu_reg_save_offsets[i];\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n+                                r->as_VMReg());\n+    }\n+  }\n+  return oop_map;\n+}\n+\n+static OopMap* save_live_registers(StubAssembler* sasm,\n+                                   bool save_fpu_registers = true) {\n+  __ block_comment(\"save_live_registers\");\n+\n+  \/\/ if the number of pushed regs is odd, one slot will be reserved for alignment\n+  __ push_reg(RegSet::range(x5, x31), sp);    \/\/ integer registers except ra(x1) & sp(x2) & gp(x3) & tp(x4)\n+\n+  if (save_fpu_registers) {\n+    \/\/ float registers\n+    __ addi(sp, sp, -(FrameMap::nof_fpu_regs * wordSize));\n+    for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n+      __ fsd(as_FloatRegister(i), Address(sp, i * wordSize));\n+    }\n+  } else {\n+    \/\/ we define reg_save_layout = 62 as the fixed frame size,\n+    \/\/ we should also sub 32 * wordSize to sp when save_fpu_registers == false\n+    __ addi(sp, sp, -32 * wordSize);\n+  }\n+\n+  return generate_oop_map(sasm, save_fpu_registers);\n+}\n+\n+static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = true) {\n+  if (restore_fpu_registers) {\n+    for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n+      __ fld(as_FloatRegister(i), Address(sp, i * wordSize));\n+    }\n+    __ addi(sp, sp, FrameMap::nof_fpu_regs * wordSize);\n+  } else {\n+    \/\/ we define reg_save_layout = 64 as the fixed frame size,\n+    \/\/ we should also add 32 * wordSize to sp when save_fpu_registers == false\n+    __ addi(sp, sp, 32 * wordSize);\n+  }\n+\n+  \/\/ if the number of popped regs is odd, the reserved slot for alignment will be removed\n+  __ pop_reg(RegSet::range(x5, x31), sp);   \/\/ integer registers except ra(x1) & sp(x2) & gp(x3) & tp(x4)\n+}\n+\n+static void restore_live_registers_except_r10(StubAssembler* sasm, bool restore_fpu_registers = true) {\n+  if (restore_fpu_registers) {\n+    for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n+      __ fld(as_FloatRegister(i), Address(sp, i * wordSize));\n+    }\n+    __ addi(sp, sp, FrameMap::nof_fpu_regs * wordSize);\n+  } else {\n+    \/\/ we define reg_save_layout = 64 as the fixed frame size,\n+    \/\/ we should also add 32 * wordSize to sp when save_fpu_registers == false\n+    __ addi(sp, sp, 32 * wordSize);\n+  }\n+\n+  \/\/ pop integer registers except ra(x1) & sp(x2) & gp(x3) & tp(x4) & x10\n+  \/\/ there is one reserved slot for alignment on the stack in save_live_registers().\n+  __ pop_reg(RegSet::range(x5, x9), sp);   \/\/ pop x5 ~ x9 with the reserved slot for alignment\n+  __ pop_reg(RegSet::range(x11, x31), sp); \/\/ pop x11 ~ x31; x10 will be automatically skipped here\n+}\n+\n+void Runtime1::initialize_pd() {\n+  int i = 0;\n+  int sp_offset = 0;\n+  const int step = 2; \/\/ SP offsets are in halfwords\n+\n+  \/\/ all float registers are saved explicitly\n+  for (i = 0; i < FrameMap::nof_fpu_regs; i++) {\n+    fpu_reg_save_offsets[i] = sp_offset;\n+    sp_offset += step;\n+  }\n+\n+  \/\/ a slot reserved for stack 16-byte alignment, see MacroAssembler::push_reg\n+  sp_offset += step;\n+  \/\/ we save x5 ~ x31, except x0 ~ x4: loop starts from x5\n+  for (i = 5; i < FrameMap::nof_cpu_regs; i++) {\n+    cpu_reg_save_offsets[i] = sp_offset;\n+    sp_offset += step;\n+  }\n+}\n+\n+\/\/ target: the entry point of the method that creates and posts the exception oop\n+\/\/ has_argument: true if the exception needs arguments (passed in t0 and t1)\n+\n+OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {\n+  \/\/ make a frame and preserve the caller's caller-save registers\n+  OopMap* oop_map = save_live_registers(sasm);\n+  assert_cond(oop_map != NULL);\n+  int call_offset = 0;\n+  if (!has_argument) {\n+    call_offset = __ call_RT(noreg, noreg, target);\n+  } else {\n+    __ mv(c_rarg1, t0);\n+    __ mv(c_rarg2, t1);\n+    call_offset = __ call_RT(noreg, noreg, target);\n+  }\n+  OopMapSet* oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  oop_maps->add_gc_map(call_offset, oop_map);\n+\n+  __ should_not_reach_here();\n+  return oop_maps;\n+}\n+\n+OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler *sasm) {\n+  __ block_comment(\"generate_handle_exception\");\n+\n+  \/\/ incoming parameters\n+  const Register exception_oop = x10;\n+  const Register exception_pc  = x13;\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  OopMap* oop_map = NULL;\n+\n+  switch (id) {\n+    case forward_exception_id:\n+      \/\/ We're handling an exception in the context of a compiled frame.\n+      \/\/ The registers have been saved in the standard places.  Perform\n+      \/\/ an exception lookup in the caller and dispatch to the handler\n+      \/\/ if found.  Otherwise unwind and dispatch to the callers\n+      \/\/ exception handler.\n+      oop_map = generate_oop_map(sasm, 1 \/* thread *\/);\n+\n+      \/\/ load and clear pending exception oop into x10\n+      __ ld(exception_oop, Address(xthread, Thread::pending_exception_offset()));\n+      __ sd(zr, Address(xthread, Thread::pending_exception_offset()));\n+\n+      \/\/ load issuing PC (the return address for this stub) into x13\n+      __ ld(exception_pc, Address(fp, frame::return_addr_offset * BytesPerWord));\n+\n+      \/\/ make sure that the vm_results are cleared (may be unnecessary)\n+      __ sd(zr, Address(xthread, JavaThread::vm_result_offset()));\n+      __ sd(zr, Address(xthread, JavaThread::vm_result_2_offset()));\n+      break;\n+    case handle_exception_nofpu_id:\n+    case handle_exception_id:\n+      \/\/ At this point all registers MAY be live.\n+      oop_map = save_live_registers(sasm, id != handle_exception_nofpu_id);\n+      break;\n+    case handle_exception_from_callee_id: {\n+      \/\/ At this point all registers except exception oop (x10) and\n+      \/\/ exception pc (ra) are dead.\n+      const int frame_size = 2 \/* fp, return address *\/;\n+      oop_map = new OopMap(frame_size * VMRegImpl::slots_per_word, 0);\n+      sasm->set_frame_size(frame_size);\n+      break;\n+    }\n+    default:\n+      __ should_not_reach_here();\n+      break;\n+  }\n+\n+  \/\/ verify that only x10 and x13 are valid at this time\n+  __ invalidate_registers(false, true, true, false, true, true);\n+  \/\/ verify that x10 contains a valid exception\n+  __ verify_not_null_oop(exception_oop);\n+\n+#ifdef ASSERT\n+  \/\/ check that fields in JavaThread for exception oop and issuing pc are\n+  \/\/ empty before writing to them\n+  Label oop_empty;\n+  __ ld(t0, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ beqz(t0, oop_empty);\n+  __ stop(\"exception oop already set\");\n+  __ bind(oop_empty);\n+\n+  Label pc_empty;\n+  __ ld(t0, Address(xthread, JavaThread::exception_pc_offset()));\n+  __ beqz(t0, pc_empty);\n+  __ stop(\"exception pc already set\");\n+  __ bind(pc_empty);\n+#endif\n+\n+  \/\/ save exception oop and issuing pc into JavaThread\n+  \/\/ (exception handler will load it from here)\n+  __ sd(exception_oop, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ sd(exception_pc, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+  \/\/ patch throwing pc into return address (has bci & oop map)\n+  __ sd(exception_pc, Address(fp, frame::return_addr_offset * BytesPerWord));\n+\n+  \/\/ compute the exception handler.\n+  \/\/ the exception oop and the throwing pc are read from the fields in JavaThread\n+  int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));\n+  guarantee(oop_map != NULL, \"NULL oop_map!\");\n+  oop_maps->add_gc_map(call_offset, oop_map);\n+\n+  \/\/ x10: handler address\n+  \/\/      will be the deopt blob if nmethod was deoptimized while we looked up\n+  \/\/      handler regardless of whether handler existed in the nmethod.\n+\n+  \/\/ only x10 is valid at this time, all other registers have been destroyed by the runtime call\n+  __ invalidate_registers(false, true, true, true, true, true);\n+\n+  \/\/ patch the return address, this stub will directly return to the exception handler\n+  __ sd(x10, Address(fp, frame::return_addr_offset * BytesPerWord));\n+\n+  switch (id) {\n+    case forward_exception_id:\n+    case handle_exception_nofpu_id:\n+    case handle_exception_id:\n+      \/\/ Restore the registers that were saved at the beginning.\n+      restore_live_registers(sasm, id != handle_exception_nofpu_id);\n+      break;\n+    case handle_exception_from_callee_id:\n+      \/\/ Pop the return address.\n+      __ leave();\n+      __ ret();  \/\/ jump to exception handler\n+      break;\n+    default: ShouldNotReachHere();\n+  }\n+\n+  return oop_maps;\n+}\n+\n+\n+void Runtime1::generate_unwind_exception(StubAssembler *sasm) {\n+  \/\/ incoming parameters\n+  const Register exception_oop = x10;\n+  \/\/ other registers used in this stub\n+  const Register handler_addr = x11;\n+\n+  \/\/ verify that only x10, is valid at this time\n+  __ invalidate_registers(false, true, true, true, true, true);\n+\n+#ifdef ASSERT\n+  \/\/ check that fields in JavaThread for exception oop and issuing pc are empty\n+  Label oop_empty;\n+  __ ld(t0, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ beqz(t0, oop_empty);\n+  __ stop(\"exception oop must be empty\");\n+  __ bind(oop_empty);\n+\n+  Label pc_empty;\n+  __ ld(t0, Address(xthread, JavaThread::exception_pc_offset()));\n+  __ beqz(t0, pc_empty);\n+  __ stop(\"exception pc must be empty\");\n+  __ bind(pc_empty);\n+#endif\n+\n+  \/\/ Save our return address because\n+  \/\/ exception_handler_for_return_address will destroy it.  We also\n+  \/\/ save exception_oop\n+  __ addi(sp, sp, -2 * wordSize);\n+  __ sd(exception_oop, Address(sp, wordSize));\n+  __ sd(ra, Address(sp));\n+\n+  \/\/ search the exception handler address of the caller (using the return address)\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), xthread, ra);\n+  \/\/ x10: exception handler address of the caller\n+\n+  \/\/ Only x10 is valid at this time; all other registers have been\n+  \/\/ destroyed by the call.\n+  __ invalidate_registers(false, true, true, true, false, true);\n+\n+  \/\/ move result of call into correct register\n+  __ mv(handler_addr, x10);\n+\n+  \/\/ get throwing pc (= return address).\n+  \/\/ ra has been destroyed by the call\n+  __ ld(ra, Address(sp));\n+  __ ld(exception_oop, Address(sp, wordSize));\n+  __ addi(sp, sp, 2 * wordSize);\n+  __ mv(x13, ra);\n+\n+  __ verify_not_null_oop(exception_oop);\n+\n+  \/\/ continue at exception handler (return address removed)\n+  \/\/ note: do *not* remove arguments when unwinding the\n+  \/\/       activation since the caller assumes having\n+  \/\/       all arguments on the stack when entering the\n+  \/\/       runtime to determine the exception handler\n+  \/\/       (GC happens at call site with arguments!)\n+  \/\/ x10: exception oop\n+  \/\/ x13: throwing pc\n+  \/\/ x11: exception handler\n+  __ jr(handler_addr);\n+}\n+\n+OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {\n+  \/\/ use the maximum number of runtime-arguments here because it is difficult to\n+  \/\/ distinguish each RT-Call.\n+  \/\/ Note: This number affects also the RT-Call in generate_handle_exception because\n+  \/\/       the oop-map is shared for all calls.\n+  DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();\n+  assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+\n+  OopMap* oop_map = save_live_registers(sasm);\n+  assert_cond(oop_map != NULL);\n+\n+  __ mv(c_rarg0, xthread);\n+  Label retaddr;\n+  __ set_last_Java_frame(sp, fp, retaddr, t0);\n+  \/\/ do the call\n+  int32_t off = 0;\n+  __ la_patchable(t0, RuntimeAddress(target), off);\n+  __ jalr(x1, t0, off);\n+  __ bind(retaddr);\n+  OopMapSet* oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  oop_maps->add_gc_map(__ offset(), oop_map);\n+  \/\/ verify callee-saved register\n+#ifdef ASSERT\n+  { Label L;\n+    __ get_thread(t0);\n+    __ beq(xthread, t0, L);\n+    __ stop(\"StubAssembler::call_RT: xthread not callee saved?\");\n+    __ bind(L);\n+  }\n+#endif\n+  __ reset_last_Java_frame(true);\n+\n+  \/\/ check for pending exceptions\n+  { Label L;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ beqz(t0, L);\n+    \/\/ exception pending => remove activation and forward to exception handler\n+\n+    { Label L1;\n+      __ bnez(x10, L1);                                 \/\/ have we deoptimized?\n+      __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));\n+      __ bind(L1);\n+    }\n+\n+    \/\/ the deopt blob expects exceptions in the special fields of\n+    \/\/ JavaThread, so copy and clear pending exception.\n+\n+    \/\/ load and clear pending exception\n+    __ ld(x10, Address(xthread, Thread::pending_exception_offset()));\n+    __ sd(zr, Address(xthread, Thread::pending_exception_offset()));\n+\n+    \/\/ check that there is really a valid exception\n+    __ verify_not_null_oop(x10);\n+\n+    \/\/ load throwing pc: this is the return address of the stub\n+    __ ld(x13, Address(fp, wordSize));\n+\n+#ifdef ASSERT\n+    \/\/ Check that fields in JavaThread for exception oop and issuing pc are empty\n+    Label oop_empty;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ beqz(t0, oop_empty);\n+    __ stop(\"exception oop must be empty\");\n+    __ bind(oop_empty);\n+\n+    Label pc_empty;\n+    __ ld(t0, Address(xthread, JavaThread::exception_pc_offset()));\n+    __ beqz(t0, pc_empty);\n+    __ stop(\"exception pc must be empty\");\n+    __ bind(pc_empty);\n+#endif\n+\n+    \/\/ store exception oop and throwing pc to JavaThread\n+    __ sd(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+    __ sd(x13, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+    restore_live_registers(sasm);\n+\n+    __ leave();\n+\n+    \/\/ Forward the exception directly to deopt blob. We can blow no\n+    \/\/ registers and must leave throwing pc on the stack.  A patch may\n+    \/\/ have values live in registers so the entry point with the\n+    \/\/ exception in tls.\n+    __ far_jump(RuntimeAddress(deopt_blob->unpack_with_exception_in_tls()));\n+\n+    __ bind(L);\n+  }\n+\n+  \/\/ Runtime will return true if the nmethod has been deoptimized during\n+  \/\/ the patching process. In that case we must do a deopt reexecute instead.\n+  Label cont;\n+\n+  __ beqz(x10, cont);                                 \/\/ have we deoptimized?\n+\n+  \/\/ Will reexecute. Proper return address is already on the stack we just restore\n+  \/\/ registers, pop all of our frame but the return address and jump to the deopt blob\n+\n+  restore_live_registers(sasm);\n+  __ leave();\n+  __ far_jump(RuntimeAddress(deopt_blob->unpack_with_reexecution()));\n+\n+  __ bind(cont);\n+  restore_live_registers(sasm);\n+  __ leave();\n+  __ ret();\n+\n+  return oop_maps;\n+}\n+\n+OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {\n+  \/\/ for better readability\n+  const bool dont_gc_arguments = false;\n+\n+  \/\/ default value; overwritten for some optimized stubs that are called from methods that do not use the fpu\n+  bool save_fpu_registers = true;\n+\n+  \/\/ stub code & info for the different stubs\n+  OopMapSet* oop_maps = NULL;\n+  switch (id) {\n+    {\n+    case forward_exception_id:\n+      {\n+        oop_maps = generate_handle_exception(id, sasm);\n+        __ leave();\n+        __ ret();\n+      }\n+      break;\n+\n+    case throw_div0_exception_id:\n+      {\n+        StubFrame f(sasm, \"throw_div0_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);\n+      }\n+      break;\n+\n+    case throw_null_pointer_exception_id:\n+      { StubFrame f(sasm, \"throw_null_pointer_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);\n+      }\n+      break;\n+\n+    case new_instance_id:\n+    case fast_new_instance_id:\n+    case fast_new_instance_init_check_id:\n+      {\n+        Register klass = x13; \/\/ Incoming\n+        Register obj   = x10; \/\/ Result\n+\n+        if (id == new_instance_id) {\n+          __ set_info(\"new_instance\", dont_gc_arguments);\n+        } else if (id == fast_new_instance_id) {\n+          __ set_info(\"fast new_instance\", dont_gc_arguments);\n+        } else {\n+          assert(id == fast_new_instance_init_check_id, \"bad StubID\");\n+          __ set_info(\"fast new_instance init check\", dont_gc_arguments);\n+        }\n+\n+        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n+        \/\/ allocations.\n+        \/\/ Otherwise, just go to the slow path.\n+        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &&\n+            !UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n+          Label slow_path;\n+          Register obj_size   = x12;\n+          Register tmp1       = x9;\n+          Register tmp2       = x14;\n+          assert_different_registers(klass, obj, obj_size, tmp1, tmp2);\n+\n+          const int sp_offset = 2;\n+          const int x9_offset = 1;\n+          const int zr_offset = 0;\n+          __ addi(sp, sp, -(sp_offset * wordSize));\n+          __ sd(x9, Address(sp, x9_offset * wordSize));\n+          __ sd(zr, Address(sp, zr_offset * wordSize));\n+\n+          if (id == fast_new_instance_init_check_id) {\n+            \/\/ make sure the klass is initialized\n+            __ lbu(t0, Address(klass, InstanceKlass::init_state_offset()));\n+            __ mv(t1, InstanceKlass::fully_initialized);\n+            __ bne(t0, t1, slow_path);\n+          }\n+\n+#ifdef ASSERT\n+          \/\/ assert object can be fast path allocated\n+          {\n+            Label ok, not_ok;\n+            __ lw(obj_size, Address(klass, Klass::layout_helper_offset()));\n+            \/\/ make sure it's an instance. For instances, layout helper is a positive number.\n+            \/\/ For arrays, layout helper is a negative number\n+            __ blez(obj_size, not_ok);\n+            __ andi(t0, obj_size, Klass::_lh_instance_slow_path_bit);\n+            __ beqz(t0, ok);\n+            __ bind(not_ok);\n+            __ stop(\"assert(can be fast path allocated)\");\n+            __ should_not_reach_here();\n+            __ bind(ok);\n+          }\n+#endif \/\/ ASSERT\n+\n+          \/\/ get the instance size\n+          __ lwu(obj_size, Address(klass, Klass::layout_helper_offset()));\n+\n+          __ eden_allocate(obj, obj_size, 0, tmp1, slow_path);\n+\n+          __ initialize_object(obj, klass, obj_size, 0, tmp1, tmp2, \/* is_tlab_allocated *\/ false);\n+          __ verify_oop(obj);\n+          __ ld(x9, Address(sp, x9_offset * wordSize));\n+          __ ld(zr, Address(sp, zr_offset * wordSize));\n+          __ addi(sp, sp, sp_offset * wordSize);\n+          __ ret();\n+\n+          __ bind(slow_path);\n+          __ ld(x9, Address(sp, x9_offset * wordSize));\n+          __ ld(zr, Address(sp, zr_offset * wordSize));\n+          __ addi(sp, sp, sp_offset * wordSize);\n+        }\n+\n+        __ enter();\n+        OopMap* map = save_live_registers(sasm);\n+        assert_cond(map != NULL);\n+        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r10(sasm);\n+        __ verify_oop(obj);\n+        __ leave();\n+        __ ret();\n+\n+        \/\/ x10: new instance\n+      }\n+\n+      break;\n+\n+    case counter_overflow_id:\n+      {\n+        Register bci = x10;\n+        Register method = x11;\n+        __ enter();\n+        OopMap* map = save_live_registers(sasm);\n+        assert_cond(map != NULL);\n+\n+        const int bci_off = 0;\n+        const int method_off = 1;\n+        \/\/ Retrieve bci\n+        __ lw(bci, Address(fp, bci_off * BytesPerWord));\n+        \/\/ And a pointer to the Method*\n+        __ ld(method, Address(fp, method_off * BytesPerWord));\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, counter_overflow), bci, method);\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm);\n+        __ leave();\n+        __ ret();\n+      }\n+      break;\n+\n+    case new_type_array_id:\n+    case new_object_array_id:\n+      {\n+        Register length   = x9;  \/\/ Incoming\n+        Register klass    = x13; \/\/ Incoming\n+        Register obj      = x10; \/\/ Result\n+\n+        if (id == new_type_array_id) {\n+          __ set_info(\"new_type_array\", dont_gc_arguments);\n+        } else {\n+          __ set_info(\"new_object_array\", dont_gc_arguments);\n+        }\n+\n+#ifdef ASSERT\n+        \/\/ assert object type is really an array of the proper kind\n+        {\n+          Label ok;\n+          Register tmp = obj;\n+          __ lwu(tmp, Address(klass, Klass::layout_helper_offset()));\n+          __ sraiw(tmp, tmp, Klass::_lh_array_tag_shift);\n+          int tag = ((id == new_type_array_id) ? Klass::_lh_array_tag_type_value : Klass::_lh_array_tag_obj_value);\n+          __ mv(t0, tag);\n+          __ beq(t0, tmp, ok);\n+          __ stop(\"assert(is an array klass)\");\n+          __ should_not_reach_here();\n+          __ bind(ok);\n+        }\n+#endif \/\/ ASSERT\n+\n+        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n+        \/\/ allocations.\n+        \/\/ Otherwise, just go to the slow path.\n+        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n+          Register arr_size   = x14;\n+          Register tmp1       = x12;\n+          Register tmp2       = x15;\n+          Label slow_path;\n+          assert_different_registers(length, klass, obj, arr_size, tmp1, tmp2);\n+\n+          \/\/ check that array length is small enough for fast path.\n+          __ mv(t0, C1_MacroAssembler::max_array_allocation_length);\n+          __ bgtu(length, t0, slow_path);\n+\n+          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n+          __ lwu(tmp1, Address(klass, Klass::layout_helper_offset()));\n+          __ andi(t0, tmp1, 0x1f);\n+          __ sll(arr_size, length, t0);\n+          int lh_header_size_width = exact_log2(Klass::_lh_header_size_mask + 1);\n+          int lh_header_size_msb = Klass::_lh_header_size_shift + lh_header_size_width;\n+          __ slli(tmp1, tmp1, XLEN - lh_header_size_msb);\n+          __ srli(tmp1, tmp1, XLEN - lh_header_size_width);\n+          __ add(arr_size, arr_size, tmp1);\n+          __ addi(arr_size, arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n+          __ andi(arr_size, arr_size, ~(uint)MinObjAlignmentInBytesMask);\n+\n+          __ eden_allocate(obj, arr_size, 0, tmp1, slow_path); \/\/ preserves arr_size\n+\n+          __ initialize_header(obj, klass, length, tmp1, tmp2);\n+          __ lbu(tmp1, Address(klass,\n+                               in_bytes(Klass::layout_helper_offset()) +\n+                               (Klass::_lh_header_size_shift \/ BitsPerByte)));\n+          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n+          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n+          __ andi(tmp1, tmp1, Klass::_lh_header_size_mask);\n+          __ sub(arr_size, arr_size, tmp1); \/\/ body length\n+          __ add(tmp1, tmp1, obj);       \/\/ body start\n+          __ initialize_body(tmp1, arr_size, 0, tmp2);\n+          __ membar(MacroAssembler::StoreStore);\n+          __ verify_oop(obj);\n+\n+          __ ret();\n+\n+          __ bind(slow_path);\n+        }\n+\n+        __ enter();\n+        OopMap* map = save_live_registers(sasm);\n+        assert_cond(map != NULL);\n+        int call_offset = 0;\n+        if (id == new_type_array_id) {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_type_array), klass, length);\n+        } else {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_object_array), klass, length);\n+        }\n+\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r10(sasm);\n+\n+        __ verify_oop(obj);\n+        __ leave();\n+        __ ret();\n+\n+        \/\/ x10: new array\n+      }\n+      break;\n+\n+    case new_multi_array_id:\n+      {\n+        StubFrame f(sasm, \"new_multi_array\", dont_gc_arguments);\n+        \/\/ x10: klass\n+        \/\/ x9: rank\n+        \/\/ x12: address of 1st dimension\n+        OopMap* map = save_live_registers(sasm);\n+        assert_cond(map != NULL);\n+        __ mv(c_rarg1, x10);\n+        __ mv(c_rarg3, x12);\n+        __ mv(c_rarg2, x9);\n+        int call_offset = __ call_RT(x10, noreg, CAST_FROM_FN_PTR(address, new_multi_array), x11, x12, x13);\n+\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r10(sasm);\n+\n+        \/\/ x10: new multi array\n+        __ verify_oop(x10);\n+      }\n+      break;\n+\n+    case register_finalizer_id:\n+      {\n+        __ set_info(\"register_finalizer\", dont_gc_arguments);\n+\n+        \/\/ This is called via call_runtime so the arguments\n+        \/\/ will be place in C abi locations\n+        __ verify_oop(c_rarg0);\n+\n+        \/\/ load the klass and check the has finalizer flag\n+        Label register_finalizer;\n+        Register t = x15;\n+        __ load_klass(t, x10);\n+        __ lwu(t, Address(t, Klass::access_flags_offset()));\n+        __ andi(t0, t, JVM_ACC_HAS_FINALIZER);\n+        __ bnez(t0, register_finalizer);\n+        __ ret();\n+\n+        __ bind(register_finalizer);\n+        __ enter();\n+        OopMap* oop_map = save_live_registers(sasm);\n+        assert_cond(oop_map != NULL);\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), x10);\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, oop_map);\n+\n+        \/\/ Now restore all the live registers\n+        restore_live_registers(sasm);\n+\n+        __ leave();\n+        __ ret();\n+      }\n+      break;\n+\n+    case throw_class_cast_exception_id:\n+      {\n+        StubFrame f(sasm, \"throw_class_cast_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);\n+      }\n+      break;\n+\n+    case throw_incompatible_class_change_error_id:\n+      {\n+        StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm,\n+                                            CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);\n+      }\n+      break;\n+\n+    case slow_subtype_check_id:\n+      {\n+        \/\/ Typical calling sequence:\n+        \/\/ push klass_RInfo (object klass or other subclass)\n+        \/\/ push sup_k_RInfo (array element klass or other superclass)\n+        \/\/ jump to slow_subtype_check\n+        \/\/ Note that the subclass is pushed first, and is therefore deepest.\n+        enum layout {\n+          x10_off, x10_off_hi,\n+          x12_off, x12_off_hi,\n+          x14_off, x14_off_hi,\n+          x15_off, x15_off_hi,\n+          sup_k_off, sup_k_off_hi,\n+          klass_off, klass_off_hi,\n+          framesize,\n+          result_off = sup_k_off\n+        };\n+\n+        __ set_info(\"slow_subtype_check\", dont_gc_arguments);\n+        __ push_reg(RegSet::of(x10, x12, x14, x15), sp);\n+\n+        __ ld(x14, Address(sp, (klass_off) * VMRegImpl::stack_slot_size)); \/\/ sub klass\n+        __ ld(x10, Address(sp, (sup_k_off) * VMRegImpl::stack_slot_size)); \/\/ super klass\n+\n+        Label miss;\n+        __ check_klass_subtype_slow_path(x14, x10, x12, x15, NULL, &miss);\n+\n+        \/\/ fallthrough on success:\n+        __ li(t0, 1);\n+        __ sd(t0, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); \/\/ result\n+        __ pop_reg(RegSet::of(x10, x12, x14, x15), sp);\n+        __ ret();\n+\n+        __ bind(miss);\n+        __ sd(zr, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); \/\/ result\n+        __ pop_reg(RegSet::of(x10, x12, x14, x15), sp);\n+        __ ret();\n+      }\n+      break;\n+\n+    case monitorenter_nofpu_id:\n+      save_fpu_registers = false;\n+      \/\/ fall through\n+    case monitorenter_id:\n+      {\n+        StubFrame f(sasm, \"monitorenter\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, save_fpu_registers);\n+        assert_cond(map != NULL);\n+\n+        \/\/ Called with store_parameter and not C abi\n+        f.load_argument(1, x10); \/\/ x10: object\n+        f.load_argument(0, x11); \/\/ x11: lock address\n+\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), x10, x11);\n+\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm, save_fpu_registers);\n+      }\n+      break;\n+\n+    case monitorexit_nofpu_id:\n+      save_fpu_registers = false;\n+      \/\/ fall through\n+    case monitorexit_id:\n+      {\n+        StubFrame f(sasm, \"monitorexit\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, save_fpu_registers);\n+        assert_cond(map != NULL);\n+\n+        \/\/ Called with store_parameter and not C abi\n+        f.load_argument(0, x10); \/\/ x10: lock address\n+\n+        \/\/ note: really a leaf routine but must setup last java sp\n+        \/\/       => use call_RT for now (speed can be improved by\n+        \/\/       doing last java sp setup manually)\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), x10);\n+\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm, save_fpu_registers);\n+      }\n+      break;\n+\n+    case deoptimize_id:\n+      {\n+        StubFrame f(sasm, \"deoptimize\", dont_gc_arguments);\n+        OopMap* oop_map = save_live_registers(sasm);\n+        assert_cond(oop_map != NULL);\n+        f.load_argument(0, c_rarg1);\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), c_rarg1);\n+\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, oop_map);\n+        restore_live_registers(sasm);\n+        DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();\n+        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+        __ leave();\n+        __ far_jump(RuntimeAddress(deopt_blob->unpack_with_reexecution()));\n+      }\n+      break;\n+\n+    case throw_range_check_failed_id:\n+      {\n+        StubFrame f(sasm, \"range_check_failed\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);\n+      }\n+      break;\n+\n+    case unwind_exception_id:\n+      {\n+        __ set_info(\"unwind_exception\", dont_gc_arguments);\n+        \/\/ note: no stubframe since we are about to leave the current\n+        \/\/       activation and we are calling a leaf VM function only.\n+        generate_unwind_exception(sasm);\n+      }\n+      break;\n+\n+    case access_field_patching_id:\n+      {\n+        StubFrame f(sasm, \"access_field_patching\", dont_gc_arguments);\n+        \/\/ we should set up register map\n+        oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));\n+      }\n+      break;\n+\n+    case load_klass_patching_id:\n+      {\n+        StubFrame f(sasm, \"load_klass_patching\", dont_gc_arguments);\n+        \/\/ we should set up register map\n+        oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));\n+      }\n+      break;\n+\n+    case load_mirror_patching_id:\n+      {\n+        StubFrame f(sasm, \"load_mirror_patching\", dont_gc_arguments);\n+        \/\/ we should set up register map\n+        oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));\n+      }\n+      break;\n+\n+    case load_appendix_patching_id:\n+      {\n+        StubFrame f(sasm, \"load_appendix_patching\", dont_gc_arguments);\n+        \/\/ we should set up register map\n+        oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));\n+      }\n+      break;\n+\n+    case handle_exception_nofpu_id:\n+    case handle_exception_id:\n+      {\n+        StubFrame f(sasm, \"handle_exception\", dont_gc_arguments);\n+        oop_maps = generate_handle_exception(id, sasm);\n+      }\n+      break;\n+\n+    case handle_exception_from_callee_id:\n+      {\n+        StubFrame f(sasm, \"handle_exception_from_callee\", dont_gc_arguments);\n+        oop_maps = generate_handle_exception(id, sasm);\n+      }\n+      break;\n+\n+    case throw_index_exception_id:\n+      {\n+        StubFrame f(sasm, \"index_range_check_failed\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);\n+      }\n+      break;\n+\n+    case throw_array_store_exception_id:\n+      {\n+        StubFrame f(sasm, \"throw_array_store_exception\", dont_gc_arguments);\n+        \/\/ tos + 0: link\n+        \/\/     + 1: return address\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);\n+      }\n+      break;\n+\n+    case predicate_failed_trap_id:\n+      {\n+        StubFrame f(sasm, \"predicate_failed_trap\", dont_gc_arguments);\n+\n+        OopMap* map = save_live_registers(sasm);\n+        assert_cond(map != NULL);\n+\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));\n+        oop_maps = new OopMapSet();\n+        assert_cond(oop_maps != NULL);\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm);\n+        __ leave();\n+        DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();\n+        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+\n+        __ far_jump(RuntimeAddress(deopt_blob->unpack_with_reexecution()));\n+      }\n+      break;\n+\n+    case dtrace_object_alloc_id:\n+      { \/\/ c_rarg0: object\n+        StubFrame f(sasm, \"dtrace_object_alloc\", dont_gc_arguments);\n+        save_live_registers(sasm);\n+\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), c_rarg0);\n+\n+        restore_live_registers(sasm);\n+      }\n+      break;\n+\n+    default:\n+      {\n+        StubFrame f(sasm, \"unimplemented entry\", dont_gc_arguments);\n+        __ li(x10, (int) id);\n+        __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, unimplemented_entry), x10);\n+        __ should_not_reach_here();\n+      }\n+      break;\n+    }\n+  }\n+  return oop_maps;\n+}\n+\n+#undef __\n+\n+const char *Runtime1::pd_name_for_address(address entry) { Unimplemented(); return 0; }\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Runtime1_riscv.cpp","additions":1210,"deletions":0,"binary":false,"changes":1210,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C1_GLOBALS_RISCV_HPP\n+#define CPU_RISCV_C1_GLOBALS_RISCV_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Sets the default values for platform dependent flags used by the client compiler.\n+\/\/ (see c1_globals.hpp)\n+\n+#ifndef TIERED\n+define_pd_global(bool, BackgroundCompilation,        true );\n+define_pd_global(bool, UseTLAB,                      true );\n+define_pd_global(bool, ResizeTLAB,                   true );\n+define_pd_global(bool, InlineIntrinsics,             true );\n+define_pd_global(bool, PreferInterpreterNativeStubs, false);\n+define_pd_global(bool, ProfileTraps,                 false);\n+define_pd_global(bool, UseOnStackReplacement,        true );\n+define_pd_global(bool, TieredCompilation,            false);\n+define_pd_global(intx, CompileThreshold,             1500 );\n+\n+define_pd_global(intx, OnStackReplacePercentage,     933  );\n+define_pd_global(intx, FreqInlineSize,               325  );\n+define_pd_global(intx, NewSizeThreadIncrease,        4*K  );\n+define_pd_global(intx, InitialCodeCacheSize,         160*K);\n+define_pd_global(intx, ReservedCodeCacheSize,        32*M );\n+define_pd_global(intx, NonProfiledCodeHeapSize,      13*M );\n+define_pd_global(intx, ProfiledCodeHeapSize,         14*M );\n+define_pd_global(intx, NonNMethodCodeHeapSize,       5*M  );\n+define_pd_global(bool, ProfileInterpreter,           false);\n+define_pd_global(intx, CodeCacheExpansionSize,       32*K );\n+define_pd_global(uintx, CodeCacheMinBlockLength,     1);\n+define_pd_global(uintx, CodeCacheMinimumUseSpace,    400*K);\n+define_pd_global(uintx, MetaspaceSize,               12*M );\n+define_pd_global(bool, NeverActAsServerClassMachine, true );\n+define_pd_global(uint64_t, MaxRAM,                   1ULL*G);\n+define_pd_global(bool, CICompileOSR,                 true );\n+#endif \/\/ !TIERED\n+define_pd_global(bool, UseTypeProfile,               false);\n+define_pd_global(bool, RoundFPResults,               true );\n+\n+define_pd_global(bool, LIRFillDelaySlots,            false);\n+define_pd_global(bool, OptimizeSinglePrecision,      true );\n+define_pd_global(bool, CSEArrayLength,               false);\n+define_pd_global(bool, TwoOperandLIRForm,            false);\n+\n+#endif \/\/ CPU_RISCV_C1_GLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_globals_riscv.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_C2_GLOBALS_RISCV_HPP\n+#define CPU_RISCV_C2_GLOBALS_RISCV_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Sets the default values for platform dependent flags used by the server compiler.\n+\/\/ (see c2_globals.hpp).  Alpha-sorted.\n+\n+define_pd_global(bool, BackgroundCompilation,        true);\n+define_pd_global(bool, UseTLAB,                      true);\n+define_pd_global(bool, ResizeTLAB,                   true);\n+define_pd_global(bool, CICompileOSR,                 true);\n+define_pd_global(bool, InlineIntrinsics,             true);\n+define_pd_global(bool, PreferInterpreterNativeStubs, false);\n+define_pd_global(bool, ProfileTraps,                 true);\n+define_pd_global(bool, UseOnStackReplacement,        true);\n+define_pd_global(bool, ProfileInterpreter,           true);\n+define_pd_global(bool, TieredCompilation,            trueInTiered);\n+define_pd_global(intx, CompileThreshold,             10000);\n+\n+define_pd_global(intx, OnStackReplacePercentage,     140);\n+define_pd_global(intx, ConditionalMoveLimit,         0);\n+define_pd_global(intx, FLOATPRESSURE,                32);\n+define_pd_global(intx, FreqInlineSize,               325);\n+define_pd_global(intx, MinJumpTableSize,             10);\n+define_pd_global(intx, INTPRESSURE,                  24);\n+define_pd_global(intx, InteriorEntryAlignment,       16);\n+define_pd_global(intx, NewSizeThreadIncrease, ScaleForWordSize(4*K));\n+define_pd_global(intx, LoopUnrollLimit,              60);\n+define_pd_global(intx, LoopPercentProfileLimit,      10);\n+\/\/ InitialCodeCacheSize derived from specjbb2000 run.\n+define_pd_global(intx, InitialCodeCacheSize,         2496*K); \/\/ Integral multiple of CodeCacheExpansionSize\n+define_pd_global(intx, CodeCacheExpansionSize,       64*K);\n+\n+\/\/ Ergonomics related flags\n+define_pd_global(uint64_t,MaxRAM,                    128ULL*G);\n+define_pd_global(intx, RegisterCostAreaRatio,        16000);\n+\n+\/\/ Peephole and CISC spilling both break the graph, and so makes the\n+\/\/ scheduler sick.\n+define_pd_global(bool, OptoPeephole,                 false);\n+define_pd_global(bool, UseCISCSpill,                 false);\n+define_pd_global(bool, OptoScheduling,               true);\n+define_pd_global(bool, OptoBundling,                 false);\n+define_pd_global(bool, OptoRegScheduling,            false);\n+define_pd_global(bool, SuperWordLoopUnrollAnalysis,  true);\n+define_pd_global(bool, IdealizeClearArrayNode,       true);\n+\n+define_pd_global(intx, ReservedCodeCacheSize,        48*M);\n+define_pd_global(intx, NonProfiledCodeHeapSize,      21*M);\n+define_pd_global(intx, ProfiledCodeHeapSize,         22*M);\n+define_pd_global(intx, NonNMethodCodeHeapSize,       5*M );\n+define_pd_global(uintx, CodeCacheMinBlockLength,     6);\n+define_pd_global(uintx, CodeCacheMinimumUseSpace,    400*K);\n+\n+\/\/ Heap related flags\n+define_pd_global(uintx,MetaspaceSize,    ScaleForWordSize(16*M));\n+\n+\/\/ Ergonomics related flags\n+define_pd_global(bool, NeverActAsServerClassMachine, false);\n+\n+define_pd_global(bool, TrapBasedRangeChecks,         false); \/\/ Not needed.\n+\n+#endif \/\/ CPU_RISCV_C2_GLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c2_globals_riscv.hpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -0,0 +1,38 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/node.hpp\"\n+\n+\/\/ processor dependent initialization for riscv\n+\n+extern void reg_mask_init();\n+\n+void Compile::pd_compiler2_init() {\n+  guarantee(CodeEntryAlignment >= InteriorEntryAlignment, \"\" );\n+  reg_mask_init();\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c2_init_riscv.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_CODEBUFFER_RISCV_HPP\n+#define CPU_RISCV_CODEBUFFER_RISCV_HPP\n+\n+private:\n+  void pd_initialize() {}\n+\n+public:\n+  void flush_bundle(bool start_new_bundle) {}\n+\n+#endif \/\/ CPU_RISCV_CODEBUFFER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/codeBuffer_riscv.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,152 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2018, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/compiledIC.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+\n+\/\/ ----------------------------------------------------------------------------\n+\n+#define __ _masm.\n+address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+  precond(cbuf.stubs()->start() != badAddress);\n+  precond(cbuf.stubs()->end() != badAddress);\n+  \/\/ Stub is fixed up when the corresponding call is converted from\n+  \/\/ calling compiled code to calling interpreted code.\n+  \/\/ mv xmethod, 0\n+  \/\/ jalr -4 # to self\n+\n+  if (mark == NULL) {\n+    mark = cbuf.insts_mark();  \/\/ Get mark within main instrs section.\n+  }\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a stub.\n+  MacroAssembler _masm(&cbuf);\n+\n+  address base = __ start_a_stub(to_interp_stub_size());\n+  int offset = __ offset();\n+  if (base == NULL) {\n+    return NULL;  \/\/ CodeBuffer::expand failed\n+  }\n+  \/\/ static stub relocation stores the instruction address of the call\n+  __ relocate(static_stub_Relocation::spec(mark));\n+\n+  __ emit_static_call_stub();\n+\n+  assert((__ offset() - offset) <= (int)to_interp_stub_size(), \"stub too big\");\n+  __ end_a_stub();\n+  return base;\n+}\n+#undef __\n+\n+int CompiledStaticCall::to_interp_stub_size() {\n+  \/\/ fence_i + fence* + (lui, addi, slli, addi, slli, addi) + (lui, addi, slli, addi, slli) + jalr\n+  return NativeFenceI::instruction_size() + 12 * NativeInstruction::instruction_size;\n+}\n+\n+int CompiledStaticCall::to_trampoline_stub_size() {\n+  \/\/ Somewhat pessimistically, we count 4 instructions here (although\n+  \/\/ there are only 3) because we sometimes emit an alignment nop.\n+  \/\/ Trampoline stubs are always word aligned.\n+  return NativeInstruction::instruction_size + NativeCallTrampolineStub::instruction_size;\n+}\n+\n+\/\/ Relocation entries for call stub, compiled java to interpreter.\n+int CompiledStaticCall::reloc_to_interp_stub() {\n+  return 4; \/\/ 3 in emit_to_interp_stub + 1 in emit_call\n+}\n+\n+void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+  address stub = find_stub(false \/* is_aot *\/);\n+  guarantee(stub != NULL, \"stub not found\");\n+\n+  if (TraceICs) {\n+    ResourceMark rm;\n+    tty->print_cr(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n+                  p2i(instruction_address()),\n+                  callee->name_and_sig_as_C_string());\n+  }\n+\n+  \/\/ Creation also verifies the object.\n+  NativeMovConstReg* method_holder\n+    = nativeMovConstReg_at(stub + NativeFenceI::instruction_size());\n+#ifndef PRODUCT\n+  NativeGeneralJump* jump = nativeGeneralJump_at(method_holder->next_instruction_address());\n+\n+  \/\/ read the value once\n+  volatile intptr_t data = method_holder->data();\n+  assert(data == 0 || data == (intptr_t)callee(),\n+         \"a) MT-unsafe modification of inline cache\");\n+  assert(data == 0 || jump->jump_destination() == entry,\n+         \"b) MT-unsafe modification of inline cache\");\n+#endif\n+  \/\/ Update stub.\n+  method_holder->set_data((intptr_t)callee());\n+  NativeGeneralJump::insert_unconditional(method_holder->next_instruction_address(), entry);\n+  ICache::invalidate_range(stub, to_interp_stub_size());\n+  \/\/ Update jump to call.\n+  set_destination_mt_safe(stub);\n+}\n+\n+void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+  assert (CompiledIC_lock->is_locked() || SafepointSynchronize::is_at_safepoint(), \"mt unsafe call\");\n+  \/\/ Reset stub.\n+  address stub = static_stub->addr();\n+  assert(stub != NULL, \"stub not found\");\n+  \/\/ Creation also verifies the object.\n+  NativeMovConstReg* method_holder\n+    = nativeMovConstReg_at(stub + NativeFenceI::instruction_size());\n+  method_holder->set_data(0);\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Non-product mode code\n+#ifndef PRODUCT\n+\n+void CompiledDirectStaticCall::verify() {\n+  \/\/ Verify call.\n+  _call->verify();\n+  _call->verify_alignment();\n+\n+  \/\/ Verify stub.\n+  address stub = find_stub(false \/* is_aot *\/);\n+  assert(stub != NULL, \"no stub found for static call\");\n+  \/\/ Creation also verifies the object.\n+  NativeMovConstReg* method_holder\n+    = nativeMovConstReg_at(stub + NativeFenceI::instruction_size());\n+  NativeJump* jump = nativeJump_at(method_holder->next_instruction_address());\n+\n+  \/\/ Verify state.\n+  assert(is_clean() || is_call_to_compiled() || is_call_to_interpreted(), \"sanity check\");\n+}\n+\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/cpu\/riscv\/compiledIC_riscv.cpp","additions":152,"deletions":0,"binary":false,"changes":152,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_COPY_RISCV_HPP\n+#define CPU_RISCV_COPY_RISCV_HPP\n+\n+\/\/ Inline functions for memory copy and fill.\n+\n+\/\/ Contains inline asm implementations\n+#include OS_CPU_HEADER_INLINE(copy)\n+\n+static void pd_fill_to_words(HeapWord* tohw, size_t count, juint value) {\n+  julong* to = (julong*) tohw;\n+  julong  v  = ((julong) value << 32) | value;\n+  while (count-- > 0) {\n+    *to++ = v;\n+  }\n+}\n+\n+static void pd_fill_to_aligned_words(HeapWord* tohw, size_t count, juint value) {\n+  pd_fill_to_words(tohw, count, value);\n+}\n+\n+static void pd_fill_to_bytes(void* to, size_t count, jubyte value) {\n+  (void)memset(to, value, count);\n+}\n+\n+static void pd_zero_to_words(HeapWord* tohw, size_t count) {\n+  pd_fill_to_words(tohw, count, 0);\n+}\n+\n+static void pd_zero_to_bytes(void* to, size_t count) {\n+  (void)memset(to, 0, count);\n+}\n+\n+#endif \/\/ CPU_RISCV_COPY_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/copy_riscv.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,32 @@\n+\/*\n+ * Copyright (c) 2002, 2010, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VM_DEPCHECKER_RISCV_HPP\n+#define CPU_RISCV_VM_DEPCHECKER_RISCV_HPP\n+\n+\/\/ Nothing to do on riscv\n+\n+#endif \/\/ CPU_RISCV_VM_DEPCHECKER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/depChecker_riscv.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"added"},{"patch":"@@ -0,0 +1,38 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_DISASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_DISASSEMBLER_RISCV_HPP\n+\n+static int pd_instruction_alignment() {\n+  return 1;\n+}\n+\n+static const char* pd_cpu_opts() {\n+  return \"\";\n+}\n+\n+#endif \/\/ CPU_RISCV_DISASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/disassembler_riscv.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"added"},{"patch":"@@ -0,0 +1,694 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/monitorChunk.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#endif\n+\n+#ifdef ASSERT\n+void RegisterMap::check_location_valid() {\n+}\n+#endif\n+\n+\n+\/\/ Profiling\/safepoint support\n+\n+bool frame::safe_for_sender(JavaThread *thread) {\n+  address   sp = (address)_sp;\n+  address   fp = (address)_fp;\n+  address   unextended_sp = (address)_unextended_sp;\n+\n+  \/\/ consider stack guards when trying to determine \"safe\" stack pointers\n+  static size_t stack_guard_size = os::uses_stack_guard_pages() ?\n+                                   (JavaThread::stack_red_zone_size() + JavaThread::stack_yellow_zone_size()) : 0;\n+  size_t usable_stack_size = thread->stack_size() - stack_guard_size;\n+\n+  \/\/ sp must be within the usable part of the stack (not in guards)\n+  bool sp_safe = (sp < thread->stack_base()) &&\n+                 (sp >= thread->stack_base() - usable_stack_size);\n+\n+\n+  if (!sp_safe) {\n+    return false;\n+  }\n+\n+  \/\/ When we are running interpreted code the machine stack pointer, SP, is\n+  \/\/ set low enough so that the Java expression stack can grow and shrink\n+  \/\/ without ever exceeding the machine stack bounds.  So, ESP >= SP.\n+\n+  \/\/ When we call out of an interpreted method, SP is incremented so that\n+  \/\/ the space between SP and ESP is removed.  The SP saved in the callee's\n+  \/\/ frame is the SP *before* this increment.  So, when we walk a stack of\n+  \/\/ interpreter frames the sender's SP saved in a frame might be less than\n+  \/\/ the SP at the point of call.\n+\n+  \/\/ So unextended sp must be within the stack but we need not to check\n+  \/\/ that unextended sp >= sp\n+\n+  bool unextended_sp_safe = (unextended_sp < thread->stack_base());\n+\n+  if (!unextended_sp_safe) {\n+    return false;\n+  }\n+\n+  \/\/ an fp must be within the stack and above (but not equal) sp\n+  \/\/ second evaluation on fp+ is added to handle situation where fp is -1\n+  bool fp_safe = (fp < thread->stack_base() && (fp > sp) && (((fp + (return_addr_offset * sizeof(void*))) < thread->stack_base())));\n+\n+  \/\/ We know sp\/unextended_sp are safe only fp is questionable here\n+\n+  \/\/ If the current frame is known to the code cache then we can attempt to\n+  \/\/ to construct the sender and do some validation of it. This goes a long way\n+  \/\/ toward eliminating issues when we get in frame construction code\n+\n+  if (_cb != NULL) {\n+\n+    \/\/ First check if frame is complete and tester is reliable\n+    \/\/ Unfortunately we can only check frame complete for runtime stubs and nmethod\n+    \/\/ other generic buffer blobs are more problematic so we just assume they are\n+    \/\/ ok. adapter blobs never have a frame complete and are never ok.\n+\n+    if (!_cb->is_frame_complete_at(_pc)) {\n+      if (_cb->is_nmethod() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ Could just be some random pointer within the codeBlob\n+    if (!_cb->code_contains(_pc)) {\n+      return false;\n+    }\n+\n+    \/\/ Entry frame checks\n+    if (is_entry_frame()) {\n+      \/\/ an entry frame must have a valid fp.\n+      return fp_safe && is_entry_frame_valid(thread);\n+    }\n+\n+    intptr_t* sender_sp = NULL;\n+    intptr_t* sender_unextended_sp = NULL;\n+    address   sender_pc = NULL;\n+    intptr_t* saved_fp =  NULL;\n+\n+    if (is_interpreted_frame()) {\n+      \/\/ fp must be safe\n+      if (!fp_safe) {\n+        return false;\n+      }\n+\n+      sender_pc = (address)this->fp()[return_addr_offset];\n+      \/\/ for interpreted frames, the value below is the sender \"raw\" sp,\n+      \/\/ which can be different from the sender unextended sp (the sp seen\n+      \/\/ by the sender) because of current frame local variables\n+      sender_sp = (intptr_t*) addr_at(sender_sp_offset);\n+      sender_unextended_sp = (intptr_t*) this->fp()[interpreter_frame_sender_sp_offset];\n+      saved_fp = (intptr_t*) this->fp()[link_offset];\n+    } else {\n+      \/\/ must be some sort of compiled\/runtime frame\n+      \/\/ fp does not have to be safe (although it could be check for c1?)\n+\n+      \/\/ check for a valid frame_size, otherwise we are unlikely to get a valid sender_pc\n+      if (_cb->frame_size() <= 0) {\n+        return false;\n+      }\n+\n+      sender_sp = _unextended_sp + _cb->frame_size();\n+      \/\/ Is sender_sp safe?\n+      if ((address)sender_sp >= thread->stack_base()) {\n+        return false;\n+      }\n+\n+      sender_unextended_sp = sender_sp;\n+      sender_pc = (address) *(sender_sp - 1);\n+      saved_fp = (intptr_t*) *(sender_sp - 2);\n+    }\n+\n+\n+    \/\/ If the potential sender is the interpreter then we can do some more checking\n+    if (Interpreter::contains(sender_pc)) {\n+\n+      \/\/ fp is always saved in a recognizable place in any code we generate. However\n+      \/\/ only if the sender is interpreted\/call_stub (c1 too?) are we certain that the saved fp\n+      \/\/ is really a frame pointer.\n+\n+      bool saved_fp_safe = ((address)saved_fp < thread->stack_base()) && (saved_fp > sender_sp);\n+\n+      if (!saved_fp_safe) {\n+        return false;\n+      }\n+\n+      \/\/ construct the potential sender\n+      frame sender(sender_sp, sender_unextended_sp, saved_fp, sender_pc);\n+\n+      return sender.is_interpreted_frame_valid(thread);\n+    }\n+\n+    \/\/ We must always be able to find a recognizable pc\n+    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    if (sender_pc == NULL || sender_blob == NULL) {\n+      return false;\n+    }\n+\n+    \/\/ Could be a zombie method\n+    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n+      return false;\n+    }\n+\n+    \/\/ Could just be some random pointer within the codeBlob\n+    if (!sender_blob->code_contains(sender_pc)) {\n+      return false;\n+    }\n+\n+    \/\/ We should never be able to see an adapter if the current frame is something from code cache\n+    if (sender_blob->is_adapter_blob()) {\n+      return false;\n+    }\n+\n+    \/\/ Could be the call_stub\n+    if (StubRoutines::returns_to_call_stub(sender_pc)) {\n+      bool saved_fp_safe = ((address)saved_fp < thread->stack_base()) && (saved_fp > sender_sp);\n+\n+      if (!saved_fp_safe) {\n+        return false;\n+      }\n+\n+      \/\/ construct the potential sender\n+      frame sender(sender_sp, sender_unextended_sp, saved_fp, sender_pc);\n+\n+      \/\/ Validate the JavaCallWrapper an entry frame must have\n+      address jcw = (address)sender.entry_frame_call_wrapper();\n+\n+      bool jcw_safe = (jcw < thread->stack_base()) && (jcw > (address)sender.fp());\n+\n+      return jcw_safe;\n+    }\n+\n+    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    if (nm != NULL) {\n+      if (nm->is_deopt_mh_entry(sender_pc) || nm->is_deopt_entry(sender_pc) ||\n+          nm->method()->is_method_handle_intrinsic()) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ If the frame size is 0 something (or less) is bad because every nmethod has a non-zero frame size\n+    \/\/ because the return address counts against the callee's frame.\n+    if (sender_blob->frame_size() <= 0) {\n+      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      return false;\n+    }\n+\n+    \/\/ We should never be able to see anything here except an nmethod. If something in the\n+    \/\/ code cache (current frame) is called by an entity within the code cache that entity\n+    \/\/ should not be anything but the call stub (already covered), the interpreter (already covered)\n+    \/\/ or an nmethod.\n+    if (!sender_blob->is_compiled()) {\n+        return false;\n+    }\n+\n+    \/\/ Could put some more validation for the potential non-interpreted sender\n+    \/\/ frame we'd create by calling sender if I could think of any. Wait for next crash in forte...\n+\n+    \/\/ One idea is seeing if the sender_pc we have is one that we'd expect to call to current cb\n+\n+    \/\/ We've validated the potential sender that would be created\n+    return true;\n+  }\n+\n+  \/\/ Must be native-compiled frame. Since sender will try and use fp to find\n+  \/\/ linkages it must be safe\n+  if (!fp_safe) {\n+    return false;\n+  }\n+\n+  \/\/ Will the pc we fetch be non-zero (which we'll find at the oldest frame)\n+  if ((address)this->fp()[return_addr_offset] == NULL) { return false; }\n+\n+  return true;\n+}\n+\n+void frame::patch_pc(Thread* thread, address pc) {\n+  address* pc_addr = &(((address*) sp())[-1]);\n+  if (TracePcPatching) {\n+    tty->print_cr(\"patch_pc at address \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \" -> \" INTPTR_FORMAT \"]\",\n+                  p2i(pc_addr), p2i(*pc_addr), p2i(pc));\n+  }\n+  \/\/ Either the return address is the original one or we are going to\n+  \/\/ patch in the same address that's already there.\n+  assert(_pc == *pc_addr || pc == *pc_addr, \"must be\");\n+  *pc_addr = pc;\n+  _cb = CodeCache::find_blob(pc);\n+  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  if (original_pc != NULL) {\n+    assert(original_pc == _pc, \"expected original PC to be stored before patching\");\n+    _deopt_state = is_deoptimized;\n+    \/\/ leave _pc as is\n+  } else {\n+    _deopt_state = not_deoptimized;\n+    _pc = pc;\n+  }\n+}\n+\n+bool frame::is_interpreted_frame() const  {\n+  return Interpreter::contains(pc());\n+}\n+\n+int frame::frame_size(RegisterMap* map) const {\n+  frame sender = this->sender(map);\n+  return sender.sp() - sp();\n+}\n+\n+intptr_t* frame::entry_frame_argument_at(int offset) const {\n+  \/\/ convert offset to index to deal with tsi\n+  int index = (Interpreter::expr_offset_in_bytes(offset)\/wordSize);\n+  \/\/ Entry frame's arguments are always in relation to unextended_sp()\n+  return &unextended_sp()[index];\n+}\n+\n+\/\/ sender_sp\n+intptr_t* frame::interpreter_frame_sender_sp() const {\n+  assert(is_interpreted_frame(), \"interpreted frame expected\");\n+  return (intptr_t*) at(interpreter_frame_sender_sp_offset);\n+}\n+\n+void frame::set_interpreter_frame_sender_sp(intptr_t* sender_sp) {\n+  assert(is_interpreted_frame(), \"interpreted frame expected\");\n+  ptr_at_put(interpreter_frame_sender_sp_offset, (intptr_t) sender_sp);\n+}\n+\n+\n+\/\/ monitor elements\n+\n+BasicObjectLock* frame::interpreter_frame_monitor_begin() const {\n+  return (BasicObjectLock*) addr_at(interpreter_frame_monitor_block_bottom_offset);\n+}\n+\n+BasicObjectLock* frame::interpreter_frame_monitor_end() const {\n+  BasicObjectLock* result = (BasicObjectLock*) *addr_at(interpreter_frame_monitor_block_top_offset);\n+  \/\/ make sure the pointer points inside the frame\n+  assert(sp() <= (intptr_t*) result, \"monitor end should be above the stack pointer\");\n+  assert((intptr_t*) result < fp(),  \"monitor end should be strictly below the frame pointer\");\n+  return result;\n+}\n+\n+void frame::interpreter_frame_set_monitor_end(BasicObjectLock* value) {\n+  *((BasicObjectLock**)addr_at(interpreter_frame_monitor_block_top_offset)) = value;\n+}\n+\n+\/\/ Used by template based interpreter deoptimization\n+void frame::interpreter_frame_set_last_sp(intptr_t* last_sp) {\n+  *((intptr_t**)addr_at(interpreter_frame_last_sp_offset)) = last_sp;\n+}\n+\n+frame frame::sender_for_entry_frame(RegisterMap* map) const {\n+  assert(map != NULL, \"map must be set\");\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = entry_frame_call_wrapper()->anchor();\n+  assert(!entry_frame_is_first(), \"next Java fp must be non zero\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  if (!jfa->walkable()) {\n+    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n+    jfa->capture_last_Java_pc();\n+  }\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_fp(), jfa->last_Java_pc());\n+  return fr;\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::verify_deopt_original_pc\n+\/\/\n+\/\/ Verifies the calculated original PC of a deoptimization PC for the\n+\/\/ given unextended SP.\n+#ifdef ASSERT\n+void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+  frame fr;\n+\n+  \/\/ This is ugly but it's better than to change {get,set}_original_pc\n+  \/\/ to take an SP value as argument.  And it's only a debugging\n+  \/\/ method anyway.\n+  fr._unextended_sp = unextended_sp;\n+\n+  assert_cond(nm != NULL);\n+  address original_pc = nm->get_original_pc(&fr);\n+  assert(nm->insts_contains_inclusive(original_pc),\n+         \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n+}\n+#endif\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::adjust_unextended_sp\n+void frame::adjust_unextended_sp() {\n+  \/\/ On riscv, sites calling method handle intrinsics and lambda forms are treated\n+  \/\/ as any other call site. Therefore, no special action is needed when we are\n+  \/\/ returning to any of these call sites.\n+\n+  if (_cb != NULL) {\n+    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n+    if (sender_cm != NULL) {\n+      \/\/ If the sender PC is a deoptimization point, get the original PC.\n+      if (sender_cm->is_deopt_entry(_pc) ||\n+          sender_cm->is_deopt_mh_entry(_pc)) {\n+        DEBUG_ONLY(verify_deopt_original_pc(sender_cm, _unextended_sp));\n+      }\n+    }\n+  }\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::update_map_with_saved_link\n+void frame::update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr) {\n+  \/\/ The interpreter and compiler(s) always save fp in a known\n+  \/\/ location on entry. We must record where that location is\n+  \/\/ so that if fp was live on callout from c2 we can find\n+  \/\/ the saved copy no matter what it called.\n+\n+  \/\/ Since the interpreter always saves fp if we record where it is then\n+  \/\/ we don't have to always save fp on entry and exit to c2 compiled\n+  \/\/ code, on entry will be enough.\n+  assert(map != NULL, \"map must be set\");\n+  map->set_location(::fp->as_VMReg(), (address) link_addr);\n+  \/\/ this is weird \"H\" ought to be at a higher address however the\n+  \/\/ oopMaps seems to have the \"H\" regs at the same address and the\n+  \/\/ vanilla register.\n+  map->set_location(::fp->as_VMReg()->next(), (address) link_addr);\n+}\n+\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender_for_interpreter_frame\n+frame frame::sender_for_interpreter_frame(RegisterMap* map) const {\n+  \/\/ SP is the raw SP from the sender after adapter or interpreter\n+  \/\/ extension.\n+  intptr_t* sender_sp = this->sender_sp();\n+\n+  \/\/ This is the sp before any possible extension (adapter\/locals).\n+  intptr_t* unextended_sp = interpreter_frame_sender_sp();\n+\n+#ifdef COMPILER2\n+  assert(map != NULL, \"map must be set\");\n+  if (map->update_map()) {\n+    update_map_with_saved_link(map, (intptr_t**) addr_at(link_offset));\n+  }\n+#endif \/\/ COMPILER2\n+\n+  return frame(sender_sp, unextended_sp, link(), sender_pc());\n+}\n+\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender_for_compiled_frame\n+frame frame::sender_for_compiled_frame(RegisterMap* map) const {\n+  \/\/ we cannot rely upon the last fp having been saved to the thread\n+  \/\/ in C2 code but it will have been pushed onto the stack. so we\n+  \/\/ have to find it relative to the unextended sp\n+\n+  assert(_cb->frame_size() >= 0, \"must have non-zero frame size\");\n+  intptr_t* l_sender_sp = unextended_sp() + _cb->frame_size();\n+  intptr_t* unextended_sp = l_sender_sp;\n+\n+  \/\/ the return_address is always the word on the stack\n+  address sender_pc = (address) *(l_sender_sp + frame::return_addr_offset);\n+\n+  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp + frame::link_offset);\n+\n+  assert(map != NULL, \"map must be set\");\n+  if (map->update_map()) {\n+    \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n+    \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n+    \/\/ outside of update_register_map.\n+    map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    if (_cb->oop_maps() != NULL) {\n+      OopMapSet::update_register_map(this, map);\n+    }\n+\n+    \/\/ Since the prolog does the save and restore of FP there is no\n+    \/\/ oopmap for it so we must fill in its location as if there was\n+    \/\/ an oopmap entry since if our caller was compiled code there\n+    \/\/ could be live jvm state in it.\n+    update_map_with_saved_link(map, saved_fp_addr);\n+  }\n+\n+  return frame(l_sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender\n+frame frame::sender(RegisterMap* map) const {\n+  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ update it accordingly\n+  assert(map != NULL, \"map must be set\");\n+  map->set_include_argument_oops(false);\n+\n+  if (is_entry_frame()) {\n+    return sender_for_entry_frame(map);\n+  }\n+  if (is_interpreted_frame()) {\n+    return sender_for_interpreter_frame(map);\n+  }\n+  assert(_cb == CodeCache::find_blob(pc()),\"Must be the same\");\n+\n+  \/\/ This test looks odd: why is it not is_compiled_frame() ?  That's\n+  \/\/ because stubs also have OOP maps.\n+  if (_cb != NULL) {\n+    return sender_for_compiled_frame(map);\n+  }\n+\n+  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n+  \/\/ methods that exists in the core system.\n+  return frame(sender_sp(), link(), sender_pc());\n+}\n+\n+bool frame::is_interpreted_frame_valid(JavaThread* thread) const {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  \/\/ These are reasonable sanity checks\n+  if (fp() == NULL || (intptr_t(fp()) & (wordSize-1)) != 0) {\n+    return false;\n+  }\n+  if (sp() == NULL || (intptr_t(sp()) & (wordSize-1)) != 0) {\n+    return false;\n+  }\n+  if (fp() + interpreter_frame_initial_sp_offset < sp()) {\n+    return false;\n+  }\n+  \/\/ These are hacks to keep us out of trouble.\n+  \/\/ The problem with these is that they mask other problems\n+  if (fp() <= sp()) {        \/\/ this attempts to deal with unsigned comparison above\n+    return false;\n+  }\n+\n+  \/\/ do some validation of frame elements\n+\n+  \/\/ first the method\n+  Method* m = *interpreter_frame_method_addr();\n+  \/\/ validate the method we'd find in this potential sender\n+  if (!Method::is_valid_method(m)) {\n+    return false;\n+  }\n+\n+  \/\/ stack frames shouldn't be much larger than max_stack elements\n+  \/\/ this test requires the use of unextended_sp which is the sp as seen by\n+  \/\/ the current frame, and not sp which is the \"raw\" pc which could point\n+  \/\/ further because of local variables of the callee method inserted after\n+  \/\/ method arguments\n+  if (fp() - unextended_sp() > 1024 + m->max_stack()*Interpreter::stackElementSize) {\n+    return false;\n+  }\n+\n+  \/\/ validate bci\/bcx\n+  address bcp = interpreter_frame_bcp();\n+  if (m->validate_bci_from_bcp(bcp) < 0) {\n+    return false;\n+  }\n+\n+  \/\/ validate constantPoolCache*\n+  ConstantPoolCache* cp = *interpreter_frame_cache_addr();\n+  if (MetaspaceObj::is_valid(cp) == false) {\n+    return false;\n+  }\n+\n+  \/\/ validate locals\n+  address locals = (address) *interpreter_frame_locals_addr();\n+  if (locals > thread->stack_base()) {\n+    return false;\n+  }\n+\n+  if (m->max_locals() > 0 && locals < (address) fp()) {\n+    \/\/ fp in interpreter frame on RISC-V is higher than that on AArch64,\n+    \/\/ pointing to sender_sp and sender_sp-2 relatively.\n+    \/\/ On RISC-V, if max_locals is 0, the 'locals' pointer may be below fp,\n+    \/\/ pointing to sender_sp-1 (with one padding slot).\n+    \/\/ So we verify the 'locals' pointer only if max_locals > 0.\n+    return false;\n+  }\n+\n+  \/\/ We'd have to be pretty unlucky to be mislead at this point\n+  return true;\n+}\n+\n+BasicType frame::interpreter_frame_result(oop* oop_result, jvalue* value_result) {\n+  assert(is_interpreted_frame(), \"interpreted frame expected\");\n+  Method* method = interpreter_frame_method();\n+  BasicType type = method->result_type();\n+\n+  intptr_t* tos_addr = NULL;\n+  if (method->is_native()) {\n+    tos_addr = (intptr_t*)sp();\n+    if (type == T_FLOAT || type == T_DOUBLE) {\n+      \/\/ This is because we do a push(ltos) after push(dtos) in generate_native_entry.\n+      tos_addr += 2 * Interpreter::stackElementWords;\n+    }\n+  } else {\n+    tos_addr = (intptr_t*)interpreter_frame_tos_address();\n+  }\n+\n+  switch (type) {\n+    case T_OBJECT  :\n+    case T_ARRAY   : {\n+      oop obj;\n+      if (method->is_native()) {\n+        obj = cast_to_oop(at(interpreter_frame_oop_temp_offset));\n+      } else {\n+        oop* obj_p = (oop*)tos_addr;\n+        obj = (obj_p == NULL) ? (oop)NULL : *obj_p;\n+      }\n+      assert(obj == NULL || Universe::heap()->is_in(obj), \"sanity check\");\n+      *oop_result = obj;\n+      break;\n+    }\n+    case T_BOOLEAN : value_result->z = *(jboolean*)tos_addr; break;\n+    case T_BYTE    : value_result->b = *(jbyte*)tos_addr; break;\n+    case T_CHAR    : value_result->c = *(jchar*)tos_addr; break;\n+    case T_SHORT   : value_result->s = *(jshort*)tos_addr; break;\n+    case T_INT     : value_result->i = *(jint*)tos_addr; break;\n+    case T_LONG    : value_result->j = *(jlong*)tos_addr; break;\n+    case T_FLOAT   : {\n+        value_result->f = *(jfloat*)tos_addr;\n+      break;\n+    }\n+    case T_DOUBLE  : value_result->d = *(jdouble*)tos_addr; break;\n+    case T_VOID    : \/* Nothing to do *\/ break;\n+    default        : ShouldNotReachHere();\n+  }\n+\n+  return type;\n+}\n+\n+\n+intptr_t* frame::interpreter_frame_tos_at(jint offset) const {\n+  int index = (Interpreter::expr_offset_in_bytes(offset)\/wordSize);\n+  return &interpreter_frame_tos_address()[index];\n+}\n+\n+#ifndef PRODUCT\n+\n+#define DESCRIBE_FP_OFFSET(name) \\\n+  values.describe(frame_no, fp() + frame::name##_offset, #name)\n+\n+void frame::describe_pd(FrameValues& values, int frame_no) {\n+  if (is_interpreted_frame()) {\n+    DESCRIBE_FP_OFFSET(interpreter_frame_sender_sp);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_last_sp);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_method);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_mdp);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_mirror);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_cache);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_locals);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_bcp);\n+    DESCRIBE_FP_OFFSET(interpreter_frame_initial_sp);\n+  }\n+}\n+#endif\n+\n+intptr_t *frame::initial_deoptimization_info() {\n+  \/\/ Not used on riscv, but we must return something.\n+  return NULL;\n+}\n+\n+intptr_t* frame::real_fp() const {\n+  if (_cb != NULL) {\n+    \/\/ use the frame size if valid\n+    int size = _cb->frame_size();\n+    if (size > 0) {\n+      return unextended_sp() + size;\n+    }\n+  }\n+  \/\/ else rely on fp()\n+  assert(!is_compiled_frame(), \"unknown compiled frame size\");\n+  return fp();\n+}\n+\n+#undef DESCRIBE_FP_OFFSET\n+\n+#ifndef PRODUCT\n+\/\/ This is a generic constructor which is only used by pns() in debug.cpp.\n+frame::frame(void* ptr_sp, void* ptr_fp, void* pc) {\n+  init((intptr_t*)ptr_sp, (intptr_t*)ptr_fp, (address)pc);\n+}\n+\n+void frame::pd_ps() {}\n+#endif\n+\n+void JavaFrameAnchor::make_walkable(JavaThread* thread) {\n+  \/\/ last frame set?\n+  if (last_Java_sp() == NULL) { return; }\n+  \/\/ already walkable?\n+  if (walkable()) { return; }\n+  vmassert(Thread::current() == (Thread*)thread, \"not current thread\");\n+  vmassert(last_Java_sp() != NULL, \"not called from Java code?\");\n+  vmassert(last_Java_pc() == NULL, \"already walkable\");\n+  capture_last_Java_pc();\n+  vmassert(walkable(), \"something went wrong\");\n+}\n+\n+void JavaFrameAnchor::capture_last_Java_pc() {\n+  vmassert(_last_Java_sp != NULL, \"no last frame set\");\n+  vmassert(_last_Java_pc == NULL, \"already walkable\");\n+  _last_Java_pc = (address)_last_Java_sp[-1];\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.cpp","additions":694,"deletions":0,"binary":false,"changes":694,"status":"added"},{"patch":"@@ -0,0 +1,199 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_FRAME_RISCV_HPP\n+#define CPU_RISCV_FRAME_RISCV_HPP\n+\n+#include \"runtime\/synchronizer.hpp\"\n+\n+\/\/ A frame represents a physical stack frame (an activation).  Frames can be\n+\/\/ C or Java frames, and the Java frames can be interpreted or compiled.\n+\/\/ In contrast, vframes represent source-level activations, so that one physical frame\n+\/\/ can correspond to multiple source level frames because of inlining.\n+\/\/ A frame is comprised of {pc, fp, sp}\n+\/\/ ------------------------------ Asm interpreter ----------------------------------------\n+\/\/ Layout of asm interpreter frame:\n+\/\/    [expression stack      ] * <- sp\n+\n+\/\/    [monitors[0]           ]   \\\n+\/\/     ...                        | monitor block size = k\n+\/\/    [monitors[k-1]         ]   \/\n+\/\/    [frame initial esp     ] ( == &monitors[0], initially here)       initial_sp_offset\n+\/\/    [byte code index\/pointr]                   = bcx()                bcx_offset\n+\n+\/\/    [pointer to locals     ]                   = locals()             locals_offset\n+\/\/    [constant pool cache   ]                   = cache()              cache_offset\n+\n+\/\/    [klass of method       ]                   = mirror()             mirror_offset\n+\/\/    [padding               ]\n+\n+\/\/    [methodData            ]                   = mdp()                mdx_offset\n+\/\/    [Method                ]                   = method()             method_offset\n+\n+\/\/    [last esp              ]                   = last_sp()            last_sp_offset\n+\/\/    [old stack pointer     ]                     (sender_sp)          sender_sp_offset\n+\n+\/\/    [old frame pointer     ]\n+\/\/    [return pc             ]\n+\n+\/\/    [last sp               ]   <- fp           = link()\n+\/\/    [oop temp              ]                     (only for native calls)\n+\n+\/\/    [padding               ]                     (to preserve machine SP alignment)\n+\/\/    [locals and parameters ]\n+\/\/                               <- sender sp\n+\/\/ ------------------------------ Asm interpreter ----------------------------------------\n+\n+\/\/ ------------------------------ C Frame ------------------------------------------------\n+\/\/ Stack: gcc with -fno-omit-frame-pointer\n+\/\/                    .\n+\/\/                    .\n+\/\/       +->          .\n+\/\/       |   +-----------------+   |\n+\/\/       |   | return address  |   |\n+\/\/       |   |   previous fp ------+\n+\/\/       |   | saved registers |\n+\/\/       |   | local variables |\n+\/\/       |   |       ...       | <-+\n+\/\/       |   +-----------------+   |\n+\/\/       |   | return address  |   |\n+\/\/       +------ previous fp   |   |\n+\/\/           | saved registers |   |\n+\/\/           | local variables |   |\n+\/\/       +-> |       ...       |   |\n+\/\/       |   +-----------------+   |\n+\/\/       |   | return address  |   |\n+\/\/       |   |   previous fp ------+\n+\/\/       |   | saved registers |\n+\/\/       |   | local variables |\n+\/\/       |   |       ...       | <-+\n+\/\/       |   +-----------------+   |\n+\/\/       |   | return address  |   |\n+\/\/       +------ previous fp   |   |\n+\/\/           | saved registers |   |\n+\/\/           | local variables |   |\n+\/\/   $fp --> |       ...       |   |\n+\/\/           +-----------------+   |\n+\/\/           | return address  |   |\n+\/\/           |   previous fp ------+\n+\/\/           | saved registers |\n+\/\/   $sp --> | local variables |\n+\/\/           +-----------------+\n+\/\/ ------------------------------ C Frame ------------------------------------------------\n+\n+ public:\n+  enum {\n+    pc_return_offset                                 =  0,\n+    \/\/ All frames\n+    link_offset                                      = -2,\n+    return_addr_offset                               = -1,\n+    sender_sp_offset                                 =  0,\n+    \/\/ Interpreter frames\n+    interpreter_frame_oop_temp_offset                =  1, \/\/ for native calls only\n+\n+    interpreter_frame_sender_sp_offset               = -3,\n+    \/\/ outgoing sp before a call to an invoked method\n+    interpreter_frame_last_sp_offset                 = interpreter_frame_sender_sp_offset - 1,\n+    interpreter_frame_method_offset                  = interpreter_frame_last_sp_offset - 1,\n+    interpreter_frame_mdp_offset                     = interpreter_frame_method_offset - 1,\n+    interpreter_frame_padding_offset                 = interpreter_frame_mdp_offset - 1,\n+    interpreter_frame_mirror_offset                  = interpreter_frame_padding_offset - 1,\n+    interpreter_frame_cache_offset                   = interpreter_frame_mirror_offset - 1,\n+    interpreter_frame_locals_offset                  = interpreter_frame_cache_offset - 1,\n+    interpreter_frame_bcp_offset                     = interpreter_frame_locals_offset - 1,\n+    interpreter_frame_initial_sp_offset              = interpreter_frame_bcp_offset - 1,\n+\n+    interpreter_frame_monitor_block_top_offset       = interpreter_frame_initial_sp_offset,\n+    interpreter_frame_monitor_block_bottom_offset    = interpreter_frame_initial_sp_offset,\n+\n+    \/\/ Entry frames\n+    \/\/ n.b. these values are determined by the layout defined in\n+    \/\/ stubGenerator for the Java call stub\n+    entry_frame_after_call_words                     =  34,\n+    entry_frame_call_wrapper_offset                  = -10,\n+\n+    \/\/ we don't need a save area\n+    arg_reg_save_area_bytes                          =  0\n+  };\n+\n+  intptr_t ptr_at(int offset) const {\n+    return *ptr_at_addr(offset);\n+  }\n+\n+  void ptr_at_put(int offset, intptr_t value) {\n+    *ptr_at_addr(offset) = value;\n+  }\n+\n+ private:\n+  \/\/ an additional field beyond _sp and _pc:\n+  intptr_t*   _fp; \/\/ frame pointer\n+  \/\/ The interpreter and adapters will extend the frame of the caller.\n+  \/\/ Since oopMaps are based on the sp of the caller before extension\n+  \/\/ we need to know that value. However in order to compute the address\n+  \/\/ of the return address we need the real \"raw\" sp. Since sparc already\n+  \/\/ uses sp() to mean \"raw\" sp and unextended_sp() to mean the caller's\n+  \/\/ original sp we use that convention.\n+\n+  intptr_t*     _unextended_sp;\n+  void adjust_unextended_sp();\n+\n+  intptr_t* ptr_at_addr(int offset) const {\n+    return (intptr_t*) addr_at(offset);\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Used in frame::sender_for_{interpreter,compiled}_frame\n+  static void verify_deopt_original_pc(   CompiledMethod* nm, intptr_t* unextended_sp);\n+#endif\n+\n+ public:\n+  \/\/ Constructors\n+\n+  frame(intptr_t* ptr_sp, intptr_t* ptr_fp, address pc);\n+\n+  frame(intptr_t* ptr_sp, intptr_t* unextended_sp, intptr_t* ptr_fp, address pc);\n+\n+  frame(intptr_t* ptr_sp, intptr_t* ptr_fp);\n+\n+  void init(intptr_t* ptr_sp, intptr_t* ptr_fp, address pc);\n+\n+  \/\/ accessors for the instance variables\n+  \/\/ Note: not necessarily the real 'frame pointer' (see real_fp)\n+  intptr_t*   fp() const { return _fp; }\n+\n+  inline address* sender_pc_addr() const;\n+\n+  \/\/ expression stack tos if we are nested in a java call\n+  intptr_t* interpreter_frame_last_sp() const;\n+\n+  \/\/ helper to update a map with callee-saved RBP\n+  static void update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr);\n+\n+  \/\/ deoptimization support\n+  void interpreter_frame_set_last_sp(intptr_t* last_sp);\n+\n+  static jint interpreter_frame_expression_stack_direction() { return -1; }\n+\n+#endif \/\/ CPU_RISCV_FRAME_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.hpp","additions":199,"deletions":0,"binary":false,"changes":199,"status":"added"},{"patch":"@@ -0,0 +1,245 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_FRAME_RISCV_INLINE_HPP\n+#define CPU_RISCV_FRAME_RISCV_INLINE_HPP\n+\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+\n+\/\/ Inline functions for RISCV frames:\n+\n+\/\/ Constructors:\n+\n+inline frame::frame() {\n+  _pc = NULL;\n+  _sp = NULL;\n+  _unextended_sp = NULL;\n+  _fp = NULL;\n+  _cb = NULL;\n+  _deopt_state = unknown;\n+}\n+\n+static int spin;\n+\n+inline void frame::init(intptr_t* ptr_sp, intptr_t* ptr_fp, address pc) {\n+  intptr_t a = intptr_t(ptr_sp);\n+  intptr_t b = intptr_t(ptr_fp);\n+  _sp = ptr_sp;\n+  _unextended_sp = ptr_sp;\n+  _fp = ptr_fp;\n+  _pc = pc;\n+  assert(pc != NULL, \"no pc?\");\n+  _cb = CodeCache::find_blob(pc);\n+  adjust_unextended_sp();\n+\n+  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  if (original_pc != NULL) {\n+    _pc = original_pc;\n+    _deopt_state = is_deoptimized;\n+  } else {\n+    _deopt_state = not_deoptimized;\n+  }\n+}\n+\n+inline frame::frame(intptr_t* ptr_sp, intptr_t* ptr_fp, address pc) {\n+  init(ptr_sp, ptr_fp, pc);\n+}\n+\n+inline frame::frame(intptr_t* ptr_sp, intptr_t* unextended_sp, intptr_t* ptr_fp, address pc) {\n+  intptr_t a = intptr_t(ptr_sp);\n+  intptr_t b = intptr_t(ptr_fp);\n+  _sp = ptr_sp;\n+  _unextended_sp = unextended_sp;\n+  _fp = ptr_fp;\n+  _pc = pc;\n+  assert(pc != NULL, \"no pc?\");\n+  _cb = CodeCache::find_blob(pc);\n+  adjust_unextended_sp();\n+\n+  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  if (original_pc != NULL) {\n+    _pc = original_pc;\n+    assert(_cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+           \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n+    _deopt_state = is_deoptimized;\n+  } else {\n+    _deopt_state = not_deoptimized;\n+  }\n+}\n+\n+inline frame::frame(intptr_t* ptr_sp, intptr_t* ptr_fp) {\n+  intptr_t a = intptr_t(ptr_sp);\n+  intptr_t b = intptr_t(ptr_fp);\n+  _sp = ptr_sp;\n+  _unextended_sp = ptr_sp;\n+  _fp = ptr_fp;\n+  _pc = (address)(ptr_sp[-1]);\n+\n+  \/\/ Here's a sticky one. This constructor can be called via AsyncGetCallTrace\n+  \/\/ when last_Java_sp is non-null but the pc fetched is junk. If we are truly\n+  \/\/ unlucky the junk value could be to a zombied method and we'll die on the\n+  \/\/ find_blob call. This is also why we can have no asserts on the validity\n+  \/\/ of the pc we find here. AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n+  \/\/ -> pd_last_frame should use a specialized version of pd_last_frame which could\n+  \/\/ call a specilaized frame constructor instead of this one.\n+  \/\/ Then we could use the assert below. However this assert is of somewhat dubious\n+  \/\/ value.\n+\n+  _cb = CodeCache::find_blob(_pc);\n+  adjust_unextended_sp();\n+\n+  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  if (original_pc != NULL) {\n+    _pc = original_pc;\n+    _deopt_state = is_deoptimized;\n+  } else {\n+    _deopt_state = not_deoptimized;\n+  }\n+}\n+\n+\/\/ Accessors\n+\n+inline bool frame::equal(frame other) const {\n+  bool ret =  sp() == other.sp() &&\n+              unextended_sp() == other.unextended_sp() &&\n+              fp() == other.fp() &&\n+              pc() == other.pc();\n+  assert(!ret || ret && cb() == other.cb() && _deopt_state == other._deopt_state, \"inconsistent construction\");\n+  return ret;\n+}\n+\n+\/\/ Return unique id for this frame. The id must have a value where we can distinguish\n+\/\/ identity and younger\/older relationship. NULL represents an invalid (incomparable)\n+\/\/ frame.\n+inline intptr_t* frame::id(void) const { return unextended_sp(); }\n+\n+\/\/ Return true if the frame is older (less recent activation) than the frame represented by id\n+inline bool frame::is_older(intptr_t* id) const   { assert(this->id() != NULL && id != NULL, \"NULL frame id\");\n+                                                    return this->id() > id ; }\n+\n+inline intptr_t* frame::link() const              { return (intptr_t*) *(intptr_t **)addr_at(link_offset); }\n+\n+inline intptr_t* frame::link_or_null() const {\n+  intptr_t** ptr = (intptr_t **)addr_at(link_offset);\n+  return os::is_readable_pointer(ptr) ? *ptr : NULL;\n+}\n+\n+inline intptr_t* frame::unextended_sp() const     { return _unextended_sp; }\n+\n+\/\/ Return address\n+inline address* frame::sender_pc_addr() const     { return (address*) addr_at(return_addr_offset); }\n+inline address  frame::sender_pc() const          { return *sender_pc_addr(); }\n+inline intptr_t* frame::sender_sp() const         { return addr_at(sender_sp_offset); }\n+\n+inline intptr_t** frame::interpreter_frame_locals_addr() const {\n+  return (intptr_t**)addr_at(interpreter_frame_locals_offset);\n+}\n+\n+inline intptr_t* frame::interpreter_frame_last_sp() const {\n+  return *(intptr_t**)addr_at(interpreter_frame_last_sp_offset);\n+}\n+\n+inline intptr_t* frame::interpreter_frame_bcp_addr() const {\n+  return (intptr_t*)addr_at(interpreter_frame_bcp_offset);\n+}\n+\n+inline intptr_t* frame::interpreter_frame_mdp_addr() const {\n+  return (intptr_t*)addr_at(interpreter_frame_mdp_offset);\n+}\n+\n+\n+\/\/ Constant pool cache\n+\n+inline ConstantPoolCache** frame::interpreter_frame_cache_addr() const {\n+  return (ConstantPoolCache**)addr_at(interpreter_frame_cache_offset);\n+}\n+\n+\/\/ Method\n+\n+inline Method** frame::interpreter_frame_method_addr() const {\n+  return (Method**)addr_at(interpreter_frame_method_offset);\n+}\n+\n+\/\/ Mirror\n+\n+inline oop* frame::interpreter_frame_mirror_addr() const {\n+  return (oop*)addr_at(interpreter_frame_mirror_offset);\n+}\n+\n+\/\/ top of expression stack\n+inline intptr_t* frame::interpreter_frame_tos_address() const {\n+  intptr_t* last_sp = interpreter_frame_last_sp();\n+  if (last_sp == NULL) {\n+    return sp();\n+  } else {\n+    \/\/ sp() may have been extended or shrunk by an adapter.  At least\n+    \/\/ check that we don't fall behind the legal region.\n+    \/\/ For top deoptimized frame last_sp == interpreter_frame_monitor_end.\n+    assert(last_sp <= (intptr_t*) interpreter_frame_monitor_end(), \"bad tos\");\n+    return last_sp;\n+  }\n+}\n+\n+inline oop* frame::interpreter_frame_temp_oop_addr() const {\n+  return (oop *)(fp() + interpreter_frame_oop_temp_offset);\n+}\n+\n+inline int frame::interpreter_frame_monitor_size() {\n+  return BasicObjectLock::size();\n+}\n+\n+\n+\/\/ expression stack\n+\/\/ (the max_stack arguments are used by the GC; see class FrameClosure)\n+\n+inline intptr_t* frame::interpreter_frame_expression_stack() const {\n+  intptr_t* monitor_end = (intptr_t*) interpreter_frame_monitor_end();\n+  return monitor_end-1;\n+}\n+\n+\n+\/\/ Entry frames\n+\n+inline JavaCallWrapper** frame::entry_frame_call_wrapper_addr() const {\n+ return (JavaCallWrapper**)addr_at(entry_frame_call_wrapper_offset);\n+}\n+\n+\n+\/\/ Compiled frames\n+inline oop frame::saved_oop_result(RegisterMap* map) const {\n+  oop* result_adr = (oop *)map->location(x10->as_VMReg());\n+  guarantee(result_adr != NULL, \"bad register save location\");\n+  return (*result_adr);\n+}\n+\n+inline void frame::set_saved_oop_result(RegisterMap* map, oop obj) {\n+  oop* result_adr = (oop *)map->location(x10->as_VMReg());\n+  guarantee(result_adr != NULL, \"bad register save location\");\n+  *result_adr = obj;\n+}\n+\n+#endif \/\/ CPU_RISCV_FRAME_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.inline.hpp","additions":245,"deletions":0,"binary":false,"changes":245,"status":"added"},{"patch":"@@ -0,0 +1,481 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/g1\/g1BarrierSet.hpp\"\n+#include \"gc\/g1\/g1BarrierSetAssembler.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+#include \"gc\/g1\/g1CardTable.hpp\"\n+#include \"gc\/g1\/g1ThreadLocalData.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/g1\/c1\/g1BarrierSetC1.hpp\"\n+#endif\n+\n+#define __ masm->\n+\n+void G1BarrierSetAssembler::gen_write_ref_array_pre_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                            Register addr, Register count, RegSet saved_regs) {\n+  assert_cond(masm != NULL);\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+  if (!dest_uninitialized) {\n+    Label done;\n+    Address in_progress(xthread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+\n+    \/\/ Is marking active?\n+    if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+      __ lwu(t0, in_progress);\n+    } else {\n+      assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+      __ lbu(t0, in_progress);\n+    }\n+    __ beqz(t0, done);\n+\n+    __ push_reg(saved_regs, sp);\n+    if (count == c_rarg0) {\n+      if (addr == c_rarg1) {\n+        \/\/ exactly backwards!!\n+        __ mv(t0, c_rarg0);\n+        __ mv(c_rarg0, c_rarg1);\n+        __ mv(c_rarg1, t0);\n+      } else {\n+        __ mv(c_rarg1, count);\n+        __ mv(c_rarg0, addr);\n+      }\n+    } else {\n+      __ mv(c_rarg0, addr);\n+      __ mv(c_rarg1, count);\n+    }\n+    if (UseCompressedOops) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_pre_narrow_oop_entry), 2);\n+    } else {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_pre_oop_entry), 2);\n+    }\n+    __ pop_reg(saved_regs, sp);\n+\n+    __ bind(done);\n+  }\n+}\n+\n+void G1BarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                             Register start, Register count, Register tmp, RegSet saved_regs) {\n+  assert_cond(masm != NULL);\n+  __ push_reg(saved_regs, sp);\n+  assert_different_registers(start, count, tmp);\n+  assert_different_registers(c_rarg0, count);\n+  __ mv(c_rarg0, start);\n+  __ mv(c_rarg1, count);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry), 2);\n+  __ pop_reg(saved_regs, sp);\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre(MacroAssembler* masm,\n+                                                 Register obj,\n+                                                 Register pre_val,\n+                                                 Register thread,\n+                                                 Register tmp,\n+                                                 bool tosca_live,\n+                                                 bool expand_call) {\n+  \/\/ If expand_call is true then we expand the call_VM_leaf macro\n+  \/\/ directly to skip generating the check by\n+  \/\/ InterpreterMacroAssembler::call_VM_leaf_base that checks _last_sp.\n+\n+  assert_cond(masm != NULL);\n+  assert(thread == xthread, \"must be\");\n+\n+  Label done;\n+  Label runtime;\n+\n+  assert_different_registers(obj, pre_val, tmp, t0);\n+  assert(pre_val != noreg &&  tmp != noreg, \"expecting a register\");\n+\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n+  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n+\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) { \/\/ 4-byte width\n+    __ lwu(tmp, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ lbu(tmp, in_progress);\n+  }\n+  __ beqz(tmp, done);\n+\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+\n+  \/\/ Is the previous value null?\n+  __ beqz(pre_val, done);\n+\n+  \/\/ Can we store original value in the thread's buffer?\n+  \/\/ Is index == 0?\n+  \/\/ (The index field is typed as size_t.)\n+\n+  __ ld(tmp, index);                       \/\/ tmp := *index_adr\n+  __ beqz(tmp, runtime);                   \/\/ tmp == 0?\n+                                           \/\/ If yes, goto runtime\n+\n+  __ sub(tmp, tmp, wordSize);              \/\/ tmp := tmp - wordSize\n+  __ sd(tmp, index);                       \/\/ *index_adr := tmp\n+  __ ld(t0, buffer);\n+  __ add(tmp, tmp, t0);                    \/\/ tmp := tmp + *buffer_adr\n+\n+  \/\/ Record the previous value\n+  __ sd(pre_val, Address(tmp, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+\n+  __ push_call_clobbered_registers();\n+  if (expand_call) {\n+    assert(pre_val != c_rarg1, \"smashed arg\");\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  }\n+  __ pop_call_clobbered_registers();\n+\n+  __ bind(done);\n+\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post(MacroAssembler* masm,\n+                                                  Register store_addr,\n+                                                  Register new_val,\n+                                                  Register thread,\n+                                                  Register tmp,\n+                                                  Register tmp2) {\n+  assert_cond(masm != NULL);\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp, tmp2,\n+                             t0);\n+  assert(store_addr != noreg && new_val != noreg && tmp != noreg &&\n+         tmp2 != noreg, \"expecting a register\");\n+\n+  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n+  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  assert(sizeof(*ct->byte_map_base()) == sizeof(jbyte), \"adjust this code\");\n+\n+  Label done;\n+  Label runtime;\n+\n+  \/\/ Does store cross heap regions?\n+\n+  __ xorr(tmp, store_addr, new_val);\n+  __ srli(tmp, tmp, HeapRegion::LogOfHRGrainBytes);\n+  __ beqz(tmp, done);\n+\n+  \/\/ crosses regions, storing NULL?\n+\n+  __ beqz(new_val, done);\n+\n+  \/\/ storing region crossing non-NULL, is card already dirty?\n+\n+  ExternalAddress cardtable((address) ct->byte_map_base());\n+  assert(sizeof(*ct->byte_map_base()) == sizeof(jbyte), \"adjust this code\");\n+  const Register card_addr = tmp;\n+\n+  __ srli(card_addr, store_addr, CardTable::card_shift);\n+\n+  \/\/ get the address of the card\n+  __ load_byte_map_base(tmp2);\n+  __ add(card_addr, card_addr, tmp2);\n+  __ lbu(tmp2, Address(card_addr));\n+  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n+  __ beq(tmp2, t0, done);\n+\n+  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+\n+  __ membar(MacroAssembler::StoreLoad);\n+\n+  __ lbu(tmp2, Address(card_addr));\n+  __ beqz(tmp2, done);\n+\n+  \/\/ storing a region crossing, non-NULL oop, card is clean.\n+  \/\/ dirty card and log.\n+\n+  __ sb(zr, Address(card_addr));\n+\n+  __ ld(t0, queue_index);\n+  __ beqz(t0, runtime);\n+  __ sub(t0, t0, wordSize);\n+  __ sd(t0, queue_index);\n+\n+  __ ld(tmp2, buffer);\n+  __ add(t0, tmp2, t0);\n+  __ sd(card_addr, Address(t0, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr, new_val);\n+  __ push_reg(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n+  __ pop_reg(saved, sp);\n+\n+  __ bind(done);\n+}\n+\n+void G1BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                    Register dst, Address src, Register tmp1, Register tmp_thread) {\n+  assert_cond(masm != NULL);\n+  bool on_oop = is_reference_type(type);\n+  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n+  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  bool on_reference = on_weak || on_phantom;\n+  ModRefBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+  if (on_oop && on_reference) {\n+    \/\/ RA is live.  It must be saved around calls.\n+    __ enter(); \/\/ barrier may call runtime\n+    \/\/ Generate the G1 pre-barrier code to log the value of\n+    \/\/ the referent field in an SATB buffer.\n+    g1_write_barrier_pre(masm \/* masm *\/,\n+                         noreg \/* obj *\/,\n+                         dst \/* pre_val *\/,\n+                         xthread \/* thread *\/,\n+                         tmp1 \/* tmp *\/,\n+                         true \/* tosca_live *\/,\n+                         true \/* expand_call *\/);\n+    __ leave();\n+  }\n+}\n+\n+void G1BarrierSetAssembler::oop_store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                         Address dst, Register val, Register tmp1, Register tmp2) {\n+  assert_cond(masm != NULL);\n+  \/\/ flatten object address if needed\n+  if (dst.offset() == 0) {\n+    if (dst.base() != x13) {\n+      __ mv(x13, dst.base());\n+    }\n+  } else {\n+    __ la(x13, dst);\n+  }\n+\n+  g1_write_barrier_pre(masm,\n+                       x13 \/* obj *\/,\n+                       tmp2 \/* pre_val *\/,\n+                       xthread \/* thread *\/,\n+                       tmp1  \/* tmp *\/,\n+                       val != noreg \/* tosca_live *\/,\n+                       false \/* expand_call *\/);\n+\n+  if (val == noreg) {\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(x13, 0), noreg, noreg, noreg);\n+  } else {\n+    \/\/ G1 barrier needs uncompressed oop for region cross check.\n+    Register new_val = val;\n+    if (UseCompressedOops) {\n+      new_val = t1;\n+      __ mv(new_val, val);\n+    }\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(x13, 0), val, noreg, noreg);\n+    g1_write_barrier_post(masm,\n+                          x13 \/* store_adr *\/,\n+                          new_val \/* new_val *\/,\n+                          xthread \/* thread *\/,\n+                          tmp1 \/* tmp *\/,\n+                          tmp2 \/* tmp2 *\/);\n+  }\n+}\n+\n+#ifdef COMPILER1\n+\n+#undef __\n+#define __ ce->masm()->\n+\n+void G1BarrierSetAssembler::gen_pre_barrier_stub(LIR_Assembler* ce, G1PreBarrierStub* stub) {\n+  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n+\n+  \/\/ At this point we know that marking is in progress.\n+  \/\/ If do_load() is true then we have to emit the\n+  \/\/ load of the previous value; otherwise it has already\n+  \/\/ been loaded into _pre_val.\n+  __ bind(*stub->entry());\n+\n+  assert(stub->pre_val()->is_register(), \"Precondition.\");\n+\n+  Register pre_val_reg = stub->pre_val()->as_register();\n+\n+  if (stub->do_load()) {\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/* wide *\/, false \/*unaligned*\/);\n+  }\n+  __ beqz(pre_val_reg, *stub->continuation(), \/* is_far *\/ true);\n+  ce->store_parameter(stub->pre_val()->as_register(), 0);\n+  __ far_call(RuntimeAddress(bs->pre_barrier_c1_runtime_code_blob()->code_begin()));\n+  __ j(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n+  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+  assert(stub->addr()->is_register(), \"Precondition\");\n+  assert(stub->new_val()->is_register(), \"Precondition\");\n+  Register new_val_reg = stub->new_val()->as_register();\n+  __ beqz(new_val_reg, *stub->continuation(), \/* is_far *\/ true);\n+  ce->store_parameter(stub->addr()->as_pointer_register(), 0);\n+  __ far_call(RuntimeAddress(bs->post_barrier_c1_runtime_code_blob()->code_begin()));\n+  __ j(*stub->continuation());\n+}\n+\n+#undef __\n+\n+#define __ sasm->\n+\n+void G1BarrierSetAssembler::generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm) {\n+  __ prologue(\"g1_pre_barrier\", false);\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+\n+  \/\/ arg0 : previous value of memory\n+  const Register pre_val = x10;\n+  const Register thread = xthread;\n+  const Register tmp = t0;\n+\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  Address queue_index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n+  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n+\n+  Label done;\n+  Label runtime;\n+\n+  \/\/ Is marking still active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {  \/\/ 4-byte width\n+    __ lwu(tmp, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ lbu(tmp, in_progress);\n+  }\n+  __ beqz(tmp, done);\n+\n+  \/\/ Can we store original value in the thread's buffer?\n+  __ ld(tmp, queue_index);\n+  __ beqz(tmp, runtime);\n+\n+  __ sub(tmp, tmp, wordSize);\n+  __ sd(tmp, queue_index);\n+  __ ld(t1, buffer);\n+  __ add(tmp, tmp, t1);\n+  __ load_parameter(0, t1);\n+  __ sd(t1, Address(tmp, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+  __ push_call_clobbered_registers();\n+  __ load_parameter(0, pre_val);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  __ pop_call_clobbered_registers();\n+  __ bind(done);\n+\n+  __ epilogue();\n+}\n+\n+void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n+  __ prologue(\"g1_post_barrier\", false);\n+\n+  \/\/ arg0 : store_address\n+  Address store_addr(fp, 2 * BytesPerWord); \/\/ 2 BytesPerWord from fp\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  assert(sizeof(*ct->byte_map_base()) == sizeof(jbyte), \"adjust this code\");\n+\n+  Label done;\n+  Label runtime;\n+\n+  \/\/ At this point we know new_value is non-NULL and the new_value crosses regions.\n+  \/\/ Must check to see if card is already dirty\n+  const Register thread = xthread;\n+\n+  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n+  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n+\n+  const Register card_offset = t1;\n+  \/\/ RA is free here, so we can use it to hold the byte_map_base.\n+  const Register byte_map_base = ra;\n+\n+  assert_different_registers(card_offset, byte_map_base, t0);\n+\n+  __ load_parameter(0, card_offset);\n+  __ srli(card_offset, card_offset, CardTable::card_shift);\n+  __ load_byte_map_base(byte_map_base);\n+\n+  \/\/ Convert card offset into an address in card_addr\n+  Register card_addr = card_offset;\n+  __ add(card_addr, byte_map_base, card_addr);\n+\n+  __ lbu(t0, Address(card_addr, 0));\n+  __ sub(t0, t0, (int)G1CardTable::g1_young_card_val());\n+  __ beqz(t0, done);\n+\n+  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+\n+  __ membar(MacroAssembler::StoreLoad);\n+  __ lbu(t0, Address(card_addr, 0));\n+  __ beqz(t0, done);\n+\n+  \/\/ storing region crossing non-NULL, card is clean.\n+  \/\/ dirty card and log.\n+  __ sb(zr, Address(card_addr, 0));\n+\n+  __ ld(t0, queue_index);\n+  __ beqz(t0, runtime);\n+  __ sub(t0, t0, wordSize);\n+  __ sd(t0, queue_index);\n+\n+  \/\/ Reuse RA to hold buffer_addr\n+  const Register buffer_addr = ra;\n+\n+  __ ld(buffer_addr, buffer);\n+  __ add(t0, buffer_addr, t0);\n+  __ sd(card_addr, Address(t0, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+  __ push_call_clobbered_registers();\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n+  __ pop_call_clobbered_registers();\n+  __ bind(done);\n+  __ epilogue();\n+}\n+\n+#undef __\n+\n+#endif \/\/ COMPILER1\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":481,"deletions":0,"binary":false,"changes":481,"status":"added"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_G1_G1BARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_G1_G1BARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/modRefBarrierSetAssembler.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+#endif\n+class StubAssembler;\n+class G1PreBarrierStub;\n+class G1PostBarrierStub;\n+\n+class G1BarrierSetAssembler: public ModRefBarrierSetAssembler {\n+protected:\n+  void gen_write_ref_array_pre_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                       Register addr, Register count, RegSet saved_regs);\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register start, Register count, Register tmp, RegSet saved_regs);\n+\n+  void g1_write_barrier_pre(MacroAssembler* masm,\n+                            Register obj,\n+                            Register pre_val,\n+                            Register thread,\n+                            Register tmp,\n+                            bool tosca_live,\n+                            bool expand_call);\n+\n+  void g1_write_barrier_post(MacroAssembler* masm,\n+                             Register store_addr,\n+                             Register new_val,\n+                             Register thread,\n+                             Register tmp,\n+                             Register tmp2);\n+\n+  virtual void oop_store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                            Address dst, Register val, Register tmp1, Register tmp2);\n+\n+public:\n+#ifdef COMPILER1\n+  void gen_pre_barrier_stub(LIR_Assembler* ce, G1PreBarrierStub* stub);\n+  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n+\n+  void generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm);\n+  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+#endif\n+\n+  void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+               Register dst, Address src, Register tmp1, Register tmp_thread);\n+};\n+\n+#endif \/\/ CPU_RISCV_GC_G1_G1BARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.hpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,31 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_G1_G1GLOBALS_RISCV_HPP\n+#define CPU_RISCV_GC_G1_G1GLOBALS_RISCV_HPP\n+\n+const size_t G1MergeHeapRootsPrefetchCacheSize = 16;\n+\n+#endif \/\/ CPU_RISCV_GC_G1_G1GLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1Globals_riscv.hpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"added"},{"patch":"@@ -0,0 +1,231 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+#define __ masm->\n+\n+void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register dst, Address src, Register tmp1, Register tmp_thread) {\n+  assert_cond(masm != NULL);\n+\n+  \/\/ RA is live. It must be saved around calls.\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool in_native = (decorators & IN_NATIVE) != 0;\n+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;\n+  switch (type) {\n+    case T_OBJECT:  \/\/ fall through\n+    case T_ARRAY: {\n+      if (in_heap) {\n+        if (UseCompressedOops) {\n+          __ lwu(dst, src);\n+          if (is_not_null) {\n+            __ decode_heap_oop_not_null(dst);\n+          } else {\n+            __ decode_heap_oop(dst);\n+          }\n+        } else {\n+          __ ld(dst, src);\n+        }\n+      } else {\n+        assert(in_native, \"why else?\");\n+        __ ld(dst, src);\n+      }\n+      break;\n+    }\n+    case T_BOOLEAN: __ load_unsigned_byte (dst, src); break;\n+    case T_BYTE:    __ load_signed_byte   (dst, src); break;\n+    case T_CHAR:    __ load_unsigned_short(dst, src); break;\n+    case T_SHORT:   __ load_signed_short  (dst, src); break;\n+    case T_INT:     __ lw                 (dst, src); break;\n+    case T_LONG:    __ ld                 (dst, src); break;\n+    case T_ADDRESS: __ ld                 (dst, src); break;\n+    case T_FLOAT:   __ flw                (f10, src); break;\n+    case T_DOUBLE:  __ fld                (f10, src); break;\n+    default: Unimplemented();\n+  }\n+}\n+\n+void BarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                   Address dst, Register val, Register tmp1, Register tmp2) {\n+  assert_cond(masm != NULL);\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool in_native = (decorators & IN_NATIVE) != 0;\n+  switch (type) {\n+    case T_OBJECT: \/\/ fall through\n+    case T_ARRAY: {\n+      val = val == noreg ? zr : val;\n+      if (in_heap) {\n+        if (UseCompressedOops) {\n+          assert(!dst.uses(val), \"not enough registers\");\n+          if (val != zr) {\n+            __ encode_heap_oop(val);\n+          }\n+          __ sw(val, dst);\n+        } else {\n+          __ sd(val, dst);\n+        }\n+      } else {\n+        assert(in_native, \"why else?\");\n+        __ sd(val, dst);\n+      }\n+      break;\n+    }\n+    case T_BOOLEAN:\n+      __ andi(val, val, 0x1);  \/\/ boolean is true if LSB is 1\n+      __ sb(val, dst);\n+      break;\n+    case T_BYTE:    __ sb(val, dst); break;\n+    case T_CHAR:    __ sh(val, dst); break;\n+    case T_SHORT:   __ sh(val, dst); break;\n+    case T_INT:     __ sw(val, dst); break;\n+    case T_LONG:    __ sd(val, dst); break;\n+    case T_ADDRESS: __ sd(val, dst); break;\n+    case T_FLOAT:   __ fsw(f10,  dst); break;\n+    case T_DOUBLE:  __ fsd(f10,  dst); break;\n+    default: Unimplemented();\n+  }\n+\n+}\n+\n+void BarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,\n+                                                        Register obj, Register tmp, Label& slowpath) {\n+  assert_cond(masm != NULL);\n+  \/\/ If mask changes we need to ensure that the inverse is still encodable as an immediate\n+  STATIC_ASSERT(JNIHandles::weak_tag_mask == 1);\n+  __ andi(obj, obj, ~JNIHandles::weak_tag_mask);\n+  __ ld(obj, Address(obj, 0));             \/\/ *obj\n+}\n+\n+\/\/ Defines obj, preserves var_size_in_bytes, okay for tmp2 == var_size_in_bytes.\n+void BarrierSetAssembler::tlab_allocate(MacroAssembler* masm, Register obj,\n+                                        Register var_size_in_bytes,\n+                                        int con_size_in_bytes,\n+                                        Register tmp1,\n+                                        Register tmp2,\n+                                        Label& slow_case,\n+                                        bool is_far) {\n+  assert_cond(masm != NULL);\n+  assert_different_registers(obj, tmp2);\n+  assert_different_registers(obj, var_size_in_bytes);\n+  Register end = tmp2;\n+\n+  __ ld(obj, Address(xthread, JavaThread::tlab_top_offset()));\n+  if (var_size_in_bytes == noreg) {\n+    __ la(end, Address(obj, con_size_in_bytes));\n+  } else {\n+    __ add(end, obj, var_size_in_bytes);\n+  }\n+  __ ld(t0, Address(xthread, JavaThread::tlab_end_offset()));\n+  __ bgtu(end, t0, slow_case, is_far);\n+\n+  \/\/ update the tlab top pointer\n+  __ sd(end, Address(xthread, JavaThread::tlab_top_offset()));\n+\n+  \/\/ recover var_size_in_bytes if necessary\n+  if (var_size_in_bytes == end) {\n+    __ sub(var_size_in_bytes, var_size_in_bytes, obj);\n+  }\n+}\n+\n+\/\/ Defines obj, preserves var_size_in_bytes\n+void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj,\n+                                        Register var_size_in_bytes,\n+                                        int con_size_in_bytes,\n+                                        Register tmp1,\n+                                        Label& slow_case,\n+                                        bool is_far) {\n+  assert_cond(masm != NULL);\n+  assert_different_registers(obj, var_size_in_bytes, tmp1);\n+  if (!Universe::heap()->supports_inline_contig_alloc()) {\n+    __ j(slow_case);\n+  } else {\n+    Register end = tmp1;\n+    Label retry;\n+    __ bind(retry);\n+\n+    \/\/ Get the current end of the heap\n+    ExternalAddress address_end((address) Universe::heap()->end_addr());\n+    {\n+      int32_t offset;\n+      __ la_patchable(t1, address_end, offset);\n+      __ ld(t1, Address(t1, offset));\n+    }\n+\n+    \/\/ Get the current top of the heap\n+    ExternalAddress address_top((address) Universe::heap()->top_addr());\n+    {\n+      int32_t offset;\n+      __ la_patchable(t0, address_top, offset);\n+      __ addi(t0, t0, offset);\n+      __ lr_d(obj, t0, Assembler::aqrl);\n+    }\n+\n+    \/\/ Adjust it my the size of our new object\n+    if (var_size_in_bytes == noreg) {\n+      __ la(end, Address(obj, con_size_in_bytes));\n+    } else {\n+      __ add(end, obj, var_size_in_bytes);\n+    }\n+\n+    \/\/ if end < obj then we wrapped around high memory\n+    __ bltu(end, obj, slow_case, is_far);\n+\n+    __ bgtu(end, t1, slow_case, is_far);\n+\n+    \/\/ If heap_top hasn't been changed by some other thread, update it.\n+    __ sc_d(t1, end, t0, Assembler::rl);\n+    __ bnez(t1, retry);\n+\n+    incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, tmp1);\n+  }\n+}\n+\n+void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm,\n+                                               Register var_size_in_bytes,\n+                                               int con_size_in_bytes,\n+                                               Register tmp1) {\n+  assert_cond(masm != NULL);\n+  assert(tmp1->is_valid(), \"need temp reg\");\n+\n+  __ ld(tmp1, Address(xthread, in_bytes(JavaThread::allocated_bytes_offset())));\n+  if (var_size_in_bytes->is_valid()) {\n+    __ add(tmp1, tmp1, var_size_in_bytes);\n+  } else {\n+    __ add(tmp1, tmp1, con_size_in_bytes);\n+  }\n+  __ sd(tmp1, Address(xthread, in_bytes(JavaThread::allocated_bytes_offset())));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":231,"deletions":0,"binary":false,"changes":231,"status":"added"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_SHARED_BARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_SHARED_BARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/access.hpp\"\n+\n+class BarrierSetAssembler: public CHeapObj<mtGC> {\n+private:\n+  void incr_allocated_bytes(MacroAssembler* masm,\n+                            Register var_size_in_bytes, int con_size_in_bytes,\n+                            Register t1 = noreg);\n+\n+public:\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register src, Register dst, Register count, RegSet saved_regs) {}\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register end, Register tmp, RegSet saved_regs) {}\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Address dst, Register val, Register tmp1, Register tmp2);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+\n+  virtual void tlab_allocate(MacroAssembler* masm,\n+    Register obj,                      \/\/ result: pointer to object after successful allocation\n+    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n+    Register tmp1,                     \/\/ temp register\n+    Register tmp2,                     \/\/ temp register\n+    Label&   slow_case,                \/\/ continuation point if fast allocation fails\n+    bool is_far = false\n+  );\n+\n+  void eden_allocate(MacroAssembler* masm,\n+    Register obj,                      \/\/ result: pointer to object after successful allocation\n+    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n+    Register tmp1,                     \/\/ temp register\n+    Label&   slow_case,                \/\/ continuation point if fast allocation fails\n+    bool is_far = false\n+  );\n+  virtual void barrier_stubs_init() {}\n+\n+  virtual ~BarrierSetAssembler() {}\n+};\n+\n+#endif \/\/ CPU_RISCV_GC_SHARED_BARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.hpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,125 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSetAssembler.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+\n+#define __ masm->\n+\n+\n+void CardTableBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj, Register tmp) {\n+  assert_cond(masm != NULL);\n+  assert_different_registers(obj, tmp);\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  assert(bs->kind() == BarrierSet::CardTableBarrierSet, \"Wrong barrier set kind\");\n+\n+  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  assert(sizeof(*ct->byte_map_base()) == sizeof(jbyte), \"adjust this code\");\n+\n+  __ srli(obj, obj, CardTable::card_shift);\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(tmp);\n+  __ add(tmp, obj, tmp);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ membar(MacroAssembler::StoreLoad);\n+    __ lbu(t1,  Address(tmp));\n+    __ beqz(t1, L_already_dirty);\n+    __ sb(zr, Address(tmp));\n+    __ bind(L_already_dirty);\n+  } else {\n+    if (ct->scanned_concurrently()) {\n+      __ membar(MacroAssembler::StoreStore);\n+    }\n+    __ sb(zr, Address(tmp));\n+  }\n+}\n+\n+void CardTableBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                    Register start, Register count, Register tmp, RegSet saved_regs) {\n+  assert_cond(masm != NULL);\n+  assert_different_registers(start, tmp);\n+  assert_different_registers(count, tmp);\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  __ beqz(count, L_done); \/\/ zero count - nothing to do\n+  \/\/ end = start + count << LogBytesPerHeapOop\n+  __ shadd(end, count, start, count, LogBytesPerHeapOop);\n+  __ sub(end, end, BytesPerHeapOop); \/\/ last element address to make inclusive\n+\n+  __ srli(start, start, CardTable::card_shift);\n+  __ srli(end, end, CardTable::card_shift);\n+  __ sub(count, end, start); \/\/ number of bytes to copy\n+\n+  __ load_byte_map_base(tmp);\n+  __ add(start, start, tmp);\n+  if (ct->scanned_concurrently()) {\n+    __ membar(MacroAssembler::StoreStore);\n+  }\n+\n+  __ bind(L_loop);\n+  __ add(tmp, start, count);\n+  __ sb(zr, Address(tmp));\n+  __ sub(count, count, 1);\n+  __ bgez(count, L_loop);\n+  __ bind(L_done);\n+}\n+\n+void CardTableBarrierSetAssembler::oop_store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                Address dst, Register val, Register tmp1, Register tmp2) {\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool is_array = (decorators & IS_ARRAY) != 0;\n+  bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool precise = is_array || on_anonymous;\n+\n+  bool needs_post_barrier = val != noreg && in_heap;\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, noreg, noreg);\n+  if (needs_post_barrier) {\n+    \/\/ flatten object address if needed\n+    if (!precise || dst.offset() == 0) {\n+      store_check(masm, dst.base(), x13);\n+    } else {\n+      assert_cond(masm != NULL);\n+      __ la(x13, dst);\n+      store_check(masm, x13, t0);\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/cardTableBarrierSetAssembler_riscv.cpp","additions":125,"deletions":0,"binary":false,"changes":125,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_SHARED_CARDTABLEBARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_SHARED_CARDTABLEBARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/modRefBarrierSetAssembler.hpp\"\n+\n+class CardTableBarrierSetAssembler: public ModRefBarrierSetAssembler {\n+protected:\n+  void store_check(MacroAssembler* masm, Register obj, Register tmp);\n+\n+  virtual void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                Register start, Register count, Register tmp, RegSet saved_regs);\n+  virtual void oop_store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                            Address dst, Register val, Register tmp1, Register tmp2);\n+};\n+\n+#endif \/\/ #ifndef CPU_RISCV_GC_SHARED_CARDTABLEBARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/cardTableBarrierSetAssembler_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shared\/modRefBarrierSetAssembler.hpp\"\n+\n+#define __ masm->\n+\n+void ModRefBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                   Register src, Register dst, Register count, RegSet saved_regs) {\n+\n+  if (is_oop) {\n+    gen_write_ref_array_pre_barrier(masm, decorators, dst, count, saved_regs);\n+  }\n+}\n+\n+void ModRefBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                   Register start, Register count, Register tmp,\n+                                                   RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n+void ModRefBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                         Address dst, Register val, Register tmp1, Register tmp2) {\n+  if (is_reference_type(type)) {\n+    oop_store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/modRefBarrierSetAssembler_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_SHARED_MODREFBARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_SHARED_MODREFBARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+\n+\/\/ The ModRefBarrierSetAssembler filters away accesses on BasicTypes other\n+\/\/ than T_OBJECT\/T_ARRAY (oops). The oop accesses call one of the protected\n+\/\/ accesses, which are overridden in the concrete BarrierSetAssembler.\n+\n+class ModRefBarrierSetAssembler: public BarrierSetAssembler {\n+protected:\n+  virtual void gen_write_ref_array_pre_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                               Register addr, Register count, RegSet saved_regs) {}\n+  virtual void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                Register start, Register count, Register tmp, RegSet saved_regs) {}\n+\n+  virtual void oop_store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                            Address dst, Register val, Register tmp1, Register tmp2) = 0;\n+\n+public:\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register src, Register dst, Register count, RegSet saved_regs);\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Address dst, Register val, Register tmp1, Register tmp2);\n+};\n+\n+#endif \/\/ CPU_RISCV_GC_SHARED_MODREFBARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/modRefBarrierSetAssembler_riscv.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,117 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+\n+#define __ masm->masm()->\n+\n+void LIR_OpShenandoahCompareAndSwap::emit_code(LIR_Assembler* masm) {\n+  Register addr = _addr->as_register_lo();\n+  Register newval = _new_value->as_register();\n+  Register cmpval = _cmp_value->as_register();\n+  Register tmp1 = _tmp1->as_register();\n+  Register tmp2 = _tmp2->as_register();\n+  Register result = result_opr()->as_register();\n+\n+  ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), newval, t1);\n+\n+  if (UseCompressedOops) {\n+    __ encode_heap_oop(tmp1, cmpval);\n+    cmpval = tmp1;\n+    __ encode_heap_oop(tmp2, newval);\n+    newval = tmp2;\n+  }\n+\n+  ShenandoahBarrierSet::assembler()->cmpxchg_oop(masm->masm(), addr, cmpval, newval, \/* acquire *\/ Assembler::aq,\n+                                                 \/* release *\/ Assembler::rl, \/* is_cae *\/ false, result);\n+}\n+\n+#undef __\n+\n+#ifdef ASSERT\n+#define __ gen->lir(__FILE__, __LINE__)->\n+#else\n+#define __ gen->lir()->\n+#endif\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n+  BasicType bt = access.type();\n+  if (access.is_oop()) {\n+    LIRGenerator *gen = access.gen();\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(gen, access.access_emit_info(), access.decorators(), access.resolved_addr(),\n+                  LIR_OprFact::illegalOpr \/* pre_val *\/);\n+    }\n+    if (ShenandoahCASBarrier) {\n+      cmp_value.load_item();\n+      new_value.load_item();\n+\n+      LIR_Opr tmp1 = gen->new_register(T_OBJECT);\n+      LIR_Opr tmp2 = gen->new_register(T_OBJECT);\n+      LIR_Opr addr = access.resolved_addr()->as_address_ptr()->base();\n+      LIR_Opr result = gen->new_register(T_INT);\n+\n+      __ append(new LIR_OpShenandoahCompareAndSwap(addr, cmp_value.result(), new_value.result(), tmp1, tmp2, result));\n+      return result;\n+    }\n+  }\n+  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+}\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value) {\n+  LIRGenerator* gen = access.gen();\n+  BasicType type = access.type();\n+\n+  LIR_Opr result = gen->new_register(type);\n+  value.load_item();\n+  LIR_Opr value_opr = value.result();\n+\n+  if (access.is_oop()) {\n+    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n+  }\n+\n+  assert(type == T_INT || is_reference_type(type) LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  LIR_Opr tmp = gen->new_register(T_INT);\n+  __ xchg(access.resolved_addr(), value_opr, result, tmp);\n+\n+  if (access.is_oop()) {\n+    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0));\n+    LIR_Opr tmp_opr = gen->new_register(type);\n+    __ move(result, tmp_opr);\n+    result = tmp_opr;\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(access.gen(), access.access_emit_info(), access.decorators(), LIR_OprFact::illegalOpr,\n+                  result \/* pre_val *\/);\n+    }\n+  }\n+\n+  return result;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_riscv.cpp","additions":117,"deletions":0,"binary":false,"changes":117,"status":"added"},{"patch":"@@ -0,0 +1,715 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahForwarding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRuntime.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+#endif\n+\n+#define __ masm->\n+\n+address ShenandoahBarrierSetAssembler::_shenandoah_lrb = NULL;\n+\n+void ShenandoahBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register src, Register dst, Register count, RegSet saved_regs) {\n+  if (is_oop) {\n+    bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahIUBarrier || ShenandoahLoadRefBarrier) {\n+\n+      Label done;\n+\n+      \/\/ Avoid calling runtime if count == 0\n+      __ beqz(count, done);\n+\n+      \/\/ Is GC active?\n+      Address gc_state(xthread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+      assert_different_registers(src, dst, count, t0);\n+\n+      __ lbu(t0, gc_state);\n+      if (ShenandoahSATBBarrier && dest_uninitialized) {\n+        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED);\n+        __ beqz(t0, done);\n+      } else {\n+        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ beqz(t0, done);\n+      }\n+\n+      __ push_reg(saved_regs, sp);\n+      if (UseCompressedOops) {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_narrow_oop_entry),\n+                        src, dst, count);\n+      } else {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_oop_entry), src, dst, count);\n+      }\n+      __ pop_reg(saved_regs, sp);\n+      __ bind(done);\n+    }\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::shenandoah_write_barrier_pre(MacroAssembler* masm,\n+                                                                 Register obj,\n+                                                                 Register pre_val,\n+                                                                 Register thread,\n+                                                                 Register tmp,\n+                                                                 bool tosca_live,\n+                                                                 bool expand_call) {\n+  if (ShenandoahSATBBarrier) {\n+    satb_write_barrier_pre(masm, obj, pre_val, thread, tmp, tosca_live, expand_call);\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::satb_write_barrier_pre(MacroAssembler* masm,\n+                                                           Register obj,\n+                                                           Register pre_val,\n+                                                           Register thread,\n+                                                           Register tmp,\n+                                                           bool tosca_live,\n+                                                           bool expand_call) {\n+  \/\/ If expand_call is true then we expand the call_VM_leaf macro\n+  \/\/ directly to skip generating the check by\n+  \/\/ InterpreterMacroAssembler::call_VM_leaf_base that checks _last_sp.\n+  assert(thread == xthread, \"must be\");\n+\n+  Label done;\n+  Label runtime;\n+\n+  assert_different_registers(obj, pre_val, tmp, t0);\n+  assert(pre_val != noreg &&  tmp != noreg, \"expecting a register\");\n+\n+  Address in_progress(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_active_offset()));\n+  Address index(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()));\n+  Address buffer(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()));\n+\n+  \/\/ Is marking active?\n+  if (in_bytes(ShenandoahSATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ lwu(tmp, in_progress);\n+  } else {\n+    assert(in_bytes(ShenandoahSATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ lbu(tmp, in_progress);\n+  }\n+  __ beqz(tmp, done);\n+\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+\n+  \/\/ Is the previous value null?\n+  __ beqz(pre_val, done);\n+\n+  \/\/ Can we store original value in the thread's buffer?\n+  \/\/ Is index == 0?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ld(tmp, index);                        \/\/ tmp := *index_adr\n+  __ beqz(tmp, runtime);                    \/\/ tmp == 0? If yes, goto runtime\n+\n+  __ sub(tmp, tmp, wordSize);               \/\/ tmp := tmp - wordSize\n+  __ sd(tmp, index);                        \/\/ *index_adr := tmp\n+  __ ld(t0, buffer);\n+  __ add(tmp, tmp, t0);                     \/\/ tmp := tmp + *buffer_adr\n+\n+  \/\/ Record the previous value\n+  __ sd(pre_val, Address(tmp, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(pre_val);\n+  if (tosca_live) saved += RegSet::of(x10);\n+  if (obj != noreg) saved += RegSet::of(obj);\n+\n+  __ push_reg(saved, sp);\n+\n+  \/\/ Calling the runtime using the regular call_VM_leaf mechanism generates\n+  \/\/ code (generated by InterpreterMacroAssember::call_VM_leaf_base)\n+  \/\/ that checks that the *(rfp+frame::interpreter_frame_last_sp) == NULL.\n+  \/\/\n+  \/\/ If we care generating the pre-barrier without a frame (e.g. in the\n+  \/\/ intrinsified Reference.get() routine) then ebp might be pointing to\n+  \/\/ the caller frame and so this check will most likely fail at runtime.\n+  \/\/\n+  \/\/ Expanding the call directly bypasses the generation of the check.\n+  \/\/ So when we do not have have a full interpreter frame on the stack\n+  \/\/ expand_call should be passed true.\n+  if (expand_call) {\n+    assert(pre_val != c_rarg1, \"smashed arg\");\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  }\n+\n+  __ pop_reg(saved, sp);\n+\n+  __ bind(done);\n+}\n+\n+void ShenandoahBarrierSetAssembler::resolve_forward_pointer(MacroAssembler* masm, Register dst, Register tmp) {\n+  assert(ShenandoahLoadRefBarrier || ShenandoahCASBarrier, \"Should be enabled\");\n+\n+  Label is_null;\n+  __ beqz(dst, is_null);\n+  resolve_forward_pointer_not_null(masm, dst, tmp);\n+  __ bind(is_null);\n+}\n+\n+\/\/ IMPORTANT: This must preserve all registers, even t0 and t1, except those explicitely\n+\/\/ passed in.\n+void ShenandoahBarrierSetAssembler::resolve_forward_pointer_not_null(MacroAssembler* masm, Register dst, Register tmp) {\n+  assert(ShenandoahLoadRefBarrier || ShenandoahCASBarrier, \"Should be enabled\");\n+  \/\/ The below loads the mark word, checks if the lowest two bits are\n+  \/\/ set, and if so, clear the lowest two bits and copy the result\n+  \/\/ to dst. Otherwise it leaves dst alone.\n+  \/\/ Implementing this is surprisingly awkward. I do it here by:\n+  \/\/ - Inverting the mark word\n+  \/\/ - Test lowest two bits == 0\n+  \/\/ - If so, set the lowest two bits\n+  \/\/ - Invert the result back, and copy to dst\n+  RegSet saved_regs = RegSet::of(t2);\n+  bool borrow_reg = (tmp == noreg);\n+  if (borrow_reg) {\n+    \/\/ No free registers available. Make one useful.\n+    tmp = t0;\n+    if (tmp == dst) {\n+      tmp = t1;\n+    }\n+    saved_regs += RegSet::of(tmp);\n+  }\n+\n+  assert_different_registers(tmp, dst, t2);\n+  __ push_reg(saved_regs, sp);\n+\n+  Label done;\n+  __ ld(tmp, Address(dst, oopDesc::mark_offset_in_bytes()));\n+  __ xori(tmp, tmp, -1); \/\/ eon with 0 is equivalent to XOR with -1\n+  __ andi(t2, tmp, markOopDesc::lock_mask_in_place);\n+  __ bnez(t2, done);\n+  __ ori(tmp, tmp, markOopDesc::marked_value);\n+  __ xori(dst, tmp, -1); \/\/ eon with 0 is equivalent to XOR with -1\n+  __ bind(done);\n+\n+  __ pop_reg(saved_regs, sp);\n+}\n+\n+void ShenandoahBarrierSetAssembler::load_reference_barrier_not_null(MacroAssembler* masm,\n+                                                                    Register dst,\n+                                                                    Address load_addr) {\n+  assert(ShenandoahLoadRefBarrier, \"Should be enabled\");\n+  assert(dst != t1 && load_addr.base() != t1, \"need t1\");\n+  assert_different_registers(load_addr.base(), t0, t1);\n+\n+  Label done;\n+  __ enter();\n+  Address gc_state(xthread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+  __ lbu(t1, gc_state);\n+\n+  \/\/ Check for heap stability\n+  __ andi(t1, t1, ShenandoahHeap::HAS_FORWARDED);\n+  __ beqz(t1, done);\n+\n+  \/\/ use x11 for load address\n+  Register result_dst = dst;\n+  if (dst == x11) {\n+    __ mv(t1, dst);\n+    dst = t1;\n+  }\n+\n+  \/\/ Save x10 and x11, unless it is an output register\n+  RegSet saved_regs = RegSet::of(x10, x11) - result_dst;\n+  __ push_reg(saved_regs, sp);\n+  __ la(x11, load_addr);\n+  __ mv(x10, dst);\n+\n+  __ far_call(RuntimeAddress(CAST_FROM_FN_PTR(address, ShenandoahBarrierSetAssembler::shenandoah_lrb())));\n+\n+  __ mv(result_dst, x10);\n+  __ pop_reg(saved_regs, sp);\n+\n+  __ bind(done);\n+  __ leave();\n+}\n+\n+void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler* masm, Register dst, Register tmp) {\n+  if (ShenandoahIUBarrier) {\n+    __ push_call_clobbered_registers();\n+\n+    satb_write_barrier_pre(masm, noreg, dst, xthread, tmp, true, false);\n+\n+    __ pop_call_clobbered_registers();\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr) {\n+  if (ShenandoahLoadRefBarrier) {\n+    Label is_null;\n+    __ beqz(dst, is_null);\n+    load_reference_barrier_not_null(masm, dst, load_addr);\n+    __ bind(is_null);\n+  }\n+}\n+\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   src:        oop location to load from, might be clobbered\n+\/\/\n+\/\/ Output:\n+\/\/   dst:        oop loaded from src location\n+\/\/\n+\/\/ Kill:\n+\/\/   x30 (tmp reg)\n+\/\/\n+\/\/ Alias:\n+\/\/   dst: x30 (might use x30 as temporary output register to avoid clobbering src)\n+\/\/\n+void ShenandoahBarrierSetAssembler::load_at(MacroAssembler* masm,\n+                                            DecoratorSet decorators,\n+                                            BasicType type,\n+                                            Register dst,\n+                                            Address src,\n+                                            Register tmp1,\n+                                            Register tmp_thread) {\n+  \/\/ 1: non-reference load, no additional barrier is needed\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+    return;\n+  }\n+\n+  \/\/ 2: load a reference from src location and apply LRB if needed\n+  if (ShenandoahBarrierSet::need_load_reference_barrier(decorators, type)) {\n+    Register result_dst = dst;\n+\n+    \/\/ Preserve src location for LRB\n+    RegSet saved_regs;\n+    if (dst == src.base()) {\n+      dst = (src.base() == x28) ? x29 : x28;\n+      saved_regs = RegSet::of(dst);\n+      __ push_reg(saved_regs, sp);\n+    }\n+    assert_different_registers(dst, src.base());\n+\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+\n+    load_reference_barrier(masm, dst, src);\n+\n+    if (dst != result_dst) {\n+      __ mv(result_dst, dst);\n+      dst = result_dst;\n+    }\n+\n+    if (saved_regs.bits() != 0) {\n+      __ pop_reg(saved_regs, sp);\n+    }\n+  } else {\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+  }\n+\n+  \/\/ 3: apply keep-alive barrier if needed\n+  if (ShenandoahBarrierSet::need_keep_alive_barrier(decorators, type)) {\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+    satb_write_barrier_pre(masm \/* masm *\/,\n+                           noreg \/* obj *\/,\n+                           dst \/* pre_val *\/,\n+                           xthread \/* thread *\/,\n+                           tmp1 \/* tmp *\/,\n+                           true \/* tosca_live *\/,\n+                           true \/* expand_call *\/);\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                             Address dst, Register val, Register tmp1, Register tmp2) {\n+  bool on_oop = is_reference_type(type);\n+  if (!on_oop) {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2);\n+    return;\n+  }\n+\n+  \/\/ flatten object address if needed\n+  if (dst.offset() == 0) {\n+    if (dst.base() != x13) {\n+      __ mv(x13, dst.base());\n+    }\n+  } else {\n+    __ la(x13, dst);\n+  }\n+\n+  shenandoah_write_barrier_pre(masm,\n+                               x13 \/* obj *\/,\n+                               tmp2 \/* pre_val *\/,\n+                               xthread \/* thread *\/,\n+                               tmp1  \/* tmp *\/,\n+                               val != noreg \/* tosca_live *\/,\n+                               false \/* expand_call *\/);\n+\n+  if (val == noreg) {\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(x13, 0), noreg, noreg, noreg);\n+  } else {\n+    iu_barrier(masm, val, tmp1);\n+    \/\/ G1 barrier needs uncompressed oop for region cross check.\n+    Register new_val = val;\n+    if (UseCompressedOops) {\n+      new_val = t1;\n+      __ mv(new_val, val);\n+    }\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(x13, 0), val, noreg, noreg);\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,\n+                                                                  Register obj, Register tmp, Label& slowpath) {\n+  Label done;\n+  \/\/ Resolve jobject\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Check for null.\n+  __ beqz(obj, done);\n+\n+  assert(obj != t1, \"need t1\");\n+  Address gc_state(jni_env, ShenandoahThreadLocalData::gc_state_offset() - JavaThread::jni_environment_offset());\n+  __ lbu(t1, gc_state);\n+\n+  \/\/ Check for heap in evacuation phase\n+  __ andi(t0, t1, ShenandoahHeap::EVACUATION);\n+  __ bnez(t0, slowpath);\n+\n+  __ bind(done);\n+}\n+\n+\/\/ Special Shenandoah CAS implementation that handles false negatives due\n+\/\/ to concurrent evacuation.  The service is more complex than a\n+\/\/ traditional CAS operation because the CAS operation is intended to\n+\/\/ succeed if the reference at addr exactly matches expected or if the\n+\/\/ reference at addr holds a pointer to a from-space object that has\n+\/\/ been relocated to the location named by expected.  There are two\n+\/\/ races that must be addressed:\n+\/\/  a) A parallel thread may mutate the contents of addr so that it points\n+\/\/     to a different object.  In this case, the CAS operation should fail.\n+\/\/  b) A parallel thread may heal the contents of addr, replacing a\n+\/\/     from-space pointer held in addr with the to-space pointer\n+\/\/     representing the new location of the object.\n+\/\/ Upon entry to cmpxchg_oop, it is assured that new_val equals NULL\n+\/\/ or it refers to an object that is not being evacuated out of\n+\/\/ from-space, or it refers to the to-space version of an object that\n+\/\/ is being evacuated out of from-space.\n+\/\/\n+\/\/ By default the value held in the result register following execution\n+\/\/ of the generated code sequence is 0 to indicate failure of CAS,\n+\/\/ non-zero to indicate success. If is_cae, the result is the value most\n+\/\/ recently fetched from addr rather than a boolean success indicator.\n+\/\/\n+\/\/ Clobbers t0, t1\n+void ShenandoahBarrierSetAssembler::cmpxchg_oop(MacroAssembler* masm,\n+                                                Register addr,\n+                                                Register expected,\n+                                                Register new_val,\n+                                                Assembler::Aqrl acquire,\n+                                                Assembler::Aqrl release,\n+                                                bool is_cae,\n+                                                Register result) {\n+  bool is_narrow = UseCompressedOops;\n+  Assembler::operand_size size = is_narrow ? Assembler::uint32 : Assembler::int64;\n+\n+  assert_different_registers(addr, expected, t0, t1);\n+  assert_different_registers(addr, new_val, t0, t1);\n+\n+  Label retry, success, fail, done;\n+\n+  __ bind(retry);\n+\n+  \/\/ Step1: Try to CAS.\n+  __ cmpxchg(addr, expected, new_val, size, acquire, release, \/* result *\/ t1);\n+\n+  \/\/ If success, then we are done.\n+  __ beq(expected, t1, success);\n+\n+  \/\/ Step2: CAS failed, check the forwared pointer.\n+  __ mv(t0, t1);\n+\n+  if (is_narrow) {\n+    __ decode_heap_oop(t0, t0);\n+  }\n+  resolve_forward_pointer(masm, t0);\n+\n+  __ encode_heap_oop(t0, t0);\n+\n+  \/\/ Report failure when the forwarded oop was not expected.\n+  __ bne(t0, expected, fail);\n+\n+  \/\/ Step 3: CAS again using the forwarded oop.\n+  __ cmpxchg(addr, t1, new_val, size, acquire, release, \/* result *\/ t0);\n+\n+  \/\/ Retry when failed.\n+  __ bne(t0, t1, retry);\n+\n+  __ bind(success);\n+  if (is_cae) {\n+    __ mv(result, expected);\n+  } else {\n+    __ addi(result, zr, 1);\n+  }\n+  __ j(done);\n+\n+  __ bind(fail);\n+  if (is_cae) {\n+    __ mv(result, t0);\n+  } else {\n+    __ mv(result, zr);\n+  }\n+\n+  __ bind(done);\n+}\n+\n+#undef __\n+\n+#ifdef COMPILER1\n+\n+#define __ ce->masm()->\n+\n+void ShenandoahBarrierSetAssembler::gen_pre_barrier_stub(LIR_Assembler* ce, ShenandoahPreBarrierStub* stub) {\n+  ShenandoahBarrierSetC1* bs = (ShenandoahBarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n+  \/\/ At this point we know that marking is in progress.\n+  \/\/ If do_load() is true then we have to emit the\n+  \/\/ load of the previous value; otherwise it has already\n+  \/\/ been loaded into _pre_val.\n+  __ bind(*stub->entry());\n+\n+  assert(stub->pre_val()->is_register(), \"Precondition.\");\n+\n+  Register pre_val_reg = stub->pre_val()->as_register();\n+\n+  if (stub->do_load()) {\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false \/* wide *\/, false \/*unaligned*\/);\n+  }\n+  __ beqz(pre_val_reg, *stub->continuation(), \/* is_far *\/ true);\n+  ce->store_parameter(stub->pre_val()->as_register(), 0);\n+  __ far_call(RuntimeAddress(bs->pre_barrier_c1_runtime_code_blob()->code_begin()));\n+  __ j(*stub->continuation());\n+}\n+\n+void ShenandoahBarrierSetAssembler::gen_load_reference_barrier_stub(LIR_Assembler* ce,\n+                                                                    ShenandoahLoadReferenceBarrierStub* stub) {\n+  ShenandoahBarrierSetC1* bs = (ShenandoahBarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+\n+  Register obj = stub->obj()->as_register();\n+  Register res = stub->result()->as_register();\n+  Register addr = stub->addr()->as_pointer_register();\n+  Register tmp1 = stub->tmp1()->as_register();\n+  Register tmp2 = stub->tmp2()->as_register();\n+\n+  assert(res == x10, \"result must arrive in x10\");\n+  assert_different_registers(tmp1, tmp2, t0);\n+\n+  if (res != obj) {\n+    __ mv(res, obj);\n+  }\n+\n+  \/\/ Check for null.\n+  __ beqz(res, *stub->continuation(), \/* is_far *\/ true);\n+\n+  \/\/ Check for object in cset.\n+  __ mv(tmp2, ShenandoahHeap::in_cset_fast_test_addr());\n+  __ srli(tmp1, res, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+  __ add(t0, tmp2, tmp1);\n+  __ lb(tmp2, Address(t0));\n+  __ beqz(tmp2, *stub->continuation(), \/* is_far *\/ true);\n+\n+  \/\/ Check if object is already forwarded.\n+  Label slow_path;\n+  __ ld(tmp1, Address(res, oopDesc::mark_offset_in_bytes()));\n+  __ xori(tmp1, tmp1, -1);\n+  __ andi(t0, tmp1, markOopDesc::lock_mask_in_place);\n+  __ bnez(t0, slow_path);\n+\n+  \/\/ Decode forwarded object.\n+  __ ori(tmp1, tmp1, markOopDesc::marked_value);\n+  __ xori(res, tmp1, -1);\n+  __ j(*stub->continuation());\n+\n+  __ bind(slow_path);\n+  ce->store_parameter(res, 0);\n+  ce->store_parameter(addr, 1);\n+  __ far_call(RuntimeAddress(bs->load_reference_barrier_rt_code_blob()->code_begin()));\n+\n+  __ j(*stub->continuation());\n+}\n+\n+#undef __\n+\n+#define __ sasm->\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm) {\n+  __ prologue(\"shenandoah_pre_barrier\", false);\n+\n+  \/\/ arg0 : previous value of memory\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+\n+  const Register pre_val = x10;\n+  const Register thread = xthread;\n+  const Register tmp = t0;\n+\n+  Address queue_index(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()));\n+  Address buffer(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()));\n+\n+  Label done;\n+  Label runtime;\n+\n+  \/\/ Is marking still active?\n+  Address gc_state(thread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+  __ lb(tmp, gc_state);\n+  __ andi(tmp, tmp, ShenandoahHeap::MARKING);\n+  __ beqz(tmp, done);\n+\n+  \/\/ Can we store original value in the thread's buffer?\n+  __ ld(tmp, queue_index);\n+  __ beqz(tmp, runtime);\n+\n+  __ sub(tmp, tmp, wordSize);\n+  __ sd(tmp, queue_index);\n+  __ ld(t1, buffer);\n+  __ add(tmp, tmp, t1);\n+  __ load_parameter(0, t1);\n+  __ sd(t1, Address(tmp, 0));\n+  __ j(done);\n+\n+  __ bind(runtime);\n+  __ push_call_clobbered_registers();\n+  __ load_parameter(0, pre_val);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), pre_val, thread);\n+  __ pop_call_clobbered_registers();\n+  __ bind(done);\n+\n+  __ epilogue();\n+}\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm) {\n+  __ prologue(\"shenandoah_load_reference_barrier\", false);\n+  \/\/ arg0 : object to be resolved\n+\n+  __ push_call_clobbered_registers();\n+  __ load_parameter(0, x10);\n+  __ load_parameter(1, x11);\n+\n+  if (UseCompressedOops) {\n+    __ mv(ra, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+  } else {\n+    __ mv(ra, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+  }\n+  __ jalr(ra);\n+  __ mv(t0, x10);\n+  __ pop_call_clobbered_registers();\n+  __ mv(x10, t0);\n+\n+  __ epilogue();\n+}\n+\n+#undef __\n+\n+#endif \/\/ COMPILER1\n+\n+address ShenandoahBarrierSetAssembler::shenandoah_lrb() {\n+  assert(_shenandoah_lrb != NULL, \"need load reference barrier stub\");\n+  return _shenandoah_lrb;\n+}\n+\n+#define __ cgen->assembler()->\n+\n+\/\/ Shenandoah load reference barrier.\n+\/\/\n+\/\/ Input:\n+\/\/   x10: OOP to evacuate.  Not null.\n+\/\/   x11: load address\n+\/\/\n+\/\/ Output:\n+\/\/   x10: Pointer to evacuated OOP.\n+\/\/\n+\/\/ Trash t0 t1  Preserve everything else.\n+address ShenandoahBarrierSetAssembler::generate_shenandoah_lrb(StubCodeGenerator* cgen) {\n+  __ align(6);\n+  StubCodeMark mark(cgen, \"StubRoutines\", \"shenandoah_lrb\");\n+  address start = __ pc();\n+\n+  Label slow_path;\n+  __ mv(t1, ShenandoahHeap::in_cset_fast_test_addr());\n+  __ srli(t0, x10, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+  __ add(t1, t1, t0);\n+  __ lbu(t1, Address(t1, 0));\n+  __ andi(t0, t1, 1);\n+  __ bnez(t0, slow_path);\n+  __ ret();\n+\n+  __ bind(slow_path);\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  __ push_call_clobbered_registers();\n+\n+  if (UseCompressedOops) {\n+    __ mv(ra, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+  } else {\n+    __ mv(ra, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+  }\n+  __ jalr(ra);\n+  __ mv(t0, x10);\n+  __ pop_call_clobbered_registers();\n+  __ mv(x10, t0);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret();\n+\n+  return start;\n+}\n+\n+#undef __\n+\n+void ShenandoahBarrierSetAssembler::barrier_stubs_init() {\n+  if (ShenandoahLoadRefBarrier) {\n+    int stub_code_size = 2048;\n+    ResourceMark rm;\n+    BufferBlob* bb = BufferBlob::create(\"shenandoah_barrier_stubs\", stub_code_size);\n+    CodeBuffer buf(bb);\n+    StubCodeGenerator cgen(&buf);\n+    _shenandoah_lrb = generate_shenandoah_lrb(&cgen);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.cpp","additions":715,"deletions":0,"binary":false,"changes":715,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class ShenandoahPreBarrierStub;\n+class ShenandoahLoadReferenceBarrierStub;\n+class StubAssembler;\n+#endif\n+class StubCodeGenerator;\n+\n+class ShenandoahBarrierSetAssembler: public BarrierSetAssembler {\n+private:\n+\n+  static address _shenandoah_lrb;\n+\n+  void satb_write_barrier_pre(MacroAssembler* masm,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register thread,\n+                              Register tmp,\n+                              bool tosca_live,\n+                              bool expand_call);\n+  void shenandoah_write_barrier_pre(MacroAssembler* masm,\n+                                    Register obj,\n+                                    Register pre_val,\n+                                    Register thread,\n+                                    Register tmp,\n+                                    bool tosca_live,\n+                                    bool expand_call);\n+\n+  void resolve_forward_pointer(MacroAssembler* masm, Register dst, Register tmp = noreg);\n+  void resolve_forward_pointer_not_null(MacroAssembler* masm, Register dst, Register tmp = noreg);\n+  void load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr);\n+  void load_reference_barrier_not_null(MacroAssembler* masm, Register dst, Address load_addr);\n+\n+  address generate_shenandoah_lrb(StubCodeGenerator* cgen);\n+\n+public:\n+\n+  static address shenandoah_lrb();\n+\n+  void iu_barrier(MacroAssembler* masm, Register dst, Register tmp);\n+\n+#ifdef COMPILER1\n+  void gen_pre_barrier_stub(LIR_Assembler* ce, ShenandoahPreBarrierStub* stub);\n+  void gen_load_reference_barrier_stub(LIR_Assembler* ce, ShenandoahLoadReferenceBarrierStub* stub);\n+  void generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm);\n+#endif\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register src, Register dst, Register count, RegSet saved_regs);\n+\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Address dst, Register val, Register tmp1, Register tmp2);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+\n+  virtual void cmpxchg_oop(MacroAssembler* masm, Register addr, Register expected, Register new_val,\n+                   Assembler::Aqrl acquire, Assembler::Aqrl release, bool is_cae, Register result);\n+\n+  virtual void barrier_stubs_init();\n+};\n+\n+#endif \/\/ CPU_RISCV_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,197 @@\n+\/\/\n+\/\/ Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n+\/\/ Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+source_hpp %{\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+%}\n+\n+instruct compareAndSwapP_shenandoah(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+\n+  format %{\n+    \"cmpxchg_shenandoah $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval with temp $tmp, #@compareAndSwapP_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapN_shenandoah(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval, iRegNNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+\n+  format %{\n+    \"cmpxchgw_shenandoah $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval with temp $tmp, #@compareAndSwapN_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapPAcq_shenandoah(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp, rFlagsReg cr) %{\n+  predicate(needs_acquiring_load_reserved(n));\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+\n+  format %{\n+    \"cmpxchg_acq_shenandoah_oop $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval with temp $tmp, #@compareAndSwapPAcq_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapNAcq_shenandoah(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval, iRegNNoSp tmp, rFlagsReg cr) %{\n+  predicate(needs_acquiring_load_reserved(n));\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+\n+  format %{\n+    \"cmpxchgw_acq_shenandoah_narrow_oop $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval with temp $tmp, #@compareAndSwapNAcq_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeN_shenandoah(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegNNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeN mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr);\n+\n+  format %{\n+    \"cmpxchgw_shenandoah $res = $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeN_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   true \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeP_shenandoah(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeP mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr);\n+  format %{\n+    \"cmpxchg_shenandoah $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval with temp $tmp, #@compareAndExchangeP_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   true \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapN_shenandoah(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval, iRegNNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahWeakCompareAndSwapN mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+  format %{\n+    \"cmpxchgw_shenandoah $res = $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval, #@weakCompareAndSwapN_shenandoah\"\n+    \"mv $res, EQ\\t# $res <-- (EQ ? 1 : 0)\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    \/\/ Weak is not current supported by ShenandoahBarrierSet::cmpxchg_oop\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapP_shenandoah(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp, rFlagsReg cr) %{\n+  match(Set res (ShenandoahWeakCompareAndSwapP mem (Binary oldval newval)));\n+  ins_cost(10 * DEFAULT_COST);\n+\n+  effect(TEMP tmp, KILL cr);\n+  format %{\n+    \"cmpxchg_shenandoah $res = $mem, $oldval, $newval\\t# (ptr, weak) if $mem == $oldval then $mem <-- $newval, #@weakCompareAndSwapP_shenandoah\"\n+  %}\n+\n+  ins_encode %{\n+    Register tmp = $tmp$$Register;\n+    __ mv(tmp, $oldval$$Register); \/\/ Must not clobber oldval.\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(&_masm, $mem$$Register, tmp, $newval$$Register,\n+                                                   Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/,\n+                                                   false \/* is_cae *\/, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoah_riscv64.ad","additions":197,"deletions":0,"binary":false,"changes":197,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GLOBALDEFINITIONS_RISCV_HPP\n+#define CPU_RISCV_GLOBALDEFINITIONS_RISCV_HPP\n+\n+const int StackAlignmentInBytes = 16;\n+\n+\/\/ Indicates whether the C calling conventions require that\n+\/\/ 32-bit integer argument values are extended to 64 bits.\n+const bool CCallingConventionRequiresIntsAsLongs = false;\n+\n+\/\/ To be safe, we deoptimize when we come across an access that needs\n+\/\/ patching. This is similar to what is done on aarch64.\n+#define DEOPTIMIZE_WHEN_PATCHING\n+\n+#define SUPPORTS_NATIVE_CX8\n+\n+#define SUPPORT_RESERVED_STACK_AREA\n+\n+#define THREAD_LOCAL_POLL\n+\n+#endif \/\/ CPU_RISCV_GLOBALDEFINITIONS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/globalDefinitions_riscv.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,110 @@\n+\/*\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GLOBALS_RISCV_HPP\n+#define CPU_RISCV_GLOBALS_RISCV_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Sets the default values for platform dependent flags used by the runtime system.\n+\/\/ (see globals.hpp)\n+\n+define_pd_global(bool, NeedsDeoptSuspend,        false); \/\/ only register window machines need this\n+\n+define_pd_global(bool, ImplicitNullChecks,       true);  \/\/ Generate code for implicit null checks\n+define_pd_global(bool, TrapBasedNullChecks,      false);\n+define_pd_global(bool, UncommonNullCast,         true);  \/\/ Uncommon-trap NULLs past to check cast\n+\n+define_pd_global(uintx, CodeCacheSegmentSize,    64 TIERED_ONLY(+64)); \/\/ Tiered compilation has large code-entry alignment.\n+define_pd_global(intx, CodeEntryAlignment,       64);\n+define_pd_global(intx, OptoLoopAlignment,        16);\n+define_pd_global(intx, InlineFrequencyCount,     100);\n+\n+#define DEFAULT_STACK_YELLOW_PAGES (2)\n+#define DEFAULT_STACK_RED_PAGES (1)\n+\/\/ Java_java_net_SocketOutputStream_socketWrite0() uses a 64k buffer on the\n+\/\/ stack if compiled for unix and LP64. To pass stack overflow tests we need\n+\/\/ 20 shadow pages.\n+#define DEFAULT_STACK_SHADOW_PAGES (20 DEBUG_ONLY(+5))\n+#define DEFAULT_STACK_RESERVED_PAGES (1)\n+\n+#define MIN_STACK_YELLOW_PAGES DEFAULT_STACK_YELLOW_PAGES\n+#define MIN_STACK_RED_PAGES    DEFAULT_STACK_RED_PAGES\n+#define MIN_STACK_SHADOW_PAGES DEFAULT_STACK_SHADOW_PAGES\n+#define MIN_STACK_RESERVED_PAGES (0)\n+\n+define_pd_global(intx, StackYellowPages, DEFAULT_STACK_YELLOW_PAGES);\n+define_pd_global(intx, StackRedPages, DEFAULT_STACK_RED_PAGES);\n+define_pd_global(intx, StackShadowPages, DEFAULT_STACK_SHADOW_PAGES);\n+define_pd_global(intx, StackReservedPages, DEFAULT_STACK_RESERVED_PAGES);\n+\n+define_pd_global(bool, RewriteBytecodes,     true);\n+define_pd_global(bool, RewriteFrequentPairs, true);\n+\n+define_pd_global(bool, UseMembar,            true);\n+\n+define_pd_global(bool, PreserveFramePointer, false);\n+\n+\/\/ GC Ergo Flags\n+define_pd_global(uintx, CMSYoungGenPerWorker, 64*M);  \/\/ default max size of CMS young gen, per GC worker thread\n+\n+define_pd_global(uintx, TypeProfileLevel, 111);\n+\n+define_pd_global(bool, CompactStrings, true);\n+\n+\/\/ Clear short arrays bigger than one word in an arch-specific way\n+define_pd_global(intx, InitArrayShortSize, BytesPerLong);\n+\n+define_pd_global(bool, ThreadLocalHandshakes, true);\n+\n+define_pd_global(intx, InlineSmallCode,          1000);\n+\n+#define ARCH_FLAGS(develop,                                                      \\\n+                   product,                                                      \\\n+                   diagnostic,                                                   \\\n+                   experimental,                                                 \\\n+                   notproduct,                                                   \\\n+                   range,                                                        \\\n+                   constraint,                                                   \\\n+                   writeable)                                                    \\\n+                                                                                 \\\n+  product(bool, NearCpool, true,                                                 \\\n+         \"constant pool is close to instructions\")                               \\\n+  product(intx, BlockZeroingLowLimit, 256,                                       \\\n+          \"Minimum size in bytes when block zeroing will be used\")               \\\n+          range(1, max_jint)                                                     \\\n+  product(bool, TraceTraps, false, \"Trace all traps the signal handler\")         \\\n+  \/* For now we're going to be safe and add the I\/O bits to userspace fences. *\/ \\\n+  product(bool, UseConservativeFence, true,                                      \\\n+          \"Extend i for r and o for w in the pred\/succ flags of fence;\"          \\\n+          \"Extend fence.i to fence.i + fence.\")                                  \\\n+  product(bool, AvoidUnalignedAccesses, true,                                    \\\n+          \"Avoid generating unaligned memory accesses\")                          \\\n+  experimental(bool, UseRVV, false, \"Use RVV instructions\")                      \\\n+  experimental(bool, UseRVB, false, \"Use RVB instructions\")                      \\\n+  experimental(bool, UseRVC, false, \"Use RVC instructions\")\n+\n+#endif \/\/ CPU_RISCV_GLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":110,"deletions":0,"binary":false,"changes":110,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 1997, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+int InlineCacheBuffer::ic_stub_code_size() {\n+  \/\/ 6: auipc + ld + auipc + jalr + address(2 * instruction_size)\n+  \/\/ 5: auipc + ld + j + address(2 * instruction_size)\n+  return (MacroAssembler::far_branches() ? 6 : 5) * NativeInstruction::instruction_size;\n+}\n+\n+#define __ masm->\n+\n+void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n+  assert_cond(code_begin != NULL && entry_point != NULL);\n+  ResourceMark rm;\n+  CodeBuffer      code(code_begin, ic_stub_code_size());\n+  MacroAssembler* masm            = new MacroAssembler(&code);\n+  \/\/ Note: even though the code contains an embedded value, we do not need reloc info\n+  \/\/ because\n+  \/\/ (1) the value is old (i.e., doesn't matter for scavenges)\n+  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear\n+\n+  address start = __ pc();\n+  Label l;\n+  __ ld(t1, l);\n+  __ far_jump(ExternalAddress(entry_point));\n+  __ align(wordSize);\n+  __ bind(l);\n+  __ emit_int64((intptr_t)cached_value);\n+  \/\/ Only need to invalidate the 1st two instructions - not the whole ic stub\n+  ICache::invalidate_range(code_begin, InlineCacheBuffer::ic_stub_code_size());\n+  assert(__ pc() - start == ic_stub_code_size(), \"must be\");\n+}\n+\n+address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n+  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n+  NativeJump* jump = nativeJump_at(move->next_instruction_address());\n+  return jump->jump_destination();\n+}\n+\n+\n+void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n+  \/\/ The word containing the cached value is at the end of this IC buffer\n+  uintptr_t *p = (uintptr_t *)(code_begin + ic_stub_code_size() - wordSize);\n+  void* o = (void*)*p;\n+  return o;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/icBuffer_riscv.cpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"runtime\/icache.hpp\"\n+\n+#define __ _masm->\n+\n+static int icache_flush(address addr, int lines, int magic) {\n+  os::icache_flush((long int) addr, (long int) (addr + (lines << ICache::log2_line_size)));\n+  return magic;\n+}\n+\n+void ICacheStubGenerator::generate_icache_flush(ICache::flush_icache_stub_t* flush_icache_stub) {\n+  address start = (address)icache_flush;\n+  *flush_icache_stub = (ICache::flush_icache_stub_t)start;\n+\n+  \/\/ ICache::invalidate_range() contains explicit condition that the first\n+  \/\/ call is invoked on the generated icache flush stub code range.\n+  ICache::invalidate_range(start, 0);\n+\n+  {\n+    StubCodeMark mark(this, \"ICache\", \"fake_stub_for_inlined_icache_flush\");\n+    __ ret();\n+  }\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/icache_riscv.cpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_ICACHE_RISCV_HPP\n+#define CPU_RISCV_ICACHE_RISCV_HPP\n+\n+\/\/ Interface for updating the instruction cache. Whenever the VM\n+\/\/ modifies code, part of the processor instruction cache potentially\n+\/\/ has to be flushed.\n+\n+class ICache : public AbstractICache {\n+public:\n+  enum {\n+    stub_size      = 16,                \/\/ Size of the icache flush stub in bytes\n+    line_size      = BytesPerWord,      \/\/ conservative\n+    log2_line_size = LogBytesPerWord    \/\/ log2(line_size)\n+  };\n+};\n+\n+#endif \/\/ CPU_RISCV_ICACHE_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/icache_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,1931 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"interp_masm_riscv.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interpreterRuntime.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/jvmtiThreadState.hpp\"\n+#include \"runtime\/basicLock.hpp\"\n+#include \"runtime\/biasedLocking.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+\n+void InterpreterMacroAssembler::narrow(Register result) {\n+  \/\/ Get method->_constMethod->_result_type\n+  ld(t0, Address(fp, frame::interpreter_frame_method_offset * wordSize));\n+  ld(t0, Address(t0, Method::const_offset()));\n+  lbu(t0, Address(t0, ConstMethod::result_type_offset()));\n+\n+  Label done, notBool, notByte, notChar;\n+\n+  \/\/ common case first\n+  mv(t1, T_INT);\n+  beq(t0, t1, done);\n+\n+  \/\/ mask integer result to narrower return type.\n+  mv(t1, T_BOOLEAN);\n+  bne(t0, t1, notBool);\n+\n+  andi(result, result, 0x1);\n+  j(done);\n+\n+  bind(notBool);\n+  mv(t1, T_BYTE);\n+  bne(t0, t1, notByte);\n+  sign_extend(result, result, 8);\n+  j(done);\n+\n+  bind(notByte);\n+  mv(t1, T_CHAR);\n+  bne(t0, t1, notChar);\n+  zero_extend(result, result, 16);\n+  j(done);\n+\n+  bind(notChar);\n+  sign_extend(result, result, 16);\n+\n+  \/\/ Nothing to do for T_INT\n+  bind(done);\n+  addw(result, result, zr);\n+}\n+\n+void InterpreterMacroAssembler::jump_to_entry(address entry) {\n+  assert(entry != NULL, \"Entry must have been generated by now\");\n+  j(entry);\n+}\n+\n+void InterpreterMacroAssembler::check_and_handle_popframe(Register java_thread) {\n+  if (JvmtiExport::can_pop_frame()) {\n+    Label L;\n+    \/\/ Initiate popframe handling only if it is not already being\n+    \/\/ processed. If the flag has the popframe_processing bit set,\n+    \/\/ it means that this code is called *during* popframe handling - we\n+    \/\/ don't want to reenter.\n+    \/\/ This method is only called just after the call into the vm in\n+    \/\/ call_VM_base, so the arg registers are available.\n+    lwu(t1, Address(xthread, JavaThread::popframe_condition_offset()));\n+    andi(t0, t1, JavaThread::popframe_pending_bit);\n+    beqz(t0, L);\n+    andi(t0, t1, JavaThread::popframe_processing_bit);\n+    bnez(t0, L);\n+    \/\/ Call Interpreter::remove_activation_preserving_args_entry() to get the\n+    \/\/ address of the same-named entrypoint in the generated interpreter code.\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, Interpreter::remove_activation_preserving_args_entry));\n+    jr(x10);\n+    bind(L);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::load_earlyret_value(TosState state) {\n+  ld(x12, Address(xthread, JavaThread::jvmti_thread_state_offset()));\n+  const Address tos_addr(x12, JvmtiThreadState::earlyret_tos_offset());\n+  const Address oop_addr(x12, JvmtiThreadState::earlyret_oop_offset());\n+  const Address val_addr(x12, JvmtiThreadState::earlyret_value_offset());\n+  switch (state) {\n+    case atos:\n+      ld(x10, oop_addr);\n+      sd(zr, oop_addr);\n+      verify_oop(x10);\n+      break;\n+    case ltos:\n+      ld(x10, val_addr);\n+      break;\n+    case btos:  \/\/ fall through\n+    case ztos:  \/\/ fall through\n+    case ctos:  \/\/ fall through\n+    case stos:  \/\/ fall through\n+    case itos:\n+      lwu(x10, val_addr);\n+      break;\n+    case ftos:\n+      flw(f10, val_addr);\n+      break;\n+    case dtos:\n+      fld(f10, val_addr);\n+      break;\n+    case vtos:\n+      \/* nothing to do *\/\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  \/\/ Clean up tos value in the thread object\n+  mvw(t0, (int) ilgl);\n+  sw(t0, tos_addr);\n+  sw(zr, val_addr);\n+}\n+\n+\n+void InterpreterMacroAssembler::check_and_handle_earlyret(Register java_thread) {\n+  if (JvmtiExport::can_force_early_return()) {\n+    Label L;\n+    ld(t0, Address(xthread, JavaThread::jvmti_thread_state_offset()));\n+    beqz(t0, L);  \/\/ if [thread->jvmti_thread_state() == NULL] then exit\n+\n+    \/\/ Initiate earlyret handling only if it is not already being processed.\n+    \/\/ If the flag has the earlyret_processing bit set, it means that this code\n+    \/\/ is called *during* earlyret handling - we don't want to reenter.\n+    lwu(t0, Address(t0, JvmtiThreadState::earlyret_state_offset()));\n+    mv(t1, JvmtiThreadState::earlyret_pending);\n+    bne(t0, t1, L);\n+\n+    \/\/ Call Interpreter::remove_activation_early_entry() to get the address of the\n+    \/\/ same-named entrypoint in the generated interpreter code.\n+    ld(t0, Address(xthread, JavaThread::jvmti_thread_state_offset()));\n+    lwu(t0, Address(t0, JvmtiThreadState::earlyret_tos_offset()));\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, Interpreter::remove_activation_early_entry), t0);\n+    jr(x10);\n+    bind(L);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::get_unsigned_2_byte_index_at_bcp(Register reg, int bcp_offset) {\n+  assert(bcp_offset >= 0, \"bcp is still pointing to start of bytecode\");\n+  lhu(reg, Address(xbcp, bcp_offset));\n+  revb_h(reg, reg);\n+}\n+\n+void InterpreterMacroAssembler::get_dispatch() {\n+  int32_t offset = 0;\n+  la_patchable(xdispatch, ExternalAddress((address)Interpreter::dispatch_table()), offset);\n+  addi(xdispatch, xdispatch, offset);\n+}\n+\n+void InterpreterMacroAssembler::get_cache_index_at_bcp(Register index,\n+                                                       int bcp_offset,\n+                                                       size_t index_size) {\n+  assert(bcp_offset > 0, \"bcp is still pointing to start of bytecode\");\n+  if (index_size == sizeof(u2)) {\n+    load_unsigned_short(index, Address(xbcp, bcp_offset));\n+  } else if (index_size == sizeof(u4)) {\n+    lwu(index, Address(xbcp, bcp_offset));\n+    \/\/ Check if the secondary index definition is still ~x, otherwise\n+    \/\/ we have to change the following assembler code to calculate the\n+    \/\/ plain index.\n+    assert(ConstantPool::decode_invokedynamic_index(~123) == 123, \"else change next line\");\n+    xori(index, index, -1);\n+    addw(index, index, zr);\n+  } else if (index_size == sizeof(u1)) {\n+    load_unsigned_byte(index, Address(xbcp, bcp_offset));\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Return\n+\/\/ Rindex: index into constant pool\n+\/\/ Rcache: address of cache entry - ConstantPoolCache::base_offset()\n+\/\/\n+\/\/ A caller must add ConstantPoolCache::base_offset() to Rcache to get\n+\/\/ the true address of the cache entry.\n+\/\/\n+void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n+                                                           Register index,\n+                                                           int bcp_offset,\n+                                                           size_t index_size) {\n+  assert_different_registers(cache, index);\n+  assert_different_registers(cache, xcpool);\n+  get_cache_index_at_bcp(index, bcp_offset, index_size);\n+  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n+  \/\/ Convert from field index to ConstantPoolCacheEntry\n+  \/\/ riscv already has the cache in xcpool so there is no need to\n+  \/\/ install it in cache. Instead we pre-add the indexed offset to\n+  \/\/ xcpool and return it in cache. All clients of this method need to\n+  \/\/ be modified accordingly.\n+  shadd(cache, index, xcpool, cache, 5);\n+}\n+\n+\n+void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n+                                                                        Register index,\n+                                                                        Register bytecode,\n+                                                                        int byte_no,\n+                                                                        int bcp_offset,\n+                                                                        size_t index_size) {\n+  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n+  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n+  \/\/ little-endian machines allow us that.\n+  \/\/ n.b. unlike x86 cache already includes the index offset\n+  la(bytecode, Address(cache,\n+                       ConstantPoolCache::base_offset() +\n+                       ConstantPoolCacheEntry::indices_offset()));\n+  membar(MacroAssembler::AnyAny);\n+  lwu(bytecode, bytecode);\n+  membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  const int shift_count = (1 + byte_no) * BitsPerByte;\n+  slli(bytecode, bytecode, XLEN - (shift_count + BitsPerByte));\n+  srli(bytecode, bytecode, XLEN - BitsPerByte);\n+}\n+\n+void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n+                                                               Register tmp,\n+                                                               int bcp_offset,\n+                                                               size_t index_size) {\n+  assert(cache != tmp, \"must use different register\");\n+  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n+  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n+  \/\/ Convert from field index to ConstantPoolCacheEntry index\n+  \/\/ and from word offset to byte offset\n+  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord,\n+         \"else change next line\");\n+  ld(cache, Address(fp, frame::interpreter_frame_cache_offset * wordSize));\n+  \/\/ skip past the header\n+  add(cache, cache, in_bytes(ConstantPoolCache::base_offset()));\n+  \/\/ construct pointer to cache entry\n+  shadd(cache, tmp, cache, tmp, 2 + LogBytesPerWord);\n+}\n+\n+\/\/ Load object from cpool->resolved_references(index)\n+void InterpreterMacroAssembler::load_resolved_reference_at_index(\n+                                Register result, Register index, Register tmp) {\n+  assert_different_registers(result, index);\n+\n+  get_constant_pool(result);\n+  \/\/ Load pointer for resolved_references[] objArray\n+  ld(result, Address(result, ConstantPool::cache_offset_in_bytes()));\n+  ld(result, Address(result, ConstantPoolCache::resolved_references_offset_in_bytes()));\n+  resolve_oop_handle(result, tmp);\n+  \/\/ Add in the index\n+  addi(index, index, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+  shadd(result, index, result, index, LogBytesPerHeapOop);\n+  load_heap_oop(result, Address(result, 0));\n+}\n+\n+void InterpreterMacroAssembler::load_resolved_klass_at_offset(\n+                                Register cpool, Register index, Register klass, Register temp) {\n+  shadd(temp, index, cpool, temp, LogBytesPerWord);\n+  lhu(temp, Address(temp, sizeof(ConstantPool))); \/\/ temp = resolved_klass_index\n+  ld(klass, Address(cpool, ConstantPool::resolved_klasses_offset_in_bytes())); \/\/ klass = cpool->_resolved_klasses\n+  shadd(klass, temp, klass, temp, LogBytesPerWord);\n+  ld(klass, Address(klass, Array<Klass*>::base_offset_in_bytes()));\n+}\n+\n+\/\/ Generate a subtype check: branch to ok_is_subtype if sub_klass is a\n+\/\/ subtype of super_klass.\n+\/\/\n+\/\/ Args:\n+\/\/      x10: superklass\n+\/\/      Rsub_klass: subklass\n+\/\/\n+\/\/ Kills:\n+\/\/      x12, x15\n+void InterpreterMacroAssembler::gen_subtype_check(Register Rsub_klass,\n+                                                  Label& ok_is_subtype) {\n+  assert(Rsub_klass != x10, \"x10 holds superklass\");\n+  assert(Rsub_klass != x12, \"x12 holds 2ndary super array length\");\n+  assert(Rsub_klass != x15, \"x15 holds 2ndary super array scan ptr\");\n+\n+  \/\/ Profile the not-null value's klass.\n+  profile_typecheck(x12, Rsub_klass, x15); \/\/ blows x12, reloads x15\n+\n+  \/\/ Do the check.\n+  check_klass_subtype(Rsub_klass, x10, x12, ok_is_subtype); \/\/ blows x12\n+\n+  \/\/ Profile the failure of the check.\n+  profile_typecheck_failed(x12); \/\/ blows x12\n+}\n+\n+\/\/ Java Expression Stack\n+\n+void InterpreterMacroAssembler::pop_ptr(Register r) {\n+  ld(r, Address(esp, 0));\n+  addi(esp, esp, wordSize);\n+}\n+\n+void InterpreterMacroAssembler::pop_i(Register r) {\n+  lw(r, Address(esp, 0)); \/\/ lw do signed extended\n+  addi(esp, esp, wordSize);\n+}\n+\n+void InterpreterMacroAssembler::pop_l(Register r) {\n+  ld(r, Address(esp, 0));\n+  addi(esp, esp, 2 * Interpreter::stackElementSize);\n+}\n+\n+void InterpreterMacroAssembler::push_ptr(Register r) {\n+  addi(esp, esp, -wordSize);\n+  sd(r, Address(esp, 0));\n+}\n+\n+void InterpreterMacroAssembler::push_i(Register r) {\n+  addi(esp, esp, -wordSize);\n+  addw(r, r, zr); \/\/ signed extended\n+  sd(r, Address(esp, 0));\n+}\n+\n+void InterpreterMacroAssembler::push_l(Register r) {\n+  addi(esp, esp, -2 * wordSize);\n+  sd(zr, Address(esp, wordSize));\n+  sd(r, Address(esp));\n+}\n+\n+void InterpreterMacroAssembler::pop_f(FloatRegister r) {\n+  flw(r, esp, 0);\n+  addi(esp, esp, wordSize);\n+}\n+\n+void InterpreterMacroAssembler::pop_d(FloatRegister r) {\n+  fld(r, esp, 0);\n+  addi(esp, esp, 2 * Interpreter::stackElementSize);\n+}\n+\n+void InterpreterMacroAssembler::push_f(FloatRegister r) {\n+  addi(esp, esp, -wordSize);\n+  fsw(r, Address(esp, 0));\n+}\n+\n+void InterpreterMacroAssembler::push_d(FloatRegister r) {\n+  addi(esp, esp, -2 * wordSize);\n+  fsd(r, Address(esp, 0));\n+}\n+\n+void InterpreterMacroAssembler::pop(TosState state) {\n+  switch (state) {\n+    case atos:\n+      pop_ptr();\n+      verify_oop(x10);\n+      break;\n+    case btos:  \/\/ fall through\n+    case ztos:  \/\/ fall through\n+    case ctos:  \/\/ fall through\n+    case stos:  \/\/ fall through\n+    case itos:\n+      pop_i();\n+      break;\n+    case ltos:\n+      pop_l();\n+      break;\n+    case ftos:\n+      pop_f();\n+      break;\n+    case dtos:\n+      pop_d();\n+      break;\n+    case vtos:\n+      \/* nothing to do *\/\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void InterpreterMacroAssembler::push(TosState state) {\n+  switch (state) {\n+    case atos:\n+      verify_oop(x10);\n+      push_ptr();\n+      break;\n+    case btos:  \/\/ fall through\n+    case ztos:  \/\/ fall through\n+    case ctos:  \/\/ fall through\n+    case stos:  \/\/ fall through\n+    case itos:\n+      push_i();\n+      break;\n+    case ltos:\n+      push_l();\n+      break;\n+    case ftos:\n+      push_f();\n+      break;\n+    case dtos:\n+      push_d();\n+      break;\n+    case vtos:\n+      \/* nothing to do *\/\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Helpers for swap and dup\n+void InterpreterMacroAssembler::load_ptr(int n, Register val) {\n+  ld(val, Address(esp, Interpreter::expr_offset_in_bytes(n)));\n+}\n+\n+void InterpreterMacroAssembler::store_ptr(int n, Register val) {\n+  sd(val, Address(esp, Interpreter::expr_offset_in_bytes(n)));\n+}\n+\n+void InterpreterMacroAssembler::load_float(Address src) {\n+  flw(f10, src);\n+}\n+\n+void InterpreterMacroAssembler::load_double(Address src) {\n+  fld(f10, src);\n+}\n+\n+void InterpreterMacroAssembler::prepare_to_jump_from_interpreted() {\n+  \/\/ set sender sp\n+  mv(x30, sp);\n+  \/\/ record last_sp\n+  sd(esp, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+}\n+\n+\/\/ Jump to from_interpreted entry of a call unless single stepping is possible\n+\/\/ in this thread in which case we must call the i2i entry\n+void InterpreterMacroAssembler::jump_from_interpreted(Register method) {\n+  prepare_to_jump_from_interpreted();\n+  if (JvmtiExport::can_post_interpreter_events()) {\n+    Label run_compiled_code;\n+    \/\/ JVMTI events, such as single-stepping, are implemented partly by avoiding running\n+    \/\/ compiled code in threads for which the event is enabled.  Check here for\n+    \/\/ interp_only_mode if these events CAN be enabled.\n+    lwu(t0, Address(xthread, JavaThread::interp_only_mode_offset()));\n+    beqz(t0, run_compiled_code);\n+    ld(t0, Address(method, Method::interpreter_entry_offset()));\n+    jr(t0);\n+    bind(run_compiled_code);\n+  }\n+\n+  ld(t0, Address(method, Method::from_interpreted_offset()));\n+  jr(t0);\n+}\n+\n+\/\/ The following two routines provide a hook so that an implementation\n+\/\/ can schedule the dispatch in two parts.  amd64 does not do this.\n+void InterpreterMacroAssembler::dispatch_prolog(TosState state, int step) {\n+}\n+\n+void InterpreterMacroAssembler::dispatch_epilog(TosState state, int step) {\n+  dispatch_next(state, step);\n+}\n+\n+void InterpreterMacroAssembler::dispatch_base(TosState state,\n+                                              address* table,\n+                                              bool verifyoop,\n+                                              bool generate_poll,\n+                                              Register Rs) {\n+  \/\/ Pay attention to the argument Rs, which is acquiesce in t0.\n+  if (VerifyActivationFrameSize) {\n+    Unimplemented();\n+  }\n+  if (verifyoop && state == atos) {\n+    verify_oop(x10);\n+  }\n+\n+  Label safepoint;\n+  address* const safepoint_table = Interpreter::safept_table(state);\n+  bool needs_thread_local_poll = generate_poll &&\n+    SafepointMechanism::uses_thread_local_poll() && table != safepoint_table;\n+\n+  if (needs_thread_local_poll) {\n+    NOT_PRODUCT(block_comment(\"Thread-local Safepoint poll\"));\n+    ld(t1, Address(xthread, Thread::polling_page_offset()));\n+    andi(t1, t1, SafepointMechanism::poll_bit());\n+    bnez(t1, safepoint);\n+  }\n+  if (table == Interpreter::dispatch_table(state)) {\n+    li(t1, Interpreter::distance_from_dispatch_table(state));\n+    add(t1, Rs, t1);\n+    shadd(t1, t1, xdispatch, t1, 3);\n+  } else {\n+    mv(t1, (address)table);\n+    shadd(t1, Rs, t1, Rs, 3);\n+  }\n+  ld(t1, Address(t1));\n+  jr(t1);\n+\n+  if (needs_thread_local_poll) {\n+    bind(safepoint);\n+    la(t1, ExternalAddress((address)safepoint_table));\n+    shadd(t1, Rs, t1, Rs, 3);\n+    ld(t1, Address(t1));\n+    jr(t1);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::dispatch_only(TosState state, bool generate_poll, Register Rs) {\n+  dispatch_base(state, Interpreter::dispatch_table(state), true, generate_poll, Rs);\n+}\n+\n+void InterpreterMacroAssembler::dispatch_only_normal(TosState state, Register Rs) {\n+  dispatch_base(state, Interpreter::normal_table(state), Rs);\n+}\n+\n+void InterpreterMacroAssembler::dispatch_only_noverify(TosState state, Register Rs) {\n+  dispatch_base(state, Interpreter::normal_table(state), false, Rs);\n+}\n+\n+void InterpreterMacroAssembler::dispatch_next(TosState state, int step, bool generate_poll) {\n+  \/\/ load next bytecode\n+  load_unsigned_byte(t0, Address(xbcp, step));\n+  add(xbcp, xbcp, step);\n+  dispatch_base(state, Interpreter::dispatch_table(state), true, generate_poll);\n+}\n+\n+void InterpreterMacroAssembler::dispatch_via(TosState state, address* table) {\n+  \/\/ load current bytecode\n+  lbu(t0, Address(xbcp, 0));\n+  dispatch_base(state, table);\n+}\n+\n+\/\/ remove activation\n+\/\/\n+\/\/ Unlock the receiver if this is a synchronized method.\n+\/\/ Unlock any Java monitors from syncronized blocks.\n+\/\/ Remove the activation from the stack.\n+\/\/\n+\/\/ If there are locked Java monitors\n+\/\/    If throw_monitor_exception\n+\/\/       throws IllegalMonitorStateException\n+\/\/    Else if install_monitor_exception\n+\/\/       installs IllegalMonitorStateException\n+\/\/    Else\n+\/\/       no error processing\n+void InterpreterMacroAssembler::remove_activation(\n+                                TosState state,\n+                                bool throw_monitor_exception,\n+                                bool install_monitor_exception,\n+                                bool notify_jvmdi) {\n+  \/\/ Note: Registers x13 may be in use for the\n+  \/\/ result check if synchronized method\n+  Label unlocked, unlock, no_unlock;\n+\n+  \/\/ get the value of _do_not_unlock_if_synchronized into x13\n+  const Address do_not_unlock_if_synchronized(xthread,\n+    in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()));\n+  lbu(x13, do_not_unlock_if_synchronized);\n+  sb(zr, do_not_unlock_if_synchronized); \/\/ reset the flag\n+\n+  \/\/ get method access flags\n+  ld(x11, Address(fp, frame::interpreter_frame_method_offset * wordSize));\n+  ld(x12, Address(x11, Method::access_flags_offset()));\n+  andi(t0, x12, JVM_ACC_SYNCHRONIZED);\n+  beqz(t0, unlocked);\n+\n+  \/\/ Don't unlock anything if the _do_not_unlock_if_synchronized flag\n+  \/\/ is set.\n+  bnez(x13, no_unlock);\n+\n+  \/\/ unlock monitor\n+  push(state); \/\/ save result\n+\n+  \/\/ BasicObjectLock will be first in list, since this is a\n+  \/\/ synchronized method. However, need to check that the object has\n+  \/\/ not been unlocked by an explicit monitorexit bytecode.\n+  const Address monitor(fp, frame::interpreter_frame_initial_sp_offset *\n+                        wordSize - (int) sizeof(BasicObjectLock));\n+  \/\/ We use c_rarg1 so that if we go slow path it will be the correct\n+  \/\/ register for unlock_object to pass to VM directly\n+  la(c_rarg1, monitor); \/\/ address of first monitor\n+\n+  ld(x10, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+  bnez(x10, unlock);\n+\n+  pop(state);\n+  if (throw_monitor_exception) {\n+    \/\/ Entry already unlocked, need to throw exception\n+    call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+    should_not_reach_here();\n+  } else {\n+    \/\/ Monitor already unlocked during a stack unroll. If requested,\n+    \/\/ install an illegal_monitor_state_exception.  Continue with\n+    \/\/ stack unrolling.\n+    if (install_monitor_exception) {\n+      call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                      InterpreterRuntime::new_illegal_monitor_state_exception));\n+    }\n+    j(unlocked);\n+  }\n+\n+  bind(unlock);\n+  unlock_object(c_rarg1);\n+  pop(state);\n+\n+  \/\/ Check that for block-structured locking (i.e., that all locked\n+  \/\/ objects has been unlocked)\n+  bind(unlocked);\n+\n+  \/\/ x10: Might contain return value\n+\n+  \/\/ Check that all monitors are unlocked\n+  {\n+    Label loop, exception, entry, restart;\n+    const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+    const Address monitor_block_top(\n+      fp, frame::interpreter_frame_monitor_block_top_offset * wordSize);\n+    const Address monitor_block_bot(\n+      fp, frame::interpreter_frame_initial_sp_offset * wordSize);\n+\n+    bind(restart);\n+    \/\/ We use c_rarg1 so that if we go slow path it will be the correct\n+    \/\/ register for unlock_object to pass to VM directly\n+    ld(c_rarg1, monitor_block_top); \/\/ points to current entry, starting\n+                                     \/\/ with top-most entry\n+    la(x9, monitor_block_bot);  \/\/ points to word before bottom of\n+                                  \/\/ monitor block\n+\n+    j(entry);\n+\n+    \/\/ Entry already locked, need to throw exception\n+    bind(exception);\n+\n+    if (throw_monitor_exception) {\n+      \/\/ Throw exception\n+      MacroAssembler::call_VM(noreg,\n+                              CAST_FROM_FN_PTR(address, InterpreterRuntime::\n+                                               throw_illegal_monitor_state_exception));\n+\n+      should_not_reach_here();\n+    } else {\n+      \/\/ Stack unrolling. Unlock object and install illegal_monitor_exception.\n+      \/\/ Unlock does not block, so don't have to worry about the frame.\n+      \/\/ We don't have to preserve c_rarg1 since we are going to throw an exception.\n+\n+      push(state);\n+      unlock_object(c_rarg1);\n+      pop(state);\n+\n+      if (install_monitor_exception) {\n+        call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                        InterpreterRuntime::\n+                                        new_illegal_monitor_state_exception));\n+      }\n+\n+      j(restart);\n+    }\n+\n+    bind(loop);\n+    \/\/ check if current entry is used\n+    add(t0, c_rarg1, BasicObjectLock::obj_offset_in_bytes());\n+    ld(t0, Address(t0, 0));\n+    bnez(t0, exception);\n+\n+    add(c_rarg1, c_rarg1, entry_size); \/\/ otherwise advance to next entry\n+    bind(entry);\n+    bne(c_rarg1, x9, loop); \/\/ check if bottom reached if not at bottom then check this entry\n+  }\n+\n+  bind(no_unlock);\n+\n+  \/\/ jvmti support\n+  if (notify_jvmdi) {\n+    notify_method_exit(state, NotifyJVMTI);    \/\/ preserve TOSCA\n+\n+  } else {\n+    notify_method_exit(state, SkipNotifyJVMTI); \/\/ preserve TOSCA\n+  }\n+\n+  \/\/ remove activation\n+  \/\/ get sender esp\n+  ld(t1,\n+     Address(fp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  if (StackReservedPages > 0) {\n+    \/\/ testing if reserved zone needs to be re-enabled\n+    Label no_reserved_zone_enabling;\n+\n+    ld(t0, Address(xthread, JavaThread::reserved_stack_activation_offset()));\n+    ble(t1, t0, no_reserved_zone_enabling);\n+\n+    call_VM_leaf(\n+      CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), xthread);\n+    call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                    InterpreterRuntime::throw_delayed_StackOverflowError));\n+    should_not_reach_here();\n+\n+    bind(no_reserved_zone_enabling);\n+  }\n+\n+  \/\/ restore sender esp\n+  mv(esp, t1);\n+\n+  \/\/ remove frame anchor\n+  leave();\n+  \/\/ If we're returning to interpreted code we will shortly be\n+  \/\/ adjusting SP to allow some space for ESP.  If we're returning to\n+  \/\/ compiled code the saved sender SP was saved in sender_sp, so this\n+  \/\/ restores it.\n+  andi(sp, esp, -16);\n+}\n+\n+\/\/ Lock object\n+\/\/\n+\/\/ Args:\n+\/\/      c_rarg1: BasicObjectLock to be used for locking\n+\/\/\n+\/\/ Kills:\n+\/\/      x10\n+\/\/      c_rarg0, c_rarg1, c_rarg2, c_rarg3, .. (param regs)\n+\/\/      t0, t1 (temp regs)\n+void InterpreterMacroAssembler::lock_object(Register lock_reg)\n+{\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be c_rarg1\");\n+  if (UseHeavyMonitors) {\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n+  } else {\n+    Label done;\n+\n+    const Register swap_reg = x10;\n+    const Register tmp = c_rarg2;\n+    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n+\n+    const int obj_offset = BasicObjectLock::obj_offset_in_bytes();\n+    const int lock_offset = BasicObjectLock::lock_offset_in_bytes ();\n+    const int mark_offset = lock_offset +\n+                            BasicLock::displaced_header_offset_in_bytes();\n+\n+    Label slow_case;\n+\n+    \/\/ Load object pointer into obj_reg c_rarg3\n+    ld(obj_reg, Address(lock_reg, obj_offset));\n+\n+    if (UseBiasedLocking) {\n+      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n+    }\n+\n+    \/\/ Load (object->mark() | 1) into swap_reg\n+    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+    ori(swap_reg, t0, 1);\n+\n+    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+    sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+    assert(lock_offset == 0,\n+           \"displached header must be first word in BasicObjectLock\");\n+\n+    if (PrintBiasedLockingStatistics) {\n+      Label fail, fast;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, fast, &fail);\n+      bind(fast);\n+      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                  t1, t0);\n+      j(done);\n+      bind(fail);\n+    } else {\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, done, \/*fallthrough*\/NULL);\n+    }\n+\n+    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+    \/\/  1) (mark & 7) == 0, and\n+    \/\/  2) sp <= mark < mark + os::pagesize()\n+    \/\/\n+    \/\/ These 3 tests can be done by evaluating the following\n+    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+    \/\/ assuming both stack pointer and pagesize have their\n+    \/\/ least significant 3 bits clear.\n+    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+    sub(swap_reg, swap_reg, sp);\n+    li(t0, (int64_t)(7 - os::vm_page_size()));\n+    andr(swap_reg, swap_reg, t0);\n+\n+    \/\/ Save the test result, for recursive case, the result is zero\n+    sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+    if (PrintBiasedLockingStatistics) {\n+      bnez(swap_reg, slow_case);\n+      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                  t1, t0);\n+    }\n+    beqz(swap_reg, done);\n+\n+    bind(slow_case);\n+\n+    \/\/ Call the runtime routine for slow case\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n+\n+    bind(done);\n+  }\n+}\n+\n+\n+\/\/ Unlocks an object. Used in monitorexit bytecode and\n+\/\/ remove_activation.  Throws an IllegalMonitorException if object is\n+\/\/ not locked by current thread.\n+\/\/\n+\/\/ Args:\n+\/\/      c_rarg1: BasicObjectLock for lock\n+\/\/\n+\/\/ Kills:\n+\/\/      x10\n+\/\/      c_rarg0, c_rarg1, c_rarg2, c_rarg3, ... (param regs)\n+\/\/      t0, t1 (temp regs)\n+void InterpreterMacroAssembler::unlock_object(Register lock_reg)\n+{\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be rarg1\");\n+\n+  if (UseHeavyMonitors) {\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),\n+            lock_reg);\n+  } else {\n+    Label done;\n+\n+    const Register swap_reg   = x10;\n+    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n+\n+    save_bcp(); \/\/ Save in case of exception\n+\n+    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+    \/\/ structure Store the BasicLock address into x10\n+    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+\n+    \/\/ Load oop into obj_reg(c_rarg3)\n+    ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+    \/\/ Free entry\n+    sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+    if (UseBiasedLocking) {\n+      biased_locking_exit(obj_reg, header_reg, done);\n+    }\n+\n+    \/\/ Load the old header from BasicLock structure\n+    ld(header_reg, Address(swap_reg,\n+                           BasicLock::displaced_header_offset_in_bytes()));\n+\n+    \/\/ Test for recursion\n+    beqz(header_reg, done);\n+\n+    \/\/ Atomic swap back the old header\n+    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, done, \/*fallthrough*\/NULL);\n+\n+    \/\/ Call the runtime routine for slow case.\n+    sd(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes())); \/\/ restore obj\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),\n+            lock_reg);\n+\n+    bind(done);\n+\n+    restore_bcp();\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::test_method_data_pointer(Register mdp,\n+                                                         Label& zero_continue) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  ld(mdp, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+  beqz(mdp, zero_continue);\n+}\n+\n+\/\/ Set the method data pointer for the current bcp.\n+void InterpreterMacroAssembler::set_method_data_pointer_for_bcp() {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  Label set_mdp;\n+  push_reg(0xc00, sp); \/\/ save x10, x11\n+\n+  \/\/ Test MDO to avoid the call if it is NULL.\n+  ld(x10, Address(xmethod, in_bytes(Method::method_data_offset())));\n+  beqz(x10, set_mdp);\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::bcp_to_di), xmethod, xbcp);\n+  \/\/ x10: mdi\n+  \/\/ mdo is guaranteed to be non-zero here, we checked for it before the call.\n+  ld(x11, Address(xmethod, in_bytes(Method::method_data_offset())));\n+  la(x11, Address(x11, in_bytes(MethodData::data_offset())));\n+  add(x10, x11, x10);\n+  sd(x10, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+  bind(set_mdp);\n+  pop_reg(0xc00, sp);\n+}\n+\n+void InterpreterMacroAssembler::verify_method_data_pointer() {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+#ifdef ASSERT\n+  Label verify_continue;\n+  add(sp, sp, -4 * wordSize);\n+  sd(x10, Address(sp, 0));\n+  sd(x11, Address(sp, wordSize));\n+  sd(x12, Address(sp, 2 * wordSize));\n+  sd(x13, Address(sp, 3 * wordSize));\n+  test_method_data_pointer(x13, verify_continue); \/\/ If mdp is zero, continue\n+  get_method(x11);\n+\n+  \/\/ If the mdp is valid, it will point to a DataLayout header which is\n+  \/\/ consistent with the bcp.  The converse is highly probable also.\n+  lh(x12, Address(x13, in_bytes(DataLayout::bci_offset())));\n+  ld(t0, Address(x11, Method::const_offset()));\n+  add(x12, x12, t0);\n+  la(x12, Address(x12, ConstMethod::codes_offset()));\n+  beq(x12, xbcp, verify_continue);\n+  \/\/ x10: method\n+  \/\/ xbcp: bcp \/\/ xbcp == 22\n+  \/\/ x13: mdp\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::verify_mdp),\n+               x11, xbcp, x13);\n+  bind(verify_continue);\n+  ld(x10, Address(sp, 0));\n+  ld(x11, Address(sp, wordSize));\n+  ld(x12, Address(sp, 2 * wordSize));\n+  ld(x13, Address(sp, 3 * wordSize));\n+  add(sp, sp, 4 * wordSize);\n+#endif \/\/ ASSERT\n+}\n+\n+\n+void InterpreterMacroAssembler::set_mdp_data_at(Register mdp_in,\n+                                                int constant,\n+                                                Register value) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  Address data(mdp_in, constant);\n+  sd(value, data);\n+}\n+\n+\n+void InterpreterMacroAssembler::increment_mdp_data_at(Register mdp_in,\n+                                                      int constant,\n+                                                      bool decrement) {\n+  increment_mdp_data_at(mdp_in, noreg, constant, decrement);\n+}\n+\n+void InterpreterMacroAssembler::increment_mdp_data_at(Register mdp_in,\n+                                                      Register reg,\n+                                                      int constant,\n+                                                      bool decrement) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  \/\/ %%% this does 64bit counters at best it is wasting space\n+  \/\/ at worst it is a rare bug when counters overflow\n+\n+  assert_different_registers(t1, t0, mdp_in, reg);\n+\n+  Address addr1(mdp_in, constant);\n+  Address addr2(t1, 0);\n+  Address &addr = addr1;\n+  if (reg != noreg) {\n+    la(t1, addr1);\n+    add(t1, t1, reg);\n+    addr = addr2;\n+  }\n+\n+  if (decrement) {\n+    ld(t0, addr);\n+    addi(t0, t0, -DataLayout::counter_increment);\n+    Label L;\n+    bltz(t0, L);      \/\/ skip store if counter underflow\n+    sd(t0, addr);\n+    bind(L);\n+  } else {\n+    assert(DataLayout::counter_increment == 1,\n+           \"flow-free idiom only works with 1\");\n+    ld(t0, addr);\n+    addi(t0, t0, DataLayout::counter_increment);\n+    Label L;\n+    blez(t0, L);       \/\/ skip store if counter overflow\n+    sd(t0, addr);\n+    bind(L);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::set_mdp_flag_at(Register mdp_in,\n+                                                int flag_byte_constant) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  int flags_offset = in_bytes(DataLayout::flags_offset());\n+  \/\/ Set the flag\n+  lbu(t1, Address(mdp_in, flags_offset));\n+  ori(t1, t1, flag_byte_constant);\n+  sb(t1, Address(mdp_in, flags_offset));\n+}\n+\n+\n+void InterpreterMacroAssembler::test_mdp_data_at(Register mdp_in,\n+                                                 int offset,\n+                                                 Register value,\n+                                                 Register test_value_out,\n+                                                 Label& not_equal_continue) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  if (test_value_out == noreg) {\n+    ld(t1, Address(mdp_in, offset));\n+    bne(value, t1, not_equal_continue);\n+  } else {\n+    \/\/ Put the test value into a register, so caller can use it:\n+    ld(test_value_out, Address(mdp_in, offset));\n+    bne(value, test_value_out, not_equal_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::update_mdp_by_offset(Register mdp_in,\n+                                                     int offset_of_disp) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  ld(t1, Address(mdp_in, offset_of_disp));\n+  add(mdp_in, mdp_in, t1);\n+  sd(mdp_in, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+}\n+\n+void InterpreterMacroAssembler::update_mdp_by_offset(Register mdp_in,\n+                                                     Register reg,\n+                                                     int offset_of_disp) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  add(t1, mdp_in, reg);\n+  ld(t1, Address(t1, offset_of_disp));\n+  add(mdp_in, mdp_in, t1);\n+  sd(mdp_in, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+}\n+\n+\n+void InterpreterMacroAssembler::update_mdp_by_constant(Register mdp_in,\n+                                                       int constant) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+  addi(mdp_in, mdp_in, (unsigned)constant);\n+  sd(mdp_in, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+}\n+\n+\n+void InterpreterMacroAssembler::update_mdp_for_ret(Register return_bci) {\n+  assert(ProfileInterpreter, \"must be profiling interpreter\");\n+\n+  \/\/ save\/restore across call_VM\n+  addi(sp, sp, -2 * wordSize);\n+  sd(zr, Address(sp, 0));\n+  sd(return_bci, Address(sp, wordSize));\n+  call_VM(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::update_mdp_for_ret),\n+          return_bci);\n+  ld(zr, Address(sp, 0));\n+  ld(return_bci, Address(sp, wordSize));\n+  addi(sp, sp, 2 * wordSize);\n+}\n+\n+void InterpreterMacroAssembler::profile_taken_branch(Register mdp,\n+                                                     Register bumped_count) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    \/\/ Otherwise, assign to mdp\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ We are taking a branch.  Increment the taken count.\n+    Address data(mdp, in_bytes(JumpData::taken_offset()));\n+    ld(bumped_count, data);\n+    assert(DataLayout::counter_increment == 1,\n+            \"flow-free idiom only works with 1\");\n+    addi(bumped_count, bumped_count, DataLayout::counter_increment);\n+    Label L;\n+    \/\/ eg: bumped_count=0x7fff ffff ffff ffff  + 1 < 0. so we use <= 0;\n+    blez(bumped_count, L);       \/\/ skip store if counter overflow,\n+    sd(bumped_count, data);\n+    bind(L);\n+    \/\/ The method data pointer needs to be updated to reflect the new target.\n+    update_mdp_by_offset(mdp, in_bytes(JumpData::displacement_offset()));\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ We are taking a branch.  Increment the not taken count.\n+    increment_mdp_data_at(mdp, in_bytes(BranchData::not_taken_offset()));\n+\n+    \/\/ The method data pointer needs to be updated to correspond to\n+    \/\/ the next bytecode\n+    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_call(Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ We are making a call.  Increment the count.\n+    increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+\n+    \/\/ The method data pointer needs to be updated to reflect the new target.\n+    update_mdp_by_constant(mdp, in_bytes(CounterData::counter_data_size()));\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_final_call(Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ We are making a call.  Increment the count.\n+    increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+\n+    \/\/ The method data pointer needs to be updated to reflect the new target.\n+    update_mdp_by_constant(mdp,\n+                           in_bytes(VirtualCallData::\n+                                    virtual_call_data_size()));\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_virtual_call(Register receiver,\n+                                                     Register mdp,\n+                                                     Register reg2,\n+                                                     bool receiver_can_be_null) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label skip_receiver_profile;\n+    if (receiver_can_be_null) {\n+      Label not_null;\n+      \/\/ We are making a call.  Increment the count for null receiver.\n+      increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+      j(skip_receiver_profile);\n+      bind(not_null);\n+    }\n+\n+    \/\/ Record the receiver type.\n+    record_klass_in_profile(receiver, mdp, reg2, true);\n+    bind(skip_receiver_profile);\n+\n+    \/\/ The method data pointer needs to be updated to reflect the new target.\n+\n+    update_mdp_by_constant(mdp,\n+                           in_bytes(VirtualCallData::\n+                                    virtual_call_data_size()));\n+    bind(profile_continue);\n+  }\n+}\n+\n+\/\/ This routine creates a state machine for updating the multi-row\n+\/\/ type profile at a virtual call site (or other type-sensitive bytecode).\n+\/\/ The machine visits each row (of receiver\/count) until the receiver type\n+\/\/ is found, or until it runs out of rows.  At the same time, it remembers\n+\/\/ the location of the first empty row.  (An empty row records null for its\n+\/\/ receiver, and can be allocated for a newly-observed receiver type.)\n+\/\/ Because there are two degrees of freedom in the state, a simple linear\n+\/\/ search will not work; it must be a decision tree.  Hence this helper\n+\/\/ function is recursive, to generate the required tree structured code.\n+\/\/ It's the interpreter, so we are trading off code space for speed.\n+\/\/ See below for example code.\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(\n+                                Register receiver, Register mdp,\n+                                Register reg2,\n+                                Label& done, bool is_virtual_call) {\n+  if (TypeProfileWidth == 0) {\n+    if (is_virtual_call) {\n+      increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+    }\n+\n+  } else {\n+    int non_profiled_offset = -1;\n+    if (is_virtual_call) {\n+      non_profiled_offset = in_bytes(CounterData::count_offset());\n+    }\n+\n+    record_item_in_profile_helper(receiver, mdp, reg2, 0, done, TypeProfileWidth,\n+      &VirtualCallData::receiver_offset, &VirtualCallData::receiver_count_offset, non_profiled_offset);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::record_item_in_profile_helper(\n+  Register item, Register mdp, Register reg2, int start_row, Label& done, int total_rows,\n+  OffsetFunction item_offset_fn, OffsetFunction item_count_offset_fn, int non_profiled_offset) {\n+  int last_row = total_rows - 1;\n+  assert(start_row <= last_row, \"must be work left to do\");\n+  \/\/ Test this row for both the item and for null.\n+  \/\/ Take any of three different outcomes:\n+  \/\/   1. found item => increment count and goto done\n+  \/\/   2. found null => keep looking for case 1, maybe allocate this cell\n+  \/\/   3. found something else => keep looking for cases 1 and 2\n+  \/\/ Case 3 is handled by a recursive call.\n+  for (int row = start_row; row <= last_row; row++) {\n+    Label next_test;\n+    bool test_for_null_also = (row == start_row);\n+\n+    \/\/ See if the item is item[n].\n+    int item_offset = in_bytes(item_offset_fn(row));\n+    test_mdp_data_at(mdp, item_offset, item,\n+                     (test_for_null_also ? reg2 : noreg),\n+                     next_test);\n+    \/\/ (Reg2 now contains the item from the CallData.)\n+\n+    \/\/ The item is item[n].  Increment count[n].\n+    int count_offset = in_bytes(item_count_offset_fn(row));\n+    increment_mdp_data_at(mdp, count_offset);\n+    j(done);\n+    bind(next_test);\n+\n+    if (test_for_null_also) {\n+      Label found_null;\n+      \/\/ Failed the equality check on item[n]...  Test for null.\n+      if (start_row == last_row) {\n+        \/\/ The only thing left to do is handle the null case.\n+        if (non_profiled_offset >= 0) {\n+          beqz(reg2, found_null);\n+          \/\/ Item did not match any saved item and there is no empty row for it.\n+          \/\/ Increment total counter to indicate polymorphic case.\n+          increment_mdp_data_at(mdp, non_profiled_offset);\n+          j(done);\n+          bind(found_null);\n+        } else {\n+          bnez(reg2, done);\n+        }\n+        break;\n+      }\n+      \/\/ Since null is rare, make it be the branch-taken case.\n+      beqz(reg2, found_null);\n+\n+      \/\/ Put all the \"Case 3\" tests here.\n+      record_item_in_profile_helper(item, mdp, reg2, start_row + 1, done, total_rows,\n+        item_offset_fn, item_count_offset_fn, non_profiled_offset);\n+\n+      \/\/ Found a null.  Keep searching for a matching item,\n+      \/\/ but remember that this is an empty (unused) slot.\n+      bind(found_null);\n+    }\n+  }\n+\n+  \/\/ In the fall-through case, we found no matching item, but we\n+  \/\/ observed the item[start_row] is NULL.\n+  \/\/ Fill in the item field and increment the count.\n+  int item_offset = in_bytes(item_offset_fn(start_row));\n+  set_mdp_data_at(mdp, item_offset, item);\n+  int count_offset = in_bytes(item_count_offset_fn(start_row));\n+  mv(reg2, DataLayout::counter_increment);\n+  set_mdp_data_at(mdp, count_offset, reg2);\n+  if (start_row > 0) {\n+    j(done);\n+  }\n+}\n+\n+\/\/ Example state machine code for three profile rows:\n+\/\/   # main copy of decision tree, rooted at row[1]\n+\/\/   if (row[0].rec == rec) then [\n+\/\/     row[0].incr()\n+\/\/     goto done\n+\/\/   ]\n+\/\/   if (row[0].rec != NULL) then [\n+\/\/     # inner copy of decision tree, rooted at row[1]\n+\/\/     if (row[1].rec == rec) then [\n+\/\/       row[1].incr()\n+\/\/       goto done\n+\/\/     ]\n+\/\/     if (row[1].rec != NULL) then [\n+\/\/       # degenerate decision tree, rooted at row[2]\n+\/\/       if (row[2].rec == rec) then [\n+\/\/         row[2].incr()\n+\/\/         goto done\n+\/\/       ]\n+\/\/       if (row[2].rec != NULL) then [\n+\/\/         count.incr()\n+\/\/         goto done\n+\/\/       ] # overflow\n+\/\/       row[2].init(rec)\n+\/\/       goto done\n+\/\/     ] else [\n+\/\/       # remember row[1] is empty\n+\/\/       if (row[2].rec == rec) then [\n+\/\/         row[2].incr()\n+\/\/         goto done\n+\/\/       ]\n+\/\/       row[1].init(rec)\n+\/\/       goto done\n+\/\/     ]\n+\/\/   else [\n+\/\/     # remember row[0] is empty\n+\/\/     if (row[1].rec == rec) then [\n+\/\/       row[1].incr()\n+\/\/       goto done\n+\/\/     ]\n+\/\/     if (row[2].rec == rec) then [\n+\/\/       row[2].incr()\n+\/\/       goto done\n+\/\/     ]\n+\/\/     row[0].init(rec)\n+\/\/     goto done\n+\/\/   ]\n+\/\/   done:\n+\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n+                                                        Register mdp, Register reg2,\n+                                                        bool is_virtual_call) {\n+  assert(ProfileInterpreter, \"must be profiling\");\n+  Label done;\n+\n+  record_klass_in_profile_helper(receiver, mdp, reg2, done, is_virtual_call);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::profile_ret(Register return_bci, Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ Update the total ret count.\n+    increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+\n+    for (uint row = 0; row < RetData::row_limit(); row++) {\n+      Label next_test;\n+\n+      \/\/ See if return_bci is equal to bci[n]:\n+      test_mdp_data_at(mdp,\n+                       in_bytes(RetData::bci_offset(row)),\n+                       return_bci, noreg,\n+                       next_test);\n+\n+      \/\/ return_bci is equal to bci[n].  Increment the count.\n+      increment_mdp_data_at(mdp, in_bytes(RetData::bci_count_offset(row)));\n+\n+      \/\/ The method data pointer needs to be updated to reflect the new target.\n+      update_mdp_by_offset(mdp,\n+                           in_bytes(RetData::bci_displacement_offset(row)));\n+      j(profile_continue);\n+      bind(next_test);\n+    }\n+\n+    update_mdp_for_ret(return_bci);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_null_seen(Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+\n+    \/\/ The method data pointer needs to be updated.\n+    int mdp_delta = in_bytes(BitData::bit_data_size());\n+    if (TypeProfileCasts) {\n+      mdp_delta = in_bytes(VirtualCallData::virtual_call_data_size());\n+    }\n+    update_mdp_by_constant(mdp, mdp_delta);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_typecheck_failed(Register mdp) {\n+    if (ProfileInterpreter && TypeProfileCasts) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    int count_offset = in_bytes(CounterData::count_offset());\n+    \/\/ Back up the address, since we have already bumped the mdp.\n+    count_offset -= in_bytes(VirtualCallData::virtual_call_data_size());\n+\n+    \/\/ *Decrement* the counter.  We expect to see zero or small negatives.\n+    increment_mdp_data_at(mdp, count_offset, true);\n+\n+    bind (profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_typecheck(Register mdp, Register klass, Register reg2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    int mdp_delta = in_bytes(BitData::bit_data_size());\n+    if (TypeProfileCasts) {\n+      mdp_delta = in_bytes(VirtualCallData::virtual_call_data_size());\n+\n+      \/\/ Record the object type.\n+      record_klass_in_profile(klass, mdp, reg2, false);\n+    }\n+    update_mdp_by_constant(mdp, mdp_delta);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_switch_default(Register mdp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ Update the default case count\n+    increment_mdp_data_at(mdp,\n+                          in_bytes(MultiBranchData::default_count_offset()));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_offset(mdp,\n+                         in_bytes(MultiBranchData::\n+                                  default_displacement_offset()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_switch_case(Register index,\n+                                                    Register mdp,\n+                                                    Register reg2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ Build the base (index * per_case_size_in_bytes()) +\n+    \/\/ case_array_offset_in_bytes()\n+    mvw(reg2, in_bytes(MultiBranchData::per_case_size()));\n+    mvw(t0, in_bytes(MultiBranchData::case_array_offset()));\n+    Assembler::mul(index, index, reg2);\n+    Assembler::add(index, index, t0);\n+\n+    \/\/ Update the case count\n+    increment_mdp_data_at(mdp,\n+                          index,\n+                          in_bytes(MultiBranchData::relative_count_offset()));\n+\n+    \/\/ The method data pointer need to be updated.\n+    update_mdp_by_offset(mdp,\n+                         index,\n+                         in_bytes(MultiBranchData::\n+                                  relative_displacement_offset()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) { ; }\n+\n+void InterpreterMacroAssembler::notify_method_entry() {\n+  \/\/ Whenever JVMTI is interp_only_mode, method entry\/exit events are sent to\n+  \/\/ track stack depth.  If it is possible to enter interp_only_mode we add\n+  \/\/ the code to check if the event should be sent.\n+  if (JvmtiExport::can_post_interpreter_events()) {\n+    Label L;\n+    lwu(x13, Address(xthread, JavaThread::interp_only_mode_offset()));\n+    beqz(x13, L);\n+    call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                    InterpreterRuntime::post_method_entry));\n+    bind(L);\n+  }\n+\n+  {\n+    SkipIfEqual skip(this, &DTraceMethodProbes, false);\n+    get_method(c_rarg1);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry),\n+                 xthread, c_rarg1);\n+  }\n+\n+  \/\/ RedefineClasses() tracing support for obsolete method entry\n+  if (log_is_enabled(Trace, redefine, class, obsolete)) {\n+    get_method(c_rarg1);\n+    call_VM_leaf(\n+      CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry),\n+      xthread, c_rarg1);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::notify_method_exit(\n+    TosState state, NotifyMethodExitMode mode) {\n+  \/\/ Whenever JVMTI is interp_only_mode, method entry\/exit events are sent to\n+  \/\/ track stack depth.  If it is possible to enter interp_only_mode we add\n+  \/\/ the code to check if the event should be sent.\n+  if (mode == NotifyJVMTI && JvmtiExport::can_post_interpreter_events()) {\n+    Label L;\n+    \/\/ Note: frame::interpreter_frame_result has a dependency on how the\n+    \/\/ method result is saved across the call to post_method_exit. If this\n+    \/\/ is changed then the interpreter_frame_result implementation will\n+    \/\/ need to be updated too.\n+\n+    \/\/ template interpreter will leave the result on the top of the stack.\n+    push(state);\n+    lwu(x13, Address(xthread, JavaThread::interp_only_mode_offset()));\n+    beqz(x13, L);\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_exit));\n+    bind(L);\n+    pop(state);\n+  }\n+\n+  {\n+    SkipIfEqual skip(this, &DTraceMethodProbes, false);\n+    push(state);\n+    get_method(c_rarg1);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),\n+                 xthread, c_rarg1);\n+    pop(state);\n+  }\n+}\n+\n+\n+\/\/ Jump if ((*counter_addr += increment) & mask) satisfies the condition.\n+void InterpreterMacroAssembler::increment_mask_and_jump(Address counter_addr,\n+                                                        int increment, Address mask,\n+                                                        Register tmp1, Register tmp2,\n+                                                        bool preloaded, Label* where) {\n+  Label done;\n+  if (!preloaded) {\n+    lwu(tmp1, counter_addr);\n+  }\n+  add(tmp1, tmp1, increment);\n+  sw(tmp1, counter_addr);\n+  lwu(tmp2, mask);\n+  andr(tmp1, tmp1, tmp2);\n+  bnez(tmp1, done);\n+  j(*where); \/\/ offset is too large so we have to use j instead of beqz here\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::call_VM_leaf_base(address entry_point,\n+                                                  int number_of_arguments) {\n+  \/\/ interpreter specific\n+  \/\/\n+  \/\/ Note: No need to save\/restore rbcp & rlocals pointer since these\n+  \/\/       are callee saved registers and no blocking\/ GC can happen\n+  \/\/       in leaf calls.\n+#ifdef ASSERT\n+  {\n+   Label L;\n+   ld(t0, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+   beqz(t0, L);\n+   stop(\"InterpreterMacroAssembler::call_VM_leaf_base:\"\n+        \" last_sp != NULL\");\n+   bind(L);\n+  }\n+#endif \/* ASSERT *\/\n+  \/\/ super call\n+  MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);\n+}\n+\n+void InterpreterMacroAssembler::call_VM_base(Register oop_result,\n+                                             Register java_thread,\n+                                             Register last_java_sp,\n+                                             address  entry_point,\n+                                             int      number_of_arguments,\n+                                             bool     check_exceptions) {\n+  \/\/ interpreter specific\n+  \/\/\n+  \/\/ Note: Could avoid restoring locals ptr (callee saved) - however doesn't\n+  \/\/       really make a difference for these runtime calls, since they are\n+  \/\/       slow anyway. Btw., bcp must be saved\/restored since it may change\n+  \/\/       due to GC.\n+  save_bcp();\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    ld(t0, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+    beqz(t0, L);\n+    stop(\"InterpreterMacroAssembler::call_VM_base:\"\n+         \" last_sp != NULL\");\n+    bind(L);\n+  }\n+#endif \/* ASSERT *\/\n+  \/\/ super call\n+  MacroAssembler::call_VM_base(oop_result, noreg, last_java_sp,\n+                               entry_point, number_of_arguments,\n+                               check_exceptions);\n+\/\/ interpreter specific\n+  restore_bcp();\n+  restore_locals();\n+}\n+\n+void InterpreterMacroAssembler::profile_obj_type(Register obj, const Address& mdo_addr, Register tmp) {\n+  assert_different_registers(obj, tmp, t0, mdo_addr.base());\n+  Label update, next, none;\n+\n+  verify_oop(obj);\n+\n+  bnez(obj, update);\n+  orptr(mdo_addr, TypeEntries::null_seen, t0, tmp);\n+  j(next);\n+\n+  bind(update);\n+  load_klass(obj, obj);\n+\n+  ld(t0, mdo_addr);\n+  xorr(obj, obj, t0);\n+  andi(t0, obj, TypeEntries::type_klass_mask);\n+  beqz(t0, next); \/\/ klass seen before, nothing to\n+                  \/\/ do. The unknown bit may have been\n+                  \/\/ set already but no need to check.\n+\n+  andi(t0, obj, TypeEntries::type_unknown);\n+  bnez(t0, next);\n+  \/\/ already unknown. Nothing to do anymore.\n+\n+  ld(t0, mdo_addr);\n+  beqz(t0, none);\n+  li(tmp, (u1)TypeEntries::null_seen);\n+  beq(t0, tmp, none);\n+  \/\/ There is a chance that the checks above (re-reading profiling\n+  \/\/ data from memory) fail if another thread has just set the\n+  \/\/ profiling to this obj's klass\n+  ld(t0, mdo_addr);\n+  xorr(obj, obj, t0);\n+  andi(t0, obj, TypeEntries::type_klass_mask);\n+  beqz(t0, next);\n+\n+  \/\/ different than before. Cannot keep accurate profile.\n+  orptr(mdo_addr, TypeEntries::type_unknown, t0, tmp);\n+  j(next);\n+\n+  bind(none);\n+  \/\/ first time here. Set profile type.\n+  sd(obj, mdo_addr);\n+\n+  bind(next);\n+}\n+\n+void InterpreterMacroAssembler::profile_arguments_type(Register mdp, Register callee, Register tmp, bool is_virtual) {\n+  if (!ProfileInterpreter) {\n+    return;\n+  }\n+\n+  if (MethodData::profile_arguments() || MethodData::profile_return()) {\n+    Label profile_continue;\n+\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    int off_to_start = is_virtual ? in_bytes(VirtualCallData::virtual_call_data_size()) : in_bytes(CounterData::counter_data_size());\n+\n+    lbu(t0, Address(mdp, in_bytes(DataLayout::tag_offset()) - off_to_start));\n+    if (is_virtual) {\n+      li(tmp, (u1)DataLayout::virtual_call_type_data_tag);\n+      bne(t0, tmp, profile_continue);\n+    } else {\n+      li(tmp, (u1)DataLayout::call_type_data_tag);\n+      bne(t0, tmp, profile_continue);\n+    }\n+\n+    \/\/ calculate slot step\n+    static int stack_slot_offset0 = in_bytes(TypeEntriesAtCall::stack_slot_offset(0));\n+    static int slot_step = in_bytes(TypeEntriesAtCall::stack_slot_offset(1)) - stack_slot_offset0;\n+\n+    \/\/ calculate type step\n+    static int argument_type_offset0 = in_bytes(TypeEntriesAtCall::argument_type_offset(0));\n+    static int type_step = in_bytes(TypeEntriesAtCall::argument_type_offset(1)) - argument_type_offset0;\n+\n+    if (MethodData::profile_arguments()) {\n+      Label done, loop, loopEnd, profileArgument, profileReturnType;\n+      RegSet pushed_registers;\n+      pushed_registers += x15;\n+      pushed_registers += x16;\n+      pushed_registers += x17;\n+      Register mdo_addr = x15;\n+      Register index = x16;\n+      Register off_to_args = x17;\n+      push_reg(pushed_registers, sp);\n+\n+      mv(off_to_args, in_bytes(TypeEntriesAtCall::args_data_offset()));\n+      mv(t0, TypeProfileArgsLimit);\n+      beqz(t0, loopEnd);\n+\n+      mv(index, zr); \/\/ index < TypeProfileArgsLimit\n+      bind(loop);\n+      bgtz(index, profileReturnType);\n+      li(t0, (int)MethodData::profile_return());\n+      beqz(t0, profileArgument); \/\/ (index > 0 || MethodData::profile_return()) == false\n+      bind(profileReturnType);\n+      \/\/ If return value type is profiled we may have no argument to profile\n+      ld(tmp, Address(mdp, in_bytes(TypeEntriesAtCall::cell_count_offset())));\n+      mv(t1, - TypeStackSlotEntries::per_arg_count());\n+      mul(t1, index, t1);\n+      add(tmp, tmp, t1);\n+      li(t1, TypeStackSlotEntries::per_arg_count());\n+      add(t0, mdp, off_to_args);\n+      blt(tmp, t1, done);\n+\n+      bind(profileArgument);\n+\n+      ld(tmp, Address(callee, Method::const_offset()));\n+      load_unsigned_short(tmp, Address(tmp, ConstMethod::size_of_parameters_offset()));\n+      \/\/ stack offset o (zero based) from the start of the argument\n+      \/\/ list, for n arguments translates into offset n - o - 1 from\n+      \/\/ the end of the argument list\n+      li(t0, stack_slot_offset0);\n+      li(t1, slot_step);\n+      mul(t1, index, t1);\n+      add(t0, t0, t1);\n+      add(t0, mdp, t0);\n+      ld(t0, Address(t0));\n+      sub(tmp, tmp, t0);\n+      addi(tmp, tmp, -1);\n+      Address arg_addr = argument_address(tmp);\n+      ld(tmp, arg_addr);\n+\n+      li(t0, argument_type_offset0);\n+      li(t1, type_step);\n+      mul(t1, index, t1);\n+      add(t0, t0, t1);\n+      add(mdo_addr, mdp, t0);\n+      Address mdo_arg_addr(mdo_addr, 0);\n+      profile_obj_type(tmp, mdo_arg_addr, t1);\n+\n+      int to_add = in_bytes(TypeStackSlotEntries::per_arg_size());\n+      addi(off_to_args, off_to_args, to_add);\n+\n+      \/\/ increment index by 1\n+      addi(index, index, 1);\n+      li(t1, TypeProfileArgsLimit);\n+      blt(index, t1, loop);\n+      bind(loopEnd);\n+\n+      if (MethodData::profile_return()) {\n+        ld(tmp, Address(mdp, in_bytes(TypeEntriesAtCall::cell_count_offset())));\n+        addi(tmp, tmp, -TypeProfileArgsLimit*TypeStackSlotEntries::per_arg_count());\n+      }\n+\n+      add(t0, mdp, off_to_args);\n+      bind(done);\n+      mv(mdp, t0);\n+\n+      \/\/ unspill the clobbered registers\n+      pop_reg(pushed_registers, sp);\n+\n+      if (MethodData::profile_return()) {\n+        \/\/ We're right after the type profile for the last\n+        \/\/ argument. tmp is the number of cells left in the\n+        \/\/ CallTypeData\/VirtualCallTypeData to reach its end. Non null\n+        \/\/ if there's a return to profile.\n+        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        shadd(mdp, tmp, mdp, tmp, exact_log2(DataLayout::cell_size));\n+      }\n+      sd(mdp, Address(fp, frame::interpreter_frame_mdp_offset * wordSize));\n+    } else {\n+      assert(MethodData::profile_return(), \"either profile call args or call ret\");\n+      update_mdp_by_constant(mdp, in_bytes(TypeEntriesAtCall::return_only_size()));\n+    }\n+\n+    \/\/ mdp points right after the end of the\n+    \/\/ CallTypeData\/VirtualCallTypeData, right after the cells for the\n+    \/\/ return value type if there's one\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_return_type(Register mdp, Register ret, Register tmp) {\n+  assert_different_registers(mdp, ret, tmp, xbcp, t0, t1);\n+  if (ProfileInterpreter && MethodData::profile_return()) {\n+    Label profile_continue, done;\n+\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    if (MethodData::profile_return_jsr292_only()) {\n+      assert(Method::intrinsic_id_size_in_bytes() == 2, \"assuming Method::_intrinsic_id is u2\");\n+\n+      \/\/ If we don't profile all invoke bytecodes we must make sure\n+      \/\/ it's a bytecode we indeed profile. We can't go back to the\n+      \/\/ begining of the ProfileData we intend to update to check its\n+      \/\/ type because we're right after it and we don't known its\n+      \/\/ length\n+      Label do_profile;\n+      lbu(t0, Address(xbcp, 0));\n+      li(tmp, (u1)Bytecodes::_invokedynamic);\n+      beq(t0, tmp, do_profile);\n+      li(tmp, (u1)Bytecodes::_invokehandle);\n+      beq(t0, tmp, do_profile);\n+      get_method(tmp);\n+      lhu(t0, Address(tmp, Method::intrinsic_id_offset_in_bytes()));\n+      li(t1, vmIntrinsics::_compiledLambdaForm);\n+      bne(t0, t1, profile_continue);\n+      bind(do_profile);\n+    }\n+\n+    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    mv(tmp, ret);\n+    profile_obj_type(tmp, mdo_ret_addr, t1);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_parameters_type(Register mdp, Register tmp1, Register tmp2, Register tmp3) {\n+  assert_different_registers(t0, t1, mdp, tmp1, tmp2, tmp3);\n+  if (ProfileInterpreter && MethodData::profile_parameters()) {\n+    Label profile_continue, done;\n+\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    \/\/ Load the offset of the area within the MDO used for\n+    \/\/ parameters. If it's negative we're not profiling any parameters\n+    lwu(tmp1, Address(mdp, in_bytes(MethodData::parameters_type_data_di_offset()) - in_bytes(MethodData::data_offset())));\n+    srli(tmp2, tmp1, 31);\n+    bnez(tmp2, profile_continue);  \/\/ i.e. sign bit set\n+\n+    \/\/ Compute a pointer to the area for parameters from the offset\n+    \/\/ and move the pointer to the slot for the last\n+    \/\/ parameters. Collect profiling from last parameter down.\n+    \/\/ mdo start + parameters offset + array length - 1\n+    add(mdp, mdp, tmp1);\n+    ld(tmp1, Address(mdp, ArrayData::array_len_offset()));\n+    add(tmp1, tmp1, - TypeStackSlotEntries::per_arg_count());\n+\n+    Label loop;\n+    bind(loop);\n+\n+    int off_base = in_bytes(ParametersTypeData::stack_slot_offset(0));\n+    int type_base = in_bytes(ParametersTypeData::type_offset(0));\n+    int per_arg_scale = exact_log2(DataLayout::cell_size);\n+    add(t0, mdp, off_base);\n+    add(t1, mdp, type_base);\n+\n+    shadd(tmp2, tmp1, t0, tmp2, per_arg_scale);\n+    \/\/ load offset on the stack from the slot for this parameter\n+    ld(tmp2, Address(tmp2, 0));\n+    neg(tmp2, tmp2);\n+\n+    \/\/ read the parameter from the local area\n+    shadd(tmp2, tmp2, xlocals, tmp2, Interpreter::logStackElementSize);\n+    ld(tmp2, Address(tmp2, 0));\n+\n+    \/\/ profile the parameter\n+    shadd(t1, tmp1, t1, t0, per_arg_scale);\n+    Address arg_type(t1, 0);\n+    profile_obj_type(tmp2, arg_type, tmp3);\n+\n+    \/\/ go to next parameter\n+    add(tmp1, tmp1, - TypeStackSlotEntries::per_arg_count());\n+    bgez(tmp1, loop);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::get_method_counters(Register method,\n+                                                    Register mcs, Label& skip) {\n+  Label has_counters;\n+  ld(mcs, Address(method, Method::method_counters_offset()));\n+  bnez(mcs, has_counters);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address,\n+          InterpreterRuntime::build_method_counters), method);\n+  ld(mcs, Address(method, Method::method_counters_offset()));\n+  beqz(mcs, skip); \/\/ No MethodCounters allocated, OutOfMemory\n+  bind(has_counters);\n+}\n+\n+#ifdef ASSERT\n+void InterpreterMacroAssembler::verify_access_flags(Register access_flags, uint32_t flag_bits,\n+                                                    const char* msg, bool stop_by_hit) {\n+  Label L;\n+  andi(t0, access_flags, flag_bits);\n+  if (stop_by_hit) {\n+    beqz(t0, L);\n+  } else {\n+    bnez(t0, L);\n+  }\n+  stop(msg);\n+  bind(L);\n+}\n+\n+void InterpreterMacroAssembler::verify_frame_setup() {\n+  Label L;\n+  const Address monitor_block_top(fp, frame::interpreter_frame_monitor_block_top_offset * wordSize);\n+  ld(t0, monitor_block_top);\n+  beq(esp, t0, L);\n+  stop(\"broken stack frame setup in interpreter\");\n+  bind(L);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":1931,"deletions":0,"binary":false,"changes":1931,"status":"added"},{"patch":"@@ -0,0 +1,283 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_INTERP_MASM_RISCV_HPP\n+#define CPU_RISCV_INTERP_MASM_RISCV_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"interpreter\/invocationCounter.hpp\"\n+#include \"runtime\/frame.hpp\"\n+\n+\/\/ This file specializes the assember with interpreter-specific macros\n+\n+typedef ByteSize (*OffsetFunction)(uint);\n+\n+class InterpreterMacroAssembler: public MacroAssembler {\n+ protected:\n+  \/\/ Interpreter specific version of call_VM_base\n+  using MacroAssembler::call_VM_leaf_base;\n+\n+  virtual void call_VM_leaf_base(address entry_point,\n+                                 int number_of_arguments);\n+\n+  virtual void call_VM_base(Register oop_result,\n+                            Register java_thread,\n+                            Register last_java_sp,\n+                            address  entry_point,\n+                            int number_of_arguments,\n+                            bool check_exceptions);\n+\n+  \/\/ base routine for all dispatches\n+  void dispatch_base(TosState state, address* table, bool verifyoop = true,\n+                     bool generate_poll = false, Register Rs = t0);\n+\n+ public:\n+  InterpreterMacroAssembler(CodeBuffer* code) : MacroAssembler(code) {}\n+  virtual ~InterpreterMacroAssembler() {}\n+\n+  void load_earlyret_value(TosState state);\n+\n+  void jump_to_entry(address entry);\n+\n+  virtual void check_and_handle_popframe(Register java_thread);\n+  virtual void check_and_handle_earlyret(Register java_thread);\n+\n+  \/\/ Interpreter-specific registers\n+  void save_bcp() {\n+    sd(xbcp, Address(fp, frame::interpreter_frame_bcp_offset * wordSize));\n+  }\n+\n+  void restore_bcp() {\n+    ld(xbcp, Address(fp, frame::interpreter_frame_bcp_offset * wordSize));\n+  }\n+\n+  void restore_locals() {\n+    ld(xlocals, Address(fp, frame::interpreter_frame_locals_offset * wordSize));\n+  }\n+\n+  void restore_constant_pool_cache() {\n+    ld(xcpool, Address(fp, frame::interpreter_frame_cache_offset * wordSize));\n+  }\n+\n+  void get_dispatch();\n+\n+  \/\/ Helpers for runtime call arguments\/results\n+  void get_method(Register reg) {\n+    ld(reg, Address(fp, frame::interpreter_frame_method_offset * wordSize));\n+  }\n+\n+  void get_const(Register reg) {\n+    get_method(reg);\n+    ld(reg, Address(reg, in_bytes(Method::const_offset())));\n+  }\n+\n+  void get_constant_pool(Register reg) {\n+    get_const(reg);\n+    ld(reg, Address(reg, in_bytes(ConstMethod::constants_offset())));\n+  }\n+\n+  void get_constant_pool_cache(Register reg) {\n+    get_constant_pool(reg);\n+    ld(reg, Address(reg, ConstantPool::cache_offset_in_bytes()));\n+  }\n+\n+  void get_cpool_and_tags(Register cpool, Register tags) {\n+    get_constant_pool(cpool);\n+    ld(tags, Address(cpool, ConstantPool::tags_offset_in_bytes()));\n+  }\n+\n+  void get_unsigned_2_byte_index_at_bcp(Register reg, int bcp_offset);\n+  void get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size = sizeof(u2));\n+  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n+  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n+  void get_cache_index_at_bcp(Register index, int bcp_offset, size_t index_size = sizeof(u2));\n+  void get_method_counters(Register method, Register mcs, Label& skip);\n+\n+  \/\/ Load cpool->resolved_references(index).\n+  void load_resolved_reference_at_index(Register result, Register index, Register tmp = x15);\n+\n+  \/\/ Load cpool->resolved_klass_at(index).\n+  void load_resolved_klass_at_offset(Register cpool, Register index, Register klass, Register temp);\n+\n+  void pop_ptr(Register r = x10);\n+  void pop_i(Register r = x10);\n+  void pop_l(Register r = x10);\n+  void pop_f(FloatRegister r = f10);\n+  void pop_d(FloatRegister r = f10);\n+  void push_ptr(Register r = x10);\n+  void push_i(Register r = x10);\n+  void push_l(Register r = x10);\n+  void push_f(FloatRegister r = f10);\n+  void push_d(FloatRegister r = f10);\n+\n+  void pop(TosState state); \/\/ transition vtos -> state\n+  void push(TosState state); \/\/ transition state -> vtos\n+\n+  void empty_expression_stack() {\n+    ld(esp, Address(fp, frame::interpreter_frame_monitor_block_top_offset * wordSize));\n+    \/\/ NULL last_sp until next java call\n+    sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  }\n+\n+  \/\/ Helpers for swap and dup\n+  void load_ptr(int n, Register val);\n+  void store_ptr(int n, Register val);\n+\n+  \/\/ Load float value from 'address'. The value is loaded onto the FPU register v0.\n+  void load_float(Address src);\n+  void load_double(Address src);\n+\n+  \/\/ Generate a subtype check: branch to ok_is_subtype if sub_klass is\n+  \/\/ a subtype of super_klass.\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+\n+  \/\/ Dispatching\n+  void dispatch_prolog(TosState state, int step = 0);\n+  void dispatch_epilog(TosState state, int step = 0);\n+  \/\/ dispatch via t0\n+  void dispatch_only(TosState state, bool generate_poll = false, Register Rs = t0);\n+  \/\/ dispatch normal table via t0 (assume t0 is loaded already)\n+  void dispatch_only_normal(TosState state, Register Rs = t0);\n+  void dispatch_only_noverify(TosState state, Register Rs = t0);\n+  \/\/ load t0 from [xbcp + step] and dispatch via t0\n+  void dispatch_next(TosState state, int step = 0, bool generate_poll = false);\n+  \/\/ load t0 from [xbcp] and dispatch via t0 and table\n+  void dispatch_via (TosState state, address* table);\n+\n+  \/\/ jump to an invoked target\n+  void prepare_to_jump_from_interpreted();\n+  void jump_from_interpreted(Register method);\n+\n+\n+  \/\/ Returning from interpreted functions\n+  \/\/\n+  \/\/ Removes the current activation (incl. unlocking of monitors)\n+  \/\/ and sets up the return address.  This code is also used for\n+  \/\/ exception unwindwing. In that case, we do not want to throw\n+  \/\/ IllegalMonitorStateExceptions, since that might get us into an\n+  \/\/ infinite rethrow exception loop.\n+  \/\/ Additionally this code is used for popFrame and earlyReturn.\n+  \/\/ In popFrame case we want to skip throwing an exception,\n+  \/\/ installing an exception, and notifying jvmdi.\n+  \/\/ In earlyReturn case we only want to skip throwing an exception\n+  \/\/ and installing an exception.\n+  void remove_activation(TosState state,\n+                         bool throw_monitor_exception = true,\n+                         bool install_monitor_exception = true,\n+                         bool notify_jvmdi = true);\n+\n+  \/\/ FIXME: Give us a valid frame at a null check.\n+  virtual void null_check(Register reg, int offset = -1) {\n+        MacroAssembler::null_check(reg, offset);\n+  }\n+\n+  \/\/ Object locking\n+  void lock_object  (Register lock_reg);\n+  void unlock_object(Register lock_reg);\n+\n+  \/\/ Interpreter profiling operations\n+  void set_method_data_pointer_for_bcp();\n+  void test_method_data_pointer(Register mdp, Label& zero_continue);\n+  void verify_method_data_pointer();\n+\n+  void set_mdp_data_at(Register mdp_in, int constant, Register value);\n+  void increment_mdp_data_at(Address data, bool decrement = false);\n+  void increment_mdp_data_at(Register mdp_in, int constant,\n+                             bool decrement = false);\n+  void increment_mdp_data_at(Register mdp_in, Register reg, int constant,\n+                             bool decrement = false);\n+  void increment_mask_and_jump(Address counter_addr,\n+                               int increment, Address mask,\n+                               Register tmp1, Register tmp2,\n+                               bool preloaded, Label* where);\n+\n+  void set_mdp_flag_at(Register mdp_in, int flag_constant);\n+  void test_mdp_data_at(Register mdp_in, int offset, Register value,\n+                        Register test_value_out,\n+                        Label& not_equal_continue);\n+\n+  void record_klass_in_profile(Register receiver, Register mdp,\n+                               Register reg2, bool is_virtual_call);\n+  void record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                      Register reg2,\n+                                      Label& done, bool is_virtual_call);\n+  void record_item_in_profile_helper(Register item, Register mdp,\n+                                     Register reg2, int start_row, Label& done, int total_rows,\n+                                     OffsetFunction item_offset_fn, OffsetFunction item_count_offset_fn,\n+                                     int non_profiled_offset);\n+\n+  void update_mdp_by_offset(Register mdp_in, int offset_of_offset);\n+  void update_mdp_by_offset(Register mdp_in, Register reg, int offset_of_disp);\n+  void update_mdp_by_constant(Register mdp_in, int constant);\n+  void update_mdp_for_ret(Register return_bci);\n+\n+  \/\/ narrow int return value\n+  void narrow(Register result);\n+\n+  void profile_taken_branch(Register mdp, Register bumped_count);\n+  void profile_not_taken_branch(Register mdp);\n+  void profile_call(Register mdp);\n+  void profile_final_call(Register mdp);\n+  void profile_virtual_call(Register receiver, Register mdp,\n+                            Register t1,\n+                            bool receiver_can_be_null = false);\n+  void profile_ret(Register return_bci, Register mdp);\n+  void profile_null_seen(Register mdp);\n+  void profile_typecheck(Register mdp, Register klass, Register temp);\n+  void profile_typecheck_failed(Register mdp);\n+  void profile_switch_default(Register mdp);\n+  void profile_switch_case(Register index_in_scratch, Register mdp,\n+                           Register temp);\n+\n+  void profile_obj_type(Register obj, const Address& mdo_addr, Register tmp);\n+  void profile_arguments_type(Register mdp, Register callee, Register tmp, bool is_virtual);\n+  void profile_return_type(Register mdp, Register ret, Register tmp);\n+  void profile_parameters_type(Register mdp, Register tmp1, Register tmp2, Register tmp3);\n+\n+  \/\/ Debugging\n+  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n+  void verify_FPU(int stack_depth, TosState state = ftos);\n+\n+  typedef enum { NotifyJVMTI, SkipNotifyJVMTI } NotifyMethodExitMode;\n+\n+  \/\/ support for jvmti\/dtrace\n+  void notify_method_entry();\n+  void notify_method_exit(TosState state, NotifyMethodExitMode mode);\n+\n+  virtual void _call_Unimplemented(address call_site) {\n+    save_bcp();\n+    set_last_Java_frame(esp, fp, (address) pc(), t0);\n+    MacroAssembler::_call_Unimplemented(call_site);\n+  }\n+\n+#ifdef ASSERT\n+  void verify_access_flags(Register access_flags, uint32_t flag_bits,\n+                           const char* msg, bool stop_by_hit = true);\n+  void verify_frame_setup();\n+#endif\n+};\n+\n+#endif \/\/ CPU_RISCV_INTERP_MASM_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.hpp","additions":283,"deletions":0,"binary":false,"changes":283,"status":"added"},{"patch":"@@ -0,0 +1,295 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interpreterRuntime.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/icache.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/signature.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ Implementation of SignatureHandlerGenerator\n+Register InterpreterRuntime::SignatureHandlerGenerator::from() { return xlocals; }\n+Register InterpreterRuntime::SignatureHandlerGenerator::to()   { return sp; }\n+Register InterpreterRuntime::SignatureHandlerGenerator::temp() { return t0; }\n+\n+Register InterpreterRuntime::SignatureHandlerGenerator::next_gpr() {\n+  if (_num_reg_int_args < Argument::n_int_register_parameters_c - 1) {\n+    return g_INTArgReg[++_num_reg_int_args];\n+  }\n+  return noreg;\n+}\n+\n+FloatRegister InterpreterRuntime::SignatureHandlerGenerator::next_fpr() {\n+  if (_num_reg_fp_args < Argument::n_float_register_parameters_c) {\n+    return g_FPArgReg[_num_reg_fp_args++];\n+  } else {\n+    return fnoreg;\n+  }\n+}\n+\n+int InterpreterRuntime::SignatureHandlerGenerator::next_stack_offset() {\n+  int ret = _stack_offset;\n+  _stack_offset += wordSize;\n+  return ret;\n+}\n+\n+InterpreterRuntime::SignatureHandlerGenerator::SignatureHandlerGenerator(\n+  const methodHandle& method, CodeBuffer* buffer) : NativeSignatureIterator(method) {\n+  _masm = new MacroAssembler(buffer); \/\/ allocate on resourse area by default\n+  _num_reg_int_args = (method->is_static() ? 1 : 0);\n+  _num_reg_fp_args = 0;\n+  _stack_offset = 0;\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_int() {\n+  const Address src(from(), Interpreter::local_offset_in_bytes(offset()));\n+\n+  Register reg = next_gpr();\n+  if (reg != noreg) {\n+    __ lw(reg, src);\n+  } else {\n+    __ lw(x10, src);\n+    __ sw(x10, Address(to(), next_stack_offset()));\n+  }\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_long() {\n+  const Address src(from(), Interpreter::local_offset_in_bytes(offset() + 1));\n+\n+  Register reg = next_gpr();\n+  if (reg != noreg) {\n+    __ ld(reg, src);\n+  } else  {\n+    __ ld(x10, src);\n+    __ sd(x10, Address(to(), next_stack_offset()));\n+  }\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_float() {\n+  const Address src(from(), Interpreter::local_offset_in_bytes(offset()));\n+\n+  FloatRegister reg = next_fpr();\n+  if (reg != fnoreg) {\n+    __ flw(reg, src);\n+  } else {\n+    \/\/ a floating-point argument is passed according to the integer calling\n+    \/\/ convention if no floating-point argument register available\n+    pass_int();\n+  }\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_double() {\n+  const Address src(from(), Interpreter::local_offset_in_bytes(offset() + 1));\n+\n+  FloatRegister reg = next_fpr();\n+  if (reg != fnoreg) {\n+    __ fld(reg, src);\n+  } else {\n+    \/\/ a floating-point argument is passed according to the integer calling\n+    \/\/ convention if no floating-point argument register available\n+    pass_long();\n+  }\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_object() {\n+  Register reg = next_gpr();\n+  if (reg == c_rarg1) {\n+    assert(offset() == 0, \"argument register 1 can only be (non-null) receiver\");\n+    __ addi(c_rarg1, from(), Interpreter::local_offset_in_bytes(offset()));\n+  } else if (reg != noreg) {\n+      \/\/ c_rarg2-c_rarg7\n+      __ addi(x10, from(), Interpreter::local_offset_in_bytes(offset()));\n+      __ mv(reg, zr); \/\/_num_reg_int_args:c_rarg -> 1:c_rarg2,  2:c_rarg3...\n+      __ ld(temp(), x10);\n+      Label L;\n+      __ beqz(temp(), L);\n+      __ mv(reg, x10);\n+      __ bind(L);\n+  } else {\n+    \/\/to stack\n+    __ addi(x10, from(), Interpreter::local_offset_in_bytes(offset()));\n+    __ ld(temp(), x10);\n+    Label L;\n+    __ bnez(temp(), L);\n+    __ mv(x10, zr);\n+    __ bind(L);\n+    assert(sizeof(jobject) == wordSize, \"\");\n+    __ sd(x10, Address(to(), next_stack_offset()));\n+  }\n+}\n+\n+void InterpreterRuntime::SignatureHandlerGenerator::generate(uint64_t fingerprint) {\n+  \/\/ generate code to handle arguments\n+  iterate(fingerprint);\n+\n+  \/\/ return result handler\n+  __ la(x10, ExternalAddress(Interpreter::result_handler(method()->result_type())));\n+  __ ret();\n+\n+  __ flush();\n+}\n+\n+\n+\/\/ Implementation of SignatureHandlerLibrary\n+\n+void SignatureHandlerLibrary::pd_set_handler(address handler) {}\n+\n+\n+class SlowSignatureHandler\n+  : public NativeSignatureIterator {\n+ private:\n+  address   _from;\n+  intptr_t* _to;\n+  intptr_t* _int_args;\n+  intptr_t* _fp_args;\n+  intptr_t* _fp_identifiers;\n+  unsigned int _num_reg_int_args;\n+  unsigned int _num_reg_fp_args;\n+\n+  intptr_t* single_slot_addr() {\n+    intptr_t* from_addr = (intptr_t*)(_from + Interpreter::local_offset_in_bytes(0));\n+    _from -= Interpreter::stackElementSize;\n+    return from_addr;\n+  }\n+\n+  intptr_t* double_slot_addr() {\n+    intptr_t* from_addr = (intptr_t*)(_from + Interpreter::local_offset_in_bytes(1));\n+    _from -= 2 * Interpreter::stackElementSize;\n+    return from_addr;\n+  }\n+\n+  int pass_gpr(intptr_t value) {\n+    if (_num_reg_int_args < Argument::n_int_register_parameters_c - 1) {\n+      *_int_args++ = value;\n+      return _num_reg_int_args++;\n+    }\n+    return -1;\n+  }\n+\n+  int pass_fpr(intptr_t value) {\n+    if (_num_reg_fp_args < Argument::n_float_register_parameters_c) {\n+      *_fp_args++ = value;\n+      return _num_reg_fp_args++;\n+    }\n+    return -1;\n+  }\n+\n+  void pass_stack(intptr_t value) {\n+    *_to++ = value;\n+  }\n+\n+  virtual void pass_int() {\n+    jint value = *(jint*)single_slot_addr();\n+    if (pass_gpr(value) < 0) {\n+      pass_stack(value);\n+    }\n+  }\n+\n+  virtual void pass_long() {\n+    intptr_t value = *double_slot_addr();\n+    if (pass_gpr(value) < 0) {\n+      pass_stack(value);\n+    }\n+  }\n+\n+  virtual void pass_object() {\n+    intptr_t* addr = single_slot_addr();\n+    intptr_t value = *addr == 0 ? NULL : (intptr_t)addr;\n+    if (pass_gpr(value) < 0) {\n+      pass_stack(value);\n+    }\n+  }\n+\n+  virtual void pass_float() {\n+    jint value = *(jint*) single_slot_addr();\n+    \/\/ a floating-point argument is passed according to the integer calling\n+    \/\/ convention if no floating-point argument register available\n+    if (pass_fpr(value) < 0 && pass_gpr(value) < 0) {\n+      pass_stack(value);\n+    }\n+  }\n+\n+  virtual void pass_double() {\n+    intptr_t value = *double_slot_addr();\n+    int arg = pass_fpr(value);\n+    if (0 <= arg) {\n+      *_fp_identifiers |= (1ull << arg); \/\/ mark as double\n+    } else if (pass_gpr(value) < 0) { \/\/ no need to mark if passing by integer registers or stack\n+      pass_stack(value);\n+    }\n+  }\n+\n+ public:\n+  SlowSignatureHandler(const methodHandle& method, address from, intptr_t* to)\n+    : NativeSignatureIterator(method)\n+  {\n+    _from = from;\n+    _to   = to;\n+\n+    _int_args = to - (method->is_static() ? 16 : 17);\n+    _fp_args  = to - 8;\n+    _fp_identifiers = to - 9;\n+    *(int*) _fp_identifiers = 0;\n+    _num_reg_int_args = (method->is_static() ? 1 : 0);\n+    _num_reg_fp_args = 0;\n+  }\n+\n+  ~SlowSignatureHandler()\n+  {\n+    _from           = NULL;\n+    _to             = NULL;\n+    _int_args       = NULL;\n+    _fp_args        = NULL;\n+    _fp_identifiers = NULL;\n+  }\n+};\n+\n+\n+IRT_ENTRY(address,\n+          InterpreterRuntime::slow_signature_handler(JavaThread* thread,\n+                                                     Method* method,\n+                                                     intptr_t* from,\n+                                                     intptr_t* to))\n+  methodHandle m(thread, (Method*)method);\n+  assert(m->is_native(), \"sanity check\");\n+\n+  \/\/ handle arguments\n+  SlowSignatureHandler ssh(m, (address)from, to);\n+  ssh.iterate(UCONST64(-1));\n+\n+  \/\/ return result handler\n+  return Interpreter::result_handler(m->result_type());\n+IRT_END\n","filename":"src\/hotspot\/cpu\/riscv\/interpreterRT_riscv.cpp","additions":295,"deletions":0,"binary":false,"changes":295,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_INTERPRETERRT_RISCV_HPP\n+#define CPU_RISCV_INTERPRETERRT_RISCV_HPP\n+\n+\/\/ This is included in the middle of class Interpreter.\n+\/\/ Do not include files here.\n+\n+\/\/ native method calls\n+\n+class SignatureHandlerGenerator: public NativeSignatureIterator {\n+ private:\n+  MacroAssembler* _masm;\n+  unsigned int _num_reg_fp_args;\n+  unsigned int _num_reg_int_args;\n+  int _stack_offset;\n+\n+  void pass_int();\n+  void pass_long();\n+  void pass_float();\n+  void pass_double();\n+  void pass_object();\n+\n+  Register next_gpr();\n+  FloatRegister next_fpr();\n+  int next_stack_offset();\n+\n+ public:\n+  \/\/ Creation\n+  SignatureHandlerGenerator(const methodHandle& method, CodeBuffer* buffer);\n+  virtual ~SignatureHandlerGenerator() {\n+    _masm = NULL;\n+  }\n+\n+  \/\/ Code generation\n+  void generate(uint64_t fingerprint);\n+\n+  \/\/ Code generation support\n+  static Register from();\n+  static Register to();\n+  static Register temp();\n+};\n+\n+#endif \/\/ CPU_RISCV_INTERPRETERRT_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/interpreterRT_riscv.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,89 @@\n+\/*\n+ * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_JAVAFRAMEANCHOR_RISCV_HPP\n+#define CPU_RISCV_JAVAFRAMEANCHOR_RISCV_HPP\n+\n+private:\n+\n+  \/\/ FP value associated with _last_Java_sp:\n+  intptr_t* volatile _last_Java_fp; \/\/ pointer is volatile not what it points to\n+\n+public:\n+  \/\/ Each arch must define reset, save, restore\n+  \/\/ These are used by objects that only care about:\n+  \/\/  1 - initializing a new state (thread creation, javaCalls)\n+  \/\/  2 - saving a current state (javaCalls)\n+  \/\/  3 - restoring an old state (javaCalls)\n+\n+  void clear(void) {\n+    \/\/ clearing _last_Java_sp must be first\n+    _last_Java_sp = NULL;\n+    OrderAccess::release();\n+    _last_Java_fp = NULL;\n+    _last_Java_pc = NULL;\n+  }\n+\n+  void copy(JavaFrameAnchor* src) {\n+    \/\/ In order to make sure the transition state is valid for \"this\"\n+    \/\/ We must clear _last_Java_sp before copying the rest of the new data\n+    \/\/\n+    \/\/ Hack Alert: Temporary bugfix for 4717480\/4721647\n+    \/\/ To act like previous version (pd_cache_state) don't NULL _last_Java_sp\n+    \/\/ unless the value is changing\n+    \/\/\n+    assert(src != NULL, \"Src should not be NULL.\");\n+    if (_last_Java_sp != src->_last_Java_sp) {\n+      _last_Java_sp = NULL;\n+      OrderAccess::release();\n+    }\n+    _last_Java_fp = src->_last_Java_fp;\n+    _last_Java_pc = src->_last_Java_pc;\n+    \/\/ Must be last so profiler will always see valid frame if has_last_frame() is true\n+    _last_Java_sp = src->_last_Java_sp;\n+  }\n+\n+  bool walkable(void)                            { return _last_Java_sp != NULL && _last_Java_pc != NULL; }\n+  void make_walkable(JavaThread* thread);\n+  void capture_last_Java_pc(void);\n+\n+  intptr_t* last_Java_sp(void) const             { return _last_Java_sp; }\n+\n+  const address last_Java_pc(void)               { return _last_Java_pc; }\n+\n+private:\n+\n+  static ByteSize last_Java_fp_offset()          { return byte_offset_of(JavaFrameAnchor, _last_Java_fp); }\n+\n+public:\n+\n+  void set_last_Java_sp(intptr_t* java_sp)       { _last_Java_sp = java_sp; OrderAccess::release(); }\n+\n+  intptr_t* last_Java_fp(void)                   { return _last_Java_fp; }\n+\n+  \/\/ Assert (last_Java_sp == NULL || fp == NULL)\n+  void set_last_Java_fp(intptr_t* fp)            { OrderAccess::release(); _last_Java_fp = fp; }\n+\n+#endif \/\/ CPU_RISCV_JAVAFRAMEANCHOR_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/javaFrameAnchor_riscv.hpp","additions":89,"deletions":0,"binary":false,"changes":89,"status":"added"},{"patch":"@@ -0,0 +1,194 @@\n+\/*\n+ * Copyright (c) 2004, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/jniFastGetField.hpp\"\n+#include \"prims\/jvm_misc.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+\n+#define __ masm->\n+\n+#define BUFFER_SIZE 30*wordSize\n+\n+\/\/ Instead of issuing a LoadLoad barrier we create an address\n+\/\/ dependency between loads; this might be more efficient.\n+\n+\/\/ Common register usage:\n+\/\/ x10\/f10:      result\n+\/\/ c_rarg0:    jni env\n+\/\/ c_rarg1:    obj\n+\/\/ c_rarg2:    jfield id\n+\n+static const Register robj          = x13;\n+static const Register rcounter      = x14;\n+static const Register roffset       = x15;\n+static const Register rcounter_addr = x16;\n+static const Register result        = x17;\n+\n+address JNI_FastGetField::generate_fast_get_int_field0(BasicType type) {\n+  const char *name;\n+  switch (type) {\n+    case T_BOOLEAN: name = \"jni_fast_GetBooleanField\"; break;\n+    case T_BYTE:    name = \"jni_fast_GetByteField\";    break;\n+    case T_CHAR:    name = \"jni_fast_GetCharField\";    break;\n+    case T_SHORT:   name = \"jni_fast_GetShortField\";   break;\n+    case T_INT:     name = \"jni_fast_GetIntField\";     break;\n+    case T_LONG:    name = \"jni_fast_GetLongField\";    break;\n+    case T_FLOAT:   name = \"jni_fast_GetFloatField\";   break;\n+    case T_DOUBLE:  name = \"jni_fast_GetDoubleField\";  break;\n+    default:        ShouldNotReachHere();\n+      name = NULL;  \/\/ unreachable\n+  }\n+  ResourceMark rm;\n+  BufferBlob* blob = BufferBlob::create(name, BUFFER_SIZE);\n+  CodeBuffer cbuf(blob);\n+  MacroAssembler* masm = new MacroAssembler(&cbuf);\n+  address fast_entry = __ pc();\n+\n+  Label slow;\n+  int32_t offset = 0;\n+  __ la_patchable(rcounter_addr, SafepointSynchronize::safepoint_counter_addr(), offset);\n+  __ addi(rcounter_addr, rcounter_addr, offset);\n+\n+  Address safepoint_counter_addr(rcounter_addr, 0);\n+  __ lwu(rcounter, safepoint_counter_addr);\n+  \/\/ An even value means there are no ongoing safepoint operations\n+  __ andi(t0, rcounter, 1);\n+  __ bnez(t0, slow);\n+  __ xorr(robj, c_rarg1, rcounter);\n+  __ xorr(robj, robj, rcounter);               \/\/ obj, since\n+                                               \/\/ robj ^ rcounter ^ rcounter == robj\n+                                               \/\/ robj is address dependent on rcounter.\n+\n+  \/\/ Both robj and t0 are clobbered by try_resolve_jobject_in_native.\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  assert_cond(bs != NULL);\n+  bs->try_resolve_jobject_in_native(masm, c_rarg0, robj, t0, slow);\n+\n+  __ srli(roffset, c_rarg2, 2);                \/\/ offset\n+\n+  assert(count < LIST_CAPACITY, \"LIST_CAPACITY too small\");\n+  speculative_load_pclist[count] = __ pc();   \/\/ Used by the segfault handler\n+  __ add(roffset, robj, roffset);\n+\n+  switch (type) {\n+    case T_BOOLEAN: __ lbu(result, Address(roffset, 0)); break;\n+    case T_BYTE:    __ lb(result, Address(roffset, 0)); break;\n+    case T_CHAR:    __ lhu(result, Address(roffset, 0)); break;\n+    case T_SHORT:   __ lh(result, Address(roffset, 0)); break;\n+    case T_INT:     __ lw(result, Address(roffset, 0)); break;\n+    case T_LONG:    __ ld(result, Address(roffset, 0)); break;\n+    case T_FLOAT: {\n+      __ flw(f28, Address(roffset, 0)); \/\/ f28 as temporaries\n+      __ fmv_x_w(result, f28); \/\/ f{31--0}-->x\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      __ fld(f28, Address(roffset, 0)); \/\/ f28 as temporaries\n+      __ fmv_x_d(result, f28); \/\/ d{63--0}-->x\n+      break;\n+    }\n+    default:        ShouldNotReachHere();\n+  }\n+\n+  __ xorr(rcounter_addr, rcounter_addr, result);\n+  __ xorr(rcounter_addr, rcounter_addr, result);\n+  __ lw(t0, safepoint_counter_addr);\n+  __ bne(rcounter, t0, slow);\n+\n+  switch (type) {\n+    case T_FLOAT:   __ fmv_w_x(f10, result); break;\n+    case T_DOUBLE:  __ fmv_d_x(f10, result); break;\n+    default:        __ mv(x10, result);   break;\n+  }\n+  __ ret();\n+\n+  slowcase_entry_pclist[count++] = __ pc();\n+  __ bind(slow);\n+  address slow_case_addr;\n+  switch (type) {\n+    case T_BOOLEAN: slow_case_addr = jni_GetBooleanField_addr(); break;\n+    case T_BYTE:    slow_case_addr = jni_GetByteField_addr();    break;\n+    case T_CHAR:    slow_case_addr = jni_GetCharField_addr();    break;\n+    case T_SHORT:   slow_case_addr = jni_GetShortField_addr();   break;\n+    case T_INT:     slow_case_addr = jni_GetIntField_addr();     break;\n+    case T_LONG:    slow_case_addr = jni_GetLongField_addr();    break;\n+    case T_FLOAT:   slow_case_addr = jni_GetFloatField_addr();   break;\n+    case T_DOUBLE:  slow_case_addr = jni_GetDoubleField_addr();  break;\n+    default:        ShouldNotReachHere();\n+      slow_case_addr = NULL;  \/\/ unreachable\n+  }\n+\n+  {\n+    __ enter();\n+    int32_t tmp_offset = 0;\n+    __ la_patchable(t0, ExternalAddress(slow_case_addr), tmp_offset);\n+    __ jalr(x1, t0, tmp_offset);\n+    __ leave();\n+    __ ret();\n+  }\n+  __ flush();\n+\n+  return fast_entry;\n+}\n+\n+\n+address JNI_FastGetField::generate_fast_get_boolean_field() {\n+  return generate_fast_get_int_field0(T_BOOLEAN);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_byte_field() {\n+  return generate_fast_get_int_field0(T_BYTE);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_char_field() {\n+  return generate_fast_get_int_field0(T_CHAR);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_short_field() {\n+  return generate_fast_get_int_field0(T_SHORT);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_int_field() {\n+  return generate_fast_get_int_field0(T_INT);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_long_field() {\n+  return generate_fast_get_int_field0(T_LONG);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_float_field() {\n+  return generate_fast_get_int_field0(T_FLOAT);\n+}\n+\n+address JNI_FastGetField::generate_fast_get_double_field() {\n+  return generate_fast_get_int_field0(T_DOUBLE);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/jniFastGetField_riscv.cpp","additions":194,"deletions":0,"binary":false,"changes":194,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_JNITYPES_RISCV_HPP\n+#define CPU_RISCV_JNITYPES_RISCV_HPP\n+\n+#include \"jni.h\"\n+#include \"oops\/oop.hpp\"\n+\n+\/\/ This file holds platform-dependent routines used to write primitive jni\n+\/\/ types to the array of arguments passed into JavaCalls::call\n+\n+class JNITypes : private AllStatic {\n+  \/\/ These functions write a java primitive type (in native format)\n+  \/\/ to a java stack slot array to be passed as an argument to JavaCalls:calls.\n+  \/\/ I.e., they are functionally 'push' operations if they have a 'pos'\n+  \/\/ formal parameter.  Note that jlong's and jdouble's are written\n+  \/\/ _in reverse_ of the order in which they appear in the interpreter\n+  \/\/ stack.  This is because call stubs (see stubGenerator_sparc.cpp)\n+  \/\/ reverse the argument list constructed by JavaCallArguments (see\n+  \/\/ javaCalls.hpp).\n+\n+public:\n+  \/\/ Ints are stored in native format in one JavaCallArgument slot at *to.\n+  static inline void    put_int(jint  from, intptr_t *to)           { *(jint *)(to +   0  ) =  from; }\n+  static inline void    put_int(jint  from, intptr_t *to, int& pos) { *(jint *)(to + pos++) =  from; }\n+  static inline void    put_int(jint *from, intptr_t *to, int& pos) { *(jint *)(to + pos++) = *from; }\n+\n+  \/\/ Longs are stored in native format in one JavaCallArgument slot at\n+  \/\/ *(to+1).\n+  static inline void put_long(jlong  from, intptr_t *to) {\n+    *(jlong*) (to + 1) = from;\n+  }\n+\n+  static inline void put_long(jlong  from, intptr_t *to, int& pos) {\n+    *(jlong*) (to + 1 + pos) = from;\n+    pos += 2;\n+  }\n+\n+  static inline void put_long(jlong *from, intptr_t *to, int& pos) {\n+    *(jlong*) (to + 1 + pos) = *from;\n+    pos += 2;\n+  }\n+\n+  \/\/ Oops are stored in native format in one JavaCallArgument slot at *to.\n+  static inline void    put_obj(oop  from, intptr_t *to)                { *(oop *)(to +   0  ) =  from; }\n+  static inline void    put_obj(oop  from, intptr_t *to, int& pos)      { *(oop *)(to + pos++) =  from; }\n+  static inline void    put_obj(oop *from, intptr_t *to, int& pos)      { *(oop *)(to + pos++) = *from; }\n+\n+  \/\/ Floats are stored in native format in one JavaCallArgument slot at *to.\n+  static inline void    put_float(jfloat  from, intptr_t *to)           { *(jfloat *)(to +   0  ) =  from;  }\n+  static inline void    put_float(jfloat  from, intptr_t *to, int& pos) { *(jfloat *)(to + pos++) =  from; }\n+  static inline void    put_float(jfloat *from, intptr_t *to, int& pos) { *(jfloat *)(to + pos++) = *from; }\n+\n+#undef _JNI_SLOT_OFFSET\n+#define _JNI_SLOT_OFFSET 1\n+  \/\/ Doubles are stored in native word format in one JavaCallArgument\n+  \/\/ slot at *(to+1).\n+  static inline void put_double(jdouble  from, intptr_t *to) {\n+    *(jdouble*) (to + 1) = from;\n+  }\n+\n+  static inline void put_double(jdouble  from, intptr_t *to, int& pos) {\n+    *(jdouble*) (to + 1 + pos) = from;\n+    pos += 2;\n+  }\n+\n+  static inline void put_double(jdouble *from, intptr_t *to, int& pos) {\n+    *(jdouble*) (to + 1 + pos) = *from;\n+    pos += 2;\n+  }\n+\n+  \/\/ The get_xxx routines, on the other hand, actually _do_ fetch\n+  \/\/ java primitive types from the interpreter stack.\n+  \/\/ No need to worry about alignment on Intel.\n+  static inline jint    get_int   (intptr_t *from) { return *(jint *)   from; }\n+  static inline jlong   get_long  (intptr_t *from) { return *(jlong *)  (from + _JNI_SLOT_OFFSET); }\n+  static inline oop     get_obj   (intptr_t *from) { return *(oop *)    from; }\n+  static inline jfloat  get_float (intptr_t *from) { return *(jfloat *) from; }\n+  static inline jdouble get_double(intptr_t *from) { return *(jdouble *)(from + _JNI_SLOT_OFFSET); }\n+#undef _JNI_SLOT_OFFSET\n+};\n+\n+#endif \/\/ CPU_RISCV_JNITYPES_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/jniTypes_riscv.hpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -0,0 +1,5425 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"compiler\/disassembler.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"interpreter\/bytecodeHistogram.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/biasedLocking.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/intrinsicnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#endif\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) block_comment(str)\n+#endif\n+#define BIND(label) bind(label); __ BLOCK_COMMENT(#label \":\")\n+\n+static void pass_arg0(MacroAssembler* masm, Register arg) {\n+  if (c_rarg0 != arg) {\n+    assert_cond(masm != NULL);\n+    masm->mv(c_rarg0, arg);\n+  }\n+}\n+\n+static void pass_arg1(MacroAssembler* masm, Register arg) {\n+  if (c_rarg1 != arg) {\n+    assert_cond(masm != NULL);\n+    masm->mv(c_rarg1, arg);\n+  }\n+}\n+\n+static void pass_arg2(MacroAssembler* masm, Register arg) {\n+  if (c_rarg2 != arg) {\n+    assert_cond(masm != NULL);\n+    masm->mv(c_rarg2, arg);\n+  }\n+}\n+\n+static void pass_arg3(MacroAssembler* masm, Register arg) {\n+  if (c_rarg3 != arg) {\n+    assert_cond(masm != NULL);\n+    masm->mv(c_rarg3, arg);\n+  }\n+}\n+\n+void MacroAssembler::align(int modulus, int extra_offset) {\n+  CompressibleRegion cr(this);\n+  while ((offset() + extra_offset) % modulus != 0) { nop(); }\n+}\n+\n+void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {\n+  call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);\n+}\n+\n+\/\/ Implementation of call_VM versions\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             address entry_point,\n+                             bool check_exceptions) {\n+  call_VM_helper(oop_result, entry_point, 0, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             address entry_point,\n+                             Register arg_1,\n+                             bool check_exceptions) {\n+  pass_arg1(this, arg_1);\n+  call_VM_helper(oop_result, entry_point, 1, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             address entry_point,\n+                             Register arg_1,\n+                             Register arg_2,\n+                             bool check_exceptions) {\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+  pass_arg1(this, arg_1);\n+  call_VM_helper(oop_result, entry_point, 2, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             address entry_point,\n+                             Register arg_1,\n+                             Register arg_2,\n+                             Register arg_3,\n+                             bool check_exceptions) {\n+  assert(arg_1 != c_rarg3, \"smashed arg\");\n+  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  pass_arg3(this, arg_3);\n+\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+\n+  pass_arg1(this, arg_1);\n+  call_VM_helper(oop_result, entry_point, 3, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             Register last_java_sp,\n+                             address entry_point,\n+                             int number_of_arguments,\n+                             bool check_exceptions) {\n+  call_VM_base(oop_result, xthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             Register last_java_sp,\n+                             address entry_point,\n+                             Register arg_1,\n+                             bool check_exceptions) {\n+  pass_arg1(this, arg_1);\n+  call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             Register last_java_sp,\n+                             address entry_point,\n+                             Register arg_1,\n+                             Register arg_2,\n+                             bool check_exceptions) {\n+\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+  pass_arg1(this, arg_1);\n+  call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);\n+}\n+\n+void MacroAssembler::call_VM(Register oop_result,\n+                             Register last_java_sp,\n+                             address entry_point,\n+                             Register arg_1,\n+                             Register arg_2,\n+                             Register arg_3,\n+                             bool check_exceptions) {\n+  assert(arg_1 != c_rarg3, \"smashed arg\");\n+  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  pass_arg3(this, arg_3);\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+  pass_arg1(this, arg_1);\n+  call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);\n+}\n+\n+\/\/ these are no-ops overridden by InterpreterMacroAssembler\n+void MacroAssembler::check_and_handle_earlyret(Register java_thread) {}\n+void MacroAssembler::check_and_handle_popframe(Register java_thread) {}\n+\n+RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,\n+                                                      Register tmp,\n+                                                      int offset) {\n+  intptr_t value = *delayed_value_addr;\n+  if (value != 0)\n+    return RegisterOrConstant(value + offset);\n+\n+  \/\/ load indirectly to solve generation ordering problem\n+  ld(tmp, ExternalAddress((address) delayed_value_addr));\n+\n+  if (offset != 0)\n+    add(tmp, tmp, offset);\n+\n+  return RegisterOrConstant(tmp);\n+}\n+\n+\/\/ Calls to C land\n+\/\/\n+\/\/ When entering C land, the fp, & esp of the last Java frame have to be recorded\n+\/\/ in the (thread-local) JavaThread object. When leaving C land, the last Java fp\n+\/\/ has to be reset to 0. This is required to allow proper stack traversal.\n+void MacroAssembler::set_last_Java_frame(Register last_java_sp,\n+                                         Register last_java_fp,\n+                                         Register last_java_pc,\n+                                         Register tmp) {\n+\n+  if (last_java_pc->is_valid()) {\n+      sd(last_java_pc, Address(xthread,\n+                               JavaThread::frame_anchor_offset() +\n+                               JavaFrameAnchor::last_Java_pc_offset()));\n+  }\n+\n+  \/\/ determine last_java_sp register\n+  if (last_java_sp == sp) {\n+    mv(tmp, sp);\n+    last_java_sp = tmp;\n+  } else if (!last_java_sp->is_valid()) {\n+    last_java_sp = esp;\n+  }\n+\n+  sd(last_java_sp, Address(xthread, JavaThread::last_Java_sp_offset()));\n+\n+  \/\/ last_java_fp is optional\n+  if (last_java_fp->is_valid()) {\n+    sd(last_java_fp, Address(xthread, JavaThread::last_Java_fp_offset()));\n+  }\n+}\n+\n+void MacroAssembler::set_last_Java_frame(Register last_java_sp,\n+                                         Register last_java_fp,\n+                                         address  last_java_pc,\n+                                         Register tmp) {\n+  assert(last_java_pc != NULL, \"must provide a valid PC\");\n+\n+  la(tmp, last_java_pc);\n+  sd(tmp, Address(xthread, JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset()));\n+\n+  set_last_Java_frame(last_java_sp, last_java_fp, noreg, tmp);\n+}\n+\n+void MacroAssembler::set_last_Java_frame(Register last_java_sp,\n+                                         Register last_java_fp,\n+                                         Label &L,\n+                                         Register tmp) {\n+  if (L.is_bound()) {\n+    set_last_Java_frame(last_java_sp, last_java_fp, target(L), tmp);\n+  } else {\n+    InstructionMark im(this);\n+    L.add_patch_at(code(), locator());\n+    set_last_Java_frame(last_java_sp, last_java_fp, pc() \/* Patched later *\/, tmp);\n+  }\n+}\n+\n+\/\/ Just like safepoint_poll, but use an acquiring load for thread-\n+\/\/ local polling.\n+\/\/\n+\/\/ We need an acquire here to ensure that any subsequent load of the\n+\/\/ global SafepointSynchronize::_state flag is ordered after this load\n+\/\/ of the local Thread::_polling page.  We don't want this poll to\n+\/\/ return false (i.e. not safepointing) and a later poll of the global\n+\/\/ SafepointSynchronize::_state spuriously to return true.\n+\/\/\n+\/\/ This is to avoid a race when we're in a native->Java transition\n+\/\/ racing the code which wakes up from a safepoint.\n+\/\/\n+void MacroAssembler::safepoint_poll_acquire(Label& slow_path) {\n+  if (SafepointMechanism::uses_thread_local_poll()) {\n+    membar(MacroAssembler::AnyAny);\n+    ld(t1, Address(xthread, Thread::polling_page_offset()));\n+    membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+    andi(t0, t1, SafepointMechanism::poll_bit());\n+    bnez(t0, slow_path);\n+  } else {\n+    safepoint_poll(slow_path);\n+  }\n+}\n+\n+void MacroAssembler::reset_last_Java_frame(bool clear_fp) {\n+  \/\/ we must set sp to zero to clear frame\n+  sd(zr, Address(xthread, JavaThread::last_Java_sp_offset()));\n+\n+  \/\/ must clear fp, so that compiled frames are not confused; it is\n+  \/\/ possible that we need it only for debugging\n+  if (clear_fp) {\n+    sd(zr, Address(xthread, JavaThread::last_Java_fp_offset()));\n+  }\n+\n+  \/\/ Always clear the pc because it could have been set by make_walkable()\n+  sd(zr, Address(xthread, JavaThread::last_Java_pc_offset()));\n+}\n+\n+void MacroAssembler::call_VM_base(Register oop_result,\n+                                  Register java_thread,\n+                                  Register last_java_sp,\n+                                  address  entry_point,\n+                                  int      number_of_arguments,\n+                                  bool     check_exceptions) {\n+   \/\/ determine java_thread register\n+  if (!java_thread->is_valid()) {\n+    java_thread = xthread;\n+  }\n+  \/\/ determine last_java_sp register\n+  if (!last_java_sp->is_valid()) {\n+    last_java_sp = esp;\n+  }\n+\n+  \/\/ debugging support\n+  assert(number_of_arguments >= 0   , \"cannot have negative number of arguments\");\n+  assert(java_thread == xthread, \"unexpected register\");\n+\n+  assert(java_thread != oop_result  , \"cannot use the same register for java_thread & oop_result\");\n+  assert(java_thread != last_java_sp, \"cannot use the same register for java_thread & last_java_sp\");\n+\n+  \/\/ push java thread (becomes first argument of C function)\n+  mv(c_rarg0, java_thread);\n+\n+  \/\/ set last Java frame before call\n+  assert(last_java_sp != fp, \"can't use fp\");\n+\n+  Label l;\n+  set_last_Java_frame(last_java_sp, fp, l, t0);\n+\n+  \/\/ do the call, remove parameters\n+  MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments, &l);\n+\n+  \/\/ reset last Java frame\n+  \/\/ Only interpreter should have to clear fp\n+  reset_last_Java_frame(true);\n+\n+   \/\/ C++ interp handles this in the interpreter\n+  check_and_handle_popframe(java_thread);\n+  check_and_handle_earlyret(java_thread);\n+\n+  if (check_exceptions) {\n+    \/\/ check for pending exceptions (java_thread is set upon return)\n+    ld(t0, Address(java_thread, in_bytes(Thread::pending_exception_offset())));\n+    Label ok;\n+    beqz(t0, ok);\n+    int32_t offset = 0;\n+    la_patchable(t0, RuntimeAddress(StubRoutines::forward_exception_entry()), offset);\n+    jalr(x0, t0, offset);\n+    bind(ok);\n+  }\n+\n+  \/\/ get oop result if there is one and reset the value in the thread\n+  if (oop_result->is_valid()) {\n+    get_vm_result(oop_result, java_thread);\n+  }\n+}\n+\n+void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {\n+  ld(oop_result, Address(java_thread, JavaThread::vm_result_offset()));\n+  sd(zr, Address(java_thread, JavaThread::vm_result_offset()));\n+  verify_oop(oop_result, \"broken oop in call_VM_base\");\n+}\n+\n+void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {\n+  ld(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));\n+  sd(zr, Address(java_thread, JavaThread::vm_result_2_offset()));\n+}\n+\n+void MacroAssembler::verify_oop(Register reg, const char* s) {\n+  if (!VerifyOops) { return; }\n+\n+  \/\/ Pass register number to verify_oop_subroutine\n+  const char* b = NULL;\n+  {\n+    ResourceMark rm;\n+    stringStream ss;\n+    ss.print(\"verify_oop: %s: %s\", reg->name(), s);\n+    b = code_string(ss.as_string());\n+  }\n+  BLOCK_COMMENT(\"verify_oop {\");\n+\n+  push_reg(RegSet::of(ra, t0, t1, c_rarg0), sp);\n+\n+  mv(c_rarg0, reg); \/\/ c_rarg0 : x10\n+  \/\/ The length of the instruction sequence emitted should be independent\n+  \/\/ of the values of the local char buffer address so that the size of mach\n+  \/\/ nodes for scratch emit and normal emit matches.\n+  mv(t0, (address)b);\n+\n+  \/\/ call indirectly to solve generation ordering problem\n+  int32_t offset = 0;\n+  la_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), offset);\n+  ld(t1, Address(t1, offset));\n+  jalr(t1);\n+\n+  pop_reg(RegSet::of(ra, t0, t1, c_rarg0), sp);\n+\n+  BLOCK_COMMENT(\"} verify_oop\");\n+}\n+\n+void MacroAssembler::verify_oop_addr(Address addr, const char* s) {\n+  if (!VerifyOops) {\n+    return;\n+  }\n+\n+  const char* b = NULL;\n+  {\n+    ResourceMark rm;\n+    stringStream ss;\n+    ss.print(\"verify_oop_addr: %s\", s);\n+    b = code_string(ss.as_string());\n+  }\n+  BLOCK_COMMENT(\"verify_oop_addr {\");\n+\n+  push_reg(RegSet::of(ra, t0, t1, c_rarg0), sp);\n+\n+  if (addr.uses(sp)) {\n+    la(x10, addr);\n+    ld(x10, Address(x10, 4 * wordSize));\n+  } else {\n+    ld(x10, addr);\n+  }\n+\n+  \/\/ The length of the instruction sequence emitted should be independent\n+  \/\/ of the values of the local char buffer address so that the size of mach\n+  \/\/ nodes for scratch emit and normal emit matches.\n+  mv(t0, (address)b);\n+\n+  \/\/ call indirectly to solve generation ordering problem\n+  int32_t offset = 0;\n+  la_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), offset);\n+  ld(t1, Address(t1, offset));\n+  jalr(t1);\n+\n+  pop_reg(RegSet::of(ra, t0, t1, c_rarg0), sp);\n+\n+  BLOCK_COMMENT(\"} verify_oop_addr\");\n+}\n+\n+Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,\n+                                         int extra_slot_offset) {\n+  \/\/ cf. TemplateTable::prepare_invoke(), if (load_receiver).\n+  int stackElementSize = Interpreter::stackElementSize;\n+  int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);\n+#ifdef ASSERT\n+  int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);\n+  assert(offset1 - offset == stackElementSize, \"correct arithmetic\");\n+#endif\n+  if (arg_slot.is_constant()) {\n+    return Address(esp, arg_slot.as_constant() * stackElementSize + offset);\n+  } else {\n+    assert_different_registers(t0, arg_slot.as_register());\n+    shadd(t0, arg_slot.as_register(), esp, t0, exact_log2(stackElementSize));\n+    return Address(t0, offset);\n+  }\n+}\n+\n+#ifndef PRODUCT\n+extern \"C\" void findpc(intptr_t x);\n+#endif\n+\n+void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[])\n+{\n+  \/\/ In order to get locks to work, we need to fake a in_VM state\n+  if (ShowMessageBoxOnError) {\n+    JavaThread* thread = JavaThread::current();\n+    JavaThreadState saved_state = thread->thread_state();\n+    thread->set_thread_state(_thread_in_vm);\n+#ifndef PRODUCT\n+    if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {\n+      ttyLocker ttyl;\n+      BytecodeCounter::print();\n+    }\n+#endif\n+    if (os::message_box(msg, \"Execution stopped, print registers?\")) {\n+      ttyLocker ttyl;\n+      tty->print_cr(\" pc = 0x%016lx\", pc);\n+#ifndef PRODUCT\n+      tty->cr();\n+      findpc(pc);\n+      tty->cr();\n+#endif\n+      tty->print_cr(\" x0 = 0x%016lx\", regs[0]);\n+      tty->print_cr(\" x1 = 0x%016lx\", regs[1]);\n+      tty->print_cr(\" x2 = 0x%016lx\", regs[2]);\n+      tty->print_cr(\" x3 = 0x%016lx\", regs[3]);\n+      tty->print_cr(\" x4 = 0x%016lx\", regs[4]);\n+      tty->print_cr(\" x5 = 0x%016lx\", regs[5]);\n+      tty->print_cr(\" x6 = 0x%016lx\", regs[6]);\n+      tty->print_cr(\" x7 = 0x%016lx\", regs[7]);\n+      tty->print_cr(\" x8 = 0x%016lx\", regs[8]);\n+      tty->print_cr(\" x9 = 0x%016lx\", regs[9]);\n+      tty->print_cr(\"x10 = 0x%016lx\", regs[10]);\n+      tty->print_cr(\"x11 = 0x%016lx\", regs[11]);\n+      tty->print_cr(\"x12 = 0x%016lx\", regs[12]);\n+      tty->print_cr(\"x13 = 0x%016lx\", regs[13]);\n+      tty->print_cr(\"x14 = 0x%016lx\", regs[14]);\n+      tty->print_cr(\"x15 = 0x%016lx\", regs[15]);\n+      tty->print_cr(\"x16 = 0x%016lx\", regs[16]);\n+      tty->print_cr(\"x17 = 0x%016lx\", regs[17]);\n+      tty->print_cr(\"x18 = 0x%016lx\", regs[18]);\n+      tty->print_cr(\"x19 = 0x%016lx\", regs[19]);\n+      tty->print_cr(\"x20 = 0x%016lx\", regs[20]);\n+      tty->print_cr(\"x21 = 0x%016lx\", regs[21]);\n+      tty->print_cr(\"x22 = 0x%016lx\", regs[22]);\n+      tty->print_cr(\"x23 = 0x%016lx\", regs[23]);\n+      tty->print_cr(\"x24 = 0x%016lx\", regs[24]);\n+      tty->print_cr(\"x25 = 0x%016lx\", regs[25]);\n+      tty->print_cr(\"x26 = 0x%016lx\", regs[26]);\n+      tty->print_cr(\"x27 = 0x%016lx\", regs[27]);\n+      tty->print_cr(\"x28 = 0x%016lx\", regs[28]);\n+      tty->print_cr(\"x30 = 0x%016lx\", regs[30]);\n+      tty->print_cr(\"x31 = 0x%016lx\", regs[31]);\n+      BREAKPOINT;\n+    }\n+  }\n+  fatal(\"DEBUG MESSAGE: %s\", msg);\n+}\n+\n+void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {\n+  Label done, not_weak;\n+  beqz(value, done);           \/\/ Use NULL as-is.\n+\n+  \/\/ Test for jweak tag.\n+  andi(t0, value, JNIHandles::weak_tag_mask);\n+  beqz(t0, not_weak);\n+\n+  \/\/ Resolve jweak.\n+  access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,\n+                 Address(value, -JNIHandles::weak_tag_value), tmp, thread);\n+  verify_oop(value);\n+  j(done);\n+\n+  bind(not_weak);\n+  \/\/ Resolve (untagged) jobject.\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);\n+  verify_oop(value);\n+  bind(done);\n+}\n+\n+void MacroAssembler::stop(const char* msg) {\n+  address ip = pc();\n+  pusha();\n+  \/\/ The length of the instruction sequence emitted should be independent\n+  \/\/ of the values of msg and ip so that the size of mach nodes for scratch\n+  \/\/ emit and normal emit matches.\n+  mv(c_rarg0, (address)msg);\n+  mv(c_rarg1, (address)ip);\n+  mv(c_rarg2, sp);\n+  mv(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));\n+  jalr(c_rarg3);\n+  ebreak();\n+}\n+\n+void MacroAssembler::unimplemented(const char* what) {\n+  const char* buf = NULL;\n+  {\n+    ResourceMark rm;\n+    stringStream ss;\n+    ss.print(\"unimplemented: %s\", what);\n+    buf = code_string(ss.as_string());\n+  }\n+  stop(buf);\n+}\n+\n+void MacroAssembler::emit_static_call_stub() {\n+  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  \/\/ exact layout of this stub.\n+\n+  ifence();\n+  mov_metadata(xmethod, (Metadata*)NULL);\n+\n+  \/\/ Jump to the entry point of the i2c stub.\n+  int32_t offset = 0;\n+  movptr_with_offset(t0, 0, offset);\n+  jalr(x0, t0, offset);\n+}\n+\n+void MacroAssembler::call_VM_leaf_base(address entry_point,\n+                                       int number_of_arguments,\n+                                       Label *retaddr) {\n+  call_native_base(entry_point, retaddr);\n+}\n+\n+void MacroAssembler::call_native(address entry_point, Register arg_0) {\n+  pass_arg0(this, arg_0);\n+  call_native_base(entry_point);\n+}\n+\n+void MacroAssembler::call_native_base(address entry_point, Label *retaddr) {\n+  Label E, L;\n+  int32_t offset = 0;\n+  push_reg(0x80000040, sp);   \/\/ push << t0 & xmethod >> to sp\n+  movptr_with_offset(t0, entry_point, offset);\n+  jalr(x1, t0, offset);\n+  if (retaddr != NULL) {\n+    bind(*retaddr);\n+  }\n+  pop_reg(0x80000040, sp);   \/\/ pop << t0 & xmethod >> from sp\n+}\n+\n+void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {\n+  call_VM_leaf_base(entry_point, number_of_arguments);\n+}\n+\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {\n+  pass_arg0(this, arg_0);\n+  call_VM_leaf_base(entry_point, 1);\n+}\n+\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {\n+  pass_arg0(this, arg_0);\n+  pass_arg1(this, arg_1);\n+  call_VM_leaf_base(entry_point, 2);\n+}\n+\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,\n+                                  Register arg_1, Register arg_2) {\n+  pass_arg0(this, arg_0);\n+  pass_arg1(this, arg_1);\n+  pass_arg2(this, arg_2);\n+  call_VM_leaf_base(entry_point, 3);\n+}\n+\n+void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {\n+  pass_arg0(this, arg_0);\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n+void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {\n+\n+  assert(arg_0 != c_rarg1, \"smashed arg\");\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  MacroAssembler::call_VM_leaf_base(entry_point, 2);\n+}\n+\n+void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {\n+  assert(arg_0 != c_rarg2, \"smashed arg\");\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+  assert(arg_0 != c_rarg1, \"smashed arg\");\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  MacroAssembler::call_VM_leaf_base(entry_point, 3);\n+}\n+\n+void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {\n+  assert(arg_0 != c_rarg3, \"smashed arg\");\n+  assert(arg_1 != c_rarg3, \"smashed arg\");\n+  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  pass_arg3(this, arg_3);\n+  assert(arg_0 != c_rarg2, \"smashed arg\");\n+  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  pass_arg2(this, arg_2);\n+  assert(arg_0 != c_rarg1, \"smashed arg\");\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  MacroAssembler::call_VM_leaf_base(entry_point, 4);\n+}\n+\n+void MacroAssembler::nop() {\n+  addi(x0, x0, 0);\n+}\n+\n+void MacroAssembler::mv(Register Rd, Register Rs) {\n+  if (Rd != Rs) {\n+    addi(Rd, Rs, 0);\n+  }\n+}\n+\n+void MacroAssembler::notr(Register Rd, Register Rs) {\n+  xori(Rd, Rs, -1);\n+}\n+\n+void MacroAssembler::neg(Register Rd, Register Rs) {\n+  sub(Rd, x0, Rs);\n+}\n+\n+void MacroAssembler::negw(Register Rd, Register Rs) {\n+  subw(Rd, x0, Rs);\n+}\n+\n+void MacroAssembler::sext_w(Register Rd, Register Rs) {\n+  addiw(Rd, Rs, 0);\n+}\n+\n+void MacroAssembler::zext_b(Register Rd, Register Rs) {\n+  andi(Rd, Rs, 0xFF);\n+}\n+\n+void MacroAssembler::seqz(Register Rd, Register Rs) {\n+  sltiu(Rd, Rs, 1);\n+}\n+\n+void MacroAssembler::snez(Register Rd, Register Rs) {\n+  sltu(Rd, x0, Rs);\n+}\n+\n+void MacroAssembler::sltz(Register Rd, Register Rs) {\n+  slt(Rd, Rs, x0);\n+}\n+\n+void MacroAssembler::sgtz(Register Rd, Register Rs) {\n+  slt(Rd, x0, Rs);\n+}\n+\n+void MacroAssembler::fmv_s(FloatRegister Rd, FloatRegister Rs) {\n+  if (Rd != Rs) {\n+    fsgnj_s(Rd, Rs, Rs);\n+  }\n+}\n+\n+void MacroAssembler::fabs_s(FloatRegister Rd, FloatRegister Rs) {\n+  fsgnjx_s(Rd, Rs, Rs);\n+}\n+\n+void MacroAssembler::fneg_s(FloatRegister Rd, FloatRegister Rs) {\n+  fsgnjn_s(Rd, Rs, Rs);\n+}\n+\n+void MacroAssembler::fmv_d(FloatRegister Rd, FloatRegister Rs) {\n+  if (Rd != Rs) {\n+    fsgnj_d(Rd, Rs, Rs);\n+  }\n+}\n+\n+void MacroAssembler::fabs_d(FloatRegister Rd, FloatRegister Rs) {\n+  fsgnjx_d(Rd, Rs, Rs);\n+}\n+\n+void MacroAssembler::fneg_d(FloatRegister Rd, FloatRegister Rs) {\n+  fsgnjn_d(Rd, Rs, Rs);\n+}\n+\n+void MacroAssembler::vmnot_m(VectorRegister vd, VectorRegister vs) {\n+  vmnand_mm(vd, vs, vs);\n+}\n+\n+void MacroAssembler::vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm) {\n+  vnsrl_wx(vd, vs, x0, vm);\n+}\n+\n+void MacroAssembler::vfneg_v(VectorRegister vd, VectorRegister vs) {\n+  vfsgnjn_vv(vd, vs, vs);\n+}\n+\n+void MacroAssembler::la(Register Rd, const address &dest) {\n+  int64_t offset = dest - pc();\n+  if (is_offset_in_range(offset, 32)) {\n+    auipc(Rd, (int32_t)offset + 0x800);  \/\/0x800, Note:the 11th sign bit\n+    addi(Rd, Rd, ((int64_t)offset << 52) >> 52);\n+  } else {\n+    movptr(Rd, dest);\n+  }\n+}\n+\n+void MacroAssembler::la(Register Rd, const Address &adr) {\n+  InstructionMark im(this);\n+  code_section()->relocate(inst_mark(), adr.rspec());\n+  relocInfo::relocType rtype = adr.rspec().reloc()->type();\n+\n+  switch (adr.getMode()) {\n+    case Address::literal: {\n+      if (rtype == relocInfo::none) {\n+        li(Rd, (intptr_t)(adr.target()));\n+      } else {\n+        movptr(Rd, adr.target());\n+      }\n+      break;\n+    }\n+    case Address::base_plus_offset: {\n+      int32_t offset = 0;\n+      baseOffset(Rd, adr, offset);\n+      addi(Rd, Rd, offset);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void MacroAssembler::la(Register Rd, Label &label) {\n+  la(Rd, target(label));\n+}\n+\n+#define INSN(NAME)                                                                \\\n+  void MacroAssembler::NAME##z(Register Rs, const address &dest) {                \\\n+    NAME(Rs, zr, dest);                                                           \\\n+  }                                                                               \\\n+  void MacroAssembler::NAME##z(Register Rs, Label &l, bool is_far) {              \\\n+    NAME(Rs, zr, l, is_far);                                                      \\\n+  }                                                                               \\\n+\n+  INSN(beq);\n+  INSN(bne);\n+  INSN(blt);\n+  INSN(ble);\n+  INSN(bge);\n+  INSN(bgt);\n+\n+#undef INSN\n+\n+\/\/ Float compare branch instructions\n+\n+#define INSN(NAME, FLOATCMP, BRANCH)                                                                                   \\\n+  void MacroAssembler::float_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far, bool is_unordered) {  \\\n+    FLOATCMP##_s(t0, Rs1, Rs2);                                                                                        \\\n+    BRANCH(t0, l, is_far);                                                                                             \\\n+  }                                                                                                                    \\\n+  void MacroAssembler::double_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far, bool is_unordered) { \\\n+    FLOATCMP##_d(t0, Rs1, Rs2);                                                                                        \\\n+    BRANCH(t0, l, is_far);                                                                                             \\\n+  }\n+\n+  INSN(beq, feq, bnez);\n+  INSN(bne, feq, beqz);\n+\n+#undef INSN\n+\n+\n+#define INSN(NAME, FLOATCMP1, FLOATCMP2)                                              \\\n+  void MacroAssembler::float_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l,   \\\n+                                    bool is_far, bool is_unordered) {                 \\\n+    if (is_unordered) {                                                               \\\n+      \/* jump if either source is NaN or condition is expected *\/                     \\\n+      FLOATCMP2##_s(t0, Rs2, Rs1);                                                    \\\n+      beqz(t0, l, is_far);                                                            \\\n+    } else {                                                                          \\\n+      \/* jump if no NaN in source and condition is expected *\/                        \\\n+      FLOATCMP1##_s(t0, Rs1, Rs2);                                                    \\\n+      bnez(t0, l, is_far);                                                            \\\n+    }                                                                                 \\\n+  }                                                                                   \\\n+  void MacroAssembler::double_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l,  \\\n+                                     bool is_far, bool is_unordered) {                \\\n+    if (is_unordered) {                                                               \\\n+      \/* jump if either source is NaN or condition is expected *\/                     \\\n+      FLOATCMP2##_d(t0, Rs2, Rs1);                                                    \\\n+      beqz(t0, l, is_far);                                                            \\\n+    } else {                                                                          \\\n+      \/* jump if no NaN in source and condition is expected *\/                        \\\n+      FLOATCMP1##_d(t0, Rs1, Rs2);                                                    \\\n+      bnez(t0, l, is_far);                                                            \\\n+    }                                                                                 \\\n+  }\n+\n+  INSN(ble, fle, flt);\n+  INSN(blt, flt, fle);\n+\n+#undef INSN\n+\n+#define INSN(NAME, CMP)                                                              \\\n+  void MacroAssembler::float_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l,  \\\n+                                    bool is_far, bool is_unordered) {                \\\n+    float_##CMP(Rs2, Rs1, l, is_far, is_unordered);                                  \\\n+  }                                                                                  \\\n+  void MacroAssembler::double_##NAME(FloatRegister Rs1, FloatRegister Rs2, Label &l, \\\n+                                     bool is_far, bool is_unordered) {               \\\n+    double_##CMP(Rs2, Rs1, l, is_far, is_unordered);                                 \\\n+  }\n+\n+  INSN(bgt, blt);\n+  INSN(bge, ble);\n+\n+#undef INSN\n+\n+\n+#define INSN(NAME, CSR)                       \\\n+  void MacroAssembler::NAME(Register Rd) {    \\\n+    csrr(Rd, CSR);                            \\\n+  }\n+\n+  INSN(rdinstret,  CSR_INSTERT);\n+  INSN(rdcycle,    CSR_CYCLE);\n+  INSN(rdtime,     CSR_TIME);\n+  INSN(frcsr,      CSR_FCSR);\n+  INSN(frrm,       CSR_FRM);\n+  INSN(frflags,    CSR_FFLAGS);\n+\n+#undef INSN\n+\n+void MacroAssembler::csrr(Register Rd, unsigned csr) {\n+  csrrs(Rd, csr, x0);\n+}\n+\n+#define INSN(NAME, OPFUN)                                      \\\n+  void MacroAssembler::NAME(unsigned csr, Register Rs) {       \\\n+    OPFUN(x0, csr, Rs);                                        \\\n+  }\n+\n+  INSN(csrw, csrrw);\n+  INSN(csrs, csrrs);\n+  INSN(csrc, csrrc);\n+\n+#undef INSN\n+\n+#define INSN(NAME, OPFUN)                                      \\\n+  void MacroAssembler::NAME(unsigned csr, unsigned imm) {      \\\n+    OPFUN(x0, csr, imm);                                       \\\n+  }\n+\n+  INSN(csrwi, csrrwi);\n+  INSN(csrsi, csrrsi);\n+  INSN(csrci, csrrci);\n+\n+#undef INSN\n+\n+#define INSN(NAME, CSR)                                      \\\n+  void MacroAssembler::NAME(Register Rd, Register Rs) {      \\\n+    csrrw(Rd, CSR, Rs);                                      \\\n+  }\n+\n+  INSN(fscsr,   CSR_FCSR);\n+  INSN(fsrm,    CSR_FRM);\n+  INSN(fsflags, CSR_FFLAGS);\n+\n+#undef INSN\n+\n+#define INSN(NAME)                              \\\n+  void MacroAssembler::NAME(Register Rs) {      \\\n+    NAME(x0, Rs);                               \\\n+  }\n+\n+  INSN(fscsr);\n+  INSN(fsrm);\n+  INSN(fsflags);\n+\n+#undef INSN\n+\n+void MacroAssembler::fsrmi(Register Rd, unsigned imm) {\n+  guarantee(imm < 5, \"Rounding Mode is invalid in Rounding Mode register\");\n+  csrrwi(Rd, CSR_FRM, imm);\n+}\n+\n+void MacroAssembler::fsflagsi(Register Rd, unsigned imm) {\n+   csrrwi(Rd, CSR_FFLAGS, imm);\n+}\n+\n+#define INSN(NAME)                             \\\n+  void MacroAssembler::NAME(unsigned imm) {    \\\n+    NAME(x0, imm);                             \\\n+  }\n+\n+  INSN(fsrmi);\n+  INSN(fsflagsi);\n+\n+#undef INSN\n+\n+void MacroAssembler::push_reg(Register Rs)\n+{\n+  addi(esp, esp, 0 - wordSize);\n+  sd(Rs, Address(esp, 0));\n+}\n+\n+void MacroAssembler::pop_reg(Register Rd)\n+{\n+  ld(Rd, esp, 0);\n+  addi(esp, esp, wordSize);\n+}\n+\n+int MacroAssembler::bitset_to_regs(unsigned int bitset, unsigned char* regs) {\n+  int count = 0;\n+  \/\/ Scan bitset to accumulate register pairs\n+  for (int reg = 31; reg >= 0; reg--) {\n+    if ((1U << 31) & bitset) {\n+      regs[count++] = reg;\n+    }\n+    bitset <<= 1;\n+  }\n+  return count;\n+}\n+\n+\/\/ Push lots of registers in the bit set supplied.  Don't push sp.\n+\/\/ Return the number of words pushed\n+int MacroAssembler::push_reg(unsigned int bitset, Register stack) {\n+  DEBUG_ONLY(int words_pushed = 0;)\n+  CompressibleRegion cr(this);\n+\n+  unsigned char regs[32];\n+  int count = bitset_to_regs(bitset, regs);\n+  \/\/ reserve one slot to align for odd count\n+  int offset = is_even(count) ? 0 : wordSize;\n+\n+  if (count) {\n+    addi(stack, stack, - count * wordSize - offset);\n+  }\n+  for (int i = count - 1; i >= 0; i--) {\n+    sd(as_Register(regs[i]), Address(stack, (count - 1 - i) * wordSize + offset));\n+    DEBUG_ONLY(words_pushed ++;)\n+  }\n+\n+  assert(words_pushed == count, \"oops, pushed != count\");\n+\n+  return count;\n+}\n+\n+int MacroAssembler::pop_reg(unsigned int bitset, Register stack) {\n+  DEBUG_ONLY(int words_popped = 0;)\n+  CompressibleRegion cr(this);\n+\n+  unsigned char regs[32];\n+  int count = bitset_to_regs(bitset, regs);\n+  \/\/ reserve one slot to align for odd count\n+  int offset = is_even(count) ? 0 : wordSize;\n+\n+  for (int i = count - 1; i >= 0; i--) {\n+    ld(as_Register(regs[i]), Address(stack, (count - 1 - i) * wordSize + offset));\n+    DEBUG_ONLY(words_popped ++;)\n+  }\n+\n+  if (count) {\n+    addi(stack, stack, count * wordSize + offset);\n+  }\n+  assert(words_popped == count, \"oops, popped != count\");\n+\n+  return count;\n+}\n+\n+\/\/ Push float registers in the bitset, except sp.\n+\/\/ Return the number of heapwords pushed.\n+int MacroAssembler::push_fp(unsigned int bitset, Register stack) {\n+  CompressibleRegion cr(this);\n+  int words_pushed = 0;\n+  unsigned char regs[32];\n+  int count = bitset_to_regs(bitset, regs);\n+  int push_slots = count + (count & 1);\n+\n+  if (count) {\n+    addi(stack, stack, -push_slots * wordSize);\n+  }\n+\n+  for (int i = count - 1; i >= 0; i--) {\n+    fsd(as_FloatRegister(regs[i]), Address(stack, (push_slots - 1 - i) * wordSize));\n+    words_pushed++;\n+  }\n+\n+  assert(words_pushed == count, \"oops, pushed(%d) != count(%d)\", words_pushed, count);\n+  return count;\n+}\n+\n+int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {\n+  CompressibleRegion cr(this);\n+  int words_popped = 0;\n+  unsigned char regs[32];\n+  int count = bitset_to_regs(bitset, regs);\n+  int pop_slots = count + (count & 1);\n+\n+  for (int i = count - 1; i >= 0; i--) {\n+    fld(as_FloatRegister(regs[i]), Address(stack, (pop_slots - 1 - i) * wordSize));\n+    words_popped++;\n+  }\n+\n+  if (count) {\n+    addi(stack, stack, pop_slots * wordSize);\n+  }\n+\n+  assert(words_popped == count, \"oops, popped(%d) != count(%d)\", words_popped, count);\n+  return count;\n+}\n+\n+void MacroAssembler::push_call_clobbered_registers_except(RegSet exclude) {\n+  CompressibleRegion cr(this);\n+  \/\/ Push integer registers x7, x10-x17, x28-x31.\n+  push_reg(RegSet::of(x7) + RegSet::range(x10, x17) + RegSet::range(x28, x31) - exclude, sp);\n+\n+  \/\/ Push float registers f0-f7, f10-f17, f28-f31.\n+  addi(sp, sp, - wordSize * 20);\n+  int offset = 0;\n+  for (int i = 0; i < 32; i++) {\n+    if (i <= f7->encoding() || i >= f28->encoding() || (i >= f10->encoding() && i <= f17->encoding())) {\n+      fsd(as_FloatRegister(i), Address(sp, wordSize * (offset ++)));\n+    }\n+  }\n+}\n+\n+void MacroAssembler::pop_call_clobbered_registers_except(RegSet exclude) {\n+  CompressibleRegion cr(this);\n+  int offset = 0;\n+  for (int i = 0; i < 32; i++) {\n+    if (i <= f7->encoding() || i >= f28->encoding() || (i >= f10->encoding() && i <= f17->encoding())) {\n+      fld(as_FloatRegister(i), Address(sp, wordSize * (offset ++)));\n+    }\n+  }\n+  addi(sp, sp, wordSize * 20);\n+\n+  pop_reg(RegSet::of(x7) + RegSet::range(x10, x17) + RegSet::range(x28, x31) - exclude, sp);\n+}\n+\n+\/\/ Push all the integer registers, except zr(x0) & sp(x2) & gp(x3) & tp(x4).\n+void MacroAssembler::pusha() {\n+  CompressibleRegion cr(this);\n+  push_reg(0xffffffe2, sp);\n+}\n+\n+\/\/ Pop all the integer registers, except zr(x0) & sp(x2) & gp(x3) & tp(x4).\n+void MacroAssembler::popa() {\n+  CompressibleRegion cr(this);\n+  pop_reg(0xffffffe2, sp);\n+}\n+\n+void MacroAssembler::push_CPU_state() {\n+  CompressibleRegion cr(this);\n+  \/\/ integer registers, except zr(x0) & ra(x1) & sp(x2) & gp(x3) & tp(x4)\n+  push_reg(0xffffffe0, sp);\n+\n+  \/\/ float registers\n+  addi(sp, sp, - 32 * wordSize);\n+  for (int i = 0; i < 32; i++) {\n+    fsd(as_FloatRegister(i), Address(sp, i * wordSize));\n+  }\n+}\n+\n+void MacroAssembler::pop_CPU_state() {\n+  CompressibleRegion cr(this);\n+\n+  \/\/ float registers\n+  for (int i = 0; i < 32; i++) {\n+    fld(as_FloatRegister(i), Address(sp, i * wordSize));\n+  }\n+  addi(sp, sp, 32 * wordSize);\n+\n+  \/\/ integer registers, except zr(x0) & ra(x1) & sp(x2) & gp(x3) & tp(x4)\n+  pop_reg(0xffffffe0, sp);\n+}\n+\n+static int patch_offset_in_jal(address branch, int64_t offset) {\n+  assert(is_imm_in_range(offset, 20, 1), \"offset is too large to be patched in one jal insrusction!\\n\");\n+  Assembler::patch(branch, 31, 31, (offset >> 20) & 0x1);                       \/\/ offset[20]    ==> branch[31]\n+  Assembler::patch(branch, 30, 21, (offset >> 1)  & 0x3ff);                     \/\/ offset[10:1]  ==> branch[30:21]\n+  Assembler::patch(branch, 20, 20, (offset >> 11) & 0x1);                       \/\/ offset[11]    ==> branch[20]\n+  Assembler::patch(branch, 19, 12, (offset >> 12) & 0xff);                      \/\/ offset[19:12] ==> branch[19:12]\n+  return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+}\n+\n+static int patch_offset_in_conditional_branch(address branch, int64_t offset) {\n+  assert(is_imm_in_range(offset, 12, 1), \"offset is too large to be patched in one beq\/bge\/bgeu\/blt\/bltu\/bne insrusction!\\n\");\n+  Assembler::patch(branch, 31, 31, (offset >> 12) & 0x1);                       \/\/ offset[12]    ==> branch[31]\n+  Assembler::patch(branch, 30, 25, (offset >> 5)  & 0x3f);                      \/\/ offset[10:5]  ==> branch[30:25]\n+  Assembler::patch(branch, 7,  7,  (offset >> 11) & 0x1);                       \/\/ offset[11]    ==> branch[7]\n+  Assembler::patch(branch, 11, 8,  (offset >> 1)  & 0xf);                       \/\/ offset[4:1]   ==> branch[11:8]\n+  return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+}\n+\n+static int patch_offset_in_pc_relative(address branch, int64_t offset) {\n+  const int PC_RELATIVE_INSTRUCTION_NUM = 2;                                    \/\/ auipc, addi\/jalr\/load\n+  Assembler::patch(branch, 31, 12, ((offset + 0x800) >> 12) & 0xfffff);         \/\/ Auipc.          offset[31:12]  ==> branch[31:12]\n+  Assembler::patch(branch + 4, 31, 20, offset & 0xfff);                         \/\/ Addi\/Jalr\/Load. offset[11:0]   ==> branch[31:20]\n+  return PC_RELATIVE_INSTRUCTION_NUM * NativeInstruction::instruction_size;\n+}\n+\n+static int patch_addr_in_movptr(address branch, address target) {\n+  const int MOVPTR_INSTRUCTIONS_NUM = 6;                                        \/\/ lui + addi + slli + addi + slli + addi\/jalr\/load\n+  int32_t lower = ((intptr_t)target << 35) >> 35;\n+  int64_t upper = ((intptr_t)target - lower) >> 29;\n+  Assembler::patch(branch + 0,  31, 12, upper & 0xfffff);                       \/\/ Lui.             target[48:29] + target[28] ==> branch[31:12]\n+  Assembler::patch(branch + 4,  31, 20, (lower >> 17) & 0xfff);                 \/\/ Addi.            target[28:17] ==> branch[31:20]\n+  Assembler::patch(branch + 12, 31, 20, (lower >> 6) & 0x7ff);                  \/\/ Addi.            target[16: 6] ==> branch[31:20]\n+  Assembler::patch(branch + 20, 31, 20, lower & 0x3f);                          \/\/ Addi\/Jalr\/Load.  target[ 5: 0] ==> branch[31:20]\n+  return MOVPTR_INSTRUCTIONS_NUM * NativeInstruction::instruction_size;\n+}\n+\n+static int patch_imm_in_li64(address branch, address target) {\n+  const int LI64_INSTRUCTIONS_NUM = 8;                                          \/\/ lui + addi + slli + addi + slli + addi + slli + addi\n+  int64_t lower = (intptr_t)target & 0xffffffff;\n+  lower = lower - ((lower << 44) >> 44);\n+  int64_t tmp_imm = ((uint64_t)((intptr_t)target & 0xffffffff00000000)) + (uint64_t)lower;\n+  int32_t upper =  (tmp_imm - (int32_t)lower) >> 32;\n+  int64_t tmp_upper = upper, tmp_lower = upper;\n+  tmp_lower = (tmp_lower << 52) >> 52;\n+  tmp_upper -= tmp_lower;\n+  tmp_upper >>= 12;\n+  \/\/ Load upper 32 bits. Upper = target[63:32], but if target[31] = 1 or (target[31:28] == 0x7ff && target[19] == 1),\n+  \/\/ upper = target[63:32] + 1.\n+  Assembler::patch(branch + 0,  31, 12, tmp_upper & 0xfffff);                       \/\/ Lui.\n+  Assembler::patch(branch + 4,  31, 20, tmp_lower & 0xfff);                         \/\/ Addi.\n+  \/\/ Load the rest 32 bits.\n+  Assembler::patch(branch + 12, 31, 20, ((int32_t)lower >> 20) & 0xfff);            \/\/ Addi.\n+  Assembler::patch(branch + 20, 31, 20, (((intptr_t)target << 44) >> 52) & 0xfff);  \/\/ Addi.\n+  Assembler::patch(branch + 28, 31, 20, (intptr_t)target & 0xff);                   \/\/ Addi.\n+  return LI64_INSTRUCTIONS_NUM * NativeInstruction::instruction_size;\n+}\n+\n+static int patch_imm_in_li32(address branch, int32_t target) {\n+  const int LI32_INSTRUCTIONS_NUM = 2;                                          \/\/ lui + addiw\n+  int64_t upper = (intptr_t)target;\n+  int32_t lower = (((int32_t)target) << 20) >> 20;\n+  upper -= lower;\n+  upper = (int32_t)upper;\n+  Assembler::patch(branch + 0,  31, 12, (upper >> 12) & 0xfffff);               \/\/ Lui.\n+  Assembler::patch(branch + 4,  31, 20, lower & 0xfff);                         \/\/ Addiw.\n+  return LI32_INSTRUCTIONS_NUM * NativeInstruction::instruction_size;\n+}\n+\n+static long get_offset_of_jal(address insn_addr) {\n+  assert_cond(insn_addr != NULL);\n+  long offset = 0;\n+  unsigned insn = *(unsigned*)insn_addr;\n+  long val = (long)Assembler::sextract(insn, 31, 12);\n+  offset |= ((val >> 19) & 0x1) << 20;\n+  offset |= (val & 0xff) << 12;\n+  offset |= ((val >> 8) & 0x1) << 11;\n+  offset |= ((val >> 9) & 0x3ff) << 1;\n+  offset = (offset << 43) >> 43;\n+  return offset;\n+}\n+\n+static long get_offset_of_conditional_branch(address insn_addr) {\n+  long offset = 0;\n+  assert_cond(insn_addr != NULL);\n+  unsigned insn = *(unsigned*)insn_addr;\n+  offset = (long)Assembler::sextract(insn, 31, 31);\n+  offset = (offset << 12) | (((long)(Assembler::sextract(insn, 7, 7) & 0x1)) << 11);\n+  offset = offset | (((long)(Assembler::sextract(insn, 30, 25) & 0x3f)) << 5);\n+  offset = offset | (((long)(Assembler::sextract(insn, 11, 8) & 0xf)) << 1);\n+  offset = (offset << 41) >> 41;\n+  return offset;\n+}\n+\n+static long get_offset_of_pc_relative(address insn_addr) {\n+  long offset = 0;\n+  assert_cond(insn_addr != NULL);\n+  offset = ((long)(Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12))) << 12;                                  \/\/ Auipc.\n+  offset += ((long)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                                         \/\/ Addi\/Jalr\/Load.\n+  offset = (offset << 32) >> 32;\n+  return offset;\n+}\n+\n+static address get_target_of_movptr(address insn_addr) {\n+  assert_cond(insn_addr != NULL);\n+  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 29;    \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 17;                        \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 6;                         \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20));                              \/\/ Addi\/Jalr\/Load.\n+  return (address) target_address;\n+}\n+\n+static address get_target_of_li64(address insn_addr) {\n+  assert_cond(insn_addr != NULL);\n+  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 44;    \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 32;                        \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 20;                        \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20)) << 8;                         \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[7], 31, 20));                              \/\/ Addi.\n+  return (address)target_address;\n+}\n+\n+static address get_target_of_li32(address insn_addr) {\n+  assert_cond(insn_addr != NULL);\n+  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 12;    \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                              \/\/ Addiw.\n+  return (address)target_address;\n+}\n+\n+\/\/ Patch any kind of instruction; there may be several instructions.\n+\/\/ Return the total length (in bytes) of the instructions.\n+int MacroAssembler::pd_patch_instruction_size(address branch, address target) {\n+  assert_cond(branch != NULL);\n+  int64_t offset = target - branch;\n+  if (NativeInstruction::is_jal_at(branch)) {                         \/\/ jal\n+    return patch_offset_in_jal(branch, offset);\n+  } else if (NativeInstruction::is_branch_at(branch)) {               \/\/ beq\/bge\/bgeu\/blt\/bltu\/bne\n+    return patch_offset_in_conditional_branch(branch, offset);\n+  } else if (NativeInstruction::is_pc_relative_at(branch)) {          \/\/ auipc, addi\/jalr\/load\n+    return patch_offset_in_pc_relative(branch, offset);\n+  } else if (NativeInstruction::is_movptr_at(branch)) {               \/\/ movptr\n+    return patch_addr_in_movptr(branch, target);\n+  } else if (NativeInstruction::is_li64_at(branch)) {                 \/\/ li64\n+    return patch_imm_in_li64(branch, target);\n+  } else if (NativeInstruction::is_li32_at(branch)) {                 \/\/ li32\n+    int64_t imm = (intptr_t)target;\n+    return patch_imm_in_li32(branch, (int32_t)imm);\n+  } else {\n+#ifdef ASSERT\n+    tty->print_cr(\"pd_patch_instruction_size: instruction 0x%x at \" INTPTR_FORMAT \" could not be patched!\\n\",\n+                  *(unsigned*)branch, p2i(branch));\n+    Disassembler::decode(branch - 16, branch + 16);\n+#endif\n+    ShouldNotReachHere();\n+    return -1;\n+  }\n+}\n+\n+address MacroAssembler::target_addr_for_insn(address insn_addr) {\n+  long offset = 0;\n+  assert_cond(insn_addr != NULL);\n+  if (NativeInstruction::is_jal_at(insn_addr)) {                     \/\/ jal\n+    offset = get_offset_of_jal(insn_addr);\n+  } else if (NativeInstruction::is_branch_at(insn_addr)) {           \/\/ beq\/bge\/bgeu\/blt\/bltu\/bne\n+    offset = get_offset_of_conditional_branch(insn_addr);\n+  } else if (NativeInstruction::is_pc_relative_at(insn_addr)) {      \/\/ auipc, addi\/jalr\/load\n+    offset = get_offset_of_pc_relative(insn_addr);\n+  } else if (NativeInstruction::is_movptr_at(insn_addr)) {           \/\/ movptr\n+    return get_target_of_movptr(insn_addr);\n+  } else if (NativeInstruction::is_li64_at(insn_addr)) {             \/\/ li64\n+    return get_target_of_li64(insn_addr);\n+  } else if (NativeInstruction::is_li32_at(insn_addr)) {             \/\/ li32\n+    return get_target_of_li32(insn_addr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return address(((uintptr_t)insn_addr + offset));\n+}\n+\n+int MacroAssembler::patch_oop(address insn_addr, address o) {\n+  \/\/ OOPs are either narrow (32 bits) or wide (48 bits).  We encode\n+  \/\/ narrow OOPs by setting the upper 16 bits in the first\n+  \/\/ instruction.\n+  if (NativeInstruction::is_li32_at(insn_addr)) {\n+    \/\/ Move narrow OOP\n+    narrowOop n = CompressedOops::encode((oop)o);\n+    return patch_imm_in_li32(insn_addr, (int32_t)n);\n+  } else if (NativeInstruction::is_movptr_at(insn_addr)) {\n+    \/\/ Move wide OOP\n+    return patch_addr_in_movptr(insn_addr, o);\n+  }\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+void MacroAssembler::reinit_heapbase() {\n+  if (UseCompressedOops) {\n+    if (Universe::is_fully_initialized()) {\n+      mv(xheapbase, Universe::narrow_ptrs_base());\n+    } else {\n+      int32_t offset = 0;\n+      la_patchable(xheapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()), offset);\n+      ld(xheapbase, Address(xheapbase, offset));\n+    }\n+  }\n+}\n+\n+void MacroAssembler::mv(Register Rd, Address dest) {\n+  assert(dest.getMode() == Address::literal, \"Address mode should be Address::literal\");\n+  code_section()->relocate(pc(), dest.rspec());\n+  movptr(Rd, dest.target());\n+}\n+\n+void MacroAssembler::mv(Register Rd, address addr) {\n+  \/\/ Here in case of use with relocation, use fix length instruction\n+  \/\/ movptr instead of li\n+  movptr(Rd, addr);\n+}\n+\n+void MacroAssembler::mv(Register Rd, RegisterOrConstant src) {\n+  if (src.is_register()) {\n+    mv(Rd, src.as_register());\n+  } else {\n+    mv(Rd, src.as_constant());\n+  }\n+}\n+\n+void MacroAssembler::andrw(Register Rd, Register Rs1, Register Rs2) {\n+  andr(Rd, Rs1, Rs2);\n+  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n+  \/\/ and the result is stored in Rd\n+  addw(Rd, Rd, zr);\n+}\n+\n+void MacroAssembler::orrw(Register Rd, Register Rs1, Register Rs2) {\n+  orr(Rd, Rs1, Rs2);\n+  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n+  \/\/ and the result is stored in Rd\n+  addw(Rd, Rd, zr);\n+}\n+\n+void MacroAssembler::xorrw(Register Rd, Register Rs1, Register Rs2) {\n+  xorr(Rd, Rs1, Rs2);\n+  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n+  \/\/ and the result is stored in Rd\n+  addw(Rd, Rd, zr);\n+}\n+\n+\/\/ Note: load_unsigned_short used to be called load_unsigned_word.\n+int MacroAssembler::load_unsigned_short(Register dst, Address src) {\n+  int off = offset();\n+  lhu(dst, src);\n+  return off;\n+}\n+\n+int MacroAssembler::load_unsigned_byte(Register dst, Address src) {\n+  int off = offset();\n+  lbu(dst, src);\n+  return off;\n+}\n+\n+int MacroAssembler::load_signed_short(Register dst, Address src) {\n+  int off = offset();\n+  lh(dst, src);\n+  return off;\n+}\n+\n+int MacroAssembler::load_signed_byte(Register dst, Address src) {\n+  int off = offset();\n+  lb(dst, src);\n+  return off;\n+}\n+\n+void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {\n+  switch (size_in_bytes) {\n+    case  8:  ld(dst, src); break;\n+    case  4:  is_signed ? lw(dst, src) : lwu(dst, src); break;\n+    case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;\n+    case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;\n+    default:  ShouldNotReachHere();\n+  }\n+}\n+\n+void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {\n+  switch (size_in_bytes) {\n+    case  8:  sd(src, dst); break;\n+    case  4:  sw(src, dst); break;\n+    case  2:  sh(src, dst); break;\n+    case  1:  sb(src, dst); break;\n+    default:  ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ reverse bytes in halfword in lower 16 bits and sign-extend\n+\/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (sign-extend to 64 bits)\n+void MacroAssembler::revb_h_h(Register Rd, Register Rs, Register tmp) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srai(Rd, Rd, 48);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp);\n+  assert_different_registers(Rd, tmp);\n+  srli(tmp, Rs, 8);\n+  andi(tmp, tmp, 0xFF);\n+  slli(Rd, Rs, 56);\n+  srai(Rd, Rd, 48); \/\/ sign-extend\n+  orr(Rd, Rd, tmp);\n+}\n+\n+\/\/ reverse bytes in lower word and sign-extend\n+\/\/ Rd[31:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] (sign-extend to 64 bits)\n+void MacroAssembler::revb_w_w(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srai(Rd, Rd, 32);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb_h_w_u(Rd, Rs, tmp1, tmp2);\n+  slli(tmp2, Rd, 48);\n+  srai(tmp2, tmp2, 32); \/\/ sign-extend\n+  srli(Rd, Rd, 16);\n+  orr(Rd, Rd, tmp2);\n+}\n+\n+\/\/ reverse bytes in halfword in lower 16 bits and zero-extend\n+\/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n+void MacroAssembler::revb_h_h_u(Register Rd, Register Rs, Register tmp) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srli(Rd, Rd, 48);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp);\n+  assert_different_registers(Rd, tmp);\n+  srli(tmp, Rs, 8);\n+  andi(tmp, tmp, 0xFF);\n+  andi(Rd, Rs, 0xFF);\n+  slli(Rd, Rd, 8);\n+  orr(Rd, Rd, tmp);\n+}\n+\n+\/\/ reverse bytes in halfwords in lower 32 bits and zero-extend\n+\/\/ Rd[31:0] = Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n+void MacroAssembler::revb_h_w_u(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    rori(Rd, Rd, 32);\n+    roriw(Rd, Rd, 16);\n+    zext_w(Rd, Rd);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  srli(tmp2, Rs, 16);\n+  revb_h_h_u(tmp2, tmp2, tmp1);\n+  revb_h_h_u(Rd, Rs, tmp1);\n+  slli(tmp2, tmp2, 16);\n+  orr(Rd, Rd, tmp2);\n+}\n+\n+\/\/ This method is only used for revb_h\n+\/\/ Rd = Rs[47:0] Rs[55:48] Rs[63:56]\n+void MacroAssembler::revb_h_helper(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1);\n+  srli(tmp1, Rs, 48);\n+  andi(tmp2, tmp1, 0xFF);\n+  slli(tmp2, tmp2, 8);\n+  srli(tmp1, tmp1, 8);\n+  orr(tmp1, tmp1, tmp2);\n+  slli(Rd, Rs, 16);\n+  orr(Rd, Rd, tmp1);\n+}\n+\n+\/\/ reverse bytes in each halfword\n+\/\/ Rd[63:0] = Rs[55:48] Rs[63:56] Rs[39:32] Rs[47:40] Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8]\n+void MacroAssembler::revb_h(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    assert_different_registers(Rs, tmp1);\n+    assert_different_registers(Rd, tmp1);\n+    rev8(Rd, Rs);\n+    zext_w(tmp1, Rd);\n+    roriw(tmp1, tmp1, 16);\n+    slli(tmp1, tmp1, 32);\n+    srli(Rd, Rd, 32);\n+    roriw(Rd, Rd, 16);\n+    zext_w(Rd, Rd);\n+    orr(Rd, Rd, tmp1);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb_h_helper(Rd, Rs, tmp1, tmp2);\n+  for (int i = 0; i < 3; ++i) {\n+    revb_h_helper(Rd, Rd, tmp1, tmp2);\n+  }\n+}\n+\n+\/\/ reverse bytes in each word\n+\/\/ Rd[63:0] = Rs[39:32] Rs[47:40] Rs[55:48] Rs[63:56] Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24]\n+void MacroAssembler::revb_w(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    rori(Rd, Rd, 32);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb(Rd, Rs, tmp1, tmp2);\n+  ror_imm(Rd, Rd, 32);\n+}\n+\n+\/\/ reverse bytes in doubleword\n+\/\/ Rd[63:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] Rs[39:32] Rs[47,40] Rs[55,48] Rs[63:56]\n+void MacroAssembler::revb(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  andi(tmp1, Rs, 0xFF);\n+  slli(tmp1, tmp1, 8);\n+  for (int step = 8; step < 56; step += 8) {\n+    srli(tmp2, Rs, step);\n+    andi(tmp2, tmp2, 0xFF);\n+    orr(tmp1, tmp1, tmp2);\n+    slli(tmp1, tmp1, 8);\n+  }\n+  srli(Rd, Rs, 56);\n+  andi(Rd, Rd, 0xFF);\n+  orr(Rd, tmp1, Rd);\n+}\n+\n+\/\/ rotate right with shift bits\n+void MacroAssembler::ror_imm(Register dst, Register src, uint32_t shift, Register tmp)\n+{\n+  if (UseRVB) {\n+    rori(dst, src, shift);\n+    return;\n+  }\n+\n+  assert_different_registers(dst, tmp);\n+  assert_different_registers(src, tmp);\n+  assert(shift < 64, \"shift amount must be < 64\");\n+  slli(tmp, src, 64 - shift);\n+  srli(dst, src, shift);\n+  orr(dst, dst, tmp);\n+}\n+\n+void MacroAssembler::andi(Register Rd, Register Rn, int64_t imm, Register tmp) {\n+  if (is_imm_in_range(imm, 12, 0)) {\n+    and_imm12(Rd, Rn, imm);\n+  } else {\n+    assert_different_registers(Rn, tmp);\n+    li(tmp, imm);\n+    andr(Rd, Rn, tmp);\n+  }\n+}\n+\n+void MacroAssembler::orptr(Address adr, RegisterOrConstant src, Register tmp1, Register tmp2) {\n+  ld(tmp1, adr);\n+  if (src.is_register()) {\n+    orr(tmp1, tmp1, src.as_register());\n+  } else {\n+    if (is_imm_in_range(src.as_constant(), 12, 0)) {\n+      ori(tmp1, tmp1, src.as_constant());\n+    } else {\n+      assert_different_registers(tmp1, tmp2);\n+      li(tmp2, src.as_constant());\n+      orr(tmp1, tmp1, tmp2);\n+    }\n+  }\n+  sd(tmp1, adr);\n+}\n+\n+void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp, Label &L) {\n+  if (UseCompressedClassPointers) {\n+      lwu(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (Universe::narrow_klass_base() == NULL) {\n+      slli(tmp, tmp, Universe::narrow_klass_shift());\n+      beq(trial_klass, tmp, L);\n+      return;\n+    }\n+    decode_klass_not_null(tmp);\n+  } else {\n+    ld(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  }\n+  beq(trial_klass, tmp, L);\n+}\n+\n+\/\/ Move an oop into a register.  immediate is true if we want\n+\/\/ immediate instructions, i.e. we are not going to patch this\n+\/\/ instruction while the code is being executed by another thread.  In\n+\/\/ that case we can use move immediates rather than the constant pool.\n+void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {\n+  int oop_index;\n+  if (obj == NULL) {\n+    oop_index = oop_recorder()->allocate_oop_index(obj);\n+  } else {\n+#ifdef ASSERT\n+    {\n+      ThreadInVMfromUnknown tiv;\n+      assert(Universe::heap()->is_in_reserved(JNIHandles::resolve(obj)), \"should be real oop\");\n+    }\n+#endif\n+    oop_index = oop_recorder()->find_index(obj);\n+  }\n+  RelocationHolder rspec = oop_Relocation::spec(oop_index);\n+  if (!immediate) {\n+    address dummy = address(uintptr_t(pc()) & -wordSize); \/\/ A nearby aligned address\n+    ld_constant(dst, Address(dummy, rspec));\n+  } else\n+    mv(dst, Address((address)obj, rspec));\n+}\n+\n+\/\/ Move a metadata address into a register.\n+void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {\n+  int oop_index;\n+  if (obj == NULL) {\n+    oop_index = oop_recorder()->allocate_metadata_index(obj);\n+  } else {\n+    oop_index = oop_recorder()->find_index(obj);\n+  }\n+  RelocationHolder rspec = metadata_Relocation::spec(oop_index);\n+  mv(dst, Address((address)obj, rspec));\n+}\n+\n+\/\/ Writes to stack successive pages until offset reached to check for\n+\/\/ stack overflow + shadow pages.  This clobbers tmp.\n+void MacroAssembler::bang_stack_size(Register size, Register tmp) {\n+  assert_different_registers(tmp, size, t0);\n+  \/\/ Bang stack for total size given plus shadow page size.\n+  \/\/ Bang one page at a time because large size can bang beyond yellow and\n+  \/\/ red zones.\n+  mv(t0, os::vm_page_size());\n+  Label loop;\n+  bind(loop);\n+  sub(tmp, sp, t0);\n+  subw(size, size, t0);\n+  sd(size, Address(tmp));\n+  bgtz(size, loop);\n+\n+  \/\/ Bang down shadow pages too.\n+  \/\/ At this point, (tmp-0) is the last address touched, so don't\n+  \/\/ touch it again.  (It was touched as (tmp-pagesize) but then tmp\n+  \/\/ was post-decremented.)  Skip this address by starting at i=1, and\n+  \/\/ touch a few more pages below.  N.B.  It is important to touch all\n+  \/\/ the way down to and including i=StackShadowPages.\n+  for (int i = 0; i < (int)(JavaThread::stack_shadow_zone_size() \/ os::vm_page_size()) - 1; i++) {\n+    \/\/ this could be any sized move but this is can be a debugging crumb\n+    \/\/ so the bigger the better.\n+    sub(tmp, tmp, os::vm_page_size());\n+    sd(size, Address(tmp, 0));\n+  }\n+}\n+\n+SkipIfEqual::SkipIfEqual(MacroAssembler* masm, const bool* flag_addr, bool value) {\n+  assert_cond(masm != NULL);\n+  int32_t offset = 0;\n+  _masm = masm;\n+  _masm->la_patchable(t0, ExternalAddress((address)flag_addr), offset);\n+  _masm->lbu(t0, Address(t0, offset));\n+  _masm->beqz(t0, _label);\n+}\n+\n+SkipIfEqual::~SkipIfEqual() {\n+  assert_cond(_masm != NULL);\n+  _masm->bind(_label);\n+  _masm = NULL;\n+}\n+\n+void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {\n+  const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n+  ld(dst, Address(xmethod, Method::const_offset()));\n+  ld(dst, Address(dst, ConstMethod::constants_offset()));\n+  ld(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));\n+  ld(dst, Address(dst, mirror_offset));\n+  resolve_oop_handle(dst, tmp);\n+}\n+\n+void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {\n+  \/\/ OopHandle::resolve is an indirection.\n+  assert_different_registers(result, tmp);\n+  access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);\n+}\n+\n+void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,\n+                                    Register dst, Address src,\n+                                    Register tmp1, Register thread_tmp) {\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  decorators = AccessInternal::decorator_fixup(decorators);\n+  bool as_raw = (decorators & AS_RAW) != 0;\n+  if (as_raw) {\n+    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+  } else {\n+    bs->load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+  }\n+}\n+\n+void MacroAssembler::null_check(Register reg, int offset) {\n+  if (needs_explicit_null_check(offset)) {\n+    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ accessing M[reg] w\/o changing any registers\n+    \/\/ NOTE: this is plenty to provoke a segv\n+    ld(zr, Address(reg, 0));\n+  } else {\n+    \/\/ nothing to do, (later) access of M[reg + offset]\n+    \/\/ will provoke OS NULL exception if reg = NULL\n+  }\n+}\n+\n+void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,\n+                                     Address dst, Register src,\n+                                     Register tmp1, Register thread_tmp) {\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  decorators = AccessInternal::decorator_fixup(decorators);\n+  bool as_raw = (decorators & AS_RAW) != 0;\n+  if (as_raw) {\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+  }\n+}\n+\n+\/\/ Algorithm must match CompressedOops::encode.\n+void MacroAssembler::encode_heap_oop(Register d, Register s) {\n+  verify_oop(s, \"broken oop in encode_heap_oop\");\n+  if (Universe::narrow_oop_base() == NULL) {\n+    if (Universe::narrow_oop_shift() != 0) {\n+      assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), \"decode alg wrong\");\n+      srli(d, s, LogMinObjAlignmentInBytes);\n+    } else {\n+      mv(d, s);\n+    }\n+  } else {\n+    Label notNull;\n+    sub(d, s, xheapbase);\n+    bgez(d, notNull);\n+    mv(d, zr);\n+    bind(notNull);\n+    if (Universe::narrow_oop_shift() != 0) {\n+      assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), \"decode alg wrong\");\n+      srli(d, d, Universe::narrow_oop_shift());\n+    }\n+  }\n+}\n+\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    lwu(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    decode_klass_not_null(dst);\n+  } else {\n+    ld(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  \/\/ FIXME: Should this be a store release? concurrent gcs assumes\n+  \/\/ klass length is valid if klass field is not null.\n+  if (UseCompressedClassPointers) {\n+    encode_klass_not_null(src);\n+    sw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    sd(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::store_klass_gap(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    \/\/ Store to klass gap in destination\n+    sw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n+  }\n+}\n+\n+void  MacroAssembler::decode_klass_not_null(Register r) {\n+  decode_klass_not_null(r, r);\n+}\n+\n+void MacroAssembler::decode_klass_not_null(Register dst, Register src, Register tmp) {\n+  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n+\n+  if (Universe::narrow_klass_base() == NULL) {\n+    if (Universe::narrow_klass_shift() != 0) {\n+      assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), \"decode alg wrong\");\n+      slli(dst, src, LogKlassAlignmentInBytes);\n+    } else {\n+      mv(dst, src);\n+    }\n+    return;\n+  }\n+\n+  Register xbase = dst;\n+  if (dst == src) {\n+    xbase = tmp;\n+  }\n+\n+  assert_different_registers(src, xbase);\n+  li(xbase, (uintptr_t)Universe::narrow_klass_base());\n+\n+  if (Universe::narrow_klass_shift() != 0) {\n+    assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), \"decode alg wrong\");\n+    assert_different_registers(t0, xbase);\n+    shadd(dst, src, xbase, t0, LogKlassAlignmentInBytes);\n+  } else {\n+    add(dst, xbase, src);\n+  }\n+\n+  if (xbase == xheapbase) { reinit_heapbase(); }\n+}\n+\n+void MacroAssembler::encode_klass_not_null(Register r) {\n+  encode_klass_not_null(r, r);\n+}\n+\n+void MacroAssembler::encode_klass_not_null(Register dst, Register src, Register tmp) {\n+  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n+\n+  if (Universe::narrow_klass_base() == NULL) {\n+    if (Universe::narrow_klass_shift() != 0) {\n+      assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), \"decode alg wrong\");\n+      srli(dst, src, LogKlassAlignmentInBytes);\n+    } else {\n+      mv(dst, src);\n+    }\n+    return;\n+  }\n+\n+  if (((uint64_t)(uintptr_t)Universe::narrow_klass_base() & 0xffffffff) == 0 &&\n+      Universe::narrow_klass_shift() == 0) {\n+    zero_extend(dst, src, 32);\n+    return;\n+  }\n+\n+  Register xbase = dst;\n+  if (dst == src) {\n+    xbase = tmp;\n+  }\n+\n+  assert_different_registers(src, xbase);\n+  li(xbase, (intptr_t)Universe::narrow_klass_base());\n+  sub(dst, src, xbase);\n+  if (Universe::narrow_klass_shift() != 0) {\n+    assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), \"decode alg wrong\");\n+    srli(dst, dst, LogKlassAlignmentInBytes);\n+  }\n+  if (xbase == xheapbase) {\n+    reinit_heapbase();\n+  }\n+}\n+\n+void  MacroAssembler::decode_heap_oop_not_null(Register r) {\n+  decode_heap_oop_not_null(r, r);\n+}\n+\n+void MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {\n+  assert(UseCompressedOops, \"should only be used for compressed headers\");\n+  assert(Universe::heap() != NULL, \"java heap should be initialized\");\n+  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n+  \/\/ Also do not verify_oop as this is called by verify_oop.\n+  if (Universe::narrow_oop_shift() != 0) {\n+    assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), \"decode alg wrong\");\n+    slli(dst, src, LogMinObjAlignmentInBytes);\n+    if (Universe::narrow_oop_base() != NULL) {\n+      add(dst, xheapbase, dst);\n+    }\n+  } else {\n+    assert(Universe::narrow_oop_base() == NULL, \"sanity\");\n+    mv(dst, src);\n+  }\n+}\n+\n+void  MacroAssembler::decode_heap_oop(Register d, Register s) {\n+  if (Universe::narrow_oop_base() == NULL) {\n+    if (Universe::narrow_oop_shift() != 0 || d != s) {\n+      slli(d, s, Universe::narrow_oop_shift());\n+    }\n+  } else {\n+    Label done;\n+    mv(d, s);\n+    beqz(s, done);\n+    shadd(d, s, xheapbase, d, LogMinObjAlignmentInBytes);\n+    bind(done);\n+  }\n+  verify_oop(d, \"broken oop in decode_heap_oop\");\n+}\n+\n+void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,\n+                                    Register thread_tmp, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+}\n+\n+void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,\n+                                   Register thread_tmp, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+}\n+\n+void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,\n+                                            Register thread_tmp, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL, dst, src, tmp1, thread_tmp);\n+}\n+\n+\/\/ Used for storing NULLs.\n+void MacroAssembler::store_heap_oop_null(Address dst) {\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+}\n+\n+int MacroAssembler::corrected_idivl(Register result, Register rs1, Register rs2,\n+                                    bool want_remainder)\n+{\n+  \/\/ Full implementation of Java idiv and irem.  The function\n+  \/\/ returns the (pc) offset of the div instruction - may be needed\n+  \/\/ for implicit exceptions.\n+  \/\/\n+  \/\/ input : rs1: dividend\n+  \/\/         rs2: divisor\n+  \/\/\n+  \/\/ result: either\n+  \/\/         quotient  (= rs1 idiv rs2)\n+  \/\/         remainder (= rs1 irem rs2)\n+\n+\n+  int idivl_offset = offset();\n+  if (!want_remainder) {\n+    divw(result, rs1, rs2);\n+  } else {\n+    remw(result, rs1, rs2); \/\/ result = rs1 % rs2;\n+  }\n+  return idivl_offset;\n+}\n+\n+int MacroAssembler::corrected_idivq(Register result, Register rs1, Register rs2,\n+                                    bool want_remainder)\n+{\n+  \/\/ Full implementation of Java ldiv and lrem.  The function\n+  \/\/ returns the (pc) offset of the div instruction - may be needed\n+  \/\/ for implicit exceptions.\n+  \/\/\n+  \/\/ input : rs1: dividend\n+  \/\/         rs2: divisor\n+  \/\/\n+  \/\/ result: either\n+  \/\/         quotient  (= rs1 idiv rs2)\n+  \/\/         remainder (= rs1 irem rs2)\n+\n+  int idivq_offset = offset();\n+  if (!want_remainder) {\n+    div(result, rs1, rs2);\n+  } else {\n+    rem(result, rs1, rs2); \/\/ result = rs1 % rs2;\n+  }\n+  return idivq_offset;\n+}\n+\n+\/\/ Look up the method for a megamorpic invkkeinterface call.\n+\/\/ The target method is determined by <intf_klass, itable_index>.\n+\/\/ The receiver klass is in recv_klass.\n+\/\/ On success, the result will be in method_result, and execution falls through.\n+\/\/ On failure, execution transfers to the given label.\n+void MacroAssembler::lookup_interface_method(Register recv_klass,\n+                                             Register intf_klass,\n+                                             RegisterOrConstant itable_index,\n+                                             Register method_result,\n+                                             Register scan_tmp,\n+                                             Label& L_no_such_interface,\n+                                             bool return_method) {\n+  assert_different_registers(recv_klass, intf_klass, scan_tmp);\n+  assert_different_registers(method_result, intf_klass, scan_tmp);\n+  assert(recv_klass != method_result || !return_method,\n+         \"recv_klass can be destroyed when mehtid isn't needed\");\n+  assert(itable_index.is_constant() || itable_index.as_register() == method_result,\n+         \"caller must be same register for non-constant itable index as for method\");\n+\n+  \/\/ Compute start of first itableOffsetEntry (which is at the end of the vtable).\n+  int vtable_base = in_bytes(Klass::vtable_start_offset());\n+  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int scan_step   = itableOffsetEntry::size() * wordSize;\n+  int vte_size    = vtableEntry::size_in_bytes();\n+  assert(vte_size == wordSize, \"else adjust times_vte_scale\");\n+\n+  lwu(scan_tmp, Address(recv_klass, Klass::vtable_length_offset()));\n+\n+  \/\/ %%% Could store the aligned, prescaled offset in the klassoop.\n+  shadd(scan_tmp, scan_tmp, recv_klass, scan_tmp, 3);\n+  add(scan_tmp, scan_tmp, vtable_base);\n+\n+  if (return_method) {\n+    \/\/ Adjust recv_klass by scaled itable_index, so we can free itable_index.\n+    assert(itableMethodEntry::size() * wordSize == wordSize, \"adjust the scaling in the code below\");\n+    if (itable_index.is_register()) {\n+      slli(t0, itable_index.as_register(), 3);\n+    } else {\n+      li(t0, itable_index.as_constant() << 3);\n+    }\n+    add(recv_klass, recv_klass, t0);\n+    if (itentry_off) {\n+      add(recv_klass, recv_klass, itentry_off);\n+    }\n+  }\n+\n+  Label search, found_method;\n+\n+  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset_in_bytes()));\n+  beq(intf_klass, method_result, found_method);\n+  bind(search);\n+  \/\/ Check that the previous entry is non-null. A null entry means that\n+  \/\/ the receiver class doens't implement the interface, and wasn't the\n+  \/\/ same as when the caller was compiled.\n+  beqz(method_result, L_no_such_interface, \/* is_far *\/ true);\n+  addi(scan_tmp, scan_tmp, scan_step);\n+  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset_in_bytes()));\n+  bne(intf_klass, method_result, search);\n+\n+  bind(found_method);\n+\n+  \/\/ Got a hit.\n+  if (return_method) {\n+    lwu(scan_tmp, Address(scan_tmp, itableOffsetEntry::offset_offset_in_bytes()));\n+    add(method_result, recv_klass, scan_tmp);\n+    ld(method_result, Address(method_result));\n+  }\n+}\n+\n+\/\/ virtual method calling\n+void MacroAssembler::lookup_virtual_method(Register recv_klass,\n+                                           RegisterOrConstant vtable_index,\n+                                           Register method_result) {\n+  const int base = in_bytes(Klass::vtable_start_offset());\n+  assert(vtableEntry::size() * wordSize == 8,\n+         \"adjust the scaling in the code below\");\n+  int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();\n+\n+  if (vtable_index.is_register()) {\n+    shadd(method_result, vtable_index.as_register(), recv_klass, method_result, LogBytesPerWord);\n+    ld(method_result, Address(method_result, vtable_offset_in_bytes));\n+  } else {\n+    vtable_offset_in_bytes += vtable_index.as_constant() * wordSize;\n+    ld(method_result, form_address(method_result, recv_klass, vtable_offset_in_bytes));\n+  }\n+}\n+\n+void MacroAssembler::membar(uint32_t order_constraint) {\n+  address prev = pc() - NativeMembar::instruction_size;\n+  address last = code()->last_insn();\n+\n+  if (last != NULL && nativeInstruction_at(last)->is_membar() && prev == last) {\n+    NativeMembar *bar = NativeMembar_at(prev);\n+    \/\/ We are merging two memory barrier instructions.  On RISCV we\n+    \/\/ can do this simply by ORing them together.\n+    bar->set_kind(bar->get_kind() | order_constraint);\n+    BLOCK_COMMENT(\"merged membar\");\n+  } else {\n+    code()->set_last_insn(pc());\n+\n+    uint32_t predecessor = 0;\n+    uint32_t successor = 0;\n+\n+    membar_mask_to_pred_succ(order_constraint, predecessor, successor);\n+    fence(predecessor, successor);\n+  }\n+}\n+\n+\/\/ Form an addres from base + offset in Rd. Rd my or may not\n+\/\/ actually be used: you must use the Address that is returned. It\n+\/\/ is up to you to ensure that the shift provided mathces the size\n+\/\/ of your data.\n+Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset) {\n+  if (is_offset_in_range(byte_offset, 12)) { \/\/ 12: imm in range 2^12\n+    return Address(base, byte_offset);\n+  }\n+\n+  \/\/ Do it the hard way\n+  mv(Rd, byte_offset);\n+  add(Rd, base, Rd);\n+  return Address(Rd);\n+}\n+\n+void MacroAssembler::check_klass_subtype(Register sub_klass,\n+                                         Register super_klass,\n+                                         Register tmp_reg,\n+                                         Label& L_success) {\n+  Label L_failure;\n+  check_klass_subtype_fast_path(sub_klass, super_klass, tmp_reg, &L_success, &L_failure, NULL);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, tmp_reg, noreg, &L_success, NULL);\n+  bind(L_failure);\n+}\n+\n+void MacroAssembler::safepoint_poll(Label& slow_path) {\n+  if (SafepointMechanism::uses_thread_local_poll()) {\n+    ld(t1, Address(xthread, Thread::polling_page_offset()));\n+    andi(t0, t1, SafepointMechanism::poll_bit());\n+    bnez(t0, slow_path);\n+  } else {\n+    int32_t offset = 0;\n+    la_patchable(t0, ExternalAddress(SafepointSynchronize::address_of_state()), offset);\n+    lwu(t0, Address(t0, offset));\n+    assert(SafepointSynchronize::_not_synchronized == 0, \"rewrite this code\");\n+    bnez(t0, slow_path);\n+  }\n+}\n+\n+void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,\n+                                Label &succeed, Label *fail) {\n+  \/\/ oldv holds comparison value\n+  \/\/ newv holds value to write in exchange\n+  \/\/ addr identifies memory word to compare against\/update\n+  Label retry_load, nope;\n+  bind(retry_load);\n+  \/\/ Load reserved from the memory location\n+  lr_d(tmp, addr, Assembler::aqrl);\n+  \/\/ Fail and exit if it is not what we expect\n+  bne(tmp, oldv, nope);\n+  \/\/ If the store conditional succeeds, tmp will be zero\n+  sc_d(tmp, newv, addr, Assembler::rl);\n+  beqz(tmp, succeed);\n+  \/\/ Retry only when the store conditional failed\n+  j(retry_load);\n+\n+  bind(nope);\n+  membar(AnyAny);\n+  mv(oldv, tmp);\n+  if (fail != NULL) {\n+    j(*fail);\n+  }\n+}\n+\n+void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,\n+                                        Label &succeed, Label *fail) {\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"assumption\");\n+  cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);\n+}\n+\n+void MacroAssembler::load_reserved(Register addr,\n+                                   enum operand_size size,\n+                                   Assembler::Aqrl acquire) {\n+  switch (size) {\n+    case int64:\n+      lr_d(t0, addr, acquire);\n+      break;\n+    case int32:\n+      lr_w(t0, addr, acquire);\n+      break;\n+    case uint32:\n+      lr_w(t0, addr, acquire);\n+      zero_extend(t0, t0, 32);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void MacroAssembler::store_conditional(Register addr,\n+                                       Register new_val,\n+                                       enum operand_size size,\n+                                       Assembler::Aqrl release) {\n+  switch (size) {\n+    case int64:\n+      sc_d(t0, new_val, addr, release);\n+      break;\n+    case int32:\n+    case uint32:\n+      sc_w(t0, new_val, addr, release);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\n+void MacroAssembler::cmpxchg_narrow_value_helper(Register addr, Register expected,\n+                                                 Register new_val,\n+                                                 enum operand_size size,\n+                                                 Register tmp1, Register tmp2, Register tmp3) {\n+  assert(size == int8 || size == int16, \"unsupported operand size\");\n+\n+  Register aligned_addr = t1, shift = tmp1, mask = tmp2, not_mask = tmp3;\n+\n+  andi(shift, addr, 3);\n+  slli(shift, shift, 3);\n+\n+  andi(aligned_addr, addr, ~3);\n+\n+  if (size == int8) {\n+    addi(mask, zr, 0xff);\n+  } else {\n+    \/\/ size == int16 case\n+    addi(mask, zr, -1);\n+    zero_extend(mask, mask, 16);\n+  }\n+  sll(mask, mask, shift);\n+\n+  xori(not_mask, mask, -1);\n+\n+  sll(expected, expected, shift);\n+  andr(expected, expected, mask);\n+\n+  sll(new_val, new_val, shift);\n+  andr(new_val, new_val, mask);\n+}\n+\n+\/\/ cmpxchg_narrow_value will kill t0, t1, expected, new_val and tmps.\n+\/\/ It's designed to implement compare and swap byte\/boolean\/char\/short by lr.w\/sc.w,\n+\/\/ which are forced to work with 4-byte aligned address.\n+void MacroAssembler::cmpxchg_narrow_value(Register addr, Register expected,\n+                                          Register new_val,\n+                                          enum operand_size size,\n+                                          Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                                          Register result, bool result_as_bool,\n+                                          Register tmp1, Register tmp2, Register tmp3) {\n+  Register aligned_addr = t1, shift = tmp1, mask = tmp2, not_mask = tmp3, old = result, tmp = t0;\n+  assert_different_registers(addr, old, mask, not_mask, new_val, expected, shift, tmp);\n+  cmpxchg_narrow_value_helper(addr, expected, new_val, size, tmp1, tmp2, tmp3);\n+\n+  Label retry, fail, done;\n+\n+  bind(retry);\n+  lr_w(old, aligned_addr, acquire);\n+  andr(tmp, old, mask);\n+  bne(tmp, expected, fail);\n+\n+  andr(tmp, old, not_mask);\n+  orr(tmp, tmp, new_val);\n+  sc_w(tmp, tmp, aligned_addr, release);\n+  bnez(tmp, retry);\n+\n+  if (result_as_bool) {\n+    addi(result, zr, 1);\n+    j(done);\n+\n+    bind(fail);\n+    mv(result, zr);\n+\n+    bind(done);\n+  } else {\n+    andr(tmp, old, mask);\n+\n+    bind(fail);\n+    srl(result, tmp, shift);\n+\n+    if (size == int8) {\n+      sign_extend(result, result, 8);\n+    } else {\n+      \/\/ size == int16 case\n+      sign_extend(result, result, 16);\n+    }\n+  }\n+}\n+\n+\/\/ weak_cmpxchg_narrow_value is a weak version of cmpxchg_narrow_value, to implement\n+\/\/ the weak CAS stuff. The major difference is that it just failed when store conditional\n+\/\/ failed.\n+void MacroAssembler::weak_cmpxchg_narrow_value(Register addr, Register expected,\n+                                               Register new_val,\n+                                               enum operand_size size,\n+                                               Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                                               Register result,\n+                                               Register tmp1, Register tmp2, Register tmp3) {\n+  Register aligned_addr = t1, shift = tmp1, mask = tmp2, not_mask = tmp3, old = result, tmp = t0;\n+  assert_different_registers(addr, old, mask, not_mask, new_val, expected, shift, tmp);\n+  cmpxchg_narrow_value_helper(addr, expected, new_val, size, tmp1, tmp2, tmp3);\n+\n+  Label succ, fail, done;\n+\n+  lr_w(old, aligned_addr, acquire);\n+  andr(tmp, old, mask);\n+  bne(tmp, expected, fail);\n+\n+  andr(tmp, old, not_mask);\n+  orr(tmp, tmp, new_val);\n+  sc_w(tmp, tmp, aligned_addr, release);\n+  beqz(tmp, succ);\n+\n+  bind(fail);\n+  addi(result, zr, 1);\n+  j(done);\n+\n+  bind(succ);\n+  mv(result, zr);\n+\n+  bind(done);\n+}\n+\n+void MacroAssembler::cmpxchg(Register addr, Register expected,\n+                             Register new_val,\n+                             enum operand_size size,\n+                             Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                             Register result, bool result_as_bool) {\n+  assert(size != int8 && size != int16, \"unsupported operand size\");\n+\n+  Label retry_load, done, ne_done;\n+  bind(retry_load);\n+  load_reserved(addr, size, acquire);\n+  bne(t0, expected, ne_done);\n+  store_conditional(addr, new_val, size, release);\n+  bnez(t0, retry_load);\n+\n+  \/\/ equal, succeed\n+  if (result_as_bool) {\n+    li(result, 1);\n+  } else {\n+    mv(result, expected);\n+  }\n+  j(done);\n+\n+  \/\/ not equal, failed\n+  bind(ne_done);\n+  if (result_as_bool) {\n+    mv(result, zr);\n+  } else {\n+    mv(result, t0);\n+  }\n+\n+  bind(done);\n+}\n+\n+void MacroAssembler::cmpxchg_weak(Register addr, Register expected,\n+                                  Register new_val,\n+                                  enum operand_size size,\n+                                  Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                                  Register result) {\n+  Label fail, done, sc_done;\n+  load_reserved(addr, size, acquire);\n+  bne(t0, expected, fail);\n+  store_conditional(addr, new_val, size, release);\n+  beqz(t0, sc_done);\n+\n+  \/\/ fail\n+  bind(fail);\n+  li(result, 1);\n+  j(done);\n+\n+  \/\/ sc_done\n+  bind(sc_done);\n+  mv(result, 0);\n+  bind(done);\n+}\n+\n+#define ATOMIC_OP(NAME, AOP, ACQUIRE, RELEASE)                                              \\\n+void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \\\n+  prev = prev->is_valid() ? prev : zr;                                                      \\\n+  if (incr.is_register()) {                                                                 \\\n+    AOP(prev, addr, incr.as_register(), (Assembler::Aqrl)(ACQUIRE | RELEASE));              \\\n+  } else {                                                                                  \\\n+    mv(t0, incr.as_constant());                                                             \\\n+    AOP(prev, addr, t0, (Assembler::Aqrl)(ACQUIRE | RELEASE));                              \\\n+  }                                                                                         \\\n+  return;                                                                                   \\\n+}\n+\n+ATOMIC_OP(add, amoadd_d, Assembler::relaxed, Assembler::relaxed)\n+ATOMIC_OP(addw, amoadd_w, Assembler::relaxed, Assembler::relaxed)\n+ATOMIC_OP(addal, amoadd_d, Assembler::aq, Assembler::rl)\n+ATOMIC_OP(addalw, amoadd_w, Assembler::aq, Assembler::rl)\n+\n+#undef ATOMIC_OP\n+\n+#define ATOMIC_XCHG(OP, AOP, ACQUIRE, RELEASE)                                       \\\n+void MacroAssembler::atomic_##OP(Register prev, Register newv, Register addr) {      \\\n+  prev = prev->is_valid() ? prev : zr;                                               \\\n+  AOP(prev, addr, newv, (Assembler::Aqrl)(ACQUIRE | RELEASE));                       \\\n+  return;                                                                            \\\n+}\n+\n+ATOMIC_XCHG(xchg, amoswap_d, Assembler::relaxed, Assembler::relaxed)\n+ATOMIC_XCHG(xchgw, amoswap_w, Assembler::relaxed, Assembler::relaxed)\n+ATOMIC_XCHG(xchgal, amoswap_d, Assembler::aq, Assembler::rl)\n+ATOMIC_XCHG(xchgalw, amoswap_w, Assembler::aq, Assembler::rl)\n+\n+#undef ATOMIC_XCHG\n+\n+#define ATOMIC_XCHGU(OP1, OP2)                                                       \\\n+void MacroAssembler::atomic_##OP1(Register prev, Register newv, Register addr) {     \\\n+  atomic_##OP2(prev, newv, addr);                                                    \\\n+  zero_extend(prev, prev, 32);                                                       \\\n+  return;                                                                            \\\n+}\n+\n+ATOMIC_XCHGU(xchgwu, xchgw)\n+ATOMIC_XCHGU(xchgalwu, xchgalw)\n+\n+#undef ATOMIC_XCHGU\n+\n+void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {\n+  assert(ReservedCodeCacheSize < 4*G, \"branch out of range\");\n+  assert(CodeCache::find_blob(entry.target()) != NULL,\n+         \"destination of far call not found in code cache\");\n+  int32_t offset = 0;\n+  if (far_branches()) {\n+    \/\/ We can use auipc + jalr here because we know that the total size of\n+    \/\/ the code cache cannot exceed 2Gb.\n+    la_patchable(tmp, entry, offset);\n+    if (cbuf != NULL) { cbuf->set_insts_mark(); }\n+    jalr(x0, tmp, offset);\n+  } else {\n+    if (cbuf != NULL) { cbuf->set_insts_mark(); }\n+    j(entry);\n+  }\n+}\n+\n+void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {\n+  assert(ReservedCodeCacheSize < 4*G, \"branch out of range\");\n+  assert(CodeCache::find_blob(entry.target()) != NULL,\n+         \"destination of far call not found in code cache\");\n+  int32_t offset = 0;\n+  if (far_branches()) {\n+    \/\/ We can use auipc + jalr here because we know that the total size of\n+    \/\/ the code cache cannot exceed 2Gb.\n+    la_patchable(tmp, entry, offset);\n+    if (cbuf != NULL) { cbuf->set_insts_mark(); }\n+    jalr(x1, tmp, offset); \/\/ link\n+  } else {\n+    if (cbuf != NULL) { cbuf->set_insts_mark(); }\n+    jal(entry); \/\/ link\n+  }\n+}\n+\n+void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register tmp_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure,\n+                                                   Label* L_slow_path,\n+                                                   Register super_check_offset) {\n+  assert_different_registers(sub_klass, super_klass, tmp_reg);\n+  bool must_load_sco = (super_check_offset == noreg);\n+  if (must_load_sco) {\n+    assert(tmp_reg != noreg, \"supply either a temp or a register offset\");\n+  } else {\n+    assert_different_registers(sub_klass, super_klass, super_check_offset);\n+  }\n+\n+  Label L_fallthrough;\n+  int label_nulls = 0;\n+  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one NULL in batch\");\n+\n+  int sc_offset = in_bytes(Klass::secondary_super_cache_offset());\n+  int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+  Address super_check_offset_addr(super_klass, sco_offset);\n+\n+  \/\/ Hacked jmp, which may only be used just before L_fallthrough.\n+#define final_jmp(label)                                                \\\n+  if (&(label) == &L_fallthrough) { \/*do nothing*\/ }                    \\\n+  else                            j(label)             \/*omit semi*\/\n+\n+  \/\/ If the pointers are equal, we are done (e.g., String[] elements).\n+  \/\/ This self-check enables sharing of secondary supertype arrays among\n+  \/\/ non-primary types such as array-of-interface. Otherwise, each such\n+  \/\/ type would need its own customized SSA.\n+  \/\/ We move this check to the front fo the fast path because many\n+  \/\/ type checks are in fact trivially successful in this manner,\n+  \/\/ so we get a nicely predicted branch right at the start of the check.\n+  beq(sub_klass, super_klass, *L_success);\n+\n+  \/\/ Check the supertype display:\n+  if (must_load_sco) {\n+    lwu(tmp_reg, super_check_offset_addr);\n+    super_check_offset = tmp_reg;\n+  }\n+  add(t0, sub_klass, super_check_offset);\n+  Address super_check_addr(t0);\n+  ld(t0, super_check_addr); \/\/ load displayed supertype\n+\n+  \/\/ Ths check has worked decisively for primary supers.\n+  \/\/ Secondary supers are sought in the super_cache ('super_cache_addr').\n+  \/\/ (Secondary supers are interfaces and very deeply nested subtypes.)\n+  \/\/ This works in the same check above because of a tricky aliasing\n+  \/\/ between the super_Cache and the primary super dispaly elements.\n+  \/\/ (The 'super_check_addr' can address either, as the case requires.)\n+  \/\/ Note that the cache is updated below if it does not help us find\n+  \/\/ what we need immediately.\n+  \/\/ So if it was a primary super, we can just fail immediately.\n+  \/\/ Otherwise, it's the slow path for us (no success at this point).\n+\n+  beq(super_klass, t0, *L_success);\n+  mv(t1, sc_offset);\n+  if (L_failure == &L_fallthrough) {\n+    beq(super_check_offset, t1, *L_slow_path);\n+  } else {\n+    bne(super_check_offset, t1, *L_failure, \/* is_far *\/ true);\n+    final_jmp(*L_slow_path);\n+  }\n+\n+  bind(L_fallthrough);\n+\n+#undef final_jmp\n+}\n+\n+\/\/ Scans count pointer sized words at [addr] for occurence of value,\n+\/\/ generic\n+void MacroAssembler::repne_scan(Register addr, Register value, Register count,\n+                                Register tmp) {\n+  Label Lloop, Lexit;\n+  beqz(count, Lexit);\n+  bind(Lloop);\n+  ld(tmp, addr);\n+  beq(value, tmp, Lexit);\n+  add(addr, addr, wordSize);\n+  sub(count, count, 1);\n+  bnez(count, Lloop);\n+  bind(Lexit);\n+}\n+\n+void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register tmp1_reg,\n+                                                   Register tmp2_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure) {\n+  assert_different_registers(sub_klass, super_klass, tmp1_reg);\n+  if (tmp2_reg != noreg) {\n+    assert_different_registers(sub_klass, super_klass, tmp1_reg, tmp2_reg, t0);\n+  }\n+#define IS_A_TEMP(reg) ((reg) == tmp1_reg || (reg) == tmp2_reg)\n+\n+  Label L_fallthrough;\n+  int label_nulls = 0;\n+  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n+\n+  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+\n+  \/\/ A couple of usefule fields in sub_klass:\n+  int ss_offset = in_bytes(Klass::secondary_supers_offset());\n+  int sc_offset = in_bytes(Klass::secondary_super_cache_offset());\n+  Address secondary_supers_addr(sub_klass, ss_offset);\n+  Address super_cache_addr(     sub_klass, sc_offset);\n+\n+  BLOCK_COMMENT(\"check_klass_subtype_slow_path\");\n+\n+  \/\/ Do a linear scan of the secondary super-klass chain.\n+  \/\/ This code is rarely used, so simplicity is a virtue here.\n+  \/\/ The repne_scan instruction uses fixed registers, which we must spill.\n+  \/\/ Don't worry too much about pre-existing connecitons with the input regs.\n+\n+  assert(sub_klass != x10, \"killed reg\"); \/\/ killed by mv(x10, super)\n+  assert(sub_klass != x12, \"killed reg\"); \/\/ killed by la(x12, &pst_counter)\n+\n+  RegSet pushed_registers;\n+  if (!IS_A_TEMP(x12)) {\n+    pushed_registers += x12;\n+  }\n+  if (!IS_A_TEMP(x15)) {\n+    pushed_registers += x15;\n+  }\n+\n+  if (super_klass != x10 || UseCompressedOops) {\n+    if (!IS_A_TEMP(x10)) {\n+      pushed_registers += x10;\n+    }\n+  }\n+\n+  push_reg(pushed_registers, sp);\n+\n+  \/\/ Get super_klass value into x10 (even if it was in x15 or x12)\n+  mv(x10, super_klass);\n+\n+#ifndef PRODUCT\n+  mv(t1, (address)&SharedRuntime::_partial_subtype_ctr);\n+  Address pst_counter_addr(t1);\n+  ld(t0, pst_counter_addr);\n+  add(t0, t0, 1);\n+  sd(t0, pst_counter_addr);\n+#endif \/\/ PRODUCT\n+\n+  \/\/ We will consult the secondary-super array.\n+  ld(x15, secondary_supers_addr);\n+  \/\/ Load the array length.\n+  lwu(x12, Address(x15, Array<Klass*>::length_offset_in_bytes()));\n+  \/\/ Skip to start of data.\n+  add(x15, x15, Array<Klass*>::base_offset_in_bytes());\n+\n+  \/\/ Set t0 to an obvious invalid value, falling through by default\n+  li(t0, -1);\n+  \/\/ Scan X12 words at [X15] for an occurrence of X10.\n+  repne_scan(x15, x10, x12, t0);\n+\n+  \/\/ pop will restore x10, so we should use a temp register to keep its value\n+  mv(t1, x10);\n+\n+  \/\/ Unspill the temp registers:\n+  pop_reg(pushed_registers, sp);\n+\n+  bne(t1, t0, *L_failure);\n+\n+  \/\/ Success. Cache the super we found an proceed in triumph.\n+  sd(super_klass, super_cache_addr);\n+\n+  if (L_success != &L_fallthrough) {\n+    j(*L_success);\n+  }\n+\n+#undef IS_A_TEMP\n+\n+  bind(L_fallthrough);\n+}\n+\n+\/\/ Defines obj, preserves var_size_in_bytes, okay for tmp2 == var_size_in_bytes.\n+void MacroAssembler::tlab_allocate(Register obj,\n+                                   Register var_size_in_bytes,\n+                                   int con_size_in_bytes,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Label& slow_case,\n+                                   bool is_far) {\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, tmp1, tmp2, slow_case, is_far);\n+}\n+\n+\/\/ Defines obj, preserves var_size_in_bytes\n+void MacroAssembler::eden_allocate(Register obj,\n+                                   Register var_size_in_bytes,\n+                                   int con_size_in_bytes,\n+                                   Register tmp,\n+                                   Label& slow_case,\n+                                   bool is_far) {\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, tmp, slow_case, is_far);\n+}\n+\n+\n+\/\/ get_thread() can be called anywhere inside generated code so we\n+\/\/ need to save whatever non-callee save context might get clobbered\n+\/\/ by the call to Thread::current() or, indeed, the call setup code.\n+void MacroAssembler::get_thread(Register thread) {\n+  \/\/ save all call-clobbered regs except thread\n+  RegSet saved_regs = RegSet::range(x5, x7) + RegSet::range(x10, x17) +\n+                      RegSet::range(x28, x31) + ra - thread;\n+  push_reg(saved_regs, sp);\n+\n+  int32_t offset = 0;\n+  movptr_with_offset(ra, CAST_FROM_FN_PTR(address, Thread::current), offset);\n+  jalr(ra, ra, offset);\n+  if (thread != x10) {\n+    mv(thread, x10);\n+  }\n+\n+  \/\/ restore pushed registers\n+  pop_reg(saved_regs, sp);\n+}\n+\n+void MacroAssembler::load_byte_map_base(Register reg) {\n+  jbyte *byte_map_base =\n+    ((CardTableBarrierSet*)(BarrierSet::barrier_set()))->card_table()->byte_map_base();\n+  li(reg, (uint64_t)byte_map_base);\n+}\n+\n+void MacroAssembler::la_patchable(Register reg1, const Address &dest, int32_t &offset) {\n+  relocInfo::relocType rtype = dest.rspec().reloc()->type();\n+  unsigned long low_address = (uintptr_t)CodeCache::low_bound();\n+  unsigned long high_address = (uintptr_t)CodeCache::high_bound();\n+  unsigned long dest_address = (uintptr_t)dest.target();\n+  long offset_low = dest_address - low_address;\n+  long offset_high = dest_address - high_address;\n+\n+  assert(is_valid_riscv64_address(dest.target()), \"bad address\");\n+  assert(dest.getMode() == Address::literal, \"la_patchable must be applied to a literal address\");\n+\n+  InstructionMark im(this);\n+  code_section()->relocate(inst_mark(), dest.rspec());\n+  \/\/ RISC-V doesn't compute a page-aligned address, in order to partially\n+  \/\/ compensate for the use of *signed* offsets in its base+disp12\n+  \/\/ addressing mode (RISC-V's PC-relative reach remains asymmetric\n+  \/\/ [-(2G + 2K), 2G - 2k).\n+  if (offset_high >= -((1L << 31) + (1L << 11)) && offset_low < (1L << 31) - (1L << 11)) {\n+    int64_t distance = dest.target() - pc();\n+    auipc(reg1, (int32_t)distance + 0x800);\n+    offset = ((int32_t)distance << 20) >> 20;\n+  } else {\n+    movptr_with_offset(reg1, dest.target(), offset);\n+  }\n+}\n+\n+void MacroAssembler::build_frame(int framesize) {\n+  assert(framesize >= 2, \"framesize must include space for FP\/RA\");\n+  assert(framesize % (2*wordSize) == 0, \"must preserve 2*wordSize alignment\");\n+  sub(sp, sp, framesize);\n+  sd(fp, Address(sp, framesize - 2 * wordSize));\n+  sd(ra, Address(sp, framesize - wordSize));\n+  if (PreserveFramePointer) { add(fp, sp, framesize); }\n+}\n+\n+void MacroAssembler::remove_frame(int framesize) {\n+  assert(framesize >= 2, \"framesize must include space for FP\/RA\");\n+  assert(framesize % (2*wordSize) == 0, \"must preserve 2*wordSize alignment\");\n+  ld(fp, Address(sp, framesize - 2 * wordSize));\n+  ld(ra, Address(sp, framesize - wordSize));\n+  add(sp, sp, framesize);\n+}\n+\n+void MacroAssembler::reserved_stack_check() {\n+    \/\/ testing if reserved zone needs to be enabled\n+    Label no_reserved_zone_enabling;\n+\n+    ld(t0, Address(xthread, JavaThread::reserved_stack_activation_offset()));\n+    bltu(sp, t0, no_reserved_zone_enabling);\n+\n+    enter();   \/\/ RA and FP are live.\n+    mv(c_rarg0, xthread);\n+    int32_t offset = 0;\n+    la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone)), offset);\n+    jalr(x1, t0, offset);\n+    leave();\n+\n+    \/\/ We have already removed our own frame.\n+    \/\/ throw_delayed_StackOverflowError will think that it's been\n+    \/\/ called by our caller.\n+    offset = 0;\n+    la_patchable(t0, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()), offset);\n+    jalr(x0, t0, offset);\n+    should_not_reach_here();\n+\n+    bind(no_reserved_zone_enabling);\n+}\n+\n+void MacroAssembler::atomic_incw(Register counter_addr, Register tmp) {\n+  Label retry_load;\n+  bind(retry_load);\n+  \/\/ flush and load exclusive from the memory location\n+  lr_w(tmp, counter_addr);\n+  addw(tmp, tmp, 1);\n+  \/\/ if we store+flush with no intervening write tmp wil be zero\n+  sc_w(tmp, tmp, counter_addr);\n+  bnez(tmp, retry_load);\n+}\n+\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ld(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n+int MacroAssembler::biased_locking_enter(Register lock_reg,\n+                                         Register obj_reg,\n+                                         Register swap_reg,\n+                                         Register tmp_reg,\n+                                         bool swap_reg_contains_mark,\n+                                         Label& done,\n+                                         Label* slow_case,\n+                                         BiasedLockingCounters* counters,\n+                                         Register flag) {\n+  assert(UseBiasedLocking, \"why call this otherwise?\");\n+  assert_different_registers(lock_reg, obj_reg, swap_reg);\n+\n+  if (PrintBiasedLockingStatistics && counters == NULL)\n+    counters = BiasedLocking::counters();\n+\n+  assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, t0);\n+  assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits, \"biased locking makes assumptions about bit layout\");\n+  Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());\n+\n+  \/\/ Biased locking\n+  \/\/ See whether the lock is currently biased toward our thread and\n+  \/\/ whether the epoch is still valid\n+  \/\/ Note that the runtime guarantees sufficient alignment of JavaThread\n+  \/\/ pointers to allow age to be placed into low bits\n+  \/\/ First check to see whether biasing is even enabled for this object\n+  Label cas_label;\n+  int null_check_offset = -1;\n+  if (!swap_reg_contains_mark) {\n+    null_check_offset = offset();\n+    ld(swap_reg, mark_addr);\n+  }\n+  andi(tmp_reg, swap_reg, markOopDesc::biased_lock_mask_in_place);\n+  li(t0, markOopDesc::biased_lock_pattern);\n+  bne(t0, tmp_reg, cas_label);\n+  \/\/ The bias pattern is present in the object's header. Need to check\n+  \/\/ whether the bias owner and the epoch are both still current.\n+  load_prototype_header(tmp_reg, obj_reg);\n+  orr(tmp_reg, tmp_reg, xthread);\n+  xorr(tmp_reg, swap_reg, tmp_reg);\n+  andi(tmp_reg, tmp_reg, ~((int) markOopDesc::age_mask_in_place));\n+  if (flag->is_valid()) {\n+    mv(flag, tmp_reg);\n+  }\n+  if (counters != NULL) {\n+    Label around;\n+    bnez(tmp_reg, around);\n+    atomic_incw(Address((address)counters->biased_lock_entry_count_addr()), tmp_reg, t0);\n+    j(done);\n+    bind(around);\n+  } else {\n+    beqz(tmp_reg, done);\n+  }\n+\n+  Label try_revoke_bias;\n+  Label try_rebias;\n+\n+  \/\/ At this point we know that the header has the bias pattern and\n+  \/\/ that we are not the bias owner in the current epoch. We need to\n+  \/\/ figure out more details about the state of the header in order to\n+  \/\/ know what operations can be legally performed on the object's\n+  \/\/ header.\n+\n+  \/\/ If the low three bits in the xor result aren't clear, that means\n+  \/\/ the prototype header is no longer biased and we have to revoke\n+  \/\/ the bias on this object.\n+  andi(t0, tmp_reg, markOopDesc::biased_lock_mask_in_place);\n+  bnez(t0, try_revoke_bias);\n+\n+  \/\/ Biasing is still enabled for this data type. See whether the\n+  \/\/ epoch of the current bias is still valid, meaning that the epoch\n+  \/\/ bits of the mark word are equal to the epoch bits of the\n+  \/\/ prototype header. (Note that the prototype header's epoch bits\n+  \/\/ only change at a safepoint.) If not, attempt to rebias the object\n+  \/\/ toward the current thread. Note that we must be absolutely sure\n+  \/\/ that the current epoch is invalid in order to do this because\n+  \/\/ otherwise the manipulations it performs on the mark word are\n+  \/\/ illegal.\n+  andi(t0, tmp_reg, markOopDesc::epoch_mask_in_place);\n+  bnez(t0, try_rebias);\n+\n+  \/\/ The epoch of the current bias is still valid but we know nothing\n+  \/\/ about the owner; it might be set or it might be clear. Try to\n+  \/\/ acquire the bias of the object using an atomic operation. If this\n+  \/\/ fails we will go in to the runtime to revoke the object's bias.\n+  \/\/ Note that we first construct the presumed unbiased header so we\n+  \/\/ don't accidentally blow away another thread's valid bias.\n+  {\n+    Label cas_success;\n+    Label counter;\n+    mv(t0, markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place);\n+    andr(swap_reg, swap_reg, t0);\n+    orr(tmp_reg, swap_reg, xthread);\n+    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, t0, cas_success, slow_case);\n+    \/\/ cas failed here if slow_cass == NULL\n+    if (flag->is_valid()) {\n+      mv(flag, 1);\n+      j(counter);\n+    }\n+    \/\/ If the biasing toward our thread failed, this means that\n+    \/\/ another thread succeeded in biasing it toward itself and we\n+    \/\/ need to revoke that bias. The revocation will occur in the\n+    \/\/ interpreter runtime in the slow case.\n+    bind(cas_success);\n+    if (flag->is_valid()) {\n+      mv(flag, 0);\n+      bind(counter);\n+    }\n+    if (counters != NULL) {\n+      atomic_incw(Address((address)counters->anonymously_biased_lock_entry_count_addr()),\n+                  tmp_reg, t0);\n+    }\n+  }\n+  j(done);\n+\n+  bind(try_rebias);\n+  \/\/ At this point we know the epoch has expired, meaning that the\n+  \/\/ current \"bias owner\", if any, is actually invalid. Under these\n+  \/\/ circumstances _only_, we are allowed to use the current header's\n+  \/\/ value as the comparison value when doing the cas to acquire the\n+  \/\/ bias in the current epoch. In other words, we allow transfer of\n+  \/\/ the bias from one thread to another directly in this situation.\n+  \/\/\n+  \/\/ FIXME: due to a lack of registers we currently blow away the age\n+  \/\/ bits in this situation. Should attempt to preserve them.\n+  {\n+    Label cas_success;\n+    Label counter;\n+    load_prototype_header(tmp_reg, obj_reg);\n+    orr(tmp_reg, xthread, tmp_reg);\n+    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, t0, cas_success, slow_case);\n+    \/\/ cas failed here if slow_cass == NULL\n+    if (flag->is_valid()) {\n+      mv(flag, 1);\n+      j(counter);\n+    }\n+\n+    \/\/ If the biasing toward our thread failed, then another thread\n+    \/\/ succeeded in biasing it toward itself and we need to revoke that\n+    \/\/ bias. The revocation will occur in the runtime in the slow case.\n+    bind(cas_success);\n+    if (flag->is_valid()) {\n+      mv(flag, 0);\n+      bind(counter);\n+    }\n+    if (counters != NULL) {\n+      atomic_incw(Address((address)counters->rebiased_lock_entry_count_addr()),\n+                  tmp_reg, t0);\n+    }\n+  }\n+  j(done);\n+\n+  bind(try_revoke_bias);\n+  \/\/ The prototype mark in the klass doesn't have the bias bit set any\n+  \/\/ more, indicating that objects of this data type are not supposed\n+  \/\/ to be biased any more. We are going to try to reset the mark of\n+  \/\/ this object to the prototype value and fall through to the\n+  \/\/ CAS-based locking scheme. Note that if our CAS fails, it means\n+  \/\/ that another thread raced us for the privilege of revoking the\n+  \/\/ bias of this particular object, so it's okay to continue in the\n+  \/\/ normal locking code.\n+  \/\/\n+  \/\/ FIXME: due to a lack of registers we currently blow away the age\n+  \/\/ bits in this situation. Should attempt to preserve them.\n+  {\n+    Label cas_success, nope;\n+    load_prototype_header(tmp_reg, obj_reg);\n+    cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, t0, cas_success, &nope);\n+    bind(cas_success);\n+\n+    \/\/ Fall through to the normal CAS-based lock, because no matter what\n+    \/\/ the result of the above CAS, some thread must have succeeded in\n+    \/\/ removing the bias bit from the object's header.\n+    if (counters != NULL) {\n+      atomic_incw(Address((address)counters->revoked_lock_entry_count_addr()), tmp_reg,\n+                  t0);\n+    }\n+    bind(nope);\n+  }\n+\n+  bind(cas_label);\n+\n+  return null_check_offset;\n+}\n+\n+void MacroAssembler::biased_locking_exit(Register obj_reg, Register tmp_reg, Label& done, Register flag) {\n+  assert(UseBiasedLocking, \"why call this otherwise?\");\n+\n+  \/\/ Check for biased locking unlock case, which is a no-op\n+  \/\/ Note: we do not have to check the thread ID for two reasons.\n+  \/\/ First, the interpreter checks for IllegalMonitorStateException at\n+  \/\/ a higher level. Second, if the bias was revoked while we held the\n+  \/\/ lock, the object could not be rebiased toward another thread, so\n+  \/\/ the bias bit would be clear.\n+  ld(tmp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+  andi(tmp_reg, tmp_reg, markOopDesc::biased_lock_mask_in_place);\n+  sub(tmp_reg, tmp_reg, markOopDesc::biased_lock_pattern);\n+  if (flag->is_valid()) { mv(flag, tmp_reg); }\n+  beqz(tmp_reg, done);\n+}\n+\n+\/\/ Move the address of the polling page into dest.\n+void MacroAssembler::get_polling_page(Register dest, address page, int32_t &offset, relocInfo::relocType rtype) {\n+  if (SafepointMechanism::uses_thread_local_poll()) {\n+    ld(dest, Address(xthread, Thread::polling_page_offset()));\n+  } else {\n+    uint64_t align = (uint64_t)page & 0xfff;\n+    assert(align == 0, \"polling page must be page aligned\");\n+    la_patchable(dest, Address(page, rtype), offset);\n+  }\n+}\n+\n+\/\/ Read the polling page.  The address of the polling page must\n+\/\/ already be in r.\n+void MacroAssembler::read_polling_page(Register dest, address page, relocInfo::relocType rtype) {\n+  int32_t offset = 0;\n+  get_polling_page(dest, page, offset, rtype);\n+  read_polling_page(dest, offset, rtype);\n+}\n+\n+\/\/ Read the polling page.  The address of the polling page must\n+\/\/ already be in r.\n+void MacroAssembler::read_polling_page(Register dest, int32_t offset, relocInfo::relocType rtype) {\n+  code_section()->relocate(pc(), rtype);\n+  lwu(zr, Address(dest, offset));\n+}\n+\n+void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {\n+#ifdef ASSERT\n+  {\n+    ThreadInVMfromUnknown tiv;\n+    assert (UseCompressedOops, \"should only be used for compressed oops\");\n+    assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+    assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert(Universe::heap()->is_in_reserved(JNIHandles::resolve(obj)), \"should be real oop\");\n+  }\n+#endif\n+  int oop_index = oop_recorder()->find_index(obj);\n+  InstructionMark im(this);\n+  RelocationHolder rspec = oop_Relocation::spec(oop_index);\n+  code_section()->relocate(inst_mark(), rspec);\n+  li32(dst, 0xDEADBEEF);\n+  zero_extend(dst, dst, 32);\n+}\n+\n+void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  int index = oop_recorder()->find_index(k);\n+  assert(!Universe::heap()->is_in_reserved(k), \"should not be an oop\");\n+\n+  InstructionMark im(this);\n+  RelocationHolder rspec = metadata_Relocation::spec(index);\n+  code_section()->relocate(inst_mark(), rspec);\n+  narrowKlass nk = Klass::encode_klass(k);\n+  li32(dst, nk);\n+  zero_extend(dst, dst, 32);\n+}\n+\n+\/\/ Maybe emit a call via a trampoline.  If the code cache is small\n+\/\/ trampolines won't be emitted.\n+address MacroAssembler::trampoline_call(Address entry, CodeBuffer* cbuf) {\n+  assert(JavaThread::current()->is_Compiler_thread(), \"just checking\");\n+  assert(entry.rspec().type() == relocInfo::runtime_call_type ||\n+         entry.rspec().type() == relocInfo::opt_virtual_call_type ||\n+         entry.rspec().type() == relocInfo::static_call_type ||\n+         entry.rspec().type() == relocInfo::virtual_call_type, \"wrong reloc type\");\n+\n+  \/\/ We need a trampoline if branches are far.\n+  if (far_branches()) {\n+    bool in_scratch_emit_size = false;\n+#ifdef COMPILER2\n+    \/\/ We don't want to emit a trampoline if C2 is generating dummy\n+    \/\/ code during its branch shortening phase.\n+    CompileTask* task = ciEnv::current()->task();\n+    in_scratch_emit_size =\n+      (task != NULL && is_c2_compile(task->comp_level()) &&\n+       Compile::current()->in_scratch_emit_size());\n+#endif\n+    if (!in_scratch_emit_size) {\n+      address stub = emit_trampoline_stub(offset(), entry.target());\n+      if (stub == NULL) {\n+        postcond(pc() == badAddress);\n+        return NULL; \/\/ CodeCache is full\n+      }\n+    }\n+  }\n+\n+  if (cbuf != NULL) { cbuf->set_insts_mark(); }\n+  relocate(entry.rspec());\n+  if (!far_branches()) {\n+    jal(entry.target());\n+  } else {\n+    jal(pc());\n+  }\n+  \/\/ just need to return a non-null address\n+  postcond(pc() != badAddress);\n+  return pc();\n+}\n+\n+address MacroAssembler::ic_call(address entry, jint method_index) {\n+  RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);\n+  movptr(t1, (address)Universe::non_oop_word());\n+  assert_cond(entry != NULL);\n+  return trampoline_call(Address(entry, rh));\n+}\n+\n+\/\/ Emit a trampoline stub for a call to a target which is too far away.\n+\/\/\n+\/\/ code sequences:\n+\/\/\n+\/\/ call-site:\n+\/\/   branch-and-link to <destination> or <trampoline stub>\n+\/\/\n+\/\/ Related trampoline stub for this call site in the stub section:\n+\/\/   load the call target from the constant pool\n+\/\/   branch (RA still points to the call site above)\n+\n+address MacroAssembler::emit_trampoline_stub(int insts_call_instruction_offset,\n+                                             address dest) {\n+  address stub = start_a_stub(NativeInstruction::instruction_size\n+                            + NativeCallTrampolineStub::instruction_size);\n+  if (stub == NULL) {\n+    return NULL;  \/\/ CodeBuffer::expand failed\n+  }\n+\n+  \/\/ Create a trampoline stub relocation which relates this trampoline stub\n+  \/\/ with the call instruction at insts_call_instruction_offset in the\n+  \/\/ instructions code-section.\n+\n+  \/\/ make sure 4 byte aligned here, so that the destination address would be\n+  \/\/ 8 byte aligned after 3 intructions\n+  \/\/ when we reach here we may get a 2-byte alignment so need to align it\n+  align(wordSize, NativeCallTrampolineStub::data_offset);\n+\n+  relocate(trampoline_stub_Relocation::spec(code()->insts()->start() +\n+                                            insts_call_instruction_offset));\n+  const int stub_start_offset = offset();\n+\n+  \/\/ Now, create the trampoline stub's code:\n+  \/\/ - load the call\n+  \/\/ - call\n+  Label target;\n+  ld(t0, target);  \/\/ auipc + ld\n+  jr(t0);          \/\/ jalr\n+  bind(target);\n+  assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,\n+         \"should be\");\n+  assert(offset() % wordSize == 0, \"bad alignment\");\n+  emit_int64((intptr_t)dest);\n+\n+  const address stub_start_addr = addr_at(stub_start_offset);\n+\n+  assert(is_NativeCallTrampolineStub_at(stub_start_addr), \"doesn't look like a trampoline\");\n+\n+  end_a_stub();\n+  return stub_start_addr;\n+}\n+\n+Address MacroAssembler::add_memory_helper(const Address dst) {\n+  switch (dst.getMode()) {\n+    case Address::base_plus_offset:\n+      \/\/ This is the expected mode, although we allow all the other\n+      \/\/ forms below.\n+      return form_address(t1, dst.base(), dst.offset());\n+    default:\n+      la(t1, dst);\n+      return Address(t1);\n+  }\n+}\n+\n+void MacroAssembler::add_memory_int64(const Address dst, int64_t imm) {\n+  Address adr = add_memory_helper(dst);\n+  assert_different_registers(adr.base(), t0);\n+  ld(t0, adr);\n+  addi(t0, t0, imm);\n+  sd(t0, adr);\n+}\n+\n+void MacroAssembler::add_memory_int32(const Address dst, int32_t imm) {\n+  Address adr = add_memory_helper(dst);\n+  assert_different_registers(adr.base(), t0);\n+  lwu(t0, adr);\n+  addiw(t0, t0, imm);\n+  sw(t0, adr);\n+}\n+\n+void MacroAssembler::cmpptr(Register src1, Address src2, Label& equal) {\n+  assert_different_registers(src1, t0);\n+  int32_t offset;\n+  la_patchable(t0, src2, offset);\n+  ld(t0, Address(t0, offset));\n+  beq(src1, t0, equal);\n+}\n+\n+\/\/ string indexof\n+\/\/ compute index by trailing zeros\n+void MacroAssembler::compute_index(Register haystack, Register trailing_zeros,\n+                                   Register match_mask, Register result,\n+                                   Register ch2, Register tmp,\n+                                   bool haystack_isL)\n+{\n+  int haystack_chr_shift = haystack_isL ? 0 : 1;\n+  srl(match_mask, match_mask, trailing_zeros);\n+  srli(match_mask, match_mask, 1);\n+  srli(tmp, trailing_zeros, LogBitsPerByte);\n+  if (!haystack_isL) andi(tmp, tmp, 0xE);\n+  add(haystack, haystack, tmp);\n+  ld(ch2, Address(haystack));\n+  if (!haystack_isL) srli(tmp, tmp, haystack_chr_shift);\n+  add(result, result, tmp);\n+}\n+\n+\/\/ string indexof\n+\/\/ Find pattern element in src, compute match mask,\n+\/\/ only the first occurrence of 0x80\/0x8000 at low bits is the valid match index\n+\/\/ match mask patterns and corresponding indices would be like:\n+\/\/ - 0x8080808080808080 (Latin1)\n+\/\/ -   7 6 5 4 3 2 1 0  (match index)\n+\/\/ - 0x8000800080008000 (UTF16)\n+\/\/ -   3   2   1   0    (match index)\n+void MacroAssembler::compute_match_mask(Register src, Register pattern, Register match_mask,\n+                                        Register mask1, Register mask2)\n+{\n+  xorr(src, pattern, src);\n+  sub(match_mask, src, mask1);\n+  orr(src, src, mask2);\n+  notr(src, src);\n+  andr(match_mask, match_mask, src);\n+}\n+\n+#ifdef COMPILER2\n+\/\/ Code for BigInteger::mulAdd instrinsic\n+\/\/ out     = x10\n+\/\/ in      = x11\n+\/\/ offset  = x12  (already out.length-offset)\n+\/\/ len     = x13\n+\/\/ k       = x14\n+\/\/ tmp     = x28\n+\/\/\n+\/\/ pseudo code from java implementation:\n+\/\/ long kLong = k & LONG_MASK;\n+\/\/ carry = 0;\n+\/\/ offset = out.length-offset - 1;\n+\/\/ for (int j = len - 1; j >= 0; j--) {\n+\/\/     product = (in[j] & LONG_MASK) * kLong + (out[offset] & LONG_MASK) + carry;\n+\/\/     out[offset--] = (int)product;\n+\/\/     carry = product >>> 32;\n+\/\/ }\n+\/\/ return (int)carry;\n+void MacroAssembler::mul_add(Register out, Register in, Register offset,\n+                             Register len, Register k, Register tmp) {\n+  Label L_tail_loop, L_unroll, L_end;\n+  mv(tmp, out);\n+  mv(out, zr);\n+  blez(len, L_end);\n+  zero_extend(k, k, 32);\n+  slliw(t0, offset, LogBytesPerInt);\n+  add(offset, tmp, t0);\n+  slliw(t0, len, LogBytesPerInt);\n+  add(in, in, t0);\n+\n+  const int unroll = 8;\n+  li(tmp, unroll);\n+  blt(len, tmp, L_tail_loop);\n+  bind(L_unroll);\n+  for (int i = 0; i < unroll; i++) {\n+    sub(in, in, BytesPerInt);\n+    lwu(t0, Address(in, 0));\n+    mul(t1, t0, k);\n+    add(t0, t1, out);\n+    sub(offset, offset, BytesPerInt);\n+    lwu(t1, Address(offset, 0));\n+    add(t0, t0, t1);\n+    sw(t0, Address(offset, 0));\n+    srli(out, t0, 32);\n+  }\n+  subw(len, len, tmp);\n+  bge(len, tmp, L_unroll);\n+\n+  bind(L_tail_loop);\n+  blez(len, L_end);\n+  sub(in, in, BytesPerInt);\n+  lwu(t0, Address(in, 0));\n+  mul(t1, t0, k);\n+  add(t0, t1, out);\n+  sub(offset, offset, BytesPerInt);\n+  lwu(t1, Address(offset, 0));\n+  add(t0, t0, t1);\n+  sw(t0, Address(offset, 0));\n+  srli(out, t0, 32);\n+  subw(len, len, 1);\n+  j(L_tail_loop);\n+\n+  bind(L_end);\n+}\n+\n+\/\/ add two unsigned input and output carry\n+void MacroAssembler::cad(Register dst, Register src1, Register src2, Register carry)\n+{\n+  assert_different_registers(dst, carry);\n+  assert_different_registers(dst, src2);\n+  add(dst, src1, src2);\n+  sltu(carry, dst, src2);\n+}\n+\n+\/\/ add two input with carry\n+void MacroAssembler::adc(Register dst, Register src1, Register src2, Register carry)\n+{\n+  assert_different_registers(dst, carry);\n+  add(dst, src1, src2);\n+  add(dst, dst, carry);\n+}\n+\n+\/\/ add two unsigned input with carry and output carry\n+void MacroAssembler::cadc(Register dst, Register src1, Register src2, Register carry)\n+{\n+  assert_different_registers(dst, src2);\n+  adc(dst, src1, src2, carry);\n+  sltu(carry, dst, src2);\n+}\n+\n+void MacroAssembler::add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,\n+                                     Register src1, Register src2, Register carry)\n+{\n+  cad(dest_lo, dest_lo, src1, carry);\n+  add(dest_hi, dest_hi, carry);\n+  cad(dest_lo, dest_lo, src2, carry);\n+  add(final_dest_hi, dest_hi, carry);\n+}\n+\n+\/**\n+ * Multiply 32 bit by 32 bit first loop.\n+ *\/\n+void MacroAssembler::multiply_32_x_32_loop(Register x, Register xstart, Register x_xstart,\n+                                           Register y, Register y_idx, Register z,\n+                                           Register carry, Register product,\n+                                           Register idx, Register kdx)\n+{\n+  \/\/ jlong carry, x[], y[], z[];\n+  \/\/ for (int idx=ystart, kdx=ystart+1+xstart; idx >= 0; idx--, kdx--) {\n+  \/\/     long product = y[idx] * x[xstart] + carry;\n+  \/\/     z[kdx] = (int)product;\n+  \/\/     carry = product >>> 32;\n+  \/\/ }\n+  \/\/ z[xstart] = (int)carry;\n+\n+  Label L_first_loop, L_first_loop_exit;\n+  blez(idx, L_first_loop_exit);\n+\n+  shadd(t0, xstart, x, t0, LogBytesPerInt);\n+  lwu(x_xstart, Address(t0, 0));\n+\n+  bind(L_first_loop);\n+  subw(idx, idx, 1);\n+  shadd(t0, idx, y, t0, LogBytesPerInt);\n+  lwu(y_idx, Address(t0, 0));\n+  mul(product, x_xstart, y_idx);\n+  add(product, product, carry);\n+  srli(carry, product, 32);\n+  subw(kdx, kdx, 1);\n+  shadd(t0, kdx, z, t0, LogBytesPerInt);\n+  sw(product, Address(t0, 0));\n+  bgtz(idx, L_first_loop);\n+\n+  bind(L_first_loop_exit);\n+}\n+\n+\/**\n+ * Multiply 64 bit by 64 bit first loop.\n+ *\/\n+void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,\n+                                           Register y, Register y_idx, Register z,\n+                                           Register carry, Register product,\n+                                           Register idx, Register kdx)\n+{\n+  \/\/\n+  \/\/  jlong carry, x[], y[], z[];\n+  \/\/  for (int idx=ystart, kdx=ystart+1+xstart; idx >= 0; idx--, kdx--) {\n+  \/\/    huge_128 product = y[idx] * x[xstart] + carry;\n+  \/\/    z[kdx] = (jlong)product;\n+  \/\/    carry  = (jlong)(product >>> 64);\n+  \/\/  }\n+  \/\/  z[xstart] = carry;\n+  \/\/\n+\n+  Label L_first_loop, L_first_loop_exit;\n+  Label L_one_x, L_one_y, L_multiply;\n+\n+  subw(xstart, xstart, 1);\n+  bltz(xstart, L_one_x);\n+\n+  shadd(t0, xstart, x, t0, LogBytesPerInt);\n+  ld(x_xstart, Address(t0, 0));\n+  ror_imm(x_xstart, x_xstart, 32); \/\/ convert big-endian to little-endian\n+\n+  bind(L_first_loop);\n+  subw(idx, idx, 1);\n+  bltz(idx, L_first_loop_exit);\n+  subw(idx, idx, 1);\n+  bltz(idx, L_one_y);\n+\n+  shadd(t0, idx, y, t0, LogBytesPerInt);\n+  ld(y_idx, Address(t0, 0));\n+  ror_imm(y_idx, y_idx, 32); \/\/ convert big-endian to little-endian\n+  bind(L_multiply);\n+\n+  mulhu(t0, x_xstart, y_idx);\n+  mul(product, x_xstart, y_idx);\n+  cad(product, product, carry, t1);\n+  adc(carry, t0, zr, t1);\n+\n+  subw(kdx, kdx, 2);\n+  ror_imm(product, product, 32); \/\/ back to big-endian\n+  shadd(t0, kdx, z, t0, LogBytesPerInt);\n+  sd(product, Address(t0, 0));\n+\n+  j(L_first_loop);\n+\n+  bind(L_one_y);\n+  lwu(y_idx, Address(y, 0));\n+  j(L_multiply);\n+\n+  bind(L_one_x);\n+  lwu(x_xstart, Address(x, 0));\n+  j(L_first_loop);\n+\n+  bind(L_first_loop_exit);\n+}\n+\n+\/**\n+ * Multiply 128 bit by 128 bit. Unrolled inner loop.\n+ *\n+ *\/\n+void MacroAssembler::multiply_128_x_128_loop(Register y, Register z,\n+                                             Register carry, Register carry2,\n+                                             Register idx, Register jdx,\n+                                             Register yz_idx1, Register yz_idx2,\n+                                             Register tmp, Register tmp3, Register tmp4,\n+                                             Register tmp6, Register product_hi)\n+{\n+  \/\/   jlong carry, x[], y[], z[];\n+  \/\/   int kdx = xstart+1;\n+  \/\/   for (int idx=ystart-2; idx >= 0; idx -= 2) { \/\/ Third loop\n+  \/\/     huge_128 tmp3 = (y[idx+1] * product_hi) + z[kdx+idx+1] + carry;\n+  \/\/     jlong carry2  = (jlong)(tmp3 >>> 64);\n+  \/\/     huge_128 tmp4 = (y[idx]   * product_hi) + z[kdx+idx] + carry2;\n+  \/\/     carry  = (jlong)(tmp4 >>> 64);\n+  \/\/     z[kdx+idx+1] = (jlong)tmp3;\n+  \/\/     z[kdx+idx] = (jlong)tmp4;\n+  \/\/   }\n+  \/\/   idx += 2;\n+  \/\/   if (idx > 0) {\n+  \/\/     yz_idx1 = (y[idx] * product_hi) + z[kdx+idx] + carry;\n+  \/\/     z[kdx+idx] = (jlong)yz_idx1;\n+  \/\/     carry  = (jlong)(yz_idx1 >>> 64);\n+  \/\/   }\n+  \/\/\n+\n+  Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;\n+\n+  srliw(jdx, idx, 2);\n+\n+  bind(L_third_loop);\n+\n+  subw(jdx, jdx, 1);\n+  bltz(jdx, L_third_loop_exit);\n+  subw(idx, idx, 4);\n+\n+  shadd(t0, idx, y, t0, LogBytesPerInt);\n+  ld(yz_idx2, Address(t0, 0));\n+  ld(yz_idx1, Address(t0, wordSize));\n+\n+  shadd(tmp6, idx, z, t0, LogBytesPerInt);\n+\n+  ror_imm(yz_idx1, yz_idx1, 32); \/\/ convert big-endian to little-endian\n+  ror_imm(yz_idx2, yz_idx2, 32);\n+\n+  ld(t1, Address(tmp6, 0));\n+  ld(t0, Address(tmp6, wordSize));\n+\n+  mul(tmp3, product_hi, yz_idx1); \/\/  yz_idx1 * product_hi -> tmp4:tmp3\n+  mulhu(tmp4, product_hi, yz_idx1);\n+\n+  ror_imm(t0, t0, 32, tmp); \/\/ convert big-endian to little-endian\n+  ror_imm(t1, t1, 32, tmp);\n+\n+  mul(tmp, product_hi, yz_idx2); \/\/  yz_idx2 * product_hi -> carry2:tmp\n+  mulhu(carry2, product_hi, yz_idx2);\n+\n+  cad(tmp3, tmp3, carry, carry);\n+  adc(tmp4, tmp4, zr, carry);\n+  cad(tmp3, tmp3, t0, t0);\n+  cadc(tmp4, tmp4, tmp, t0);\n+  adc(carry, carry2, zr, t0);\n+  cad(tmp4, tmp4, t1, carry2);\n+  adc(carry, carry, zr, carry2);\n+\n+  ror_imm(tmp3, tmp3, 32); \/\/ convert little-endian to big-endian\n+  ror_imm(tmp4, tmp4, 32);\n+  sd(tmp4, Address(tmp6, 0));\n+  sd(tmp3, Address(tmp6, wordSize));\n+\n+  j(L_third_loop);\n+\n+  bind(L_third_loop_exit);\n+\n+  andi(idx, idx, 0x3);\n+  beqz(idx, L_post_third_loop_done);\n+\n+  Label L_check_1;\n+  subw(idx, idx, 2);\n+  bltz(idx, L_check_1);\n+\n+  shadd(t0, idx, y, t0, LogBytesPerInt);\n+  ld(yz_idx1, Address(t0, 0));\n+  ror_imm(yz_idx1, yz_idx1, 32);\n+\n+  mul(tmp3, product_hi, yz_idx1); \/\/  yz_idx1 * product_hi -> tmp4:tmp3\n+  mulhu(tmp4, product_hi, yz_idx1);\n+\n+  shadd(t0, idx, z, t0, LogBytesPerInt);\n+  ld(yz_idx2, Address(t0, 0));\n+  ror_imm(yz_idx2, yz_idx2, 32, tmp);\n+\n+  add2_with_carry(carry, tmp4, tmp3, carry, yz_idx2, tmp);\n+\n+  ror_imm(tmp3, tmp3, 32, tmp);\n+  sd(tmp3, Address(t0, 0));\n+\n+  bind(L_check_1);\n+\n+  andi(idx, idx, 0x1);\n+  subw(idx, idx, 1);\n+  bltz(idx, L_post_third_loop_done);\n+  shadd(t0, idx, y, t0, LogBytesPerInt);\n+  lwu(tmp4, Address(t0, 0));\n+  mul(tmp3, tmp4, product_hi); \/\/  tmp4 * product_hi -> carry2:tmp3\n+  mulhu(carry2, tmp4, product_hi);\n+\n+  shadd(t0, idx, z, t0, LogBytesPerInt);\n+  lwu(tmp4, Address(t0, 0));\n+\n+  add2_with_carry(carry2, carry2, tmp3, tmp4, carry, t0);\n+\n+  shadd(t0, idx, z, t0, LogBytesPerInt);\n+  sw(tmp3, Address(t0, 0));\n+\n+  slli(t0, carry2, 32);\n+  srli(carry, tmp3, 32);\n+  orr(carry, carry, t0);\n+\n+  bind(L_post_third_loop_done);\n+}\n+\n+\/**\n+ * Code for BigInteger::multiplyToLen() intrinsic.\n+ *\n+ * x10: x\n+ * x11: xlen\n+ * x12: y\n+ * x13: ylen\n+ * x14: z\n+ * x15: zlen\n+ * x16: tmp1\n+ * x17: tmp2\n+ * x7:  tmp3\n+ * x28: tmp4\n+ * x29: tmp5\n+ * x30: tmp6\n+ * x31: tmp7\n+ *\/\n+void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen,\n+                                     Register z, Register zlen,\n+                                     Register tmp1, Register tmp2, Register tmp3, Register tmp4,\n+                                     Register tmp5, Register tmp6, Register product_hi)\n+{\n+  assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6);\n+\n+  const Register idx = tmp1;\n+  const Register kdx = tmp2;\n+  const Register xstart = tmp3;\n+\n+  const Register y_idx = tmp4;\n+  const Register carry = tmp5;\n+  const Register product = xlen;\n+  const Register x_xstart = zlen; \/\/ reuse register\n+\n+  mv(idx, ylen); \/\/ idx = ylen;\n+  mv(kdx, zlen); \/\/ kdx = xlen+ylen;\n+  mv(carry, zr); \/\/ carry = 0;\n+\n+  Label L_multiply_64_x_64_loop, L_done;\n+\n+  subw(xstart, xlen, 1);\n+  bltz(xstart, L_done);\n+\n+  const Register jdx = tmp1;\n+\n+  if (AvoidUnalignedAccesses) {\n+    \/\/ Check if x and y are both 8-byte aligned.\n+    orr(t0, xlen, ylen);\n+    andi(t0, t0, 0x1);\n+    beqz(t0, L_multiply_64_x_64_loop);\n+\n+    multiply_32_x_32_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);\n+    shadd(t0, xstart, z, t0, LogBytesPerInt);\n+    sw(carry, Address(t0, 0));\n+\n+    Label L_second_loop_unaligned;\n+    bind(L_second_loop_unaligned);\n+    mv(carry, zr);\n+    mv(jdx, ylen);\n+    subw(xstart, xstart, 1);\n+    bltz(xstart, L_done);\n+    sub(sp, sp, 2 * wordSize);\n+    sd(z, Address(sp, 0));\n+    sd(zr, Address(sp, wordSize));\n+    shadd(t0, xstart, z, t0, LogBytesPerInt);\n+    addi(z, t0, 4);\n+    shadd(t0, xstart, x, t0, LogBytesPerInt);\n+    lwu(product, Address(t0, 0));\n+    Label L_third_loop, L_third_loop_exit;\n+\n+    blez(jdx, L_third_loop_exit);\n+\n+    bind(L_third_loop);\n+    subw(jdx, jdx, 1);\n+    shadd(t0, jdx, y, t0, LogBytesPerInt);\n+    lwu(t0, Address(t0, 0));\n+    mul(t1, t0, product);\n+    add(t0, t1, carry);\n+    shadd(tmp6, jdx, z, t1, LogBytesPerInt);\n+    lwu(t1, Address(tmp6, 0));\n+    add(t0, t0, t1);\n+    sw(t0, Address(tmp6, 0));\n+    srli(carry, t0, 32);\n+    bgtz(jdx, L_third_loop);\n+\n+    bind(L_third_loop_exit);\n+    ld(z, Address(sp, 0));\n+    addi(sp, sp, 2 * wordSize);\n+    shadd(t0, xstart, z, t0, LogBytesPerInt);\n+    sw(carry, Address(t0, 0));\n+\n+    j(L_second_loop_unaligned);\n+  }\n+\n+  bind(L_multiply_64_x_64_loop);\n+  multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);\n+\n+  Label L_second_loop_aligned;\n+  beqz(kdx, L_second_loop_aligned);\n+\n+  Label L_carry;\n+  subw(kdx, kdx, 1);\n+  beqz(kdx, L_carry);\n+\n+  shadd(t0, kdx, z, t0, LogBytesPerInt);\n+  sw(carry, Address(t0, 0));\n+  srli(carry, carry, 32);\n+  subw(kdx, kdx, 1);\n+\n+  bind(L_carry);\n+  shadd(t0, kdx, z, t0, LogBytesPerInt);\n+  sw(carry, Address(t0, 0));\n+\n+  \/\/ Second and third (nested) loops.\n+  \/\/\n+  \/\/ for (int i = xstart-1; i >= 0; i--) { \/\/ Second loop\n+  \/\/   carry = 0;\n+  \/\/   for (int jdx=ystart, k=ystart+1+i; jdx >= 0; jdx--, k--) { \/\/ Third loop\n+  \/\/     long product = (y[jdx] & LONG_MASK) * (x[i] & LONG_MASK) +\n+  \/\/                    (z[k] & LONG_MASK) + carry;\n+  \/\/     z[k] = (int)product;\n+  \/\/     carry = product >>> 32;\n+  \/\/   }\n+  \/\/   z[i] = (int)carry;\n+  \/\/ }\n+  \/\/\n+  \/\/ i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = product_hi\n+\n+  bind(L_second_loop_aligned);\n+  mv(carry, zr); \/\/ carry = 0;\n+  mv(jdx, ylen); \/\/ j = ystart+1\n+\n+  subw(xstart, xstart, 1); \/\/ i = xstart-1;\n+  bltz(xstart, L_done);\n+\n+  sub(sp, sp, 4 * wordSize);\n+  sd(z, Address(sp, 0));\n+\n+  Label L_last_x;\n+  shadd(t0, xstart, z, t0, LogBytesPerInt);\n+  addi(z, t0, 4);\n+  subw(xstart, xstart, 1); \/\/ i = xstart-1;\n+  bltz(xstart, L_last_x);\n+\n+  shadd(t0, xstart, x, t0, LogBytesPerInt);\n+  ld(product_hi, Address(t0, 0));\n+  ror_imm(product_hi, product_hi, 32); \/\/ convert big-endian to little-endian\n+\n+  Label L_third_loop_prologue;\n+  bind(L_third_loop_prologue);\n+\n+  sd(ylen, Address(sp, wordSize));\n+  sd(x, Address(sp, 2 * wordSize));\n+  sd(xstart, Address(sp, 3 * wordSize));\n+  multiply_128_x_128_loop(y, z, carry, x, jdx, ylen, product,\n+                          tmp2, x_xstart, tmp3, tmp4, tmp6, product_hi);\n+  ld(z, Address(sp, 0));\n+  ld(ylen, Address(sp, wordSize));\n+  ld(x, Address(sp, 2 * wordSize));\n+  ld(xlen, Address(sp, 3 * wordSize)); \/\/ copy old xstart -> xlen\n+  addi(sp, sp, 4 * wordSize);\n+\n+  addiw(tmp3, xlen, 1);\n+  shadd(t0, tmp3, z, t0, LogBytesPerInt);\n+  sw(carry, Address(t0, 0));\n+\n+  subw(tmp3, tmp3, 1);\n+  bltz(tmp3, L_done);\n+\n+  srli(carry, carry, 32);\n+  shadd(t0, tmp3, z, t0, LogBytesPerInt);\n+  sw(carry, Address(t0, 0));\n+  j(L_second_loop_aligned);\n+\n+  \/\/ Next infrequent code is moved outside loops.\n+  bind(L_last_x);\n+  lwu(product_hi, Address(x, 0));\n+  j(L_third_loop_prologue);\n+\n+  bind(L_done);\n+}\n+#endif\n+\n+\/\/ Count bits of trailing zero chars from lsb to msb until first non-zero element.\n+\/\/ For LL case, one byte for one element, so shift 8 bits once, and for other case,\n+\/\/ shift 16 bits once.\n+void MacroAssembler::ctzc_bit(Register Rd, Register Rs, bool isLL, Register tmp1, Register tmp2)\n+{\n+  if (UseRVB) {\n+    assert_different_registers(Rd, Rs, tmp1);\n+    int step = isLL ? 8 : 16;\n+    ctz(Rd, Rs);\n+    andi(tmp1, Rd, step - 1);\n+    sub(Rd, Rd, tmp1);\n+    return;\n+  }\n+  assert_different_registers(Rd, Rs, tmp1, tmp2);\n+  Label Loop;\n+  int step = isLL ? 8 : 16;\n+  li(Rd, -step);\n+  mv(tmp2, Rs);\n+\n+  bind(Loop);\n+  addi(Rd, Rd, step);\n+  andi(tmp1, tmp2, ((1 << step) - 1));\n+  srli(tmp2, tmp2, step);\n+  beqz(tmp1, Loop);\n+}\n+\n+\/\/ This instruction reads adjacent 4 bytes from the lower half of source register,\n+\/\/ inflate into a register, for example:\n+\/\/ Rs: A7A6A5A4A3A2A1A0\n+\/\/ Rd: 00A300A200A100A0\n+void MacroAssembler::inflate_lo32(Register Rd, Register Rs, Register tmp1, Register tmp2)\n+{\n+  assert_different_registers(Rd, Rs, tmp1, tmp2);\n+  li(tmp1, 0xFF);\n+  mv(Rd, zr);\n+  for (int i = 0; i <= 3; i++)\n+  {\n+    andr(tmp2, Rs, tmp1);\n+    if (i) {\n+      slli(tmp2, tmp2, i * 8);\n+    }\n+    orr(Rd, Rd, tmp2);\n+    if (i != 3) {\n+      slli(tmp1, tmp1, 8);\n+    }\n+  }\n+}\n+\n+\/\/ This instruction reads adjacent 4 bytes from the upper half of source register,\n+\/\/ inflate into a register, for example:\n+\/\/ Rs: A7A6A5A4A3A2A1A0\n+\/\/ Rd: 00A700A600A500A4\n+void MacroAssembler::inflate_hi32(Register Rd, Register Rs, Register tmp1, Register tmp2)\n+{\n+  assert_different_registers(Rd, Rs, tmp1, tmp2);\n+  li(tmp1, 0xFF00000000);\n+  mv(Rd, zr);\n+  for (int i = 0; i <= 3; i++)\n+  {\n+    andr(tmp2, Rs, tmp1);\n+    orr(Rd, Rd, tmp2);\n+    srli(Rd, Rd, 8);\n+    if (i != 3) {\n+      slli(tmp1, tmp1, 8);\n+    }\n+  }\n+}\n+\n+\/\/ The size of the blocks erased by the zero_blocks stub.  We must\n+\/\/ handle anything smaller than this ourselves in zero_words().\n+const int MacroAssembler::zero_words_block_size = 8;\n+\n+\/\/ zero_words() is used by C2 ClearArray patterns.  It is as small as\n+\/\/ possible, handling small word counts locally and delegating\n+\/\/ anything larger to the zero_blocks stub.  It is expanded many times\n+\/\/ in compiled code, so it is important to keep it short.\n+\n+\/\/ ptr:   Address of a buffer to be zeroed.\n+\/\/ cnt:   Count in HeapWords.\n+\/\/\n+\/\/ ptr, cnt, and t0 are clobbered.\n+address MacroAssembler::zero_words(Register ptr, Register cnt)\n+{\n+  assert(is_power_of_2(zero_words_block_size), \"adjust this\");\n+  assert(ptr == x28 && cnt == x29, \"mismatch in register usage\");\n+  assert_different_registers(cnt, t0);\n+\n+  BLOCK_COMMENT(\"zero_words {\");\n+  mv(t0, zero_words_block_size);\n+  Label around, done, done16;\n+  bltu(cnt, t0, around);\n+  {\n+    RuntimeAddress zero_blocks = RuntimeAddress(StubRoutines::riscv::zero_blocks());\n+    assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+    if (StubRoutines::riscv::complete()) {\n+      address tpc = trampoline_call(zero_blocks);\n+      if (tpc == NULL) {\n+        DEBUG_ONLY(reset_labels1(around));\n+        postcond(pc() == badAddress);\n+        return NULL;\n+      }\n+    } else {\n+      jal(zero_blocks);\n+    }\n+  }\n+  bind(around);\n+  for (int i = zero_words_block_size >> 1; i > 1; i >>= 1) {\n+    Label l;\n+    andi(t0, cnt, i);\n+    beqz(t0, l);\n+    for (int j = 0; j < i; j++) {\n+      sd(zr, Address(ptr, 0));\n+      addi(ptr, ptr, 8);\n+    }\n+    bind(l);\n+  }\n+  {\n+    Label l;\n+    andi(t0, cnt, 1);\n+    beqz(t0, l);\n+    sd(zr, Address(ptr, 0));\n+    bind(l);\n+  }\n+  BLOCK_COMMENT(\"} zero_words\");\n+  postcond(pc() != badAddress);\n+  return pc();\n+}\n+\n+#define SmallArraySize (18 * BytesPerLong)\n+\n+\/\/ base:  Address of a buffer to be zeroed, 8 bytes aligned.\n+\/\/ cnt:   Immediate count in HeapWords.\n+void MacroAssembler::zero_words(Register base, u_int64_t cnt)\n+{\n+  assert_different_registers(base, t0, t1);\n+\n+  BLOCK_COMMENT(\"zero_words {\");\n+\n+  if (cnt <= SmallArraySize \/ BytesPerLong) {\n+    for (int i = 0; i < (int)cnt; i++) {\n+      sd(zr, Address(base, i * wordSize));\n+    }\n+  } else {\n+    const int unroll = 8; \/\/ Number of sd(zr, adr), instructions we'll unroll\n+    int remainder = cnt % unroll;\n+    for (int i = 0; i < remainder; i++) {\n+      sd(zr, Address(base, i * wordSize));\n+    }\n+\n+    Label loop;\n+    Register cnt_reg = t0;\n+    Register loop_base = t1;\n+    cnt = cnt - remainder;\n+    li(cnt_reg, cnt);\n+    add(loop_base, base, remainder * wordSize);\n+    bind(loop);\n+    sub(cnt_reg, cnt_reg, unroll);\n+    for (int i = 0; i < unroll; i++) {\n+      sd(zr, Address(loop_base, i * wordSize));\n+    }\n+    add(loop_base, loop_base, unroll * wordSize);\n+    bnez(cnt_reg, loop);\n+  }\n+\n+  BLOCK_COMMENT(\"} zero_words\");\n+}\n+\n+\/\/ base:   Address of a buffer to be filled, 8 bytes aligned.\n+\/\/ cnt:    Count in 8-byte unit.\n+\/\/ value:  Value to be filled with.\n+\/\/ base will point to the end of the buffer after filling.\n+void MacroAssembler::fill_words(Register base, Register cnt, Register value)\n+{\n+\/\/  Algorithm:\n+\/\/\n+\/\/    t0 = cnt & 7\n+\/\/    cnt -= t0\n+\/\/    p += t0\n+\/\/    switch (t0):\n+\/\/      switch start:\n+\/\/      do while cnt\n+\/\/        cnt -= 8\n+\/\/          p[-8] = value\n+\/\/        case 7:\n+\/\/          p[-7] = value\n+\/\/        case 6:\n+\/\/          p[-6] = value\n+\/\/          \/\/ ...\n+\/\/        case 1:\n+\/\/          p[-1] = value\n+\/\/        case 0:\n+\/\/          p += 8\n+\/\/      do-while end\n+\/\/    switch end\n+\n+  assert_different_registers(base, cnt, value, t0, t1);\n+\n+  Label fini, skip, entry, loop;\n+  const int unroll = 8; \/\/ Number of sd instructions we'll unroll\n+\n+  beqz(cnt, fini);\n+\n+  andi(t0, cnt, unroll - 1);\n+  sub(cnt, cnt, t0);\n+  \/\/ align 8, so first sd n % 8 = mod, next loop sd 8 * n.\n+  shadd(base, t0, base, t1, 3);\n+  la(t1, entry);\n+  slli(t0, t0, 2); \/\/ sd_inst_nums * 4; t0 is cnt % 8, so t1 = t1 - sd_inst_nums * 4, 4 is sizeof(inst)\n+  sub(t1, t1, t0);\n+  jr(t1);\n+\n+  bind(loop);\n+  add(base, base, unroll * 8);\n+  for (int i = -unroll; i < 0; i++) {\n+    sd(value, Address(base, i * 8));\n+  }\n+  bind(entry);\n+  sub(cnt, cnt, unroll);\n+  bgez(cnt, loop);\n+\n+  bind(fini);\n+}\n+\n+#define FCVT_SAFE(FLOATCVT, FLOATEQ)                                                             \\\n+void MacroAssembler:: FLOATCVT##_safe(Register dst, FloatRegister src, Register tmp) {           \\\n+  Label L_Okay;                                                                                  \\\n+  fscsr(zr);                                                                                     \\\n+  FLOATCVT(dst, src);                                                                            \\\n+  frcsr(tmp);                                                                                    \\\n+  andi(tmp, tmp, 0x1E);                                                                          \\\n+  beqz(tmp, L_Okay);                                                                             \\\n+  FLOATEQ(tmp, src, src);                                                                        \\\n+  bnez(tmp, L_Okay);                                                                             \\\n+  mv(dst, zr);                                                                                   \\\n+  bind(L_Okay);                                                                                  \\\n+}\n+\n+FCVT_SAFE(fcvt_w_s, feq_s)\n+FCVT_SAFE(fcvt_l_s, feq_s)\n+FCVT_SAFE(fcvt_w_d, feq_d)\n+FCVT_SAFE(fcvt_l_d, feq_d)\n+\n+#undef FCVT_SAFE\n+\n+#define FCMP(FLOATTYPE, FLOATSIG)                                                       \\\n+void MacroAssembler::FLOATTYPE##_compare(Register result, FloatRegister Rs1,            \\\n+                                         FloatRegister Rs2, int unordered_result) {     \\\n+  Label Ldone;                                                                          \\\n+  if (unordered_result < 0) {                                                           \\\n+    \/* we want -1 for unordered or less than, 0 for equal and 1 for greater than. *\/    \\\n+    \/* installs 1 if gt else 0 *\/                                                       \\\n+    flt_##FLOATSIG(result, Rs2, Rs1);                                                   \\\n+    \/* Rs1 > Rs2, install 1 *\/                                                          \\\n+    bgtz(result, Ldone);                                                                \\\n+    feq_##FLOATSIG(result, Rs1, Rs2);                                                   \\\n+    addi(result, result, -1);                                                           \\\n+    \/* Rs1 = Rs2, install 0 *\/                                                          \\\n+    \/* NaN or Rs1 < Rs2, install -1 *\/                                                  \\\n+    bind(Ldone);                                                                        \\\n+  } else {                                                                              \\\n+    \/* we want -1 for less than, 0 for equal and 1 for unordered or greater than. *\/    \\\n+    \/* installs 1 if gt or unordered else 0 *\/                                          \\\n+    flt_##FLOATSIG(result, Rs1, Rs2);                                                   \\\n+    \/* Rs1 < Rs2, install -1 *\/                                                         \\\n+    bgtz(result, Ldone);                                                                \\\n+    feq_##FLOATSIG(result, Rs1, Rs2);                                                   \\\n+    addi(result, result, -1);                                                           \\\n+    \/* Rs1 = Rs2, install 0 *\/                                                          \\\n+    \/* NaN or Rs1 > Rs2, install 1 *\/                                                   \\\n+    bind(Ldone);                                                                        \\\n+    neg(result, result);                                                                \\\n+  }                                                                                     \\\n+}\n+\n+FCMP(float, s);\n+FCMP(double, d);\n+\n+#undef FCMP\n+\n+\/\/ Zero words; len is in bytes\n+\/\/ Destroys all registers except addr\n+\/\/ len must be a nonzero multiple of wordSize\n+void MacroAssembler::zero_memory(Register addr, Register len, Register tmp) {\n+  assert_different_registers(addr, len, tmp, t0, t1);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    andi(t0, len, BytesPerWord - 1);\n+    beqz(t0, L);\n+    stop(\"len is not a multiple of BytesPerWord\");\n+    bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+#ifndef PRODUCT\n+  block_comment(\"zero memory\");\n+#endif \/\/ PRODUCT\n+\n+  Label loop;\n+  Label entry;\n+\n+  \/\/ Algorithm:\n+  \/\/\n+  \/\/  t0 = cnt & 7\n+  \/\/  cnt -= t0\n+  \/\/  p += t0\n+  \/\/  switch (t0) {\n+  \/\/    do {\n+  \/\/      cnt -= 8\n+  \/\/        p[-8] = 0\n+  \/\/      case 7:\n+  \/\/        p[-7] = 0\n+  \/\/      case 6:\n+  \/\/        p[-6] = 0\n+  \/\/        ...\n+  \/\/      case 1:\n+  \/\/        p[-1] = 0\n+  \/\/      case 0:\n+  \/\/        p += 8\n+  \/\/     } while (cnt)\n+  \/\/  }\n+\n+  const int unroll = 8;   \/\/ Number of sd(zr) instructions we'll unroll\n+\n+  srli(len, len, LogBytesPerWord);\n+  andi(t0, len, unroll - 1);  \/\/ t0 = cnt % unroll\n+  sub(len, len, t0);          \/\/ cnt -= unroll\n+  \/\/ tmp always points to the end of the region we're about to zero\n+  shadd(tmp, t0, addr, t1, LogBytesPerWord);\n+  la(t1, entry);\n+  slli(t0, t0, 2);\n+  sub(t1, t1, t0);\n+  jr(t1);\n+  bind(loop);\n+  sub(len, len, unroll);\n+  for (int i = -unroll; i < 0; i++) {\n+    Assembler::sd(zr, Address(tmp, i * wordSize));\n+  }\n+  bind(entry);\n+  add(tmp, tmp, unroll * wordSize);\n+  bnez(len, loop);\n+}\n+\n+\/\/ shift left by shamt and add\n+\/\/ Rd = (Rs1 << shamt) + Rs2\n+void MacroAssembler::shadd(Register Rd, Register Rs1, Register Rs2, Register tmp, int shamt) {\n+  if (UseRVB) {\n+    if (shamt == 1) {\n+      sh1add(Rd, Rs1, Rs2);\n+      return;\n+    } else if (shamt == 2) {\n+      sh2add(Rd, Rs1, Rs2);\n+      return;\n+    } else if (shamt == 3) {\n+      sh3add(Rd, Rs1, Rs2);\n+      return;\n+    }\n+  }\n+\n+  if (shamt != 0) {\n+    slli(tmp, Rs1, shamt);\n+    add(Rd, Rs2, tmp);\n+  } else {\n+    add(Rd, Rs1, Rs2);\n+  }\n+}\n+\n+void MacroAssembler::zero_extend(Register dst, Register src, int bits) {\n+  if (UseRVB) {\n+    if (bits == 16) {\n+      zext_h(dst, src);\n+      return;\n+    } else if (bits == 32) {\n+      zext_w(dst, src);\n+      return;\n+    }\n+  }\n+\n+  if (bits == 8) {\n+    zext_b(dst, src);\n+  } else {\n+    slli(dst, src, XLEN - bits);\n+    srli(dst, dst, XLEN - bits);\n+  }\n+}\n+\n+void MacroAssembler::sign_extend(Register dst, Register src, int bits) {\n+  if (UseRVB) {\n+    if (bits == 8) {\n+      sext_b(dst, src);\n+      return;\n+    } else if (bits == 16) {\n+      sext_h(dst, src);\n+      return;\n+    }\n+  }\n+\n+  if (bits == 32) {\n+    sext_w(dst, src);\n+  } else {\n+    slli(dst, src, XLEN - bits);\n+    srai(dst, dst, XLEN - bits);\n+  }\n+}\n+\n+void MacroAssembler::cmp_l2i(Register dst, Register src1, Register src2, Register tmp)\n+{\n+  if (src1 == src2) {\n+    mv(dst, zr);\n+    return;\n+  }\n+  Label done;\n+  Register left = src1;\n+  Register right = src2;\n+  if (dst == src1) {\n+    assert_different_registers(dst, src2, tmp);\n+    mv(tmp, src1);\n+    left = tmp;\n+  } else if (dst == src2) {\n+    assert_different_registers(dst, src1, tmp);\n+    mv(tmp, src2);\n+    right = tmp;\n+  }\n+\n+  \/\/ installs 1 if gt else 0\n+  slt(dst, right, left);\n+  bnez(dst, done);\n+  slt(dst, left, right);\n+  \/\/ dst = -1 if lt; else if eq , dst = 0\n+  neg(dst, dst);\n+  bind(done);\n+}\n+\n+void MacroAssembler::safepoint_ifence() {\n+  ifence();\n+}\n+\n+#ifdef COMPILER2\n+\/\/ short string\n+\/\/ StringUTF16.indexOfChar\n+\/\/ StringLatin1.indexOfChar\n+void MacroAssembler::string_indexof_char_short(Register str1, Register cnt1,\n+                                                  Register ch, Register result,\n+                                                  bool isL)\n+{\n+  Register ch1 = t0;\n+  Register index = t1;\n+\n+  BLOCK_COMMENT(\"string_indexof_char_short {\");\n+\n+  Label LOOP, LOOP1, LOOP4, LOOP8;\n+  Label MATCH,  MATCH1, MATCH2, MATCH3,\n+          MATCH4, MATCH5, MATCH6, MATCH7, NOMATCH;\n+\n+  mv(result, -1);\n+  mv(index, zr);\n+\n+  bind(LOOP);\n+  addi(t0, index, 8);\n+  ble(t0, cnt1, LOOP8);\n+  addi(t0, index, 4);\n+  ble(t0, cnt1, LOOP4);\n+  j(LOOP1);\n+\n+  bind(LOOP8);\n+  isL ? lbu(ch1, Address(str1, 0)) : lhu(ch1, Address(str1, 0));\n+  beq(ch, ch1, MATCH);\n+  isL ? lbu(ch1, Address(str1, 1)) : lhu(ch1, Address(str1, 2));\n+  beq(ch, ch1, MATCH1);\n+  isL ? lbu(ch1, Address(str1, 2)) : lhu(ch1, Address(str1, 4));\n+  beq(ch, ch1, MATCH2);\n+  isL ? lbu(ch1, Address(str1, 3)) : lhu(ch1, Address(str1, 6));\n+  beq(ch, ch1, MATCH3);\n+  isL ? lbu(ch1, Address(str1, 4)) : lhu(ch1, Address(str1, 8));\n+  beq(ch, ch1, MATCH4);\n+  isL ? lbu(ch1, Address(str1, 5)) : lhu(ch1, Address(str1, 10));\n+  beq(ch, ch1, MATCH5);\n+  isL ? lbu(ch1, Address(str1, 6)) : lhu(ch1, Address(str1, 12));\n+  beq(ch, ch1, MATCH6);\n+  isL ? lbu(ch1, Address(str1, 7)) : lhu(ch1, Address(str1, 14));\n+  beq(ch, ch1, MATCH7);\n+  addi(index, index, 8);\n+  addi(str1, str1, isL ? 8 : 16);\n+  blt(index, cnt1, LOOP);\n+  j(NOMATCH);\n+\n+  bind(LOOP4);\n+  isL ? lbu(ch1, Address(str1, 0)) : lhu(ch1, Address(str1, 0));\n+  beq(ch, ch1, MATCH);\n+  isL ? lbu(ch1, Address(str1, 1)) : lhu(ch1, Address(str1, 2));\n+  beq(ch, ch1, MATCH1);\n+  isL ? lbu(ch1, Address(str1, 2)) : lhu(ch1, Address(str1, 4));\n+  beq(ch, ch1, MATCH2);\n+  isL ? lbu(ch1, Address(str1, 3)) : lhu(ch1, Address(str1, 6));\n+  beq(ch, ch1, MATCH3);\n+  addi(index, index, 4);\n+  addi(str1, str1, isL ? 4 : 8);\n+  bge(index, cnt1, NOMATCH);\n+\n+  bind(LOOP1);\n+  isL ? lbu(ch1, Address(str1)) : lhu(ch1, Address(str1));\n+  beq(ch, ch1, MATCH);\n+  addi(index, index, 1);\n+  addi(str1, str1, isL ? 1 : 2);\n+  blt(index, cnt1, LOOP1);\n+  j(NOMATCH);\n+\n+  bind(MATCH1);\n+  addi(index, index, 1);\n+  j(MATCH);\n+\n+  bind(MATCH2);\n+  addi(index, index, 2);\n+  j(MATCH);\n+\n+  bind(MATCH3);\n+  addi(index, index, 3);\n+  j(MATCH);\n+\n+  bind(MATCH4);\n+  addi(index, index, 4);\n+  j(MATCH);\n+\n+  bind(MATCH5);\n+  addi(index, index, 5);\n+  j(MATCH);\n+\n+  bind(MATCH6);\n+  addi(index, index, 6);\n+  j(MATCH);\n+\n+  bind(MATCH7);\n+  addi(index, index, 7);\n+\n+  bind(MATCH);\n+  mv(result, index);\n+  bind(NOMATCH);\n+  BLOCK_COMMENT(\"} string_indexof_char_short\");\n+}\n+\n+\/\/ StringUTF16.indexOfChar\n+\/\/ StringLatin1.indexOfChar\n+void MacroAssembler::string_indexof_char(Register str1, Register cnt1,\n+                                            Register ch, Register result,\n+                                            Register tmp1, Register tmp2,\n+                                            Register tmp3, Register tmp4,\n+                                            bool isL)\n+{\n+  Label CH1_LOOP, HIT, NOMATCH, DONE, DO_LONG;\n+  Register ch1 = t0;\n+  Register orig_cnt = t1;\n+  Register mask1 = tmp3;\n+  Register mask2 = tmp2;\n+  Register match_mask = tmp1;\n+  Register trailing_char = tmp4;\n+  Register unaligned_elems = tmp4;\n+\n+  BLOCK_COMMENT(\"string_indexof_char {\");\n+  beqz(cnt1, NOMATCH);\n+\n+  addi(t0, cnt1, isL ? -32 : -16);\n+  bgtz(t0, DO_LONG);\n+  string_indexof_char_short(str1, cnt1, ch, result, isL);\n+  j(DONE);\n+\n+  bind(DO_LONG);\n+  mv(orig_cnt, cnt1);\n+  if (AvoidUnalignedAccesses) {\n+    Label ALIGNED;\n+    andi(unaligned_elems, str1, 0x7);\n+    beqz(unaligned_elems, ALIGNED);\n+    sub(unaligned_elems, unaligned_elems, 8);\n+    neg(unaligned_elems, unaligned_elems);\n+    if (!isL) {\n+      srli(unaligned_elems, unaligned_elems, 1);\n+    }\n+    \/\/ do unaligned part per element\n+    string_indexof_char_short(str1, unaligned_elems, ch, result, isL);\n+    bgez(result, DONE);\n+    mv(orig_cnt, cnt1);\n+    sub(cnt1, cnt1, unaligned_elems);\n+    bind(ALIGNED);\n+  }\n+\n+  \/\/ duplicate ch\n+  if (isL) {\n+    slli(ch1, ch, 8);\n+    orr(ch, ch1, ch);\n+  }\n+  slli(ch1, ch, 16);\n+  orr(ch, ch1, ch);\n+  slli(ch1, ch, 32);\n+  orr(ch, ch1, ch);\n+\n+  if (!isL) {\n+    slli(cnt1, cnt1, 1);\n+  }\n+\n+  uint64_t mask0101 = UCONST64(0x0101010101010101);\n+  uint64_t mask0001 = UCONST64(0x0001000100010001);\n+  mv(mask1, isL ? mask0101 : mask0001);\n+  uint64_t mask7f7f = UCONST64(0x7f7f7f7f7f7f7f7f);\n+  uint64_t mask7fff = UCONST64(0x7fff7fff7fff7fff);\n+  mv(mask2, isL ? mask7f7f : mask7fff);\n+\n+  bind(CH1_LOOP);\n+  ld(ch1, Address(str1));\n+  addi(str1, str1, 8);\n+  addi(cnt1, cnt1, -8);\n+  compute_match_mask(ch1, ch, match_mask, mask1, mask2);\n+  bnez(match_mask, HIT);\n+  bgtz(cnt1, CH1_LOOP);\n+  j(NOMATCH);\n+\n+  bind(HIT);\n+  ctzc_bit(trailing_char, match_mask, isL, ch1, result);\n+  srli(trailing_char, trailing_char, 3);\n+  addi(cnt1, cnt1, 8);\n+  ble(cnt1, trailing_char, NOMATCH);\n+  \/\/ match case\n+  if (!isL) {\n+    srli(cnt1, cnt1, 1);\n+    srli(trailing_char, trailing_char, 1);\n+  }\n+\n+  sub(result, orig_cnt, cnt1);\n+  add(result, result, trailing_char);\n+  j(DONE);\n+\n+  bind(NOMATCH);\n+  mv(result, -1);\n+\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} string_indexof_char\");\n+}\n+\n+typedef void (MacroAssembler::* load_chr_insn)(Register rd, const Address &adr, Register temp);\n+\n+\/\/ Search for needle in haystack and return index or -1\n+\/\/ x10: result\n+\/\/ x11: haystack\n+\/\/ x12: haystack_len\n+\/\/ x13: needle\n+\/\/ x14: needle_len\n+void MacroAssembler::string_indexof(Register haystack, Register needle,\n+                                       Register haystack_len, Register needle_len,\n+                                       Register tmp1, Register tmp2,\n+                                       Register tmp3, Register tmp4,\n+                                       Register tmp5, Register tmp6,\n+                                       Register result, int ae)\n+{\n+  assert(ae != StrIntrinsicNode::LU, \"Invalid encoding\");\n+\n+  Label LINEARSEARCH, LINEARSTUB, DONE, NOMATCH;\n+\n+  Register ch1 = t0;\n+  Register ch2 = t1;\n+  Register nlen_tmp = tmp1; \/\/ needle len tmp\n+  Register hlen_tmp = tmp2; \/\/ haystack len tmp\n+  Register result_tmp = tmp4;\n+\n+  bool isLL = ae == StrIntrinsicNode::LL;\n+\n+  bool needle_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL;\n+  bool haystack_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::LU;\n+  int needle_chr_shift = needle_isL ? 0 : 1;\n+  int haystack_chr_shift = haystack_isL ? 0 : 1;\n+  int needle_chr_size = needle_isL ? 1 : 2;\n+  int haystack_chr_size = haystack_isL ? 1 : 2;\n+  load_chr_insn needle_load_1chr = needle_isL ? (load_chr_insn)&MacroAssembler::lbu :\n+                                   (load_chr_insn)&MacroAssembler::lhu;\n+  load_chr_insn haystack_load_1chr = haystack_isL ? (load_chr_insn)&MacroAssembler::lbu :\n+                                     (load_chr_insn)&MacroAssembler::lhu;\n+\n+  BLOCK_COMMENT(\"string_indexof {\");\n+\n+  \/\/ Note, inline_string_indexOf() generates checks:\n+  \/\/ if (pattern.count > src.count) return -1;\n+  \/\/ if (pattern.count == 0) return 0;\n+\n+  \/\/ We have two strings, a source string in haystack, haystack_len and a pattern string\n+  \/\/ in needle, needle_len. Find the first occurence of pattern in source or return -1.\n+\n+  \/\/ For larger pattern and source we use a simplified Boyer Moore algorithm.\n+  \/\/ With a small pattern and source we use linear scan.\n+\n+  \/\/ needle_len >=8 && needle_len < 256 && needle_len < haystack_len\/4, use bmh algorithm.\n+  sub(result_tmp, haystack_len, needle_len);\n+  \/\/ needle_len < 8, use linear scan\n+  sub(t0, needle_len, 8);\n+  bltz(t0, LINEARSEARCH);\n+  \/\/ needle_len >= 256, use linear scan\n+  sub(t0, needle_len, 256);\n+  bgez(t0, LINEARSTUB);\n+  \/\/ needle_len >= haystack_len\/4, use linear scan\n+  srli(t0, haystack_len, 2);\n+  bge(needle_len, t0, LINEARSTUB);\n+\n+  \/\/ Boyer-Moore-Horspool introduction:\n+  \/\/ The Boyer Moore alogorithm is based on the description here:-\n+  \/\/\n+  \/\/ http:\/\/en.wikipedia.org\/wiki\/Boyer%E2%80%93Moore_string_search_algorithm\n+  \/\/\n+  \/\/ This describes and algorithm with 2 shift rules. The 'Bad Character' rule\n+  \/\/ and the 'Good Suffix' rule.\n+  \/\/\n+  \/\/ These rules are essentially heuristics for how far we can shift the\n+  \/\/ pattern along the search string.\n+  \/\/\n+  \/\/ The implementation here uses the 'Bad Character' rule only because of the\n+  \/\/ complexity of initialisation for the 'Good Suffix' rule.\n+  \/\/\n+  \/\/ This is also known as the Boyer-Moore-Horspool algorithm:\n+  \/\/\n+  \/\/ http:\/\/en.wikipedia.org\/wiki\/Boyer-Moore-Horspool_algorithm\n+  \/\/\n+  \/\/ #define ASIZE 256\n+  \/\/\n+  \/\/    int bm(unsigned char *pattern, int m, unsigned char *src, int n) {\n+  \/\/      int i, j;\n+  \/\/      unsigned c;\n+  \/\/      unsigned char bc[ASIZE];\n+  \/\/\n+  \/\/      \/* Preprocessing *\/\n+  \/\/      for (i = 0; i < ASIZE; ++i)\n+  \/\/        bc[i] = m;\n+  \/\/      for (i = 0; i < m - 1; ) {\n+  \/\/        c = pattern[i];\n+  \/\/        ++i;\n+  \/\/        \/\/ c < 256 for Latin1 string, so, no need for branch\n+  \/\/        #ifdef PATTERN_STRING_IS_LATIN1\n+  \/\/        bc[c] = m - i;\n+  \/\/        #else\n+  \/\/        if (c < ASIZE) bc[c] = m - i;\n+  \/\/        #endif\n+  \/\/      }\n+  \/\/\n+  \/\/      \/* Searching *\/\n+  \/\/      j = 0;\n+  \/\/      while (j <= n - m) {\n+  \/\/        c = src[i+j];\n+  \/\/        if (pattern[m-1] == c)\n+  \/\/          int k;\n+  \/\/          for (k = m - 2; k >= 0 && pattern[k] == src[k + j]; --k);\n+  \/\/          if (k < 0) return j;\n+  \/\/          \/\/ c < 256 for Latin1 string, so, no need for branch\n+  \/\/          #ifdef SOURCE_STRING_IS_LATIN1_AND_PATTERN_STRING_IS_LATIN1\n+  \/\/          \/\/ LL case: (c< 256) always true. Remove branch\n+  \/\/          j += bc[pattern[j+m-1]];\n+  \/\/          #endif\n+  \/\/          #ifdef SOURCE_STRING_IS_UTF_AND_PATTERN_STRING_IS_UTF\n+  \/\/          \/\/ UU case: need if (c<ASIZE) check. Skip 1 character if not.\n+  \/\/          if (c < ASIZE)\n+  \/\/            j += bc[pattern[j+m-1]];\n+  \/\/          else\n+  \/\/            j += 1\n+  \/\/          #endif\n+  \/\/          #ifdef SOURCE_IS_UTF_AND_PATTERN_IS_LATIN1\n+  \/\/          \/\/ UL case: need if (c<ASIZE) check. Skip <pattern length> if not.\n+  \/\/          if (c < ASIZE)\n+  \/\/            j += bc[pattern[j+m-1]];\n+  \/\/          else\n+  \/\/            j += m\n+  \/\/          #endif\n+  \/\/      }\n+  \/\/      return -1;\n+  \/\/    }\n+\n+  \/\/ temp register:t0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, result\n+  Label BCLOOP, BCSKIP, BMLOOPSTR2, BMLOOPSTR1, BMSKIP, BMADV, BMMATCH,\n+          BMLOOPSTR1_LASTCMP, BMLOOPSTR1_CMP, BMLOOPSTR1_AFTER_LOAD, BM_INIT_LOOP;\n+\n+  Register haystack_end = haystack_len;\n+  Register skipch = tmp2;\n+\n+  \/\/ pattern length is >=8, so, we can read at least 1 register for cases when\n+  \/\/ UTF->Latin1 conversion is not needed(8 LL or 4UU) and half register for\n+  \/\/ UL case. We'll re-read last character in inner pre-loop code to have\n+  \/\/ single outer pre-loop load\n+  const int firstStep = isLL ? 7 : 3;\n+\n+  const int ASIZE = 256;\n+  const int STORE_BYTES = 8; \/\/ 8 bytes stored per instruction(sd)\n+\n+  sub(sp, sp, ASIZE);\n+\n+  \/\/ init BC offset table with default value: needle_len\n+  slli(t0, needle_len, 8);\n+  orr(t0, t0, needle_len); \/\/ [63...16][needle_len][needle_len]\n+  slli(tmp1, t0, 16);\n+  orr(t0, tmp1, t0); \/\/ [63...32][needle_len][needle_len][needle_len][needle_len]\n+  slli(tmp1, t0, 32);\n+  orr(tmp5, tmp1, t0); \/\/ tmp5: 8 elements [needle_len]\n+\n+  mv(ch1, sp);  \/\/ ch1 is t0\n+  mv(tmp6, ASIZE \/ STORE_BYTES); \/\/ loop iterations\n+\n+  bind(BM_INIT_LOOP);\n+  \/\/ for (i = 0; i < ASIZE; ++i)\n+  \/\/   bc[i] = m;\n+  for (int i = 0; i < 4; i++) {\n+    sd(tmp5, Address(ch1, i * wordSize));\n+  }\n+  add(ch1, ch1, 32);\n+  sub(tmp6, tmp6, 4);\n+  bgtz(tmp6, BM_INIT_LOOP);\n+\n+  sub(nlen_tmp, needle_len, 1); \/\/ m - 1, index of the last element in pattern\n+  Register orig_haystack = tmp5;\n+  mv(orig_haystack, haystack);\n+  \/\/ result_tmp = tmp4\n+  shadd(haystack_end, result_tmp, haystack, haystack_end, haystack_chr_shift);\n+  sub(ch2, needle_len, 1); \/\/ bc offset init value, ch2 is t1\n+  mv(tmp3, needle);\n+\n+  \/\/  for (i = 0; i < m - 1; ) {\n+  \/\/    c = pattern[i];\n+  \/\/    ++i;\n+  \/\/    \/\/ c < 256 for Latin1 string, so, no need for branch\n+  \/\/    #ifdef PATTERN_STRING_IS_LATIN1\n+  \/\/    bc[c] = m - i;\n+  \/\/    #else\n+  \/\/    if (c < ASIZE) bc[c] = m - i;\n+  \/\/    #endif\n+  \/\/  }\n+  bind(BCLOOP);\n+  (this->*needle_load_1chr)(ch1, Address(tmp3), noreg);\n+  add(tmp3, tmp3, needle_chr_size);\n+  if (!needle_isL) {\n+    \/\/ ae == StrIntrinsicNode::UU\n+    mv(tmp6, ASIZE);\n+    bgeu(ch1, tmp6, BCSKIP);\n+  }\n+  add(tmp4, sp, ch1);\n+  sb(ch2, Address(tmp4)); \/\/ store skip offset to BC offset table\n+\n+  bind(BCSKIP);\n+  sub(ch2, ch2, 1); \/\/ for next pattern element, skip distance -1\n+  bgtz(ch2, BCLOOP);\n+\n+  \/\/ tmp6: pattern end, address after needle\n+  shadd(tmp6, needle_len, needle, tmp6, needle_chr_shift);\n+  if (needle_isL == haystack_isL) {\n+    \/\/ load last 8 bytes (8LL\/4UU symbols)\n+    ld(tmp6, Address(tmp6, -wordSize));\n+  } else {\n+    \/\/ UL: from UTF-16(source) search Latin1(pattern)\n+    lwu(tmp6, Address(tmp6, -wordSize \/ 2)); \/\/ load last 4 bytes(4 symbols)\n+    \/\/ convert Latin1 to UTF. eg: 0x0000abcd -> 0x0a0b0c0d\n+    \/\/ We'll have to wait until load completed, but it's still faster than per-character loads+checks\n+    srli(tmp3, tmp6, BitsPerByte * (wordSize \/ 2 - needle_chr_size)); \/\/ pattern[m-1], eg:0x0000000a\n+    slli(ch2, tmp6, XLEN - 24);\n+    srli(ch2, ch2, XLEN - 8); \/\/ pattern[m-2], 0x0000000b\n+    slli(ch1, tmp6, XLEN - 16);\n+    srli(ch1, ch1, XLEN - 8); \/\/ pattern[m-3], 0x0000000c\n+    andi(tmp6, tmp6, 0xff); \/\/ pattern[m-4], 0x0000000d\n+    slli(ch2, ch2, 16);\n+    orr(ch2, ch2, ch1); \/\/ 0x00000b0c\n+    slli(result, tmp3, 48); \/\/ use result as temp register\n+    orr(tmp6, tmp6, result); \/\/ 0x0a00000d\n+    slli(result, ch2, 16);\n+    orr(tmp6, tmp6, result); \/\/ UTF-16:0x0a0b0c0d\n+  }\n+\n+  \/\/ i = m - 1;\n+  \/\/ skipch = j + i;\n+  \/\/ if (skipch == pattern[m - 1]\n+  \/\/   for (k = m - 2; k >= 0 && pattern[k] == src[k + j]; --k);\n+  \/\/ else\n+  \/\/   move j with bad char offset table\n+  bind(BMLOOPSTR2);\n+  \/\/ compare pattern to source string backward\n+  shadd(result, nlen_tmp, haystack, result, haystack_chr_shift);\n+  (this->*haystack_load_1chr)(skipch, Address(result), noreg);\n+  sub(nlen_tmp, nlen_tmp, firstStep); \/\/ nlen_tmp is positive here, because needle_len >= 8\n+  if (needle_isL == haystack_isL) {\n+    \/\/ re-init tmp3. It's for free because it's executed in parallel with\n+    \/\/ load above. Alternative is to initialize it before loop, but it'll\n+    \/\/ affect performance on in-order systems with 2 or more ld\/st pipelines\n+    srli(tmp3, tmp6, BitsPerByte * (wordSize - needle_chr_size)); \/\/ UU\/LL: pattern[m-1]\n+  }\n+  if (!isLL) { \/\/ UU\/UL case\n+    slli(ch2, nlen_tmp, 1); \/\/ offsets in bytes\n+  }\n+  bne(tmp3, skipch, BMSKIP); \/\/ if not equal, skipch is bad char\n+  add(result, haystack, isLL ? nlen_tmp : ch2);\n+  ld(ch2, Address(result)); \/\/ load 8 bytes from source string\n+  mv(ch1, tmp6);\n+  if (isLL) {\n+    j(BMLOOPSTR1_AFTER_LOAD);\n+  } else {\n+    sub(nlen_tmp, nlen_tmp, 1); \/\/ no need to branch for UU\/UL case. cnt1 >= 8\n+    j(BMLOOPSTR1_CMP);\n+  }\n+\n+  bind(BMLOOPSTR1);\n+  shadd(ch1, nlen_tmp, needle, ch1, needle_chr_shift);\n+  (this->*needle_load_1chr)(ch1, Address(ch1), noreg);\n+  shadd(ch2, nlen_tmp, haystack, ch2, haystack_chr_shift);\n+  (this->*haystack_load_1chr)(ch2, Address(ch2), noreg);\n+\n+  bind(BMLOOPSTR1_AFTER_LOAD);\n+  sub(nlen_tmp, nlen_tmp, 1);\n+  bltz(nlen_tmp, BMLOOPSTR1_LASTCMP);\n+\n+  bind(BMLOOPSTR1_CMP);\n+  beq(ch1, ch2, BMLOOPSTR1);\n+\n+  bind(BMSKIP);\n+  if (!isLL) {\n+    \/\/ if we've met UTF symbol while searching Latin1 pattern, then we can\n+    \/\/ skip needle_len symbols\n+    if (needle_isL != haystack_isL) {\n+      mv(result_tmp, needle_len);\n+    } else {\n+      mv(result_tmp, 1);\n+    }\n+    mv(t0, ASIZE);\n+    bgeu(skipch, t0, BMADV);\n+  }\n+  add(result_tmp, sp, skipch);\n+  lbu(result_tmp, Address(result_tmp)); \/\/ load skip offset\n+\n+  bind(BMADV);\n+  sub(nlen_tmp, needle_len, 1);\n+  \/\/ move haystack after bad char skip offset\n+  shadd(haystack, result_tmp, haystack, result, haystack_chr_shift);\n+  ble(haystack, haystack_end, BMLOOPSTR2);\n+  add(sp, sp, ASIZE);\n+  j(NOMATCH);\n+\n+  bind(BMLOOPSTR1_LASTCMP);\n+  bne(ch1, ch2, BMSKIP);\n+\n+  bind(BMMATCH);\n+  sub(result, haystack, orig_haystack);\n+  if (!haystack_isL) {\n+    srli(result, result, 1);\n+  }\n+  add(sp, sp, ASIZE);\n+  j(DONE);\n+\n+  bind(LINEARSTUB);\n+  sub(t0, needle_len, 16); \/\/ small patterns still should be handled by simple algorithm\n+  bltz(t0, LINEARSEARCH);\n+  mv(result, zr);\n+  RuntimeAddress stub = NULL;\n+  if (isLL) {\n+    stub = RuntimeAddress(StubRoutines::riscv::string_indexof_linear_ll());\n+    assert(stub.target() != NULL, \"string_indexof_linear_ll stub has not been generated\");\n+  } else if (needle_isL) {\n+    stub = RuntimeAddress(StubRoutines::riscv::string_indexof_linear_ul());\n+    assert(stub.target() != NULL, \"string_indexof_linear_ul stub has not been generated\");\n+  } else {\n+    stub = RuntimeAddress(StubRoutines::riscv::string_indexof_linear_uu());\n+    assert(stub.target() != NULL, \"string_indexof_linear_uu stub has not been generated\");\n+  }\n+  trampoline_call(stub);\n+  j(DONE);\n+\n+  bind(NOMATCH);\n+  mv(result, -1);\n+  j(DONE);\n+\n+  bind(LINEARSEARCH);\n+  string_indexof_linearscan(haystack, needle, haystack_len, needle_len, tmp1, tmp2, tmp3, tmp4, -1, result, ae);\n+\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} string_indexof\");\n+}\n+\n+\/\/ string_indexof\n+\/\/ result: x10\n+\/\/ src: x11\n+\/\/ src_count: x12\n+\/\/ pattern: x13\n+\/\/ pattern_count: x14 or 1\/2\/3\/4\n+void MacroAssembler::string_indexof_linearscan(Register haystack, Register needle,\n+                                                  Register haystack_len, Register needle_len,\n+                                                  Register tmp1, Register tmp2,\n+                                                  Register tmp3, Register tmp4,\n+                                                  int needle_con_cnt, Register result, int ae)\n+{\n+  \/\/ Note:\n+  \/\/ needle_con_cnt > 0 means needle_len register is invalid, needle length is constant\n+  \/\/ for UU\/LL: needle_con_cnt[1, 4], UL: needle_con_cnt = 1\n+  assert(needle_con_cnt <= 4, \"Invalid needle constant count\");\n+  assert(ae != StrIntrinsicNode::LU, \"Invalid encoding\");\n+\n+  Register ch1 = t0;\n+  Register ch2 = t1;\n+  Register hlen_neg = haystack_len, nlen_neg = needle_len;\n+  Register nlen_tmp = tmp1, hlen_tmp = tmp2, result_tmp = tmp4;\n+\n+  bool isLL = ae == StrIntrinsicNode::LL;\n+\n+  bool needle_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL;\n+  bool haystack_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::LU;\n+  int needle_chr_shift = needle_isL ? 0 : 1;\n+  int haystack_chr_shift = haystack_isL ? 0 : 1;\n+  int needle_chr_size = needle_isL ? 1 : 2;\n+  int haystack_chr_size = haystack_isL ? 1 : 2;\n+\n+  load_chr_insn needle_load_1chr = needle_isL ? (load_chr_insn)&MacroAssembler::lbu :\n+                                   (load_chr_insn)&MacroAssembler::lhu;\n+  load_chr_insn haystack_load_1chr = haystack_isL ? (load_chr_insn)&MacroAssembler::lbu :\n+                                     (load_chr_insn)&MacroAssembler::lhu;\n+  load_chr_insn load_2chr = isLL ? (load_chr_insn)&MacroAssembler::lhu : (load_chr_insn)&MacroAssembler::lwu;\n+  load_chr_insn load_4chr = isLL ? (load_chr_insn)&MacroAssembler::lwu : (load_chr_insn)&MacroAssembler::ld;\n+\n+  Label DO1, DO2, DO3, MATCH, NOMATCH, DONE;\n+\n+  Register first = tmp3;\n+\n+  if (needle_con_cnt == -1) {\n+    Label DOSHORT, FIRST_LOOP, STR2_NEXT, STR1_LOOP, STR1_NEXT;\n+\n+    sub(t0, needle_len, needle_isL == haystack_isL ? 4 : 2);\n+    bltz(t0, DOSHORT);\n+\n+    (this->*needle_load_1chr)(first, Address(needle), noreg);\n+    slli(t0, needle_len, needle_chr_shift);\n+    add(needle, needle, t0);\n+    neg(nlen_neg, t0);\n+    slli(t0, result_tmp, haystack_chr_shift);\n+    add(haystack, haystack, t0);\n+    neg(hlen_neg, t0);\n+\n+    bind(FIRST_LOOP);\n+    add(t0, haystack, hlen_neg);\n+    (this->*haystack_load_1chr)(ch2, Address(t0), noreg);\n+    beq(first, ch2, STR1_LOOP);\n+\n+    bind(STR2_NEXT);\n+    add(hlen_neg, hlen_neg, haystack_chr_size);\n+    blez(hlen_neg, FIRST_LOOP);\n+    j(NOMATCH);\n+\n+    bind(STR1_LOOP);\n+    add(nlen_tmp, nlen_neg, needle_chr_size);\n+    add(hlen_tmp, hlen_neg, haystack_chr_size);\n+    bgez(nlen_tmp, MATCH);\n+\n+    bind(STR1_NEXT);\n+    add(ch1, needle, nlen_tmp);\n+    (this->*needle_load_1chr)(ch1, Address(ch1), noreg);\n+    add(ch2, haystack, hlen_tmp);\n+    (this->*haystack_load_1chr)(ch2, Address(ch2), noreg);\n+    bne(ch1, ch2, STR2_NEXT);\n+    add(nlen_tmp, nlen_tmp, needle_chr_size);\n+    add(hlen_tmp, hlen_tmp, haystack_chr_size);\n+    bltz(nlen_tmp, STR1_NEXT);\n+    j(MATCH);\n+\n+    bind(DOSHORT);\n+    if (needle_isL == haystack_isL) {\n+      sub(t0, needle_len, 2);\n+      bltz(t0, DO1);\n+      bgtz(t0, DO3);\n+    }\n+  }\n+\n+  if (needle_con_cnt == 4) {\n+    Label CH1_LOOP;\n+    (this->*load_4chr)(ch1, Address(needle), noreg);\n+    sub(result_tmp, haystack_len, 4);\n+    slli(tmp3, result_tmp, haystack_chr_shift); \/\/ result as tmp\n+    add(haystack, haystack, tmp3);\n+    neg(hlen_neg, tmp3);\n+\n+    bind(CH1_LOOP);\n+    add(ch2, haystack, hlen_neg);\n+    (this->*load_4chr)(ch2, Address(ch2), noreg);\n+    beq(ch1, ch2, MATCH);\n+    add(hlen_neg, hlen_neg, haystack_chr_size);\n+    blez(hlen_neg, CH1_LOOP);\n+    j(NOMATCH);\n+  }\n+\n+  if ((needle_con_cnt == -1 && needle_isL == haystack_isL) || needle_con_cnt == 2) {\n+    Label CH1_LOOP;\n+    BLOCK_COMMENT(\"string_indexof DO2 {\");\n+    bind(DO2);\n+    (this->*load_2chr)(ch1, Address(needle), noreg);\n+    if (needle_con_cnt == 2) {\n+      sub(result_tmp, haystack_len, 2);\n+    }\n+    slli(tmp3, result_tmp, haystack_chr_shift);\n+    add(haystack, haystack, tmp3);\n+    neg(hlen_neg, tmp3);\n+\n+    bind(CH1_LOOP);\n+    add(tmp3, haystack, hlen_neg);\n+    (this->*load_2chr)(ch2, Address(tmp3), noreg);\n+    beq(ch1, ch2, MATCH);\n+    add(hlen_neg, hlen_neg, haystack_chr_size);\n+    blez(hlen_neg, CH1_LOOP);\n+    j(NOMATCH);\n+    BLOCK_COMMENT(\"} string_indexof DO2\");\n+  }\n+\n+  if ((needle_con_cnt == -1 && needle_isL == haystack_isL) || needle_con_cnt == 3) {\n+    Label FIRST_LOOP, STR2_NEXT, STR1_LOOP;\n+    BLOCK_COMMENT(\"string_indexof DO3 {\");\n+\n+    bind(DO3);\n+    (this->*load_2chr)(first, Address(needle), noreg);\n+    (this->*needle_load_1chr)(ch1, Address(needle, 2 * needle_chr_size), noreg);\n+    if (needle_con_cnt == 3) {\n+      sub(result_tmp, haystack_len, 3);\n+    }\n+    slli(hlen_tmp, result_tmp, haystack_chr_shift);\n+    add(haystack, haystack, hlen_tmp);\n+    neg(hlen_neg, hlen_tmp);\n+\n+    bind(FIRST_LOOP);\n+    add(ch2, haystack, hlen_neg);\n+    (this->*load_2chr)(ch2, Address(ch2), noreg);\n+    beq(first, ch2, STR1_LOOP);\n+\n+    bind(STR2_NEXT);\n+    add(hlen_neg, hlen_neg, haystack_chr_size);\n+    blez(hlen_neg, FIRST_LOOP);\n+    j(NOMATCH);\n+\n+    bind(STR1_LOOP);\n+    add(hlen_tmp, hlen_neg, 2 * haystack_chr_size);\n+    add(ch2, haystack, hlen_tmp);\n+    (this->*haystack_load_1chr)(ch2, Address(ch2), noreg);\n+    bne(ch1, ch2, STR2_NEXT);\n+    j(MATCH);\n+    BLOCK_COMMENT(\"} string_indexof DO3\");\n+  }\n+\n+  if (needle_con_cnt == -1 || needle_con_cnt == 1) {\n+    Label DO1_LOOP;\n+\n+    BLOCK_COMMENT(\"string_indexof DO1 {\");\n+    bind(DO1);\n+    (this->*needle_load_1chr)(ch1, Address(needle), noreg);\n+    sub(result_tmp, haystack_len, 1);\n+    mv(tmp3, result_tmp);\n+    if (haystack_chr_shift) {\n+      slli(tmp3, result_tmp, haystack_chr_shift);\n+    }\n+    add(haystack, haystack, tmp3);\n+    neg(hlen_neg, tmp3);\n+\n+    bind(DO1_LOOP);\n+    add(tmp3, haystack, hlen_neg);\n+    (this->*haystack_load_1chr)(ch2, Address(tmp3), noreg);\n+    beq(ch1, ch2, MATCH);\n+    add(hlen_neg, hlen_neg, haystack_chr_size);\n+    blez(hlen_neg, DO1_LOOP);\n+    BLOCK_COMMENT(\"} string_indexof DO1\");\n+  }\n+\n+  bind(NOMATCH);\n+  mv(result, -1);\n+  j(DONE);\n+\n+  bind(MATCH);\n+  srai(t0, hlen_neg, haystack_chr_shift);\n+  add(result, result_tmp, t0);\n+\n+  bind(DONE);\n+}\n+\n+\/\/ Compare strings.\n+void MacroAssembler::string_compare(Register str1, Register str2,\n+                                       Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,\n+                                       Register tmp3, int ae)\n+{\n+  Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,\n+          DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,\n+          SHORT_LOOP_START, TAIL_CHECK, L;\n+\n+  const int STUB_THRESHOLD = 64 + 8;\n+  bool isLL = ae == StrIntrinsicNode::LL;\n+  bool isLU = ae == StrIntrinsicNode::LU;\n+  bool isUL = ae == StrIntrinsicNode::UL;\n+\n+  bool str1_isL = isLL || isLU;\n+  bool str2_isL = isLL || isUL;\n+\n+  \/\/ for L strings, 1 byte for 1 character\n+  \/\/ for U strings, 2 bytes for 1 character\n+  int str1_chr_size = str1_isL ? 1 : 2;\n+  int str2_chr_size = str2_isL ? 1 : 2;\n+  int minCharsInWord = isLL ? wordSize : wordSize \/ 2;\n+\n+  load_chr_insn str1_load_chr = str1_isL ? (load_chr_insn)&MacroAssembler::lbu : (load_chr_insn)&MacroAssembler::lhu;\n+  load_chr_insn str2_load_chr = str2_isL ? (load_chr_insn)&MacroAssembler::lbu : (load_chr_insn)&MacroAssembler::lhu;\n+\n+  BLOCK_COMMENT(\"string_compare {\");\n+\n+  \/\/ Bizzarely, the counts are passed in bytes, regardless of whether they\n+  \/\/ are L or U strings, however the result is always in characters.\n+  if (!str1_isL) {\n+    sraiw(cnt1, cnt1, 1);\n+  }\n+  if (!str2_isL) {\n+    sraiw(cnt2, cnt2, 1);\n+  }\n+\n+  \/\/ Compute the minimum of the string lengths and save the difference in result.\n+  sub(result, cnt1, cnt2);\n+  bgt(cnt1, cnt2, L);\n+  mv(cnt2, cnt1);\n+  bind(L);\n+\n+  \/\/ A very short string\n+  li(t0, minCharsInWord);\n+  ble(cnt2, t0, SHORT_STRING);\n+\n+  \/\/ Compare longwords\n+  \/\/ load first parts of strings and finish initialization while loading\n+  {\n+    if (str1_isL == str2_isL) { \/\/ LL or UU\n+      \/\/ load 8 bytes once to compare\n+      ld(tmp1, Address(str1));\n+      beq(str1, str2, DONE);\n+      ld(tmp2, Address(str2));\n+      li(t0, STUB_THRESHOLD);\n+      bge(cnt2, t0, STUB);\n+      sub(cnt2, cnt2, minCharsInWord);\n+      beqz(cnt2, TAIL_CHECK);\n+      \/\/ convert cnt2 from characters to bytes\n+      if (!str1_isL) {\n+        slli(cnt2, cnt2, 1);\n+      }\n+      add(str2, str2, cnt2);\n+      add(str1, str1, cnt2);\n+      sub(cnt2, zr, cnt2);\n+    } else if (isLU) { \/\/ LU case\n+      lwu(tmp1, Address(str1));\n+      ld(tmp2, Address(str2));\n+      li(t0, STUB_THRESHOLD);\n+      bge(cnt2, t0, STUB);\n+      addi(cnt2, cnt2, -4);\n+      add(str1, str1, cnt2);\n+      sub(cnt1, zr, cnt2);\n+      slli(cnt2, cnt2, 1);\n+      add(str2, str2, cnt2);\n+      inflate_lo32(tmp3, tmp1);\n+      mv(tmp1, tmp3);\n+      sub(cnt2, zr, cnt2);\n+      addi(cnt1, cnt1, 4);\n+    } else { \/\/ UL case\n+      ld(tmp1, Address(str1));\n+      lwu(tmp2, Address(str2));\n+      li(t0, STUB_THRESHOLD);\n+      bge(cnt2, t0, STUB);\n+      addi(cnt2, cnt2, -4);\n+      slli(t0, cnt2, 1);\n+      sub(cnt1, zr, t0);\n+      add(str1, str1, t0);\n+      add(str2, str2, cnt2);\n+      inflate_lo32(tmp3, tmp2);\n+      mv(tmp2, tmp3);\n+      sub(cnt2, zr, cnt2);\n+      addi(cnt1, cnt1, 8);\n+    }\n+    addi(cnt2, cnt2, isUL ? 4 : 8);\n+    bgez(cnt2, TAIL);\n+    xorr(tmp3, tmp1, tmp2);\n+    bnez(tmp3, DIFFERENCE);\n+\n+    \/\/ main loop\n+    bind(NEXT_WORD);\n+    if (str1_isL == str2_isL) { \/\/ LL or UU\n+      add(t0, str1, cnt2);\n+      ld(tmp1, Address(t0));\n+      add(t0, str2, cnt2);\n+      ld(tmp2, Address(t0));\n+      addi(cnt2, cnt2, 8);\n+    } else if (isLU) { \/\/ LU case\n+      add(t0, str1, cnt1);\n+      lwu(tmp1, Address(t0));\n+      add(t0, str2, cnt2);\n+      ld(tmp2, Address(t0));\n+      addi(cnt1, cnt1, 4);\n+      inflate_lo32(tmp3, tmp1);\n+      mv(tmp1, tmp3);\n+      addi(cnt2, cnt2, 8);\n+    } else { \/\/ UL case\n+      add(t0, str2, cnt2);\n+      lwu(tmp2, Address(t0));\n+      add(t0, str1, cnt1);\n+      ld(tmp1, Address(t0));\n+      inflate_lo32(tmp3, tmp2);\n+      mv(tmp2, tmp3);\n+      addi(cnt1, cnt1, 8);\n+      addi(cnt2, cnt2, 4);\n+    }\n+    bgez(cnt2, TAIL);\n+\n+    xorr(tmp3, tmp1, tmp2);\n+    beqz(tmp3, NEXT_WORD);\n+    j(DIFFERENCE);\n+    bind(TAIL);\n+    xorr(tmp3, tmp1, tmp2);\n+    bnez(tmp3, DIFFERENCE);\n+    \/\/ Last longword.  In the case where length == 4 we compare the\n+    \/\/ same longword twice, but that's still faster than another\n+    \/\/ conditional branch.\n+    if (str1_isL == str2_isL) { \/\/ LL or UU\n+      ld(tmp1, Address(str1));\n+      ld(tmp2, Address(str2));\n+    } else if (isLU) { \/\/ LU case\n+      lwu(tmp1, Address(str1));\n+      ld(tmp2, Address(str2));\n+      inflate_lo32(tmp3, tmp1);\n+      mv(tmp1, tmp3);\n+    } else { \/\/ UL case\n+      lwu(tmp2, Address(str2));\n+      ld(tmp1, Address(str1));\n+      inflate_lo32(tmp3, tmp2);\n+      mv(tmp2, tmp3);\n+    }\n+    bind(TAIL_CHECK);\n+    xorr(tmp3, tmp1, tmp2);\n+    beqz(tmp3, DONE);\n+\n+    \/\/ Find the first different characters in the longwords and\n+    \/\/ compute their difference.\n+    bind(DIFFERENCE);\n+    ctzc_bit(result, tmp3, isLL); \/\/ count zero from lsb to msb\n+    srl(tmp1, tmp1, result);\n+    srl(tmp2, tmp2, result);\n+    if (isLL) {\n+      andi(tmp1, tmp1, 0xFF);\n+      andi(tmp2, tmp2, 0xFF);\n+    } else {\n+      andi(tmp1, tmp1, 0xFFFF);\n+      andi(tmp2, tmp2, 0xFFFF);\n+    }\n+    sub(result, tmp1, tmp2);\n+    j(DONE);\n+  }\n+\n+  bind(STUB);\n+  RuntimeAddress stub = NULL;\n+  switch (ae) {\n+    case StrIntrinsicNode::LL:\n+      stub = RuntimeAddress(StubRoutines::riscv::compare_long_string_LL());\n+      break;\n+    case StrIntrinsicNode::UU:\n+      stub = RuntimeAddress(StubRoutines::riscv::compare_long_string_UU());\n+      break;\n+    case StrIntrinsicNode::LU:\n+      stub = RuntimeAddress(StubRoutines::riscv::compare_long_string_LU());\n+      break;\n+    case StrIntrinsicNode::UL:\n+      stub = RuntimeAddress(StubRoutines::riscv::compare_long_string_UL());\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  assert(stub.target() != NULL, \"compare_long_string stub has not been generated\");\n+  trampoline_call(stub);\n+  j(DONE);\n+\n+  bind(SHORT_STRING);\n+  \/\/ Is the minimum length zero?\n+  beqz(cnt2, DONE);\n+  \/\/ arrange code to do most branches while loading and loading next characters\n+  \/\/ while comparing previous\n+  (this->*str1_load_chr)(tmp1, Address(str1), t0);\n+  addi(str1, str1, str1_chr_size);\n+  addi(cnt2, cnt2, -1);\n+  beqz(cnt2, SHORT_LAST_INIT);\n+  (this->*str2_load_chr)(cnt1, Address(str2), t0);\n+  addi(str2, str2, str2_chr_size);\n+  j(SHORT_LOOP_START);\n+  bind(SHORT_LOOP);\n+  addi(cnt2, cnt2, -1);\n+  beqz(cnt2, SHORT_LAST);\n+  bind(SHORT_LOOP_START);\n+  (this->*str1_load_chr)(tmp2, Address(str1), t0);\n+  addi(str1, str1, str1_chr_size);\n+  (this->*str2_load_chr)(t0, Address(str2), t0);\n+  addi(str2, str2, str2_chr_size);\n+  bne(tmp1, cnt1, SHORT_LOOP_TAIL);\n+  addi(cnt2, cnt2, -1);\n+  beqz(cnt2, SHORT_LAST2);\n+  (this->*str1_load_chr)(tmp1, Address(str1), t0);\n+  addi(str1, str1, str1_chr_size);\n+  (this->*str2_load_chr)(cnt1, Address(str2), t0);\n+  addi(str2, str2, str2_chr_size);\n+  beq(tmp2, t0, SHORT_LOOP);\n+  sub(result, tmp2, t0);\n+  j(DONE);\n+  bind(SHORT_LOOP_TAIL);\n+  sub(result, tmp1, cnt1);\n+  j(DONE);\n+  bind(SHORT_LAST2);\n+  beq(tmp2, t0, DONE);\n+  sub(result, tmp2, t0);\n+\n+  j(DONE);\n+  bind(SHORT_LAST_INIT);\n+  (this->*str2_load_chr)(cnt1, Address(str2), t0);\n+  addi(str2, str2, str2_chr_size);\n+  bind(SHORT_LAST);\n+  beq(tmp1, cnt1, DONE);\n+  sub(result, tmp1, cnt1);\n+\n+  bind(DONE);\n+\n+  BLOCK_COMMENT(\"} string_compare\");\n+}\n+\n+void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,\n+                                      Register tmp4, Register tmp5, Register tmp6, Register result,\n+                                      Register cnt1, int elem_size) {\n+  Label DONE, SAME, NEXT_DWORD, SHORT, TAIL, TAIL2, IS_TMP5_ZR;\n+  Register tmp1 = t0;\n+  Register tmp2 = t1;\n+  Register cnt2 = tmp2;  \/\/ cnt2 only used in array length compare\n+  Register elem_per_word = tmp6;\n+  int log_elem_size = exact_log2(elem_size);\n+  int length_offset = arrayOopDesc::length_offset_in_bytes();\n+  int base_offset   = arrayOopDesc::base_offset_in_bytes(elem_size == 2 ? T_CHAR : T_BYTE);\n+\n+  assert(elem_size == 1 || elem_size == 2, \"must be char or byte\");\n+  assert_different_registers(a1, a2, result, cnt1, t0, t1, tmp3, tmp4, tmp5, tmp6);\n+  li(elem_per_word, wordSize \/ elem_size);\n+\n+  BLOCK_COMMENT(\"arrays_equals {\");\n+\n+  \/\/ if (a1 == a2), return true\n+  beq(a1, a2, SAME);\n+\n+  mv(result, false);\n+  beqz(a1, DONE);\n+  beqz(a2, DONE);\n+  lwu(cnt1, Address(a1, length_offset));\n+  lwu(cnt2, Address(a2, length_offset));\n+  bne(cnt2, cnt1, DONE);\n+  beqz(cnt1, SAME);\n+\n+  slli(tmp5, cnt1, 3 + log_elem_size);\n+  sub(tmp5, zr, tmp5);\n+  add(a1, a1, base_offset);\n+  add(a2, a2, base_offset);\n+  ld(tmp3, Address(a1, 0));\n+  ld(tmp4, Address(a2, 0));\n+  ble(cnt1, elem_per_word, SHORT); \/\/ short or same\n+\n+  \/\/ Main 16 byte comparison loop with 2 exits\n+  bind(NEXT_DWORD); {\n+    ld(tmp1, Address(a1, wordSize));\n+    ld(tmp2, Address(a2, wordSize));\n+    sub(cnt1, cnt1, 2 * wordSize \/ elem_size);\n+    blez(cnt1, TAIL);\n+    bne(tmp3, tmp4, DONE);\n+    ld(tmp3, Address(a1, 2 * wordSize));\n+    ld(tmp4, Address(a2, 2 * wordSize));\n+    add(a1, a1, 2 * wordSize);\n+    add(a2, a2, 2 * wordSize);\n+    ble(cnt1, elem_per_word, TAIL2);\n+  } beq(tmp1, tmp2, NEXT_DWORD);\n+  j(DONE);\n+\n+  bind(TAIL);\n+  xorr(tmp4, tmp3, tmp4);\n+  xorr(tmp2, tmp1, tmp2);\n+  sll(tmp2, tmp2, tmp5);\n+  orr(tmp5, tmp4, tmp2);\n+  j(IS_TMP5_ZR);\n+\n+  bind(TAIL2);\n+  bne(tmp1, tmp2, DONE);\n+\n+  bind(SHORT);\n+  xorr(tmp4, tmp3, tmp4);\n+  sll(tmp5, tmp4, tmp5);\n+\n+  bind(IS_TMP5_ZR);\n+  bnez(tmp5, DONE);\n+\n+  bind(SAME);\n+  mv(result, true);\n+  \/\/ That's it.\n+  bind(DONE);\n+\n+  BLOCK_COMMENT(\"} array_equals\");\n+}\n+\n+\/\/ Compare Strings\n+\n+\/\/ For Strings we're passed the address of the first characters in a1\n+\/\/ and a2 and the length in cnt1.\n+\/\/ elem_size is the element size in bytes: either 1 or 2.\n+\/\/ There are two implementations.  For arrays >= 8 bytes, all\n+\/\/ comparisons (including the final one, which may overlap) are\n+\/\/ performed 8 bytes at a time.  For strings < 8 bytes, we compare a\n+\/\/ halfword, then a short, and then a byte.\n+\n+void MacroAssembler::string_equals(Register a1, Register a2,\n+                                      Register result, Register cnt1, int elem_size)\n+{\n+  Label SAME, DONE, SHORT, NEXT_WORD;\n+  Register tmp1 = t0;\n+  Register tmp2 = t1;\n+\n+  assert(elem_size == 1 || elem_size == 2, \"must be 2 or 1 byte\");\n+  assert_different_registers(a1, a2, result, cnt1, t0, t1);\n+\n+  BLOCK_COMMENT(\"string_equals {\");\n+\n+  mv(result, false);\n+\n+  \/\/ Check for short strings, i.e. smaller than wordSize.\n+  sub(cnt1, cnt1, wordSize);\n+  bltz(cnt1, SHORT);\n+\n+  \/\/ Main 8 byte comparison loop.\n+  bind(NEXT_WORD); {\n+    ld(tmp1, Address(a1, 0));\n+    add(a1, a1, wordSize);\n+    ld(tmp2, Address(a2, 0));\n+    add(a2, a2, wordSize);\n+    sub(cnt1, cnt1, wordSize);\n+    bne(tmp1, tmp2, DONE);\n+  } bgtz(cnt1, NEXT_WORD);\n+\n+  \/\/ Last longword.  In the case where length == 4 we compare the\n+  \/\/ same longword twice, but that's still faster than another\n+  \/\/ conditional branch.\n+  \/\/ cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when\n+  \/\/ length == 4.\n+  add(tmp1, a1, cnt1);\n+  ld(tmp1, Address(tmp1, 0));\n+  add(tmp2, a2, cnt1);\n+  ld(tmp2, Address(tmp2, 0));\n+  bne(tmp1, tmp2, DONE);\n+  j(SAME);\n+\n+  bind(SHORT);\n+  Label TAIL03, TAIL01;\n+\n+  \/\/ 0-7 bytes left.\n+  andi(t0, cnt1, 4);\n+  beqz(t0, TAIL03);\n+  {\n+    lwu(tmp1, Address(a1, 0));\n+    add(a1, a1, 4);\n+    lwu(tmp2, Address(a2, 0));\n+    add(a2, a2, 4);\n+    bne(tmp1, tmp2, DONE);\n+  }\n+\n+  bind(TAIL03);\n+  \/\/ 0-3 bytes left.\n+  andi(t0, cnt1, 2);\n+  beqz(t0, TAIL01);\n+  {\n+    lhu(tmp1, Address(a1, 0));\n+    add(a1, a1, 2);\n+    lhu(tmp2, Address(a2, 0));\n+    add(a2, a2, 2);\n+    bne(tmp1, tmp2, DONE);\n+  }\n+\n+  bind(TAIL01);\n+  if (elem_size == 1) { \/\/ Only needed when comparing 1-byte elements\n+    \/\/ 0-1 bytes left.\n+    andi(t0, cnt1, 1);\n+    beqz(t0, SAME);\n+    {\n+      lbu(tmp1, a1, 0);\n+      lbu(tmp2, a2, 0);\n+      bne(tmp1, tmp2, DONE);\n+    }\n+  }\n+\n+  \/\/ Arrays are equal.\n+  bind(SAME);\n+  mv(result, true);\n+\n+  \/\/ That's it.\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} string_equals\");\n+}\n+\n+typedef void (Assembler::*conditional_branch_insn)(Register op1, Register op2, Label& label, bool is_far);\n+typedef void (MacroAssembler::*float_conditional_branch_insn)(FloatRegister op1, FloatRegister op2, Label& label,\n+                                                              bool is_far, bool is_unordered);\n+\n+static conditional_branch_insn conditional_branches[] =\n+{\n+  \/* SHORT branches *\/\n+  (conditional_branch_insn)&Assembler::beq,\n+  (conditional_branch_insn)&Assembler::bgt,\n+  NULL, \/\/ BoolTest::overflow\n+  (conditional_branch_insn)&Assembler::blt,\n+  (conditional_branch_insn)&Assembler::bne,\n+  (conditional_branch_insn)&Assembler::ble,\n+  NULL, \/\/ BoolTest::no_overflow\n+  (conditional_branch_insn)&Assembler::bge,\n+\n+  \/* UNSIGNED branches *\/\n+  (conditional_branch_insn)&Assembler::beq,\n+  (conditional_branch_insn)&Assembler::bgtu,\n+  NULL,\n+  (conditional_branch_insn)&Assembler::bltu,\n+  (conditional_branch_insn)&Assembler::bne,\n+  (conditional_branch_insn)&Assembler::bleu,\n+  NULL,\n+  (conditional_branch_insn)&Assembler::bgeu\n+};\n+\n+static float_conditional_branch_insn float_conditional_branches[] =\n+{\n+  \/* FLOAT SHORT branches *\/\n+  (float_conditional_branch_insn)&MacroAssembler::float_beq,\n+  (float_conditional_branch_insn)&MacroAssembler::float_bgt,\n+  NULL,  \/\/ BoolTest::overflow\n+  (float_conditional_branch_insn)&MacroAssembler::float_blt,\n+  (float_conditional_branch_insn)&MacroAssembler::float_bne,\n+  (float_conditional_branch_insn)&MacroAssembler::float_ble,\n+  NULL, \/\/ BoolTest::no_overflow\n+  (float_conditional_branch_insn)&MacroAssembler::float_bge,\n+\n+  \/* DOUBLE SHORT branches *\/\n+  (float_conditional_branch_insn)&MacroAssembler::double_beq,\n+  (float_conditional_branch_insn)&MacroAssembler::double_bgt,\n+  NULL,\n+  (float_conditional_branch_insn)&MacroAssembler::double_blt,\n+  (float_conditional_branch_insn)&MacroAssembler::double_bne,\n+  (float_conditional_branch_insn)&MacroAssembler::double_ble,\n+  NULL,\n+  (float_conditional_branch_insn)&MacroAssembler::double_bge\n+};\n+\n+void MacroAssembler::cmp_branch(int cmpFlag, Register op1, Register op2, Label& label, bool is_far) {\n+  assert(cmpFlag >= 0 && cmpFlag < (int)(sizeof(conditional_branches) \/ sizeof(conditional_branches[0])),\n+         \"invalid conditional branch index\");\n+  (this->*conditional_branches[cmpFlag])(op1, op2, label, is_far);\n+}\n+\n+\/\/ This is a function should only be used by C2. Flip the unordered when unordered-greater, C2 would use\n+\/\/ unordered-lesser instead of unordered-greater. Finally, commute the result bits at function do_one_bytecode().\n+void MacroAssembler::float_cmp_branch(int cmpFlag, FloatRegister op1, FloatRegister op2, Label& label, bool is_far) {\n+  assert(cmpFlag >= 0 && cmpFlag < (int)(sizeof(float_conditional_branches) \/ sizeof(float_conditional_branches[0])),\n+         \"invalid float conditional branch index\");\n+  int booltest_flag = cmpFlag & ~(MacroAssembler::double_branch_mask);\n+  (this->*float_conditional_branches[cmpFlag])(op1, op2, label, is_far,\n+                                               (booltest_flag == (BoolTest::ge) || booltest_flag == (BoolTest::gt)) ? false : true);\n+}\n+\n+void MacroAssembler::enc_cmpUEqNeLeGt_imm0_branch(int cmpFlag, Register op1, Label& L, bool is_far) {\n+  switch (cmpFlag) {\n+    case BoolTest::eq:\n+    case BoolTest::le:\n+      beqz(op1, L, is_far);\n+      break;\n+    case BoolTest::ne:\n+    case BoolTest::gt:\n+      bnez(op1, L, is_far);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void MacroAssembler::enc_cmpEqNe_imm0_branch(int cmpFlag, Register op1, Label& L, bool is_far) {\n+  switch (cmpFlag) {\n+    case BoolTest::eq:\n+      beqz(op1, L, is_far);\n+      break;\n+    case BoolTest::ne:\n+      bnez(op1, L, is_far);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void MacroAssembler::enc_cmove(int cmpFlag, Register op1, Register op2, Register dst, Register src) {\n+  Label L;\n+  cmp_branch(cmpFlag ^ (1 << neg_cond_bits), op1, op2, L);\n+  mv(dst, src);\n+  bind(L);\n+}\n+\n+\/\/ Set dst to NaN if any NaN input.\n+void MacroAssembler::minmax_FD(FloatRegister dst, FloatRegister src1, FloatRegister src2,\n+                                  bool is_double, bool is_min) {\n+  assert_different_registers(dst, src1, src2);\n+\n+  Label Done;\n+  fsflags(zr);\n+  if (is_double) {\n+    is_min ? fmin_d(dst, src1, src2)\n+           : fmax_d(dst, src1, src2);\n+    \/\/ Checking NaNs\n+    flt_d(zr, src1, src2);\n+  } else {\n+    is_min ? fmin_s(dst, src1, src2)\n+           : fmax_s(dst, src1, src2);\n+    \/\/ Checking NaNs\n+    flt_s(zr, src1, src2);\n+  }\n+\n+  frflags(t0);\n+  beqz(t0, Done);\n+\n+  \/\/ In case of NaNs\n+  is_double ? fadd_d(dst, src1, src2)\n+            : fadd_s(dst, src1, src2);\n+\n+  bind(Done);\n+}\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":5425,"deletions":0,"binary":false,"changes":5425,"status":"added"},{"patch":"@@ -0,0 +1,976 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_MACROASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_MACROASSEMBLER_RISCV_HPP\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+\n+\/\/ MacroAssembler extends Assembler by frequently used macros.\n+\/\/\n+\/\/ Instructions for which a 'better' code sequence exists depending\n+\/\/ on arguments should also go in here.\n+\n+class MacroAssembler: public Assembler {\n+\n+ public:\n+  MacroAssembler(CodeBuffer* code) : Assembler(code) {\n+  }\n+  virtual ~MacroAssembler() {}\n+\n+  void safepoint_poll(Label& slow_path);\n+  void safepoint_poll_acquire(Label& slow_path);\n+\n+  \/\/ Biased locking support\n+  \/\/ lock_reg and obj_reg must be loaded up with the appropriate values.\n+  \/\/ swap_reg is killed.\n+  \/\/ tmp_reg must be supplied and must not be rscratch1 or rscratch2\n+  \/\/ Optional slow case is for implementations (interpreter and C1) which branch to\n+  \/\/ slow case directly. Leaves condition codes set for C2's Fast_Lock node.\n+  \/\/ Returns offset of first potentially-faulting instruction for null\n+  \/\/ check info (currently consumed only by C1). If\n+  \/\/ swap_reg_contains_mark is true then returns -1 as it is assumed\n+  \/\/ the calling code has already passed any potential faults.\n+  int biased_locking_enter(Register lock_reg, Register obj_reg,\n+                           Register swap_reg, Register tmp_reg,\n+                           bool swap_reg_contains_mark,\n+                           Label& done, Label* slow_case = NULL,\n+                           BiasedLockingCounters* counters = NULL,\n+                           Register flag = noreg);\n+  void biased_locking_exit (Register obj_reg, Register temp_reg, Label& done, Register flag = noreg);\n+\n+  \/\/ Helper functions for statistics gathering.\n+  \/\/ Unconditional atomic increment.\n+  void atomic_incw(Register counter_addr, Register tmp);\n+  void atomic_incw(Address counter_addr, Register tmp1, Register tmp2) {\n+    la(tmp1, counter_addr);\n+    atomic_incw(tmp1, tmp2);\n+  }\n+\n+  \/\/ Place a fence.i after code may have been modified due to a safepoint.\n+  void safepoint_ifence();\n+\n+  \/\/ Alignment\n+  void align(int modulus, int extra_offset = 0);\n+\n+  \/\/ Stack frame creation\/removal\n+  \/\/ Note that SP must be updated to the right place before saving\/restoring RA and FP\n+  \/\/ because signal based thread suspend\/resume could happen asynchronously.\n+  void enter() {\n+    addi(sp, sp, - 2 * wordSize);\n+    sd(ra, Address(sp, wordSize));\n+    sd(fp, Address(sp));\n+    addi(fp, sp, 2 * wordSize);\n+  }\n+\n+  void leave() {\n+    addi(sp, fp, - 2 * wordSize);\n+    ld(fp, Address(sp));\n+    ld(ra, Address(sp, wordSize));\n+    addi(sp, sp, 2 * wordSize);\n+  }\n+\n+\n+  \/\/ Support for getting the JavaThread pointer (i.e.; a reference to thread-local information)\n+  \/\/ The pointer will be loaded into the thread register.\n+  void get_thread(Register thread);\n+\n+  \/\/ Support for VM calls\n+  \/\/\n+  \/\/ It is imperative that all calls into the VM are handled via the call_VM macros.\n+  \/\/ They make sure that the stack linkage is setup correctly. call_VM's correspond\n+  \/\/ to ENTRY\/ENTRY_X entry points while call_VM_leaf's correspond to LEAF entry points.\n+\n+  void call_VM(Register oop_result,\n+               address entry_point,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               address entry_point,\n+               Register arg_1,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               address entry_point,\n+               Register arg_1, Register arg_2,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               address entry_point,\n+               Register arg_1, Register arg_2, Register arg_3,\n+               bool check_exceptions = true);\n+\n+  \/\/ Overloadings with last_Java_sp\n+  void call_VM(Register oop_result,\n+               Register last_java_sp,\n+               address entry_point,\n+               int number_of_arguments = 0,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               Register last_java_sp,\n+               address entry_point,\n+               Register arg_1,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               Register last_java_sp,\n+               address entry_point,\n+               Register arg_1, Register arg_2,\n+               bool check_exceptions = true);\n+  void call_VM(Register oop_result,\n+               Register last_java_sp,\n+               address entry_point,\n+               Register arg_1, Register arg_2, Register arg_3,\n+               bool check_exceptions = true);\n+\n+  void get_vm_result(Register oop_result, Register java_thread);\n+  void get_vm_result_2(Register metadata_result, Register java_thread);\n+\n+  \/\/ These always tightly bind to MacroAssembler::call_VM_leaf_base\n+  \/\/ bypassing the virtual implementation\n+  void call_VM_leaf(address entry_point,\n+                    int number_of_arguments = 0);\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_0);\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_0, Register arg_1);\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_0, Register arg_1, Register arg_2);\n+\n+  \/\/ These always tightly bind to MacroAssembler::call_VM_base\n+  \/\/ bypassing the virtual implementation\n+  void super_call_VM_leaf(address entry_point, Register arg_0);\n+  void super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1);\n+  void super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2);\n+  void super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3);\n+\n+  \/\/ last Java Frame (fills frame anchor)\n+  void set_last_Java_frame(Register last_java_sp, Register last_java_fp, address last_java_pc, Register tmp);\n+  void set_last_Java_frame(Register last_java_sp, Register last_java_fp, Label &last_java_pc, Register tmp);\n+  void set_last_Java_frame(Register last_java_sp, Register last_java_fp, Register last_java_pc, Register tmp);\n+\n+  \/\/ thread in the default location (xthread)\n+  void reset_last_Java_frame(bool clear_fp);\n+\n+  void call_native(address entry_point,\n+                   Register arg_0);\n+  void call_native_base(\n+    address entry_point,                \/\/ the entry point\n+    Label*  retaddr = NULL\n+  );\n+\n+  virtual void call_VM_leaf_base(\n+    address entry_point,                \/\/ the entry point\n+    int     number_of_arguments,        \/\/ the number of arguments to pop after the call\n+    Label*  retaddr = NULL\n+  );\n+\n+  virtual void call_VM_leaf_base(\n+    address entry_point,                \/\/ the entry point\n+    int     number_of_arguments,        \/\/ the number of arguments to pop after the call\n+    Label&  retaddr) {\n+    call_VM_leaf_base(entry_point, number_of_arguments, &retaddr);\n+  }\n+\n+  virtual void call_VM_base(           \/\/ returns the register containing the thread upon return\n+    Register oop_result,               \/\/ where an oop-result ends up if any; use noreg otherwise\n+    Register java_thread,              \/\/ the thread if computed before     ; use noreg otherwise\n+    Register last_java_sp,             \/\/ to set up last_Java_frame in stubs; use noreg otherwise\n+    address  entry_point,              \/\/ the entry point\n+    int      number_of_arguments,      \/\/ the number of arguments (w\/o thread) to pop after the call\n+    bool     check_exceptions          \/\/ whether to check for pending exceptions after return\n+  );\n+\n+  void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions);\n+\n+  virtual void check_and_handle_earlyret(Register java_thread);\n+  virtual void check_and_handle_popframe(Register java_thread);\n+\n+  void resolve_oop_handle(Register result, Register tmp = x15);\n+  void resolve_jobject(Register value, Register thread, Register tmp);\n+\n+  void movoop(Register dst, jobject obj, bool immediate = false);\n+  void mov_metadata(Register dst, Metadata* obj);\n+  void bang_stack_size(Register size, Register tmp);\n+  void set_narrow_oop(Register dst, jobject obj);\n+  void set_narrow_klass(Register dst, Klass* k);\n+\n+  void load_mirror(Register dst, Register method, Register tmp = x15);\n+  void access_load_at(BasicType type, DecoratorSet decorators, Register dst,\n+                      Address src, Register tmp1, Register thread_tmp);\n+  void access_store_at(BasicType type, DecoratorSet decorators, Address dst,\n+                       Register src, Register tmp1, Register thread_tmp);\n+  void load_klass(Register dst, Register src);\n+  void store_klass(Register dst, Register src);\n+  void cmp_klass(Register oop, Register trial_klass, Register tmp, Label &L);\n+\n+  void encode_klass_not_null(Register r);\n+  void decode_klass_not_null(Register r);\n+  void encode_klass_not_null(Register dst, Register src, Register tmp = xheapbase);\n+  void decode_klass_not_null(Register dst, Register src, Register tmp = xheapbase);\n+  void decode_heap_oop_not_null(Register r);\n+  void decode_heap_oop_not_null(Register dst, Register src);\n+  void decode_heap_oop(Register d, Register s);\n+  void decode_heap_oop(Register r) { decode_heap_oop(r, r); }\n+  void encode_heap_oop(Register d, Register s);\n+  void encode_heap_oop(Register r) { encode_heap_oop(r, r); };\n+  void load_heap_oop(Register dst, Address src, Register tmp1 = noreg,\n+                     Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+  void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg,\n+                              Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+  void store_heap_oop(Address dst, Register src, Register tmp1 = noreg,\n+                      Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+\n+  void store_klass_gap(Register dst, Register src);\n+\n+  \/\/ currently unimplemented\n+  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ stored using routines that take a jobject.\n+  void store_heap_oop_null(Address dst);\n+\n+  void load_prototype_header(Register dst, Register src);\n+\n+  \/\/ This dummy is to prevent a call to store_heap_oop from\n+  \/\/ converting a zero (linke NULL) into a Register by giving\n+  \/\/ the compiler two choices it can't resolve\n+\n+  void store_heap_oop(Address dst, void* dummy);\n+\n+  \/\/ Support for NULL-checks\n+  \/\/\n+  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ If the accessed location is M[reg + offset] and the offset is known, provide the\n+  \/\/ offset. No explicit code generateion is needed if the offset is within a certain\n+  \/\/ range (0 <= offset <= page_size).\n+\n+  virtual void null_check(Register reg, int offset = -1);\n+  static bool needs_explicit_null_check(intptr_t offset);\n+  static bool uses_implicit_null_check(void* address);\n+\n+  \/\/ idiv variant which deals with MINLONG as dividend and -1 as divisor\n+  int corrected_idivl(Register result, Register rs1, Register rs2,\n+                      bool want_remainder);\n+  int corrected_idivq(Register result, Register rs1, Register rs2,\n+                      bool want_remainder);\n+\n+  \/\/ interface method calling\n+  void lookup_interface_method(Register recv_klass,\n+                               Register intf_klass,\n+                               RegisterOrConstant itable_index,\n+                               Register method_result,\n+                               Register scan_tmp,\n+                               Label& no_such_interface,\n+                               bool return_method = true);\n+\n+  \/\/ virtual method calling\n+  \/\/ n.n. x86 allows RegisterOrConstant for vtable_index\n+  void lookup_virtual_method(Register recv_klass,\n+                             RegisterOrConstant vtable_index,\n+                             Register method_result);\n+\n+  \/\/ Form an addres from base + offset in Rd. Rd my or may not\n+  \/\/ actually be used: you must use the Address that is returned. It\n+  \/\/ is up to you to ensure that the shift provided mathces the size\n+  \/\/ of your data.\n+  Address form_address(Register Rd, Register base, long byte_offset);\n+\n+  \/\/ allocation\n+  void tlab_allocate(\n+    Register obj,                   \/\/ result: pointer to object after successful allocation\n+    Register var_size_in_bytes,     \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,     \/\/ object size in bytes if   known at compile time\n+    Register tmp1,                  \/\/ temp register\n+    Register tmp2,                  \/\/ temp register\n+    Label&   slow_case,             \/\/ continuation point of fast allocation fails\n+    bool is_far = false\n+  );\n+\n+  void eden_allocate(\n+    Register obj,                   \/\/ result: pointer to object after successful allocation\n+    Register var_size_in_bytes,     \/\/ object size in bytes if unknown at compile time; invalid otherwise\n+    int      con_size_in_bytes,     \/\/ object size in bytes if   known at compile time\n+    Register tmp,                   \/\/ temp register\n+    Label&   slow_case,             \/\/ continuation point if fast allocation fails\n+    bool is_far = false\n+  );\n+\n+  \/\/ Test sub_klass against super_klass, with fast and slow paths.\n+\n+  \/\/ The fast path produces a tri-state answer: yes \/ no \/ maybe-slow.\n+  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ If super_check_offset is -1, the value is loaded up from super_klass.\n+  \/\/ No registers are killed, except tmp_reg\n+  void check_klass_subtype_fast_path(Register sub_klass,\n+                                     Register super_klass,\n+                                     Register tmp_reg,\n+                                     Label* L_success,\n+                                     Label* L_failure,\n+                                     Label* L_slow_path,\n+                                     Register super_check_offset = noreg);\n+\n+  \/\/ The reset of the type cehck; must be wired to a corresponding fast path.\n+  \/\/ It does not repeat the fast path logic, so don't use it standalone.\n+  \/\/ The tmp1_reg and tmp2_reg can be noreg, if no temps are avaliable.\n+  \/\/ Updates the sub's secondary super cache as necessary.\n+  void check_klass_subtype_slow_path(Register sub_klass,\n+                                     Register super_klass,\n+                                     Register tmp1_reg,\n+                                     Register tmp2_reg,\n+                                     Label* L_success,\n+                                     Label* L_failure);\n+\n+  void check_klass_subtype(Register sub_klass,\n+                           Register super_klass,\n+                           Register tmp_reg,\n+                           Label& L_success);\n+\n+  Address argument_address(RegisterOrConstant arg_slot, int extra_slot_offset = 0);\n+\n+  \/\/ only if +VerifyOops\n+  void verify_oop(Register reg, const char* s = \"broken oop\");\n+  void verify_oop_addr(Address addr, const char* s = \"broken oop addr\");\n+\n+  void _verify_method_ptr(Register reg, const char* msg, const char* file, int line) {}\n+  void _verify_klass_ptr(Register reg, const char* msg, const char* file, int line) {}\n+\n+#define verify_method_ptr(reg) _verify_method_ptr(reg, \"broken method \" #reg, __FILE__, __LINE__)\n+#define verify_klass_ptr(reg) _verify_method_ptr(reg, \"broken klass \" #reg, __FILE__, __LINE__)\n+\n+  \/\/ A more convenient access to fence for our purposes\n+  \/\/ We used four bit to indicate the read and write bits in the predecessors and successors,\n+  \/\/ and extended i for r, o for w if UseConservativeFence enabled.\n+  enum Membar_mask_bits {\n+    StoreStore = 0b0101,               \/\/ (pred = ow   + succ =   ow)\n+    LoadStore  = 0b1001,               \/\/ (pred = ir   + succ =   ow)\n+    StoreLoad  = 0b0110,               \/\/ (pred = ow   + succ =   ir)\n+    LoadLoad   = 0b1010,               \/\/ (pred = ir   + succ =   ir)\n+    AnyAny     = LoadStore | StoreLoad \/\/ (pred = iorw + succ = iorw)\n+  };\n+\n+  void membar(uint32_t order_constraint);\n+\n+  static void membar_mask_to_pred_succ(uint32_t order_constraint, uint32_t& predecessor, uint32_t& successor) {\n+    predecessor = (order_constraint >> 2) & 0x3;\n+    successor = order_constraint & 0x3;\n+\n+    \/\/ extend rw -> iorw:\n+    \/\/ 01(w) -> 0101(ow)\n+    \/\/ 10(r) -> 1010(ir)\n+    \/\/ 11(rw)-> 1111(iorw)\n+    if (UseConservativeFence) {\n+      predecessor |= predecessor << 2;\n+      successor |= successor << 2;\n+    }\n+  }\n+\n+  static int pred_succ_to_membar_mask(uint32_t predecessor, uint32_t successor) {\n+    return ((predecessor & 0x3) << 2) | (successor & 0x3);\n+  }\n+\n+  \/\/ prints msg, dumps registers and stops execution\n+  void stop(const char* msg);\n+\n+  static void debug64(char* msg, int64_t pc, int64_t regs[]);\n+\n+  void unimplemented(const char* what = \"\");\n+\n+  void should_not_reach_here() { stop(\"should not reach here\"); }\n+\n+  static address target_addr_for_insn(address insn_addr);\n+\n+  \/\/ Required platform-specific helpers for Label::patch_instructions.\n+  \/\/ They _shadow_ the declarations in AbstractAssembler, which are undefined.\n+  static int pd_patch_instruction_size(address branch, address target);\n+  static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {\n+    pd_patch_instruction_size(branch, target);\n+  }\n+  static address pd_call_destination(address branch) {\n+    return target_addr_for_insn(branch);\n+  }\n+\n+  static int patch_oop(address insn_addr, address o);\n+  address emit_trampoline_stub(int insts_call_instruction_offset, address target);\n+  void emit_static_call_stub();\n+\n+  \/\/ The following 4 methods return the offset of the appropriate move instruction\n+\n+  \/\/ Support for fast byte\/short loading with zero extension (depending on particular CPU)\n+  int load_unsigned_byte(Register dst, Address src);\n+  int load_unsigned_short(Register dst, Address src);\n+\n+  \/\/ Support for fast byte\/short loading with sign extension (depending on particular CPU)\n+  int load_signed_byte(Register dst, Address src);\n+  int load_signed_short(Register dst, Address src);\n+\n+  \/\/ Load and store values by size and signed-ness\n+  void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);\n+  void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);\n+\n+ public:\n+  \/\/ Standard pseudoinstruction\n+  void nop();\n+  void mv(Register Rd, Register Rs);\n+  void notr(Register Rd, Register Rs);\n+  void neg(Register Rd, Register Rs);\n+  void negw(Register Rd, Register Rs);\n+  void sext_w(Register Rd, Register Rs);\n+  void zext_b(Register Rd, Register Rs);\n+  void seqz(Register Rd, Register Rs);          \/\/ set if = zero\n+  void snez(Register Rd, Register Rs);          \/\/ set if != zero\n+  void sltz(Register Rd, Register Rs);          \/\/ set if < zero\n+  void sgtz(Register Rd, Register Rs);          \/\/ set if > zero\n+\n+  \/\/ Float pseudoinstruction\n+  void fmv_s(FloatRegister Rd, FloatRegister Rs);\n+  void fabs_s(FloatRegister Rd, FloatRegister Rs);    \/\/ single-precision absolute value\n+  void fneg_s(FloatRegister Rd, FloatRegister Rs);\n+\n+  \/\/ Double pseudoinstruction\n+  void fmv_d(FloatRegister Rd, FloatRegister Rs);\n+  void fabs_d(FloatRegister Rd, FloatRegister Rs);\n+  void fneg_d(FloatRegister Rd, FloatRegister Rs);\n+\n+  \/\/ Pseudoinstruction for control and status register\n+  void rdinstret(Register Rd);                  \/\/ read instruction-retired counter\n+  void rdcycle(Register Rd);                    \/\/ read cycle counter\n+  void rdtime(Register Rd);                     \/\/ read time\n+  void csrr(Register Rd, unsigned csr);         \/\/ read csr\n+  void csrw(unsigned csr, Register Rs);         \/\/ write csr\n+  void csrs(unsigned csr, Register Rs);         \/\/ set bits in csr\n+  void csrc(unsigned csr, Register Rs);         \/\/ clear bits in csr\n+  void csrwi(unsigned csr, unsigned imm);\n+  void csrsi(unsigned csr, unsigned imm);\n+  void csrci(unsigned csr, unsigned imm);\n+  void frcsr(Register Rd);                      \/\/ read float-point csr\n+  void fscsr(Register Rd, Register Rs);         \/\/ swap float-point csr\n+  void fscsr(Register Rs);                      \/\/ write float-point csr\n+  void frrm(Register Rd);                       \/\/ read float-point rounding mode\n+  void fsrm(Register Rd, Register Rs);          \/\/ swap float-point rounding mode\n+  void fsrm(Register Rs);                       \/\/ write float-point rounding mode\n+  void fsrmi(Register Rd, unsigned imm);\n+  void fsrmi(unsigned imm);\n+  void frflags(Register Rd);                    \/\/ read float-point exception flags\n+  void fsflags(Register Rd, Register Rs);       \/\/ swap float-point exception flags\n+  void fsflags(Register Rs);                    \/\/ write float-point exception flags\n+  void fsflagsi(Register Rd, unsigned imm);\n+  void fsflagsi(unsigned imm);\n+\n+  void beqz(Register Rs, const address &dest);\n+  void bnez(Register Rs, const address &dest);\n+  void blez(Register Rs, const address &dest);\n+  void bgez(Register Rs, const address &dest);\n+  void bltz(Register Rs, const address &dest);\n+  void bgtz(Register Rs, const address &dest);\n+  void la(Register Rd, Label &label);\n+  void la(Register Rd, const address &dest);\n+  void la(Register Rd, const Address &adr);\n+  \/\/label\n+  void beqz(Register Rs, Label &l, bool is_far = false);\n+  void bnez(Register Rs, Label &l, bool is_far = false);\n+  void blez(Register Rs, Label &l, bool is_far = false);\n+  void bgez(Register Rs, Label &l, bool is_far = false);\n+  void bltz(Register Rs, Label &l, bool is_far = false);\n+  void bgtz(Register Rs, Label &l, bool is_far = false);\n+  void float_beq(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void float_bne(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void float_ble(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void float_bge(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void float_blt(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void float_bgt(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_beq(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_bne(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_ble(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_bge(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_blt(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+  void double_bgt(FloatRegister Rs1, FloatRegister Rs2, Label &l, bool is_far = false, bool is_unordered = false);\n+\n+  void push_reg(RegSet regs, Register stack) { if (regs.bits()) { push_reg(regs.bits(), stack); } }\n+  void pop_reg(RegSet regs, Register stack) { if (regs.bits()) { pop_reg(regs.bits(), stack); } }\n+  void push_reg(Register Rs);\n+  void pop_reg(Register Rd);\n+  int  push_reg(unsigned int bitset, Register stack);\n+  int  pop_reg(unsigned int bitset, Register stack);\n+\n+  \/\/ Push and pop everything that might be clobbered by a native\n+  \/\/ runtime call except t0 and t1. (They are always\n+  \/\/ temporary registers, so we don't have to protect them.)\n+  \/\/ Additional registers can be excluded in a passed RegSet.\n+  void push_call_clobbered_registers_except(RegSet exclude);\n+  void pop_call_clobbered_registers_except(RegSet exclude);\n+\n+  void push_call_clobbered_registers() {\n+    push_call_clobbered_registers_except(RegSet());\n+  }\n+  void pop_call_clobbered_registers() {\n+    pop_call_clobbered_registers_except(RegSet());\n+  }\n+\n+  void pusha();\n+  void popa();\n+  void push_CPU_state();\n+  void pop_CPU_state();\n+\n+  \/\/ if heap base register is used - reinit it with the correct value\n+  void reinit_heapbase();\n+\n+  void bind(Label& L) {\n+    Assembler::bind(L);\n+    \/\/ fences across basic blocks should not be merged\n+    code()->clear_last_insn();\n+  }\n+\n+  \/\/ mv\n+  inline void mv(Register Rd, int imm64)                { li(Rd, (int64_t)imm64); }\n+  inline void mv(Register Rd, long imm64)               { li(Rd, (int64_t)imm64); }\n+  inline void mv(Register Rd, long long imm64)          { li(Rd, (int64_t)imm64); }\n+  inline void mv(Register Rd, unsigned int imm64)       { li(Rd, (int64_t)imm64); }\n+  inline void mv(Register Rd, unsigned long imm64)      { li(Rd, (int64_t)imm64); }\n+  inline void mv(Register Rd, unsigned long long imm64) { li(Rd, (int64_t)imm64); }\n+\n+  inline void mvw(Register Rd, int32_t imm32) { mv(Rd, imm32); }\n+\n+  void mv(Register Rd, Address dest);\n+  void mv(Register Rd, address dest);\n+  void mv(Register Rd, RegisterOrConstant src);\n+\n+  \/\/ logic\n+  void andrw(Register Rd, Register Rs1, Register Rs2);\n+  void orrw(Register Rd, Register Rs1, Register Rs2);\n+  void xorrw(Register Rd, Register Rs1, Register Rs2);\n+\n+  \/\/ revb\n+  void revb_h_h(Register Rd, Register Rs, Register tmp = t0);                           \/\/ reverse bytes in halfword in lower 16 bits, sign-extend\n+  void revb_w_w(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);      \/\/ reverse bytes in lower word, sign-extend\n+  void revb_h_h_u(Register Rd, Register Rs, Register tmp = t0);                         \/\/ reverse bytes in halfword in lower 16 bits, zero-extend\n+  void revb_h_w_u(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);    \/\/ reverse bytes in halfwords in lower 32 bits, zero-extend\n+  void revb_h_helper(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);  \/\/ reverse bytes in upper 16 bits (48:63) and move to lower\n+  void revb_h(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);         \/\/ reverse bytes in each halfword\n+  void revb_w(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);         \/\/ reverse bytes in each word\n+  void revb(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);          \/\/ reverse bytes in doubleword\n+\n+  void ror_imm(Register dst, Register src, uint32_t shift, Register tmp = t0);\n+  void andi(Register Rd, Register Rn, int64_t imm, Register tmp = t0);\n+  void orptr(Address adr, RegisterOrConstant src, Register tmp1 = t0, Register tmp2 = t1);\n+\n+  void cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp, Label &succeed, Label *fail);\n+  void cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp, Label &succeed, Label *fail);\n+  void cmpxchg(Register addr, Register expected,\n+               Register new_val,\n+               enum operand_size size,\n+               Assembler::Aqrl acquire, Assembler::Aqrl release,\n+               Register result, bool result_as_bool = false);\n+  void cmpxchg_weak(Register addr, Register expected,\n+                    Register new_val,\n+                    enum operand_size size,\n+                    Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                    Register result);\n+  void cmpxchg_narrow_value_helper(Register addr, Register expected,\n+                                   Register new_val,\n+                                   enum operand_size size,\n+                                   Register tmp1, Register tmp2, Register tmp3);\n+  void cmpxchg_narrow_value(Register addr, Register expected,\n+                            Register new_val,\n+                            enum operand_size size,\n+                            Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                            Register result, bool result_as_bool,\n+                            Register tmp1, Register tmp2, Register tmp3);\n+  void weak_cmpxchg_narrow_value(Register addr, Register expected,\n+                                 Register new_val,\n+                                 enum operand_size size,\n+                                 Assembler::Aqrl acquire, Assembler::Aqrl release,\n+                                 Register result,\n+                                 Register tmp1, Register tmp2, Register tmp3);\n+\n+  void atomic_add(Register prev, RegisterOrConstant incr, Register addr);\n+  void atomic_addw(Register prev, RegisterOrConstant incr, Register addr);\n+  void atomic_addal(Register prev, RegisterOrConstant incr, Register addr);\n+  void atomic_addalw(Register prev, RegisterOrConstant incr, Register addr);\n+\n+  void atomic_xchg(Register prev, Register newv, Register addr);\n+  void atomic_xchgw(Register prev, Register newv, Register addr);\n+  void atomic_xchgal(Register prev, Register newv, Register addr);\n+  void atomic_xchgalw(Register prev, Register newv, Register addr);\n+  void atomic_xchgwu(Register prev, Register newv, Register addr);\n+  void atomic_xchgalwu(Register prev, Register newv, Register addr);\n+\n+  static bool far_branches() {\n+    return ReservedCodeCacheSize > branch_range;\n+  }\n+\n+  \/\/ Jumps that can reach anywhere in the code cache.\n+  \/\/ Trashes tmp.\n+  void far_call(Address entry, CodeBuffer *cbuf = NULL, Register tmp = t0);\n+  void far_jump(Address entry, CodeBuffer *cbuf = NULL, Register tmp = t0);\n+\n+  static int far_branch_size() {\n+    if (far_branches()) {\n+      return 2 * 4;  \/\/ auipc + jalr, see far_call() & far_jump()\n+    } else {\n+      return 4;\n+    }\n+  }\n+\n+  void load_byte_map_base(Register reg);\n+\n+  void bang_stack_with_offset(int offset) {\n+    \/\/ stack grows down, caller passes positive offset\n+    assert(offset > 0, \"must bang with negative offset\");\n+    sub(t0, sp, offset);\n+    sd(zr, Address(t0));\n+  }\n+\n+  void la_patchable(Register reg1, const Address &dest, int32_t &offset);\n+\n+  virtual void _call_Unimplemented(address call_site) {\n+    mv(t1, call_site);\n+  }\n+\n+  #define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)\n+\n+  \/\/ Frame creation and destruction shared between JITs.\n+  void build_frame(int framesize);\n+  void remove_frame(int framesize);\n+\n+  void reserved_stack_check();\n+\n+  virtual RegisterOrConstant delayed_value_impl(intptr_t* delayed_value_addr,\n+                                                Register tmp,\n+                                                int offset);\n+\n+  void get_polling_page(Register dest, address page, int32_t &offset, relocInfo::relocType rtype);\n+  void read_polling_page(Register r, address page, relocInfo::relocType rtype);\n+  void read_polling_page(Register r, int32_t offset, relocInfo::relocType rtype);\n+\n+  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL);\n+  address ic_call(address entry, jint method_index = 0);\n+\n+  void add_memory_int64(const Address dst, int64_t imm);\n+  void add_memory_int32(const Address dst, int32_t imm);\n+\n+  void cmpptr(Register src1, Address src2, Label& equal);\n+\n+  void compute_index(Register str1, Register trailing_zeros, Register match_mask,\n+                     Register result, Register char_tmp, Register tmp,\n+                     bool haystack_isL);\n+  void compute_match_mask(Register src, Register pattern, Register match_mask,\n+                          Register mask1, Register mask2);\n+\n+#ifdef COMPILER2\n+  void mul_add(Register out, Register in, Register offset,\n+               Register len, Register k, Register tmp);\n+  void cad(Register dst, Register src1, Register src2, Register carry);\n+  void cadc(Register dst, Register src1, Register src2, Register carry);\n+  void adc(Register dst, Register src1, Register src2, Register carry);\n+  void add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,\n+                       Register src1, Register src2, Register carry);\n+  void multiply_32_x_32_loop(Register x, Register xstart, Register x_xstart,\n+                             Register y, Register y_idx, Register z,\n+                             Register carry, Register product,\n+                             Register idx, Register kdx);\n+  void multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,\n+                             Register y, Register y_idx, Register z,\n+                             Register carry, Register product,\n+                             Register idx, Register kdx);\n+  void multiply_128_x_128_loop(Register y, Register z,\n+                               Register carry, Register carry2,\n+                               Register idx, Register jdx,\n+                               Register yz_idx1, Register yz_idx2,\n+                               Register tmp, Register tmp3, Register tmp4,\n+                               Register tmp6, Register product_hi);\n+  void multiply_to_len(Register x, Register xlen, Register y, Register ylen,\n+                       Register z, Register zlen,\n+                       Register tmp1, Register tmp2, Register tmp3, Register tmp4,\n+                       Register tmp5, Register tmp6, Register product_hi);\n+#endif\n+\n+  void inflate_lo32(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);\n+  void inflate_hi32(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);\n+\n+  void ctzc_bit(Register Rd, Register Rs, bool isLL = false, Register tmp1 = t0, Register tmp2 = t1);\n+\n+  void zero_words(Register base, u_int64_t cnt);\n+  address zero_words(Register ptr, Register cnt);\n+  void fill_words(Register base, Register cnt, Register value);\n+  void zero_memory(Register addr, Register len, Register tmp);\n+\n+  \/\/ shift left by shamt and add\n+  void shadd(Register Rd, Register Rs1, Register Rs2, Register tmp, int shamt);\n+\n+  \/\/ Here the float instructions with safe deal with some exceptions.\n+  \/\/ e.g. convert from NaN, +Inf, -Inf to int, float, double\n+  \/\/ will trigger exception, we need to deal with these situations\n+  \/\/ to get correct results.\n+  void fcvt_w_s_safe(Register dst, FloatRegister src, Register tmp = t0);\n+  void fcvt_l_s_safe(Register dst, FloatRegister src, Register tmp = t0);\n+  void fcvt_w_d_safe(Register dst, FloatRegister src, Register tmp = t0);\n+  void fcvt_l_d_safe(Register dst, FloatRegister src, Register tmp = t0);\n+\n+  \/\/ vector load\/store unit-stride instructions\n+  void vlex_v(VectorRegister vd, Register base, Assembler::SEW sew, VectorMask vm = unmasked) {\n+    switch (sew) {\n+      case Assembler::e64:\n+        vle64_v(vd, base, vm);\n+        break;\n+      case Assembler::e32:\n+        vle32_v(vd, base, vm);\n+        break;\n+      case Assembler::e16:\n+        vle16_v(vd, base, vm);\n+        break;\n+      case Assembler::e8: \/\/ fall through\n+      default:\n+        vle8_v(vd, base, vm);\n+        break;\n+    }\n+  }\n+\n+  void vsex_v(VectorRegister store_data, Register base, Assembler::SEW sew, VectorMask vm = unmasked) {\n+    switch (sew) {\n+      case Assembler::e64:\n+        vse64_v(store_data, base, vm);\n+        break;\n+      case Assembler::e32:\n+        vse32_v(store_data, base, vm);\n+        break;\n+      case Assembler::e16:\n+        vse16_v(store_data, base, vm);\n+        break;\n+      case Assembler::e8: \/\/ fall through\n+      default:\n+        vse8_v(store_data, base, vm);\n+        break;\n+    }\n+  }\n+\n+  static const int zero_words_block_size;\n+\n+  void cast_primitive_type(BasicType type, Register Rt) {\n+    switch (type) {\n+      case T_BOOLEAN:\n+        sltu(Rt, zr, Rt);\n+        break;\n+      case T_CHAR   :\n+        zero_extend(Rt, Rt, 16);\n+        break;\n+      case T_BYTE   :\n+        sign_extend(Rt, Rt, 8);\n+        break;\n+      case T_SHORT  :\n+        sign_extend(Rt, Rt, 16);\n+        break;\n+      case T_INT    :\n+        addw(Rt, Rt, zr);\n+        break;\n+      case T_LONG   : \/* nothing to do *\/        break;\n+      case T_VOID   : \/* nothing to do *\/        break;\n+      case T_FLOAT  : \/* nothing to do *\/        break;\n+      case T_DOUBLE : \/* nothing to do *\/        break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ float cmp with unordered_result\n+  void float_compare(Register result, FloatRegister Rs1, FloatRegister Rs2, int unordered_result);\n+  void double_compare(Register result, FloatRegister Rs1, FloatRegister Rs2, int unordered_result);\n+\n+  \/\/ Zero\/Sign-extend\n+  void zero_extend(Register dst, Register src, int bits);\n+  void sign_extend(Register dst, Register src, int bits);\n+\n+  \/\/ compare src1 and src2 and get -1\/0\/1 in dst.\n+  \/\/ if [src1 > src2], dst = 1;\n+  \/\/ if [src1 == src2], dst = 0;\n+  \/\/ if [src1 < src2], dst = -1;\n+  void cmp_l2i(Register dst, Register src1, Register src2, Register tmp = t0);\n+\n+  int push_fp(unsigned int bitset, Register stack);\n+  int pop_fp(unsigned int bitset, Register stack);\n+\n+  \/\/ vext\n+  void vmnot_m(VectorRegister vd, VectorRegister vs);\n+  void vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm = unmasked);\n+  void vfneg_v(VectorRegister vd, VectorRegister vs);\n+\n+private:\n+\n+#ifdef ASSERT\n+  \/\/ Macro short-hand support to clean-up after a failed call to trampoline\n+  \/\/ call generation (see trampoline_call() below), when a set of Labels must\n+  \/\/ be reset (before returning).\n+#define reset_labels1(L1) L1.reset()\n+#define reset_labels2(L1, L2) L1.reset(); L2.reset()\n+#define reset_labels3(L1, L2, L3) L1.reset(); reset_labels2(L2, L3)\n+#define reset_labels5(L1, L2, L3, L4, L5) reset_labels2(L1, L2); reset_labels3(L3, L4, L5)\n+#endif\n+  void repne_scan(Register addr, Register value, Register count, Register tmp);\n+\n+  \/\/ Return true if an address is within the 48-bit RISCV64 address space.\n+  bool is_valid_riscv64_address(address addr) {\n+    \/\/ sv48: must have bits 63–48 all equal to bit 47\n+    return ((uintptr_t)addr >> 47) == 0;\n+  }\n+\n+  void ld_constant(Register dest, const Address &const_addr) {\n+    if (NearCpool) {\n+      ld(dest, const_addr);\n+    } else {\n+      int32_t offset = 0;\n+      la_patchable(dest, InternalAddress(const_addr.target()), offset);\n+      ld(dest, Address(dest, offset));\n+    }\n+  }\n+\n+  int bitset_to_regs(unsigned int bitset, unsigned char* regs);\n+  Address add_memory_helper(const Address dst);\n+\n+  void load_reserved(Register addr, enum operand_size size, Assembler::Aqrl acquire);\n+  void store_conditional(Register addr, Register new_val, enum operand_size size, Assembler::Aqrl release);\n+\n+public:\n+  void string_compare(Register str1, Register str2,\n+                      Register cnt1, Register cnt2, Register result,\n+                      Register tmp1, Register tmp2, Register tmp3,\n+                      int ae);\n+\n+  void string_indexof_char_short(Register str1, Register cnt1,\n+                                 Register ch, Register result,\n+                                 bool isL);\n+\n+  void string_indexof_char(Register str1, Register cnt1,\n+                           Register ch, Register result,\n+                           Register tmp1, Register tmp2,\n+                           Register tmp3, Register tmp4,\n+                           bool isL);\n+\n+  void string_indexof(Register str1, Register str2,\n+                      Register cnt1, Register cnt2,\n+                      Register tmp1, Register tmp2,\n+                      Register tmp3, Register tmp4,\n+                      Register tmp5, Register tmp6,\n+                      Register result, int ae);\n+\n+  void string_indexof_linearscan(Register haystack, Register needle,\n+                                 Register haystack_len, Register needle_len,\n+                                 Register tmp1, Register tmp2,\n+                                 Register tmp3, Register tmp4,\n+                                 int needle_con_cnt, Register result, int ae);\n+\n+  void arrays_equals(Register r1, Register r2,\n+                     Register tmp3, Register tmp4,\n+                     Register tmp5, Register tmp6,\n+                     Register result, Register cnt1,\n+                     int elem_size);\n+\n+  void string_equals(Register r1, Register r2,\n+                     Register result, Register cnt1,\n+                     int elem_size);\n+\n+  \/\/ refer to conditional_branches and float_conditional_branches\n+  static const int bool_test_bits = 3;\n+  static const int neg_cond_bits = 2;\n+  static const int unsigned_branch_mask = 1 << bool_test_bits;\n+  static const int double_branch_mask = 1 << bool_test_bits;\n+\n+  \/\/ cmp\n+  void cmp_branch(int cmpFlag,\n+                  Register op1, Register op2,\n+                  Label& label, bool is_far = false);\n+\n+  void float_cmp_branch(int cmpFlag,\n+                        FloatRegister op1, FloatRegister op2,\n+                        Label& label, bool is_far = false);\n+\n+  void enc_cmpUEqNeLeGt_imm0_branch(int cmpFlag, Register op,\n+                                    Label& L, bool is_far = false);\n+\n+  void enc_cmpEqNe_imm0_branch(int cmpFlag, Register op,\n+                               Label& L, bool is_far = false);\n+\n+  void enc_cmove(int cmpFlag,\n+                 Register op1, Register op2,\n+                 Register dst, Register src);\n+\n+  void spill(Register r, bool is64, int offset) {\n+    is64 ? sd(r, Address(sp, offset))\n+         : sw(r, Address(sp, offset));\n+  }\n+\n+  void spill(FloatRegister f, bool is64, int offset) {\n+    is64 ? fsd(f, Address(sp, offset))\n+         : fsw(f, Address(sp, offset));\n+  }\n+\n+  void spill(VectorRegister v, int offset) {\n+    add(t0, sp, offset);\n+    vs1r_v(v, t0);\n+  }\n+\n+  void unspill(Register r, bool is64, int offset) {\n+    is64 ? ld(r, Address(sp, offset))\n+         : lw(r, Address(sp, offset));\n+  }\n+\n+  void unspillu(Register r, bool is64, int offset) {\n+    is64 ? ld(r, Address(sp, offset))\n+         : lwu(r, Address(sp, offset));\n+  }\n+\n+  void unspill(FloatRegister f, bool is64, int offset) {\n+    is64 ? fld(f, Address(sp, offset))\n+         : flw(f, Address(sp, offset));\n+  }\n+\n+  void unspill(VectorRegister v, int offset) {\n+    add(t0, sp, offset);\n+    vl1r_v(v, t0);\n+  }\n+\n+  void minmax_FD(FloatRegister dst,\n+                 FloatRegister src1, FloatRegister src2,\n+                 bool is_double, bool is_min);\n+\n+};\n+\n+#ifdef ASSERT\n+inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }\n+#endif\n+\n+\/**\n+ * class SkipIfEqual:\n+ *\n+ * Instantiating this class will result in assembly code being output that will\n+ * jump around any code emitted between the creation of the instance and it's\n+ * automatic destruction at the end of a scope block, depending on the value of\n+ * the flag passed to the constructor, which will be checked at run-time.\n+ *\/\n+class SkipIfEqual {\n+ private:\n+  MacroAssembler* _masm;\n+  Label _label;\n+\n+ public:\n+   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);\n+   ~SkipIfEqual();\n+};\n+\n+#endif \/\/ CPU_RISCV_MACROASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":976,"deletions":0,"binary":false,"changes":976,"status":"added"},{"patch":"@@ -0,0 +1,31 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_MACROASSEMBLER_RISCV_INLINE_HPP\n+#define CPU_RISCV_MACROASSEMBLER_RISCV_INLINE_HPP\n+\n+\/\/ Still empty.\n+\n+#endif \/\/ CPU_RISCV_MACROASSEMBLER_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.inline.hpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"added"},{"patch":"@@ -0,0 +1,450 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interpreterRuntime.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/flags\/flagSetting.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ _masm->\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+void MethodHandles::load_klass_from_Class(MacroAssembler* _masm, Register klass_reg) {\n+  assert_cond(_masm != NULL);\n+  if (VerifyMethodHandles) {\n+    verify_klass(_masm, klass_reg, SystemDictionary::WK_KLASS_ENUM_NAME(java_lang_Class),\n+                 \"MH argument is a Class\");\n+  }\n+  __ ld(klass_reg, Address(klass_reg, java_lang_Class::klass_offset_in_bytes()));\n+}\n+\n+#ifdef ASSERT\n+static int check_nonzero(const char* xname, int x) {\n+  assert(x != 0, \"%s should be nonzero\", xname);\n+  return x;\n+}\n+#define NONZERO(x) check_nonzero(#x, x)\n+#else \/\/ASSERT\n+#define NONZERO(x) (x)\n+#endif \/\/PRODUCT\n+\n+#ifdef ASSERT\n+void MethodHandles::verify_klass(MacroAssembler* _masm,\n+                                 Register obj, SystemDictionary::WKID klass_id,\n+                                 const char* error_message) {\n+  assert_cond(_masm != NULL);\n+  InstanceKlass** klass_addr = SystemDictionary::well_known_klass_addr(klass_id);\n+  Klass* klass = SystemDictionary::well_known_klass(klass_id);\n+  Register temp = t1;\n+  Register temp2 = t0; \/\/ used by MacroAssembler::cmpptr\n+  Label L_ok, L_bad;\n+  BLOCK_COMMENT(\"verify_klass {\");\n+  __ verify_oop(obj);\n+  __ beqz(obj, L_bad);\n+  __ push_reg(RegSet::of(temp, temp2), sp);\n+  __ load_klass(temp, obj);\n+  __ cmpptr(temp, ExternalAddress((address) klass_addr), L_ok);\n+  intptr_t super_check_offset = klass->super_check_offset();\n+  __ ld(temp, Address(temp, super_check_offset));\n+  __ cmpptr(temp, ExternalAddress((address) klass_addr), L_ok);\n+  __ pop_reg(RegSet::of(temp, temp2), sp);\n+  __ bind(L_bad);\n+  __ stop(error_message);\n+  __ BIND(L_ok);\n+  __ pop_reg(RegSet::of(temp, temp2), sp);\n+  BLOCK_COMMENT(\"} verify_klass\");\n+}\n+\n+void MethodHandles::verify_ref_kind(MacroAssembler* _masm, int ref_kind, Register member_reg, Register temp) {}\n+\n+#endif \/\/ASSERT\n+\n+void MethodHandles::jump_from_method_handle(MacroAssembler* _masm, Register method, Register temp,\n+                                            bool for_compiler_entry) {\n+  assert_cond(_masm != NULL);\n+  assert(method == xmethod, \"interpreter calling convention\");\n+  Label L_no_such_method;\n+  __ beqz(xmethod, L_no_such_method);\n+  __ verify_method_ptr(method);\n+\n+  if (!for_compiler_entry && JvmtiExport::can_post_interpreter_events()) {\n+    Label run_compiled_code;\n+    \/\/ JVMTI events, such as single-stepping, are implemented partly by avoiding running\n+    \/\/ compiled code in threads for which the event is enabled.  Check here for\n+    \/\/ interp_only_mode if these events CAN be enabled.\n+\n+    __ lwu(t0, Address(xthread, JavaThread::interp_only_mode_offset()));\n+    __ beqz(t0, run_compiled_code);\n+    __ ld(t0, Address(method, Method::interpreter_entry_offset()));\n+    __ jr(t0);\n+    __ BIND(run_compiled_code);\n+  }\n+\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+                                                     Method::from_interpreted_offset();\n+  __ ld(t0,Address(method, entry_offset));\n+  __ jr(t0);\n+  __ bind(L_no_such_method);\n+  __ far_jump(RuntimeAddress(StubRoutines::throw_AbstractMethodError_entry()));\n+}\n+\n+void MethodHandles::jump_to_lambda_form(MacroAssembler* _masm,\n+                                        Register recv, Register method_temp,\n+                                        Register temp2,\n+                                        bool for_compiler_entry) {\n+  assert_cond(_masm != NULL);\n+  BLOCK_COMMENT(\"jump_to_lambda_form {\");\n+  \/\/ This is the initial entry point of a lazy method handle.\n+  \/\/ After type checking, it picks up the invoker from the LambdaForm.\n+  assert_different_registers(recv, method_temp, temp2);\n+  assert(recv != noreg, \"required register\");\n+  assert(method_temp == xmethod, \"required register for loading method\");\n+\n+  \/\/ Load the invoker, as MH -> MH.form -> LF.vmentry\n+  __ verify_oop(recv);\n+  __ load_heap_oop(method_temp, Address(recv, NONZERO(java_lang_invoke_MethodHandle::form_offset_in_bytes())), temp2);\n+  __ verify_oop(method_temp);\n+  __ load_heap_oop(method_temp, Address(method_temp, NONZERO(java_lang_invoke_LambdaForm::vmentry_offset_in_bytes())), temp2);\n+  __ verify_oop(method_temp);\n+  __ load_heap_oop(method_temp, Address(method_temp, NONZERO(java_lang_invoke_MemberName::method_offset_in_bytes())), temp2);\n+  __ verify_oop(method_temp);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, method_temp, Address(method_temp, NONZERO(java_lang_invoke_ResolvedMethodName::vmtarget_offset_in_bytes())), noreg, noreg);\n+\n+  if (VerifyMethodHandles && !for_compiler_entry) {\n+    \/\/ make sure recv is already on stack\n+    __ ld(temp2, Address(method_temp, Method::const_offset()));\n+    __ load_sized_value(temp2,\n+                        Address(temp2, ConstMethod::size_of_parameters_offset()),\n+                        sizeof(u2), \/*is_signed*\/ false);\n+    Label L;\n+    __ ld(t0, __ argument_address(temp2, -1));\n+    __ beq(recv, t0, L);\n+    __ ld(x10, __ argument_address(temp2, -1));\n+    __ ebreak();\n+    __ BIND(L);\n+  }\n+\n+  jump_from_method_handle(_masm, method_temp, temp2, for_compiler_entry);\n+  BLOCK_COMMENT(\"} jump_to_lambda_form\");\n+}\n+\n+\/\/ Code generation\n+address MethodHandles::generate_method_handle_interpreter_entry(MacroAssembler* _masm,\n+                                                                vmIntrinsics::ID iid) {\n+  assert_cond(_masm != NULL);\n+  const bool not_for_compiler_entry = false;  \/\/ this is the interpreter entry\n+  assert(is_signature_polymorphic(iid), \"expected invoke iid\");\n+  if (iid == vmIntrinsics::_invokeGeneric ||\n+      iid == vmIntrinsics::_compiledLambdaForm) {\n+    \/\/ Perhaps surprisingly, the symbolic references visible to Java are not directly used.\n+    \/\/ They are linked to Java-generated adapters via MethodHandleNatives.linkMethod.\n+    \/\/ They all allow an appendix argument.\n+    __ ebreak();           \/\/ empty stubs make SG sick\n+    return NULL;\n+  }\n+\n+  \/\/ x30: sender SP (must preserve; see prepare_to_jump_from_interpreted)\n+  \/\/ xmethod: Method*\n+  \/\/ x13: argument locator (parameter slot count, added to sp)\n+  \/\/ x11: used as temp to hold mh or receiver\n+  \/\/ x10, x29: garbage temps, blown away\n+  Register argp   = x13;   \/\/ argument list ptr, live on error paths\n+  Register mh     = x11;   \/\/ MH receiver; dies quickly and is recycled\n+\n+  \/\/ here's where control starts out:\n+  __ align(CodeEntryAlignment);\n+  address entry_point = __ pc();\n+\n+  if (VerifyMethodHandles) {\n+    assert(Method::intrinsic_id_size_in_bytes() == 2, \"assuming Method::_intrinsic_id is u2\");\n+\n+    Label L;\n+    BLOCK_COMMENT(\"verify_intrinsic_id {\");\n+    __ lhu(t0, Address(xmethod, Method::intrinsic_id_offset_in_bytes()));\n+    __ mv(t1, (int) iid);\n+    __ beq(t0, t1, L);\n+    if (iid == vmIntrinsics::_linkToVirtual ||\n+        iid == vmIntrinsics::_linkToSpecial) {\n+      \/\/ could do this for all kinds, but would explode assembly code size\n+      trace_method_handle(_masm, \"bad Method*::intrinsic_id\");\n+    }\n+    __ ebreak();\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} verify_intrinsic_id\");\n+  }\n+\n+  \/\/ First task:  Find out how big the argument list is.\n+  Address x13_first_arg_addr;\n+  int ref_kind = signature_polymorphic_intrinsic_ref_kind(iid);\n+  assert(ref_kind != 0 || iid == vmIntrinsics::_invokeBasic, \"must be _invokeBasic or a linkTo intrinsic\");\n+  if (ref_kind == 0 || MethodHandles::ref_kind_has_receiver(ref_kind)) {\n+    __ ld(argp, Address(xmethod, Method::const_offset()));\n+    __ load_sized_value(argp,\n+                        Address(argp, ConstMethod::size_of_parameters_offset()),\n+                        sizeof(u2), \/*is_signed*\/ false);\n+    x13_first_arg_addr = __ argument_address(argp, -1);\n+  } else {\n+    DEBUG_ONLY(argp = noreg);\n+  }\n+\n+  if (!is_signature_polymorphic_static(iid)) {\n+    __ ld(mh, x13_first_arg_addr);\n+    DEBUG_ONLY(argp = noreg);\n+  }\n+\n+  \/\/ x13_first_arg_addr is live!\n+\n+  trace_method_handle_interpreter_entry(_masm, iid);\n+  if (iid == vmIntrinsics::_invokeBasic) {\n+    generate_method_handle_dispatch(_masm, iid, mh, noreg, not_for_compiler_entry);\n+  } else {\n+    \/\/ Adjust argument list by popping the trailing MemberName argument.\n+    Register recv = noreg;\n+    if (MethodHandles::ref_kind_has_receiver(ref_kind)) {\n+      \/\/ Load the receiver (not the MH; the actual MemberName's receiver) up from the interpreter stack.\n+      __ ld(recv = x12, x13_first_arg_addr);\n+    }\n+    DEBUG_ONLY(argp = noreg);\n+    Register xmember = xmethod;  \/\/ MemberName ptr; incoming method ptr is dead now\n+    __ pop_reg(xmember);             \/\/ extract last argument\n+    generate_method_handle_dispatch(_masm, iid, recv, xmember, not_for_compiler_entry);\n+  }\n+\n+  return entry_point;\n+}\n+\n+\n+void MethodHandles::generate_method_handle_dispatch(MacroAssembler* _masm,\n+                                                    vmIntrinsics::ID iid,\n+                                                    Register receiver_reg,\n+                                                    Register member_reg,\n+                                                    bool for_compiler_entry) {\n+  assert_cond(_masm != NULL);\n+  assert(is_signature_polymorphic(iid), \"expected invoke iid\");\n+  \/\/ temps used in this code are not used in *either* compiled or interpreted calling sequences\n+  Register temp1 = x7;\n+  Register temp2 = x28;\n+  Register temp3 = x29;  \/\/ x30 is live by this point: it contains the sender SP\n+  if (for_compiler_entry) {\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert_different_registers(temp1, j_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4, j_rarg5, j_rarg6, j_rarg7);\n+    assert_different_registers(temp2, j_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4, j_rarg5, j_rarg6, j_rarg7);\n+    assert_different_registers(temp3, j_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4, j_rarg5, j_rarg6, j_rarg7);\n+  }\n+\n+  assert_different_registers(temp1, temp2, temp3, receiver_reg);\n+  assert_different_registers(temp1, temp2, temp3, member_reg);\n+\n+  if (iid == vmIntrinsics::_invokeBasic) {\n+    \/\/ indirect through MH.form.vmentry.vmtarget\n+    jump_to_lambda_form(_masm, receiver_reg, xmethod, temp1, for_compiler_entry);\n+  } else {\n+    \/\/ The method is a member invoker used by direct method handles.\n+    if (VerifyMethodHandles) {\n+      \/\/ make sure the trailing argument really is a MemberName (caller responsibility)\n+      verify_klass(_masm, member_reg, SystemDictionary::WK_KLASS_ENUM_NAME(java_lang_invoke_MemberName),\n+                   \"MemberName required for invokeVirtual etc.\");\n+    }\n+\n+    Address member_clazz(    member_reg, NONZERO(java_lang_invoke_MemberName::clazz_offset_in_bytes()));\n+    Address member_vmindex(  member_reg, NONZERO(java_lang_invoke_MemberName::vmindex_offset_in_bytes()));\n+    Address member_vmtarget( member_reg, NONZERO(java_lang_invoke_MemberName::method_offset_in_bytes()));\n+    Address vmtarget_method( xmethod, NONZERO(java_lang_invoke_ResolvedMethodName::vmtarget_offset_in_bytes()));\n+\n+    Register temp1_recv_klass = temp1;\n+    if (iid != vmIntrinsics::_linkToStatic) {\n+      __ verify_oop(receiver_reg);\n+      if (iid == vmIntrinsics::_linkToSpecial) {\n+        \/\/ Don't actually load the klass; just null-check the receiver.\n+        __ null_check(receiver_reg);\n+      } else {\n+        \/\/ load receiver klass itself\n+        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n+        __ load_klass(temp1_recv_klass, receiver_reg);\n+        __ verify_klass_ptr(temp1_recv_klass);\n+      }\n+      BLOCK_COMMENT(\"check_receiver {\");\n+      \/\/ The receiver for the MemberName must be in receiver_reg.\n+      \/\/ Check the receiver against the MemberName.clazz\n+      if (VerifyMethodHandles && iid == vmIntrinsics::_linkToSpecial) {\n+        \/\/ Did not load it above...\n+        __ load_klass(temp1_recv_klass, receiver_reg);\n+        __ verify_klass_ptr(temp1_recv_klass);\n+      }\n+      if (VerifyMethodHandles && iid != vmIntrinsics::_linkToInterface) {\n+        Label L_ok;\n+        Register temp2_defc = temp2;\n+        __ load_heap_oop(temp2_defc, member_clazz, temp3);\n+        load_klass_from_Class(_masm, temp2_defc);\n+        __ verify_klass_ptr(temp2_defc);\n+        __ check_klass_subtype(temp1_recv_klass, temp2_defc, temp3, L_ok);\n+        \/\/ If we get here, the type check failed!\n+        __ ebreak();\n+        __ bind(L_ok);\n+      }\n+      BLOCK_COMMENT(\"} check_receiver\");\n+    }\n+    if (iid == vmIntrinsics::_linkToSpecial ||\n+        iid == vmIntrinsics::_linkToStatic) {\n+      DEBUG_ONLY(temp1_recv_klass = noreg);  \/\/ these guys didn't load the recv_klass\n+    }\n+\n+    \/\/ Live registers at this point:\n+    \/\/  member_reg - MemberName that was the trailing argument\n+    \/\/  temp1_recv_klass - klass of stacked receiver, if needed\n+    \/\/  x30 - interpreter linkage (if interpreted)\n+    \/\/  x11 ... x10 - compiler arguments (if compiled)\n+\n+    Label L_incompatible_class_change_error;\n+    switch (iid) {\n+      case vmIntrinsics::_linkToSpecial:\n+        if (VerifyMethodHandles) {\n+          verify_ref_kind(_masm, JVM_REF_invokeSpecial, member_reg, temp3);\n+        }\n+        __ load_heap_oop(xmethod, member_vmtarget);\n+        __ access_load_at(T_ADDRESS, IN_HEAP, xmethod, vmtarget_method, noreg, noreg);\n+        break;\n+\n+      case vmIntrinsics::_linkToStatic:\n+        if (VerifyMethodHandles) {\n+          verify_ref_kind(_masm, JVM_REF_invokeStatic, member_reg, temp3);\n+        }\n+        __ load_heap_oop(xmethod, member_vmtarget);\n+        __ access_load_at(T_ADDRESS, IN_HEAP, xmethod, vmtarget_method, noreg, noreg);\n+        break;\n+\n+      case vmIntrinsics::_linkToVirtual:\n+      {\n+        \/\/ same as TemplateTable::invokevirtual,\n+        \/\/ minus the CP setup and profiling:\n+\n+        if (VerifyMethodHandles) {\n+          verify_ref_kind(_masm, JVM_REF_invokeVirtual, member_reg, temp3);\n+        }\n+\n+        \/\/ pick out the vtable index from the MemberName, and then we can discard it:\n+        Register temp2_index = temp2;\n+        __ access_load_at(T_ADDRESS, IN_HEAP, temp2_index, member_vmindex, noreg, noreg);\n+\n+        if (VerifyMethodHandles) {\n+          Label L_index_ok;\n+          __ bgez(temp2_index, L_index_ok);\n+          __ ebreak();\n+          __ BIND(L_index_ok);\n+        }\n+\n+        \/\/ Note:  The verifier invariants allow us to ignore MemberName.clazz and vmtarget\n+        \/\/ at this point.  And VerifyMethodHandles has already checked clazz, if needed.\n+\n+        \/\/ get target Method* & entry point\n+        __ lookup_virtual_method(temp1_recv_klass, temp2_index, xmethod);\n+        break;\n+      }\n+\n+      case vmIntrinsics::_linkToInterface:\n+      {\n+        \/\/ same as TemplateTable::invokeinterface\n+        \/\/ (minus the CP setup and profiling, with different argument motion)\n+        if (VerifyMethodHandles) {\n+          verify_ref_kind(_masm, JVM_REF_invokeInterface, member_reg, temp3);\n+        }\n+\n+        Register temp3_intf = temp3;\n+        __ load_heap_oop(temp3_intf, member_clazz);\n+        load_klass_from_Class(_masm, temp3_intf);\n+        __ verify_klass_ptr(temp3_intf);\n+\n+        Register rindex = xmethod;\n+        __ access_load_at(T_ADDRESS, IN_HEAP, rindex, member_vmindex, noreg, noreg);\n+        if (VerifyMethodHandles) {\n+          Label L;\n+          __ bgez(rindex, L);\n+          __ ebreak();\n+          __ bind(L);\n+        }\n+\n+        \/\/ given intf, index, and recv klass, dispatch to the implementation method\n+        __ lookup_interface_method(temp1_recv_klass, temp3_intf,\n+                                   \/\/ note: next two args must be the same:\n+                                   rindex, xmethod,\n+                                   temp2,\n+                                   L_incompatible_class_change_error);\n+        break;\n+      }\n+\n+      default:\n+        fatal(\"unexpected intrinsic %d: %s\", iid, vmIntrinsics::name_at(iid));\n+        break;\n+    }\n+\n+    \/\/ live at this point:  xmethod, x30 (if interpreted)\n+\n+    \/\/ After figuring out which concrete method to call, jump into it.\n+    \/\/ Note that this works in the interpreter with no data motion.\n+    \/\/ But the compiled version will require that r2_recv be shifted out.\n+    __ verify_method_ptr(xmethod);\n+    jump_from_method_handle(_masm, xmethod, temp1, for_compiler_entry);\n+    if (iid == vmIntrinsics::_linkToInterface) {\n+      __ bind(L_incompatible_class_change_error);\n+      __ far_jump(RuntimeAddress(StubRoutines::throw_IncompatibleClassChangeError_entry()));\n+    }\n+  }\n+\n+}\n+\n+#ifndef PRODUCT\n+void trace_method_handle_stub(const char* adaptername,\n+                              oopDesc* mh,\n+                              intptr_t* saved_regs,\n+                              intptr_t* entry_sp) {  }\n+\n+\/\/ The stub wraps the arguments in a struct on the stack to avoid\n+\/\/ dealing with the different calling conventions for passing 6\n+\/\/ arguments.\n+struct MethodHandleStubArguments {\n+  const char* adaptername;\n+  oopDesc* mh;\n+  intptr_t* saved_regs;\n+  intptr_t* entry_sp;\n+};\n+void trace_method_handle_stub_wrapper(MethodHandleStubArguments* args) {  }\n+\n+void MethodHandles::trace_method_handle(MacroAssembler* _masm, const char* adaptername) {  }\n+#endif \/\/PRODUCT\n","filename":"src\/hotspot\/cpu\/riscv\/methodHandles_riscv.cpp","additions":450,"deletions":0,"binary":false,"changes":450,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2010, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/\/ Platform-specific definitions for method handles.\n+\/\/ These definitions are inlined into class MethodHandles.\n+\n+\/\/ Adapters\n+enum \/* platform_dependent_constants *\/ {\n+  adapter_code_size = 32000 DEBUG_ONLY(+ 120000)\n+};\n+\n+public:\n+\n+  static void load_klass_from_Class(MacroAssembler* _masm, Register klass_reg);\n+\n+  static void verify_klass(MacroAssembler* _masm,\n+                           Register obj, SystemDictionary::WKID klass_id,\n+                           const char* error_message = \"wrong klass\") NOT_DEBUG_RETURN;\n+\n+  static void verify_method_handle(MacroAssembler* _masm, Register mh_reg) {\n+    verify_klass(_masm, mh_reg, SystemDictionary::WK_KLASS_ENUM_NAME(java_lang_invoke_MethodHandle),\n+                 \"reference is a MH\");\n+  }\n+\n+  static void verify_ref_kind(MacroAssembler* _masm, int ref_kind, Register member_reg, Register temp) NOT_DEBUG_RETURN;\n+\n+  \/\/ Similar to InterpreterMacroAssembler::jump_from_interpreted.\n+  \/\/ Takes care of special dispatch from single stepping too.\n+  static void jump_from_method_handle(MacroAssembler* _masm, Register method, Register temp,\n+                                      bool for_compiler_entry);\n+\n+  static void jump_to_lambda_form(MacroAssembler* _masm,\n+                                  Register recv, Register method_temp,\n+                                  Register temp2,\n+                                  bool for_compiler_entry);\n","filename":"src\/hotspot\/cpu\/riscv\/methodHandles_riscv.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,417 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/compiledIC.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n+\n+Register NativeInstruction::extract_rs1(address instr) {\n+  assert_cond(instr != NULL);\n+  return as_Register(Assembler::extract(((unsigned*)instr)[0], 19, 15));\n+}\n+\n+Register NativeInstruction::extract_rs2(address instr) {\n+  assert_cond(instr != NULL);\n+  return as_Register(Assembler::extract(((unsigned*)instr)[0], 24, 20));\n+}\n+\n+Register NativeInstruction::extract_rd(address instr) {\n+  assert_cond(instr != NULL);\n+  return as_Register(Assembler::extract(((unsigned*)instr)[0], 11, 7));\n+}\n+\n+uint32_t NativeInstruction::extract_opcode(address instr) {\n+  assert_cond(instr != NULL);\n+  return Assembler::extract(((unsigned*)instr)[0], 6, 0);\n+}\n+\n+uint32_t NativeInstruction::extract_funct3(address instr) {\n+  assert_cond(instr != NULL);\n+  return Assembler::extract(((unsigned*)instr)[0], 14, 12);\n+}\n+\n+bool NativeInstruction::is_pc_relative_at(address instr) {\n+  \/\/ auipc + jalr\n+  \/\/ auipc + addi\n+  \/\/ auipc + load\n+  \/\/ auipc + fload_load\n+  return (is_auipc_at(instr)) &&\n+         (is_addi_at(instr + instruction_size) ||\n+          is_jalr_at(instr + instruction_size) ||\n+          is_load_at(instr + instruction_size) ||\n+          is_float_load_at(instr + instruction_size)) &&\n+         check_pc_relative_data_dependency(instr);\n+}\n+\n+\/\/ ie:ld(Rd, Label)\n+bool NativeInstruction::is_load_pc_relative_at(address instr) {\n+  return is_auipc_at(instr) && \/\/ auipc\n+         is_ld_at(instr + instruction_size) && \/\/ ld\n+         check_load_pc_relative_data_dependency(instr);\n+}\n+\n+bool NativeInstruction::is_movptr_at(address instr) {\n+  return is_lui_at(instr) && \/\/ Lui\n+         is_addi_at(instr + instruction_size) && \/\/ Addi\n+         is_slli_shift_at(instr + instruction_size * 2, 11) && \/\/ Slli Rd, Rs, 11\n+         is_addi_at(instr + instruction_size * 3) && \/\/ Addi\n+         is_slli_shift_at(instr + instruction_size * 4, 6) && \/\/ Slli Rd, Rs, 6\n+         (is_addi_at(instr + instruction_size * 5) ||\n+          is_jalr_at(instr + instruction_size * 5) ||\n+          is_load_at(instr + instruction_size * 5)) && \/\/ Addi\/Jalr\/Load\n+         check_movptr_data_dependency(instr);\n+}\n+\n+bool NativeInstruction::is_li32_at(address instr) {\n+  return is_lui_at(instr) && \/\/ lui\n+         is_addiw_at(instr + instruction_size) && \/\/ addiw\n+         check_li32_data_dependency(instr);\n+}\n+\n+bool NativeInstruction::is_li64_at(address instr) {\n+  return is_lui_at(instr) && \/\/ lui\n+         is_addi_at(instr + instruction_size) && \/\/ addi\n+         is_slli_shift_at(instr + instruction_size * 2, 12) &&  \/\/ Slli Rd, Rs, 12\n+         is_addi_at(instr + instruction_size * 3) && \/\/ addi\n+         is_slli_shift_at(instr + instruction_size * 4, 12) &&  \/\/ Slli Rd, Rs, 12\n+         is_addi_at(instr + instruction_size * 5) && \/\/ addi\n+         is_slli_shift_at(instr + instruction_size * 6, 8) &&   \/\/ Slli Rd, Rs, 8\n+         is_addi_at(instr + instruction_size * 7) && \/\/ addi\n+         check_li64_data_dependency(instr);\n+}\n+\n+void NativeCall::verify() {\n+  assert(NativeCall::is_call_at((address)this), \"unexpected code at call site\");\n+}\n+\n+address NativeCall::destination() const {\n+  address addr = (address)this;\n+  assert(NativeInstruction::is_jal_at(instruction_address()), \"inst must be jal.\");\n+  address destination = MacroAssembler::target_addr_for_insn(instruction_address());\n+\n+  \/\/ Do we use a trampoline stub for this call?\n+  CodeBlob* cb = CodeCache::find_blob_unsafe(addr);   \/\/ Else we get assertion if nmethod is zombie.\n+  assert(cb && cb->is_nmethod(), \"sanity\");\n+  nmethod *nm = (nmethod *)cb;\n+  if (nm != NULL && nm->stub_contains(destination) && is_NativeCallTrampolineStub_at(destination)) {\n+    \/\/ Yes we do, so get the destination from the trampoline stub.\n+    const address trampoline_stub_addr = destination;\n+    destination = nativeCallTrampolineStub_at(trampoline_stub_addr)->destination();\n+  }\n+\n+  return destination;\n+}\n+\n+\/\/ Similar to replace_mt_safe, but just changes the destination. The\n+\/\/ important thing is that free-running threads are able to execute this\n+\/\/ call instruction at all times.\n+\/\/\n+\/\/ Used in the runtime linkage of calls; see class CompiledIC.\n+\/\/\n+\/\/ Add parameter assert_lock to switch off assertion\n+\/\/ during code generation, where no patching lock is needed.\n+void NativeCall::set_destination_mt_safe(address dest, bool assert_lock) {\n+  assert(!assert_lock ||\n+         (Patching_lock->is_locked() || SafepointSynchronize::is_at_safepoint()),\n+         \"concurrent code patching\");\n+\n+  ResourceMark rm;\n+  address addr_call = addr_at(0);\n+  assert(NativeCall::is_call_at(addr_call), \"unexpected code at call site\");\n+\n+  \/\/ Patch the constant in the call's trampoline stub.\n+  address trampoline_stub_addr = get_trampoline();\n+  if (trampoline_stub_addr != NULL) {\n+    assert (!is_NativeCallTrampolineStub_at(dest), \"chained trampolines\");\n+    nativeCallTrampolineStub_at(trampoline_stub_addr)->set_destination(dest);\n+  }\n+\n+  \/\/ Patch the call.\n+  if (Assembler::reachable_from_branch_at(addr_call, dest)) {\n+    set_destination(dest);\n+  } else {\n+    assert (trampoline_stub_addr != NULL, \"we need a trampoline\");\n+    set_destination(trampoline_stub_addr);\n+  }\n+\n+  ICache::invalidate_range(addr_call, instruction_size);\n+}\n+\n+address NativeCall::get_trampoline() {\n+  address call_addr = addr_at(0);\n+\n+  CodeBlob *code = CodeCache::find_blob(call_addr);\n+  assert(code != NULL, \"Could not find the containing code blob\");\n+\n+  address jal_destination = MacroAssembler::pd_call_destination(call_addr);\n+  if (code != NULL && code->contains(jal_destination) && is_NativeCallTrampolineStub_at(jal_destination)) {\n+    return jal_destination;\n+  }\n+\n+  if (code != NULL && code->is_nmethod()) {\n+    return trampoline_stub_Relocation::get_trampoline_for(call_addr, (nmethod*)code);\n+  }\n+\n+  return NULL;\n+}\n+\n+\/\/ Inserts a native call instruction at a given pc\n+void NativeCall::insert(address code_pos, address entry) { Unimplemented(); }\n+\n+\/\/-------------------------------------------------------------------\n+\n+void NativeMovConstReg::verify() {\n+  if (!(nativeInstruction_at(instruction_address())->is_movptr() ||\n+        is_auipc_at(instruction_address()))) {\n+    fatal(\"should be MOVPTR or AUIPC\");\n+  }\n+}\n+\n+intptr_t NativeMovConstReg::data() const {\n+  address addr = MacroAssembler::target_addr_for_insn(instruction_address());\n+  if (maybe_cpool_ref(instruction_address())) {\n+    return *(intptr_t*)addr;\n+  } else {\n+    return (intptr_t)addr;\n+  }\n+}\n+\n+void NativeMovConstReg::set_data(intptr_t x) {\n+  if (maybe_cpool_ref(instruction_address())) {\n+    address addr = MacroAssembler::target_addr_for_insn(instruction_address());\n+    *(intptr_t*)addr = x;\n+  } else {\n+    \/\/ Store x into the instruction stream.\n+    MacroAssembler::pd_patch_instruction_size(instruction_address(), (address)x);\n+    ICache::invalidate_range(instruction_address(), movptr_instruction_size);\n+  }\n+\n+  \/\/ Find and replace the oop\/metadata corresponding to this\n+  \/\/ instruction in oops section.\n+  CodeBlob* cb = CodeCache::find_blob(instruction_address());\n+  nmethod* nm = cb->as_nmethod_or_null();\n+  if (nm != NULL) {\n+    RelocIterator iter(nm, instruction_address(), next_instruction_address());\n+    while (iter.next()) {\n+      if (iter.type() == relocInfo::oop_type) {\n+        oop* oop_addr = iter.oop_reloc()->oop_addr();\n+        *oop_addr = cast_to_oop(x);\n+        break;\n+      } else if (iter.type() == relocInfo::metadata_type) {\n+        Metadata** metadata_addr = iter.metadata_reloc()->metadata_addr();\n+        *metadata_addr = (Metadata*)x;\n+        break;\n+      }\n+    }\n+  }\n+}\n+\n+void NativeMovConstReg::print() {\n+  tty->print_cr(PTR_FORMAT \": mov reg, \" INTPTR_FORMAT,\n+                p2i(instruction_address()), data());\n+}\n+\n+\/\/-------------------------------------------------------------------\n+\n+int NativeMovRegMem::offset() const  {\n+  Unimplemented();\n+  return 0;\n+}\n+\n+void NativeMovRegMem::set_offset(int x) { Unimplemented(); }\n+\n+void NativeMovRegMem::verify() {\n+  Unimplemented();\n+}\n+\n+\/\/--------------------------------------------------------------------------------\n+\n+void NativeJump::verify() { }\n+\n+\n+void NativeJump::check_verified_entry_alignment(address entry, address verified_entry) {\n+}\n+\n+\n+address NativeJump::jump_destination() const {\n+  address dest = MacroAssembler::target_addr_for_insn(instruction_address());\n+\n+  \/\/ We use jump to self as the unresolved address which the inline\n+  \/\/ cache code (and relocs) know about\n+\n+  \/\/ return -1 if jump to self\n+  dest = (dest == (address) this) ? (address) -1 : dest;\n+  return dest;\n+};\n+\n+void NativeJump::set_jump_destination(address dest) {\n+  \/\/ We use jump to self as the unresolved address which the inline\n+  \/\/ cache code (and relocs) know about\n+  if (dest == (address) -1)\n+    dest = instruction_address();\n+\n+  MacroAssembler::pd_patch_instruction(instruction_address(), dest);\n+  ICache::invalidate_range(instruction_address(), instruction_size);\n+}\n+\n+\/\/-------------------------------------------------------------------\n+\n+address NativeGeneralJump::jump_destination() const {\n+  NativeMovConstReg* move = nativeMovConstReg_at(instruction_address());\n+  address dest = (address) move->data();\n+\n+  \/\/ We use jump to self as the unresolved address which the inline\n+  \/\/ cache code (and relocs) know about\n+\n+  \/\/ return -1 if jump to self\n+  dest = (dest == (address) this) ? (address) -1 : dest;\n+  return dest;\n+}\n+\n+\/\/-------------------------------------------------------------------\n+\n+bool NativeInstruction::is_safepoint_poll() {\n+  return is_lwu_to_zr(address(this));\n+}\n+\n+bool NativeInstruction::is_lwu_to_zr(address instr) {\n+  assert_cond(instr != NULL);\n+  return (extract_opcode(instr) == 0b0000011 &&\n+          extract_funct3(instr) == 0b110 &&\n+          extract_rd(instr) == zr);         \/\/ zr\n+}\n+\n+\/\/ A 16-bit instruction with all bits ones is permanently reserved as an illegal instruction.\n+bool NativeInstruction::is_sigill_zombie_not_entrant() {\n+  \/\/ jvmci\n+  return uint_at(0) == 0xffffffff;\n+}\n+\n+void NativeIllegalInstruction::insert(address code_pos) {\n+  assert_cond(code_pos != NULL);\n+  *(juint*)code_pos = 0xffffffff; \/\/ all bits ones is permanently reserved as an illegal instruction\n+}\n+\n+bool NativeInstruction::is_stop() {\n+  return uint_at(0) == 0xffffffff; \/\/ an illegal instruction\n+}\n+\n+\/\/-------------------------------------------------------------------\n+\n+\/\/ MT-safe inserting of a jump over a jump or a nop (used by\n+\/\/ nmethod::make_not_entrant_or_zombie)\n+\n+void NativeJump::patch_verified_entry(address entry, address verified_entry, address dest) {\n+\n+  assert(dest == SharedRuntime::get_handle_wrong_method_stub(), \"expected fixed destination of patch\");\n+\n+  assert(nativeInstruction_at(verified_entry)->is_jump_or_nop() ||\n+         nativeInstruction_at(verified_entry)->is_sigill_zombie_not_entrant(),\n+         \"riscv cannot replace non-jump with jump\");\n+\n+  \/\/ Patch this nmethod atomically.\n+  if (Assembler::reachable_from_branch_at(verified_entry, dest)) {\n+    ptrdiff_t offset = dest - verified_entry;\n+    guarantee(is_imm_in_range(offset, 20, 1), \"offset is too large to be patched in one jal insrusction.\"); \/\/ 1M\n+\n+    uint32_t insn = 0;\n+    address pInsn = (address)&insn;\n+    Assembler::patch(pInsn, 31, 31, (offset >> 20) & 0x1);\n+    Assembler::patch(pInsn, 30, 21, (offset >> 1) & 0x3ff);\n+    Assembler::patch(pInsn, 20, 20, (offset >> 11) & 0x1);\n+    Assembler::patch(pInsn, 19, 12, (offset >> 12) & 0xff);\n+    Assembler::patch(pInsn, 11, 7, 0); \/\/ zero, no link jump\n+    Assembler::patch(pInsn, 6, 0, 0b1101111); \/\/ j, (jal x0 offset)\n+    *(unsigned int*)verified_entry = insn;\n+  } else {\n+    \/\/ We use an illegal instruction for marking a method as\n+    \/\/ not_entrant or zombie.\n+    NativeIllegalInstruction::insert(verified_entry);\n+  }\n+\n+  ICache::invalidate_range(verified_entry, instruction_size);\n+}\n+\n+void NativeGeneralJump::insert_unconditional(address code_pos, address entry) {\n+  CodeBuffer cb(code_pos, instruction_size);\n+  MacroAssembler a(&cb);\n+\n+  int32_t offset = 0;\n+  a.movptr_with_offset(t0, entry, offset); \/\/ lui, addi, slli, addi, slli\n+  a.jalr(x0, t0, offset); \/\/ jalr\n+\n+  ICache::invalidate_range(code_pos, instruction_size);\n+}\n+\n+\/\/ MT-safe patching of a long jump instruction.\n+void NativeGeneralJump::replace_mt_safe(address instr_addr, address code_buffer) {\n+  ShouldNotCallThis();\n+}\n+\n+\n+address NativeCallTrampolineStub::destination(nmethod *nm) const {\n+  return ptr_at(data_offset);\n+}\n+\n+void NativeCallTrampolineStub::set_destination(address new_destination) {\n+  set_ptr_at(data_offset, new_destination);\n+  OrderAccess::release();\n+}\n+\n+uint32_t NativeMembar::get_kind() {\n+  uint32_t insn = uint_at(0);\n+\n+  uint32_t predecessor = Assembler::extract(insn, 27, 24);\n+  uint32_t successor = Assembler::extract(insn, 23, 20);\n+\n+  return MacroAssembler::pred_succ_to_membar_mask(predecessor, successor);\n+}\n+\n+void NativeMembar::set_kind(uint32_t order_kind) {\n+  uint32_t predecessor = 0;\n+  uint32_t successor = 0;\n+\n+  MacroAssembler::membar_mask_to_pred_succ(order_kind, predecessor, successor);\n+\n+  uint32_t insn = uint_at(0);\n+  address pInsn = (address) &insn;\n+  Assembler::patch(pInsn, 27, 24, predecessor);\n+  Assembler::patch(pInsn, 23, 20, successor);\n+\n+  address membar = addr_at(0);\n+  *(unsigned int*) membar = insn;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.cpp","additions":417,"deletions":0,"binary":false,"changes":417,"status":"added"},{"patch":"@@ -0,0 +1,572 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2018, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_NATIVEINST_RISCV_HPP\n+#define CPU_RISCV_NATIVEINST_RISCV_HPP\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"runtime\/icache.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+\/\/ We have interfaces for the following instructions:\n+\/\/ - NativeInstruction\n+\/\/ - - NativeCall\n+\/\/ - - NativeMovConstReg\n+\/\/ - - NativeMovRegMem\n+\/\/ - - NativeJump\n+\/\/ - - NativeGeneralJump\n+\/\/ - - NativeIllegalInstruction\n+\/\/ - - NativeCallTrampolineStub\n+\/\/ - - NativeMembar\n+\/\/ - - NativeFenceI\n+\n+\/\/ The base class for different kinds of native instruction abstractions.\n+\/\/ Provides the primitive operations to manipulate code relative to this.\n+\n+class NativeCall;\n+\n+class NativeInstruction {\n+  friend class Relocation;\n+  friend bool is_NativeCallTrampolineStub_at(address);\n+ public:\n+  enum {\n+    instruction_size = 4,\n+    compressed_instruction_size = 2,\n+  };\n+\n+  juint encoding() const {\n+    return uint_at(0);\n+  }\n+\n+  bool is_jal()                             const { return is_jal_at(addr_at(0));         }\n+  bool is_movptr()                          const { return is_movptr_at(addr_at(0));      }\n+  bool is_call()                            const { return is_call_at(addr_at(0));        }\n+  bool is_jump()                            const { return is_jump_at(addr_at(0));        }\n+\n+  static bool is_jal_at(address instr)        { assert_cond(instr != NULL); return extract_opcode(instr) == 0b1101111; }\n+  static bool is_jalr_at(address instr)       { assert_cond(instr != NULL); return extract_opcode(instr) == 0b1100111 && extract_funct3(instr) == 0b000; }\n+  static bool is_branch_at(address instr)     { assert_cond(instr != NULL); return extract_opcode(instr) == 0b1100011; }\n+  static bool is_ld_at(address instr)         { assert_cond(instr != NULL); return is_load_at(instr) && extract_funct3(instr) == 0b011; }\n+  static bool is_load_at(address instr)       { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0000011; }\n+  static bool is_float_load_at(address instr) { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0000111; }\n+  static bool is_auipc_at(address instr)      { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0010111; }\n+  static bool is_jump_at(address instr)       { assert_cond(instr != NULL); return is_branch_at(instr) || is_jal_at(instr) || is_jalr_at(instr); }\n+  static bool is_addi_at(address instr)       { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0010011 && extract_funct3(instr) == 0b000; }\n+  static bool is_addiw_at(address instr)      { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0011011 && extract_funct3(instr) == 0b000; }\n+  static bool is_lui_at(address instr)        { assert_cond(instr != NULL); return extract_opcode(instr) == 0b0110111; }\n+  static bool is_slli_shift_at(address instr, uint32_t shift) {\n+    assert_cond(instr != NULL);\n+    return (extract_opcode(instr) == 0b0010011 && \/\/ opcode field\n+            extract_funct3(instr) == 0b001 &&     \/\/ funct3 field, select the type of operation\n+            Assembler::extract(((unsigned*)instr)[0], 25, 20) == shift);    \/\/ shamt field\n+  }\n+\n+  static Register extract_rs1(address instr);\n+  static Register extract_rs2(address instr);\n+  static Register extract_rd(address instr);\n+  static uint32_t extract_opcode(address instr);\n+  static uint32_t extract_funct3(address instr);\n+\n+  \/\/ the instruction sequence of movptr is as below:\n+  \/\/     lui\n+  \/\/     addi\n+  \/\/     slli\n+  \/\/     addi\n+  \/\/     slli\n+  \/\/     addi\/jalr\/load\n+  static bool check_movptr_data_dependency(address instr) {\n+    address lui = instr;\n+    address addi1 = lui + instruction_size;\n+    address slli1 = addi1 + instruction_size;\n+    address addi2 = slli1 + instruction_size;\n+    address slli2 = addi2 + instruction_size;\n+    address last_instr = slli2 + instruction_size;\n+    return extract_rs1(addi1) == extract_rd(lui) &&\n+           extract_rs1(addi1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(slli2) &&\n+           extract_rs1(last_instr) == extract_rd(slli2);\n+  }\n+\n+  \/\/ the instruction sequence of li64 is as below:\n+  \/\/     lui\n+  \/\/     addi\n+  \/\/     slli\n+  \/\/     addi\n+  \/\/     slli\n+  \/\/     addi\n+  \/\/     slli\n+  \/\/     addi\n+  static bool check_li64_data_dependency(address instr) {\n+    address lui = instr;\n+    address addi1 = lui + instruction_size;\n+    address slli1 = addi1 + instruction_size;\n+    address addi2 = slli1 + instruction_size;\n+    address slli2 = addi2 + instruction_size;\n+    address addi3 = slli2 + instruction_size;\n+    address slli3 = addi3 + instruction_size;\n+    address addi4 = slli3 + instruction_size;\n+    return extract_rs1(addi1) == extract_rd(lui) &&\n+           extract_rs1(addi1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(slli2) &&\n+           extract_rs1(addi3) == extract_rd(slli2) &&\n+           extract_rs1(addi3) == extract_rd(addi3) &&\n+           extract_rs1(slli3) == extract_rd(addi3) &&\n+           extract_rs1(slli3) == extract_rd(slli3) &&\n+           extract_rs1(addi4) == extract_rd(slli3) &&\n+           extract_rs1(addi4) == extract_rd(addi4);\n+  }\n+\n+  \/\/ the instruction sequence of li32 is as below:\n+  \/\/     lui\n+  \/\/     addiw\n+  static bool check_li32_data_dependency(address instr) {\n+    address lui = instr;\n+    address addiw = lui + instruction_size;\n+\n+    return extract_rs1(addiw) == extract_rd(lui) &&\n+           extract_rs1(addiw) == extract_rd(addiw);\n+  }\n+\n+  \/\/ the instruction sequence of pc-relative is as below:\n+  \/\/     auipc\n+  \/\/     jalr\/addi\/load\/float_load\n+  static bool check_pc_relative_data_dependency(address instr) {\n+    address auipc = instr;\n+    address last_instr = auipc + instruction_size;\n+\n+    return extract_rs1(last_instr) == extract_rd(auipc);\n+  }\n+\n+  \/\/ the instruction sequence of load_label is as below:\n+  \/\/     auipc\n+  \/\/     load\n+  static bool check_load_pc_relative_data_dependency(address instr) {\n+    address auipc = instr;\n+    address load = auipc + instruction_size;\n+\n+    return extract_rd(load) == extract_rd(auipc) &&\n+           extract_rs1(load) == extract_rd(load);\n+  }\n+\n+  static bool is_movptr_at(address instr);\n+  static bool is_li32_at(address instr);\n+  static bool is_li64_at(address instr);\n+  static bool is_pc_relative_at(address branch);\n+  static bool is_load_pc_relative_at(address branch);\n+\n+  static bool is_call_at(address instr) {\n+    if (is_jal_at(instr) || is_jalr_at(instr)) {\n+      return true;\n+    }\n+    return false;\n+  }\n+  static bool is_lwu_to_zr(address instr);\n+\n+  inline bool is_nop();\n+  inline bool is_jump_or_nop();\n+  bool is_safepoint_poll();\n+  bool is_sigill_zombie_not_entrant();\n+  bool is_stop();\n+\n+ protected:\n+  address addr_at(int offset) const    { return address(this) + offset; }\n+\n+  jint int_at(int offset) const        { return *(jint*) addr_at(offset); }\n+  juint uint_at(int offset) const      { return *(juint*) addr_at(offset); }\n+\n+  address ptr_at(int offset) const     { return *(address*) addr_at(offset); }\n+\n+  oop  oop_at (int offset) const       { return *(oop*) addr_at(offset); }\n+\n+\n+  void set_int_at(int offset, jint  i)        { *(jint*)addr_at(offset) = i; }\n+  void set_uint_at(int offset, jint  i)       { *(juint*)addr_at(offset) = i; }\n+  void set_ptr_at (int offset, address  ptr)  { *(address*) addr_at(offset) = ptr; }\n+  void set_oop_at (int offset, oop  o)        { *(oop*) addr_at(offset) = o; }\n+\n+ public:\n+\n+  inline friend NativeInstruction* nativeInstruction_at(address addr);\n+\n+  static bool maybe_cpool_ref(address instr) {\n+    return is_auipc_at(instr);\n+  }\n+\n+  bool is_membar() {\n+    return (uint_at(0) & 0x7f) == 0b1111 && extract_funct3(addr_at(0)) == 0;\n+  }\n+};\n+\n+inline NativeInstruction* nativeInstruction_at(address addr) {\n+  return (NativeInstruction*)addr;\n+}\n+\n+\/\/ The natural type of an RISCV instruction is uint32_t\n+inline NativeInstruction* nativeInstruction_at(uint32_t *addr) {\n+  return (NativeInstruction*)addr;\n+}\n+\n+inline NativeCall* nativeCall_at(address addr);\n+\/\/ The NativeCall is an abstraction for accessing\/manipulating native\n+\/\/ call instructions (used to manipulate inline caches, primitive &\n+\/\/ DSO calls, etc.).\n+\n+class NativeCall: public NativeInstruction {\n+ public:\n+  enum RISCV_specific_constants {\n+    instruction_size            =    4,\n+    instruction_offset          =    0,\n+    displacement_offset         =    0,\n+    return_address_offset       =    4\n+  };\n+\n+  address instruction_address() const       { return addr_at(instruction_offset); }\n+  address next_instruction_address() const  { return addr_at(return_address_offset); }\n+  address return_address() const            { return addr_at(return_address_offset); }\n+  address destination() const;\n+\n+  void set_destination(address dest) {\n+    assert(is_jal(), \"Should be jal instruction!\");\n+    intptr_t offset = (intptr_t)(dest - instruction_address());\n+    assert((offset & 0x1) == 0, \"bad alignment\");\n+    assert(is_imm_in_range(offset, 20, 1), \"encoding constraint\");\n+    unsigned int insn = 0b1101111; \/\/ jal\n+    address pInsn = (address)(&insn);\n+    Assembler::patch(pInsn, 31, 31, (offset >> 20) & 0x1);\n+    Assembler::patch(pInsn, 30, 21, (offset >> 1) & 0x3ff);\n+    Assembler::patch(pInsn, 20, 20, (offset >> 11) & 0x1);\n+    Assembler::patch(pInsn, 19, 12, (offset >> 12) & 0xff);\n+    Assembler::patch(pInsn, 11, 7, ra->encoding()); \/\/ Rd must be x1, need ra\n+    set_int_at(displacement_offset, insn);\n+  }\n+\n+  void verify_alignment() {} \/\/ do nothing on riscv\n+  void verify();\n+  void print();\n+\n+  \/\/ Creation\n+  inline friend NativeCall* nativeCall_at(address addr);\n+  inline friend NativeCall* nativeCall_before(address return_address);\n+\n+  static bool is_call_before(address return_address) {\n+    return is_call_at(return_address - NativeCall::return_address_offset);\n+  }\n+\n+  \/\/ MT-safe patching of a call instruction.\n+  static void insert(address code_pos, address entry);\n+\n+  static void replace_mt_safe(address instr_addr, address code_buffer);\n+\n+  \/\/ Similar to replace_mt_safe, but just changes the destination.  The\n+  \/\/ important thing is that free-running threads are able to execute\n+  \/\/ this call instruction at all times.  If the call is an immediate BL\n+  \/\/ instruction we can simply rely on atomicity of 32-bit writes to\n+  \/\/ make sure other threads will see no intermediate states.\n+\n+  \/\/ We cannot rely on locks here, since the free-running threads must run at\n+  \/\/ full speed.\n+  \/\/\n+  \/\/ Used in the runtime linkage of calls; see class CompiledIC.\n+  \/\/ (Cf. 4506997 and 4479829, where threads witnessed garbage displacements.)\n+\n+  \/\/ The parameter assert_lock disables the assertion during code generation.\n+  void set_destination_mt_safe(address dest, bool assert_lock = true);\n+\n+  address get_trampoline();\n+};\n+\n+inline NativeCall* nativeCall_at(address addr) {\n+  assert_cond(addr != NULL);\n+  NativeCall* call = (NativeCall*)(addr - NativeCall::instruction_offset);\n+#ifdef ASSERT\n+  call->verify();\n+#endif\n+  return call;\n+}\n+\n+inline NativeCall* nativeCall_before(address return_address) {\n+  assert_cond(return_address != NULL);\n+  NativeCall* call = (NativeCall*)(return_address - NativeCall::return_address_offset);\n+#ifdef ASSERT\n+  call->verify();\n+#endif\n+  return call;\n+}\n+\n+\/\/ An interface for accessing\/manipulating native mov reg, imm instructions.\n+\/\/ (used to manipulate inlined 64-bit data calls, etc.)\n+class NativeMovConstReg: public NativeInstruction {\n+ public:\n+  enum RISCV_specific_constants {\n+    movptr_instruction_size             =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, addi.  See movptr().\n+    movptr_with_offset_instruction_size =    5 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli. See movptr_with_offset().\n+    load_pc_relative_instruction_size   =    2 * NativeInstruction::instruction_size, \/\/ auipc, ld\n+    instruction_offset                  =    0,\n+    displacement_offset                 =    0\n+  };\n+\n+  address instruction_address() const       { return addr_at(instruction_offset); }\n+  address next_instruction_address() const  {\n+    \/\/ if the instruction at 5 * instruction_size is addi,\n+    \/\/ it means a lui + addi + slli + addi + slli + addi instruction sequence,\n+    \/\/ and the next instruction address should be addr_at(6 * instruction_size).\n+    \/\/ However, when the instruction at 5 * instruction_size isn't addi,\n+    \/\/ the next instruction address should be addr_at(5 * instruction_size)\n+    if (nativeInstruction_at(instruction_address())->is_movptr()) {\n+      if (is_addi_at(addr_at(movptr_with_offset_instruction_size))) {\n+        \/\/ Assume: lui, addi, slli, addi, slli, addi\n+        return addr_at(movptr_instruction_size);\n+      } else {\n+        \/\/ Assume: lui, addi, slli, addi, slli\n+        return addr_at(movptr_with_offset_instruction_size);\n+      }\n+    } else if (is_load_pc_relative_at(instruction_address())) {\n+      \/\/ Assume: auipc, ld\n+      return addr_at(load_pc_relative_instruction_size);\n+    }\n+    guarantee(false, \"Unknown instruction in NativeMovConstReg\");\n+    return NULL;\n+  }\n+\n+  intptr_t data() const;\n+  void  set_data(intptr_t x);\n+\n+  void flush() {\n+    if (!maybe_cpool_ref(instruction_address())) {\n+      ICache::invalidate_range(instruction_address(), movptr_instruction_size);\n+    }\n+  }\n+\n+  void  verify();\n+  void  print();\n+\n+  \/\/ Creation\n+  inline friend NativeMovConstReg* nativeMovConstReg_at(address addr);\n+  inline friend NativeMovConstReg* nativeMovConstReg_before(address addr);\n+};\n+\n+inline NativeMovConstReg* nativeMovConstReg_at(address addr) {\n+  assert_cond(addr != NULL);\n+  NativeMovConstReg* test = (NativeMovConstReg*)(addr - NativeMovConstReg::instruction_offset);\n+#ifdef ASSERT\n+  test->verify();\n+#endif\n+  return test;\n+}\n+\n+inline NativeMovConstReg* nativeMovConstReg_before(address addr) {\n+  assert_cond(addr != NULL);\n+  NativeMovConstReg* test = (NativeMovConstReg*)(addr - NativeMovConstReg::instruction_size - NativeMovConstReg::instruction_offset);\n+#ifdef ASSERT\n+  test->verify();\n+#endif\n+  return test;\n+}\n+\n+\/\/ RISCV should not use C1 runtime patching, so just leave NativeMovRegMem Unimplemented.\n+class NativeMovRegMem: public NativeInstruction {\n+ public:\n+  int instruction_start() const {\n+    Unimplemented();\n+    return 0;\n+  }\n+\n+  address instruction_address() const {\n+    Unimplemented();\n+    return NULL;\n+  }\n+\n+  int num_bytes_to_end_of_patch() const {\n+    Unimplemented();\n+    return 0;\n+  }\n+\n+  int offset() const;\n+\n+  void set_offset(int x);\n+\n+  void add_offset_in_bytes(int add_offset) { Unimplemented(); }\n+\n+  void verify();\n+  void print();\n+\n+ private:\n+  inline friend NativeMovRegMem* nativeMovRegMem_at (address addr);\n+};\n+\n+inline NativeMovRegMem* nativeMovRegMem_at (address addr) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n+class NativeJump: public NativeInstruction {\n+ public:\n+  enum RISCV_specific_constants {\n+    instruction_size            =    NativeInstruction::instruction_size,\n+    instruction_offset          =    0,\n+    data_offset                 =    0,\n+    next_instruction_offset     =    NativeInstruction::instruction_size\n+  };\n+\n+  address instruction_address() const       { return addr_at(instruction_offset); }\n+  address next_instruction_address() const  { return addr_at(instruction_size); }\n+  address jump_destination() const;\n+  void set_jump_destination(address dest);\n+\n+  \/\/ Creation\n+  inline friend NativeJump* nativeJump_at(address address);\n+\n+  void verify();\n+\n+  \/\/ Insertion of native jump instruction\n+  static void insert(address code_pos, address entry);\n+  \/\/ MT-safe insertion of native jump at verified method entry\n+  static void check_verified_entry_alignment(address entry, address verified_entry);\n+  static void patch_verified_entry(address entry, address verified_entry, address dest);\n+};\n+\n+inline NativeJump* nativeJump_at(address addr) {\n+  NativeJump* jump = (NativeJump*)(addr - NativeJump::instruction_offset);\n+#ifdef ASSERT\n+  jump->verify();\n+#endif\n+  return jump;\n+}\n+\n+class NativeGeneralJump: public NativeJump {\n+public:\n+  enum RISCV_specific_constants {\n+    instruction_size            =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, jalr\n+    instruction_offset          =    0,\n+    data_offset                 =    0,\n+    next_instruction_offset     =    6 * NativeInstruction::instruction_size  \/\/ lui, addi, slli, addi, slli, jalr\n+  };\n+\n+  address jump_destination() const;\n+\n+  static void insert_unconditional(address code_pos, address entry);\n+  static void replace_mt_safe(address instr_addr, address code_buffer);\n+};\n+\n+inline NativeGeneralJump* nativeGeneralJump_at(address addr) {\n+  assert_cond(addr != NULL);\n+  NativeGeneralJump* jump = (NativeGeneralJump*)(addr);\n+  debug_only(jump->verify();)\n+  return jump;\n+}\n+\n+class NativeIllegalInstruction: public NativeInstruction {\n+ public:\n+  \/\/ Insert illegal opcode as specific address\n+  static void insert(address code_pos);\n+};\n+\n+inline bool NativeInstruction::is_nop()         {\n+  uint32_t insn = *(uint32_t*)addr_at(0);\n+  return insn == 0x13;\n+}\n+\n+inline bool NativeInstruction::is_jump_or_nop() {\n+  return is_nop() || is_jump();\n+}\n+\n+\/\/ Call trampoline stubs.\n+class NativeCallTrampolineStub : public NativeInstruction {\n+ public:\n+\n+  enum RISCV_specific_constants {\n+    \/\/ Refer to function emit_trampoline_stub.\n+    instruction_size = 3 * NativeInstruction::instruction_size + wordSize, \/\/ auipc + ld + jr + target address\n+    data_offset      = 3 * NativeInstruction::instruction_size,            \/\/ auipc + ld + jr\n+  };\n+\n+  address destination(nmethod *nm = NULL) const;\n+  void set_destination(address new_destination);\n+  ptrdiff_t destination_offset() const;\n+};\n+\n+inline bool is_NativeCallTrampolineStub_at(address addr) {\n+  \/\/ Ensure that the stub is exactly\n+  \/\/      ld   t0, L--->auipc + ld\n+  \/\/      jr   t0\n+  \/\/ L:\n+\n+  \/\/ judge inst + register + imm\n+  \/\/ 1). check the instructions: auipc + ld + jalr\n+  \/\/ 2). check if auipc[11:7] == t0 and ld[11:7] == t0 and ld[19:15] == t0 && jr[19:15] == t0\n+  \/\/ 3). check if the offset in ld[31:20] equals the data_offset\n+  assert_cond(addr != NULL);\n+  const int instr_size = NativeInstruction::instruction_size;\n+  if (NativeInstruction::is_auipc_at(addr) &&\n+      NativeInstruction::is_ld_at(addr + instr_size) &&\n+      NativeInstruction::is_jalr_at(addr + 2 * instr_size) &&\n+      (NativeInstruction::extract_rd(addr)                    == x5) &&\n+      (NativeInstruction::extract_rd(addr + instr_size)       == x5) &&\n+      (NativeInstruction::extract_rs1(addr + instr_size)      == x5) &&\n+      (NativeInstruction::extract_rs1(addr + 2 * instr_size)  == x5) &&\n+      (Assembler::extract(((unsigned*)addr)[1], 31, 20) == NativeCallTrampolineStub::data_offset)) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+inline NativeCallTrampolineStub* nativeCallTrampolineStub_at(address addr) {\n+  assert_cond(addr != NULL);\n+  assert(is_NativeCallTrampolineStub_at(addr), \"no call trampoline found\");\n+  return (NativeCallTrampolineStub*)addr;\n+}\n+\n+class NativeMembar : public NativeInstruction {\n+public:\n+  uint32_t get_kind();\n+  void set_kind(uint32_t order_kind);\n+};\n+\n+inline NativeMembar *NativeMembar_at(address addr) {\n+  assert_cond(addr != NULL);\n+  assert(nativeInstruction_at(addr)->is_membar(), \"no membar found\");\n+  return (NativeMembar*)addr;\n+}\n+\n+class NativeFenceI : public NativeInstruction {\n+public:\n+  static inline int instruction_size() {\n+    \/\/ 2 for fence.i + fence\n+    return (UseConservativeFence ? 2 : 1) * NativeInstruction::instruction_size;\n+  }\n+};\n+\n+#endif \/\/ CPU_RISCV_NATIVEINST_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.hpp","additions":572,"deletions":0,"binary":false,"changes":572,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_REGISTERMAP_RISCV_HPP\n+#define CPU_RISCV_REGISTERMAP_RISCV_HPP\n+\n+\/\/ machine-dependent implemention for register maps\n+  friend class frame;\n+\n+ private:\n+  \/\/ This is the hook for finding a register in an \"well-known\" location,\n+  \/\/ such as a register block of a predetermined format.\n+  address pd_location(VMReg reg) const { return NULL; }\n+\n+  \/\/ no PD state to clear or copy:\n+  void pd_clear() {}\n+  void pd_initialize() {}\n+  void pd_initialize_from(const RegisterMap* map) {}\n+\n+#endif \/\/ CPU_RISCV_REGISTERMAP_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/registerMap_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,192 @@\n+\/*\n+ * Copyright (c) 2002, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/register.hpp\"\n+#include \"interp_masm_riscv.hpp\"\n+#include \"register_riscv.hpp\"\n+\n+REGISTER_DEFINITION(Register, noreg);\n+\n+REGISTER_DEFINITION(Register, x0);\n+REGISTER_DEFINITION(Register, x1);\n+REGISTER_DEFINITION(Register, x2);\n+REGISTER_DEFINITION(Register, x3);\n+REGISTER_DEFINITION(Register, x4);\n+REGISTER_DEFINITION(Register, x5);\n+REGISTER_DEFINITION(Register, x6);\n+REGISTER_DEFINITION(Register, x7);\n+REGISTER_DEFINITION(Register, x8);\n+REGISTER_DEFINITION(Register, x9);\n+REGISTER_DEFINITION(Register, x10);\n+REGISTER_DEFINITION(Register, x11);\n+REGISTER_DEFINITION(Register, x12);\n+REGISTER_DEFINITION(Register, x13);\n+REGISTER_DEFINITION(Register, x14);\n+REGISTER_DEFINITION(Register, x15);\n+REGISTER_DEFINITION(Register, x16);\n+REGISTER_DEFINITION(Register, x17);\n+REGISTER_DEFINITION(Register, x18);\n+REGISTER_DEFINITION(Register, x19);\n+REGISTER_DEFINITION(Register, x20);\n+REGISTER_DEFINITION(Register, x21);\n+REGISTER_DEFINITION(Register, x22);\n+REGISTER_DEFINITION(Register, x23);\n+REGISTER_DEFINITION(Register, x24);\n+REGISTER_DEFINITION(Register, x25);\n+REGISTER_DEFINITION(Register, x26);\n+REGISTER_DEFINITION(Register, x27);\n+REGISTER_DEFINITION(Register, x28);\n+REGISTER_DEFINITION(Register, x29);\n+REGISTER_DEFINITION(Register, x30);\n+REGISTER_DEFINITION(Register, x31);\n+\n+REGISTER_DEFINITION(FloatRegister, fnoreg);\n+\n+REGISTER_DEFINITION(FloatRegister, f0);\n+REGISTER_DEFINITION(FloatRegister, f1);\n+REGISTER_DEFINITION(FloatRegister, f2);\n+REGISTER_DEFINITION(FloatRegister, f3);\n+REGISTER_DEFINITION(FloatRegister, f4);\n+REGISTER_DEFINITION(FloatRegister, f5);\n+REGISTER_DEFINITION(FloatRegister, f6);\n+REGISTER_DEFINITION(FloatRegister, f7);\n+REGISTER_DEFINITION(FloatRegister, f8);\n+REGISTER_DEFINITION(FloatRegister, f9);\n+REGISTER_DEFINITION(FloatRegister, f10);\n+REGISTER_DEFINITION(FloatRegister, f11);\n+REGISTER_DEFINITION(FloatRegister, f12);\n+REGISTER_DEFINITION(FloatRegister, f13);\n+REGISTER_DEFINITION(FloatRegister, f14);\n+REGISTER_DEFINITION(FloatRegister, f15);\n+REGISTER_DEFINITION(FloatRegister, f16);\n+REGISTER_DEFINITION(FloatRegister, f17);\n+REGISTER_DEFINITION(FloatRegister, f18);\n+REGISTER_DEFINITION(FloatRegister, f19);\n+REGISTER_DEFINITION(FloatRegister, f20);\n+REGISTER_DEFINITION(FloatRegister, f21);\n+REGISTER_DEFINITION(FloatRegister, f22);\n+REGISTER_DEFINITION(FloatRegister, f23);\n+REGISTER_DEFINITION(FloatRegister, f24);\n+REGISTER_DEFINITION(FloatRegister, f25);\n+REGISTER_DEFINITION(FloatRegister, f26);\n+REGISTER_DEFINITION(FloatRegister, f27);\n+REGISTER_DEFINITION(FloatRegister, f28);\n+REGISTER_DEFINITION(FloatRegister, f29);\n+REGISTER_DEFINITION(FloatRegister, f30);\n+REGISTER_DEFINITION(FloatRegister, f31);\n+\n+REGISTER_DEFINITION(VectorRegister, vnoreg);\n+\n+REGISTER_DEFINITION(VectorRegister, v0);\n+REGISTER_DEFINITION(VectorRegister, v1);\n+REGISTER_DEFINITION(VectorRegister, v2);\n+REGISTER_DEFINITION(VectorRegister, v3);\n+REGISTER_DEFINITION(VectorRegister, v4);\n+REGISTER_DEFINITION(VectorRegister, v5);\n+REGISTER_DEFINITION(VectorRegister, v6);\n+REGISTER_DEFINITION(VectorRegister, v7);\n+REGISTER_DEFINITION(VectorRegister, v8);\n+REGISTER_DEFINITION(VectorRegister, v9);\n+REGISTER_DEFINITION(VectorRegister, v10);\n+REGISTER_DEFINITION(VectorRegister, v11);\n+REGISTER_DEFINITION(VectorRegister, v12);\n+REGISTER_DEFINITION(VectorRegister, v13);\n+REGISTER_DEFINITION(VectorRegister, v14);\n+REGISTER_DEFINITION(VectorRegister, v15);\n+REGISTER_DEFINITION(VectorRegister, v16);\n+REGISTER_DEFINITION(VectorRegister, v17);\n+REGISTER_DEFINITION(VectorRegister, v18);\n+REGISTER_DEFINITION(VectorRegister, v19);\n+REGISTER_DEFINITION(VectorRegister, v20);\n+REGISTER_DEFINITION(VectorRegister, v21);\n+REGISTER_DEFINITION(VectorRegister, v22);\n+REGISTER_DEFINITION(VectorRegister, v23);\n+REGISTER_DEFINITION(VectorRegister, v24);\n+REGISTER_DEFINITION(VectorRegister, v25);\n+REGISTER_DEFINITION(VectorRegister, v26);\n+REGISTER_DEFINITION(VectorRegister, v27);\n+REGISTER_DEFINITION(VectorRegister, v28);\n+REGISTER_DEFINITION(VectorRegister, v29);\n+REGISTER_DEFINITION(VectorRegister, v30);\n+REGISTER_DEFINITION(VectorRegister, v31);\n+\n+REGISTER_DEFINITION(Register, c_rarg0);\n+REGISTER_DEFINITION(Register, c_rarg1);\n+REGISTER_DEFINITION(Register, c_rarg2);\n+REGISTER_DEFINITION(Register, c_rarg3);\n+REGISTER_DEFINITION(Register, c_rarg4);\n+REGISTER_DEFINITION(Register, c_rarg5);\n+REGISTER_DEFINITION(Register, c_rarg6);\n+REGISTER_DEFINITION(Register, c_rarg7);\n+\n+REGISTER_DEFINITION(FloatRegister, c_farg0);\n+REGISTER_DEFINITION(FloatRegister, c_farg1);\n+REGISTER_DEFINITION(FloatRegister, c_farg2);\n+REGISTER_DEFINITION(FloatRegister, c_farg3);\n+REGISTER_DEFINITION(FloatRegister, c_farg4);\n+REGISTER_DEFINITION(FloatRegister, c_farg5);\n+REGISTER_DEFINITION(FloatRegister, c_farg6);\n+REGISTER_DEFINITION(FloatRegister, c_farg7);\n+\n+REGISTER_DEFINITION(Register, j_rarg0);\n+REGISTER_DEFINITION(Register, j_rarg1);\n+REGISTER_DEFINITION(Register, j_rarg2);\n+REGISTER_DEFINITION(Register, j_rarg3);\n+REGISTER_DEFINITION(Register, j_rarg4);\n+REGISTER_DEFINITION(Register, j_rarg5);\n+REGISTER_DEFINITION(Register, j_rarg6);\n+REGISTER_DEFINITION(Register, j_rarg7);\n+\n+REGISTER_DEFINITION(FloatRegister, j_farg0);\n+REGISTER_DEFINITION(FloatRegister, j_farg1);\n+REGISTER_DEFINITION(FloatRegister, j_farg2);\n+REGISTER_DEFINITION(FloatRegister, j_farg3);\n+REGISTER_DEFINITION(FloatRegister, j_farg4);\n+REGISTER_DEFINITION(FloatRegister, j_farg5);\n+REGISTER_DEFINITION(FloatRegister, j_farg6);\n+REGISTER_DEFINITION(FloatRegister, j_farg7);\n+\n+REGISTER_DEFINITION(Register, zr);\n+REGISTER_DEFINITION(Register, gp);\n+REGISTER_DEFINITION(Register, tp);\n+REGISTER_DEFINITION(Register, xmethod);\n+REGISTER_DEFINITION(Register, ra);\n+REGISTER_DEFINITION(Register, sp);\n+REGISTER_DEFINITION(Register, fp);\n+REGISTER_DEFINITION(Register, xheapbase);\n+REGISTER_DEFINITION(Register, xcpool);\n+REGISTER_DEFINITION(Register, xmonitors);\n+REGISTER_DEFINITION(Register, xlocals);\n+REGISTER_DEFINITION(Register, xthread);\n+REGISTER_DEFINITION(Register, xbcp);\n+REGISTER_DEFINITION(Register, xdispatch);\n+REGISTER_DEFINITION(Register, esp);\n+\n+REGISTER_DEFINITION(Register, t0);\n+REGISTER_DEFINITION(Register, t1);\n+REGISTER_DEFINITION(Register, t2);\n","filename":"src\/hotspot\/cpu\/riscv\/register_definitions_riscv.cpp","additions":192,"deletions":0,"binary":false,"changes":192,"status":"added"},{"patch":"@@ -0,0 +1,64 @@\n+\/*\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"register_riscv.hpp\"\n+\n+const int ConcreteRegisterImpl::max_gpr = RegisterImpl::number_of_registers *\n+                                          RegisterImpl::max_slots_per_register;\n+\n+const int ConcreteRegisterImpl::max_fpr =\n+    ConcreteRegisterImpl::max_gpr +\n+    FloatRegisterImpl::number_of_registers * FloatRegisterImpl::max_slots_per_register;\n+\n+const char* RegisterImpl::name() const {\n+  static const char *const names[number_of_registers] = {\n+    \"zr\", \"ra\", \"sp\", \"gp\", \"tp\", \"t0\", \"t1\", \"t2\", \"fp\", \"x9\",\n+    \"c_rarg0\", \"c_rarg1\", \"c_rarg2\", \"c_rarg3\", \"c_rarg4\", \"c_rarg5\", \"c_rarg6\", \"c_rarg7\",\n+    \"x18\", \"x19\", \"esp\", \"xdispatch\", \"xbcp\", \"xthread\", \"xlocals\",\n+    \"xmonitors\", \"xcpool\", \"xheapbase\", \"x28\", \"x29\", \"x30\", \"xmethod\"\n+  };\n+  return is_valid() ? names[encoding()] : \"noreg\";\n+}\n+\n+const char* FloatRegisterImpl::name() const {\n+  static const char *const names[number_of_registers] = {\n+    \"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\",\n+    \"f8\", \"f9\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\",\n+    \"f16\", \"f17\", \"f18\", \"f19\", \"f20\", \"f21\", \"f22\", \"f23\",\n+    \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\"\n+  };\n+  return is_valid() ? names[encoding()] : \"noreg\";\n+}\n+\n+const char* VectorRegisterImpl::name() const {\n+  static const char *const names[number_of_registers] = {\n+    \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\",\n+    \"v8\", \"v9\", \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\",\n+    \"v16\", \"v17\", \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\",\n+    \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\", \"v31\"\n+  };\n+  return is_valid() ? names[encoding()] : \"noreg\";\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/register_riscv.cpp","additions":64,"deletions":0,"binary":false,"changes":64,"status":"added"},{"patch":"@@ -0,0 +1,381 @@\n+\/*\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_REGISTER_RISCV_HPP\n+#define CPU_RISCV_REGISTER_RISCV_HPP\n+\n+#include \"asm\/register.hpp\"\n+\n+#define CSR_FFLAGS   0x001        \/\/ Floating-Point Accrued Exceptions.\n+#define CSR_FRM      0x002        \/\/ Floating-Point Dynamic Rounding Mode.\n+#define CSR_FCSR     0x003        \/\/ Floating-Point Control and Status Register (frm + fflags).\n+#define CSR_VSTART   0x008        \/\/ Vector start position\n+#define CSR_VXSAT    0x009        \/\/ Fixed-Point Saturate Flag\n+#define CSR_VXRM     0x00A        \/\/ Fixed-Point Rounding Mode\n+#define CSR_VCSR     0x00F        \/\/ Vector control and status register\n+#define CSR_VL       0xC20        \/\/ Vector length\n+#define CSR_VTYPE    0xC21        \/\/ Vector data type register\n+#define CSR_VLENB    0xC22        \/\/ VLEN\/8 (vector register length in bytes)\n+#define CSR_CYCLE    0xc00        \/\/ Cycle counter for RDCYCLE instruction.\n+#define CSR_TIME     0xc01        \/\/ Timer for RDTIME instruction.\n+#define CSR_INSTERT  0xc02        \/\/ Instructions-retired counter for RDINSTRET instruction.\n+\n+class VMRegImpl;\n+typedef VMRegImpl* VMReg;\n+\n+\/\/ Use Register as shortcut\n+class RegisterImpl;\n+typedef RegisterImpl* Register;\n+\n+inline Register as_Register(int encoding) {\n+  return (Register)(intptr_t) encoding;\n+}\n+\n+class RegisterImpl: public AbstractRegisterImpl {\n+ public:\n+  enum {\n+    number_of_registers      = 32,\n+    max_slots_per_register   = 2,\n+\n+    \/\/ integer registers x8 - x15 and floating-point registers f8 - f15 are allocatable\n+    \/\/ for compressed instructions. See Table 17.2 in spec.\n+    compressed_register_base = 8,\n+    compressed_register_top  = 15,\n+  };\n+\n+  \/\/ derived registers, offsets, and addresses\n+  const Register successor() const { return as_Register(encoding() + 1); }\n+\n+  \/\/ construction\n+  inline friend Register as_Register(int encoding);\n+\n+  VMReg as_VMReg() const;\n+\n+  \/\/ accessors\n+  int encoding() const            { assert(is_valid(), \"invalid register\"); return encoding_nocheck(); }\n+  int encoding_nocheck() const    { return (intptr_t)this; }\n+  bool is_valid() const           { return (unsigned)encoding_nocheck() < number_of_registers; }\n+  const char* name() const;\n+\n+  \/\/ for rvc\n+  int compressed_encoding() const {\n+    assert(is_compressed_valid(), \"invalid compressed register\");\n+    return encoding() - compressed_register_base;\n+  }\n+\n+  int compressed_encoding_nocheck() const {\n+    return encoding_nocheck() - compressed_register_base;\n+  }\n+\n+  bool is_compressed_valid() const {\n+    return encoding_nocheck() >= compressed_register_base &&\n+           encoding_nocheck() <= compressed_register_top;\n+  }\n+\n+  \/\/ Return the bit which represents this register.  This is intended\n+  \/\/ to be ORed into a bitmask: for usage see class RegSet below.\n+  uint64_t bit(bool should_set = true) const { return should_set ? 1 << encoding() : 0; }\n+};\n+\n+\/\/ The integer registers of the RISCV architecture\n+\n+CONSTANT_REGISTER_DECLARATION(Register, noreg, (-1));\n+\n+CONSTANT_REGISTER_DECLARATION(Register, x0,    (0));\n+CONSTANT_REGISTER_DECLARATION(Register, x1,    (1));\n+CONSTANT_REGISTER_DECLARATION(Register, x2,    (2));\n+CONSTANT_REGISTER_DECLARATION(Register, x3,    (3));\n+CONSTANT_REGISTER_DECLARATION(Register, x4,    (4));\n+CONSTANT_REGISTER_DECLARATION(Register, x5,    (5));\n+CONSTANT_REGISTER_DECLARATION(Register, x6,    (6));\n+CONSTANT_REGISTER_DECLARATION(Register, x7,    (7));\n+CONSTANT_REGISTER_DECLARATION(Register, x8,    (8));\n+CONSTANT_REGISTER_DECLARATION(Register, x9,    (9));\n+CONSTANT_REGISTER_DECLARATION(Register, x10,  (10));\n+CONSTANT_REGISTER_DECLARATION(Register, x11,  (11));\n+CONSTANT_REGISTER_DECLARATION(Register, x12,  (12));\n+CONSTANT_REGISTER_DECLARATION(Register, x13,  (13));\n+CONSTANT_REGISTER_DECLARATION(Register, x14,  (14));\n+CONSTANT_REGISTER_DECLARATION(Register, x15,  (15));\n+CONSTANT_REGISTER_DECLARATION(Register, x16,  (16));\n+CONSTANT_REGISTER_DECLARATION(Register, x17,  (17));\n+CONSTANT_REGISTER_DECLARATION(Register, x18,  (18));\n+CONSTANT_REGISTER_DECLARATION(Register, x19,  (19));\n+CONSTANT_REGISTER_DECLARATION(Register, x20,  (20));\n+CONSTANT_REGISTER_DECLARATION(Register, x21,  (21));\n+CONSTANT_REGISTER_DECLARATION(Register, x22,  (22));\n+CONSTANT_REGISTER_DECLARATION(Register, x23,  (23));\n+CONSTANT_REGISTER_DECLARATION(Register, x24,  (24));\n+CONSTANT_REGISTER_DECLARATION(Register, x25,  (25));\n+CONSTANT_REGISTER_DECLARATION(Register, x26,  (26));\n+CONSTANT_REGISTER_DECLARATION(Register, x27,  (27));\n+CONSTANT_REGISTER_DECLARATION(Register, x28,  (28));\n+CONSTANT_REGISTER_DECLARATION(Register, x29,  (29));\n+CONSTANT_REGISTER_DECLARATION(Register, x30,  (30));\n+CONSTANT_REGISTER_DECLARATION(Register, x31,  (31));\n+\n+\/\/ Use FloatRegister as shortcut\n+class FloatRegisterImpl;\n+typedef FloatRegisterImpl* FloatRegister;\n+\n+inline FloatRegister as_FloatRegister(int encoding) {\n+  return (FloatRegister)(intptr_t) encoding;\n+}\n+\n+\/\/ The implementation of floating point registers for the architecture\n+class FloatRegisterImpl: public AbstractRegisterImpl {\n+ public:\n+  enum {\n+    number_of_registers     = 32,\n+    max_slots_per_register  = 2,\n+\n+    \/\/ float registers in the range of [f8~f15] correspond to RVC. Please see Table 16.2 in spec.\n+    compressed_register_base = 8,\n+    compressed_register_top  = 15,\n+  };\n+\n+  \/\/ construction\n+  inline friend FloatRegister as_FloatRegister(int encoding);\n+\n+  VMReg as_VMReg() const;\n+\n+  \/\/ derived registers, offsets, and addresses\n+  FloatRegister successor() const { return as_FloatRegister(encoding() + 1); }\n+\n+  \/\/ accessors\n+  int encoding() const            { assert(is_valid(), \"invalid register\"); return encoding_nocheck(); }\n+  int encoding_nocheck() const    { return (intptr_t)this; }\n+  int is_valid() const            { return (unsigned)encoding_nocheck() < number_of_registers; }\n+  const char* name() const;\n+\n+  \/\/ for rvc\n+  int compressed_encoding() const {\n+    assert(is_compressed_valid(), \"invalid compressed register\");\n+    return encoding() - compressed_register_base;\n+  }\n+\n+  int compressed_encoding_nocheck() const {\n+    return encoding_nocheck() - compressed_register_base;\n+  }\n+\n+  bool is_compressed_valid() const {\n+    return encoding_nocheck() >= compressed_register_base &&\n+           encoding_nocheck() <= compressed_register_top;\n+  }\n+};\n+\n+\/\/ The float registers of the RISCV architecture\n+\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, fnoreg , (-1));\n+\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f0     , ( 0));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f1     , ( 1));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f2     , ( 2));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f3     , ( 3));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f4     , ( 4));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f5     , ( 5));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f6     , ( 6));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f7     , ( 7));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f8     , ( 8));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f9     , ( 9));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f10    , (10));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f11    , (11));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f12    , (12));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f13    , (13));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f14    , (14));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f15    , (15));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f16    , (16));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f17    , (17));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f18    , (18));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f19    , (19));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f20    , (20));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f21    , (21));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f22    , (22));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f23    , (23));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f24    , (24));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f25    , (25));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f26    , (26));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f27    , (27));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f28    , (28));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f29    , (29));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f30    , (30));\n+CONSTANT_REGISTER_DECLARATION(FloatRegister, f31    , (31));\n+\n+\/\/ Use VectorRegister as shortcut\n+class VectorRegisterImpl;\n+typedef VectorRegisterImpl* VectorRegister;\n+\n+inline VectorRegister as_VectorRegister(int encoding) {\n+  return (VectorRegister)(intptr_t) encoding;\n+}\n+\n+\/\/ The implementation of vector registers for RVV\n+class VectorRegisterImpl: public AbstractRegisterImpl {\n+ public:\n+  enum {\n+    number_of_registers    = 32,\n+    max_slots_per_register = 4\n+  };\n+\n+  \/\/ construction\n+  inline friend VectorRegister as_VectorRegister(int encoding);\n+\n+  VMReg as_VMReg() const;\n+\n+  \/\/ derived registers, offsets, and addresses\n+  VectorRegister successor() const { return as_VectorRegister(encoding() + 1); }\n+\n+  \/\/ accessors\n+  int encoding() const            { assert(is_valid(), \"invalid register\"); return encoding_nocheck(); }\n+  int encoding_nocheck() const    { return (intptr_t)this; }\n+  bool is_valid() const           { return (unsigned)encoding_nocheck() < number_of_registers; }\n+  const char* name() const;\n+\n+};\n+\n+\/\/ The vector registers of RVV\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, vnoreg , (-1));\n+\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v0     , ( 0));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v1     , ( 1));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v2     , ( 2));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v3     , ( 3));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v4     , ( 4));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v5     , ( 5));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v6     , ( 6));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v7     , ( 7));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v8     , ( 8));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v9     , ( 9));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v10    , (10));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v11    , (11));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v12    , (12));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v13    , (13));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v14    , (14));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v15    , (15));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v16    , (16));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v17    , (17));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v18    , (18));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v19    , (19));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v20    , (20));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v21    , (21));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v22    , (22));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v23    , (23));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v24    , (24));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v25    , (25));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v26    , (26));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v27    , (27));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v28    , (28));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v29    , (29));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v30    , (30));\n+CONSTANT_REGISTER_DECLARATION(VectorRegister, v31    , (31));\n+\n+\n+\/\/ Need to know the total number of registers of all sorts for SharedInfo.\n+\/\/ Define a class that exports it.\n+class ConcreteRegisterImpl : public AbstractRegisterImpl {\n+ public:\n+  enum {\n+  \/\/ A big enough number for C2: all the registers plus flags\n+  \/\/ This number must be large enough to cover REG_COUNT (defined by c2) registers.\n+  \/\/ There is no requirement that any ordering here matches any ordering c2 gives\n+  \/\/ it's optoregs.\n+\n+    number_of_registers = (RegisterImpl::max_slots_per_register * RegisterImpl::number_of_registers +\n+                           FloatRegisterImpl::max_slots_per_register * FloatRegisterImpl::number_of_registers)\n+  };\n+\n+  \/\/ added to make it compile\n+  static const int max_gpr;\n+  static const int max_fpr;\n+};\n+\n+\/\/ A set of registers\n+class RegSet {\n+  uint32_t _bitset;\n+\n+  RegSet(uint32_t bitset) : _bitset(bitset) { }\n+\n+public:\n+\n+  RegSet() : _bitset(0) { }\n+\n+  RegSet(Register r1) : _bitset(r1->bit()) { }\n+\n+  RegSet operator+(const RegSet aSet) const {\n+    RegSet result(_bitset | aSet._bitset);\n+    return result;\n+  }\n+\n+  RegSet operator-(const RegSet aSet) const {\n+    RegSet result(_bitset & ~aSet._bitset);\n+    return result;\n+  }\n+\n+  RegSet &operator+=(const RegSet aSet) {\n+    *this = *this + aSet;\n+    return *this;\n+  }\n+\n+  RegSet &operator-=(const RegSet aSet) {\n+    *this = *this - aSet;\n+    return *this;\n+  }\n+\n+  static RegSet of(Register r1) {\n+    return RegSet(r1);\n+  }\n+\n+  static RegSet of(Register r1, Register r2) {\n+    return of(r1) + r2;\n+  }\n+\n+  static RegSet of(Register r1, Register r2, Register r3) {\n+    return of(r1, r2) + r3;\n+  }\n+\n+  static RegSet of(Register r1, Register r2, Register r3, Register r4) {\n+    return of(r1, r2, r3) + r4;\n+  }\n+\n+  static RegSet range(Register start, Register end) {\n+    uint32_t bits = ~0;\n+    bits <<= start->encoding();\n+    bits <<= 31 - end->encoding();\n+    bits >>= 31 - end->encoding();\n+\n+    return RegSet(bits);\n+  }\n+\n+  uint32_t bits() const { return _bitset; }\n+\n+private:\n+\n+  Register first() {\n+    uint32_t first = _bitset & -_bitset;\n+    return first ? as_Register(exact_log2(first)) : noreg;\n+  }\n+};\n+\n+#endif \/\/ CPU_RISCV_REGISTER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/register_riscv.hpp","additions":381,"deletions":0,"binary":false,"changes":381,"status":"added"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/relocInfo.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+\n+void Relocation::pd_set_data_value(address x, intptr_t o, bool verify_only) {\n+  if (verify_only) {\n+    return;\n+  }\n+\n+  int bytes;\n+\n+  switch (type()) {\n+    case relocInfo::oop_type: {\n+      oop_Relocation *reloc = (oop_Relocation *)this;\n+      if (NativeInstruction::is_load_pc_relative_at(addr())) {\n+        address constptr = (address)code()->oop_addr_at(reloc->oop_index());\n+        bytes = MacroAssembler::pd_patch_instruction_size(addr(), constptr);\n+        assert(*(address*)constptr == x, \"error in oop relocation\");\n+      } else {\n+        bytes = MacroAssembler::patch_oop(addr(), x);\n+      }\n+      break;\n+    }\n+    default:\n+      bytes = MacroAssembler::pd_patch_instruction_size(addr(), x);\n+      break;\n+  }\n+  ICache::invalidate_range(addr(), bytes);\n+}\n+\n+address Relocation::pd_call_destination(address orig_addr) {\n+  assert(is_call(), \"should be an address instruction here\");\n+  if (NativeCall::is_call_at(addr())) {\n+    address trampoline = nativeCall_at(addr())->get_trampoline();\n+    if (trampoline != NULL) {\n+      return nativeCallTrampolineStub_at(trampoline)->destination();\n+    }\n+  }\n+  if (orig_addr != NULL) {\n+    \/\/ the extracted address from the instructions in address orig_addr\n+    address new_addr = MacroAssembler::pd_call_destination(orig_addr);\n+    \/\/ If call is branch to self, don't try to relocate it, just leave it\n+    \/\/ as branch to self. This happens during code generation if the code\n+    \/\/ buffer expands. It will be relocated to the trampoline above once\n+    \/\/ code generation is complete.\n+    new_addr = (new_addr == orig_addr) ? addr() : new_addr;\n+    return new_addr;\n+  }\n+  return MacroAssembler::pd_call_destination(addr());\n+}\n+\n+void Relocation::pd_set_call_destination(address x) {\n+  assert(is_call(), \"should be an address instruction here\");\n+  if (NativeCall::is_call_at(addr())) {\n+    address trampoline = nativeCall_at(addr())->get_trampoline();\n+    if (trampoline != NULL) {\n+      nativeCall_at(addr())->set_destination_mt_safe(x, \/* assert_lock *\/false);\n+      return;\n+    }\n+  }\n+  MacroAssembler::pd_patch_instruction_size(addr(), x);\n+  address pd_call = pd_call_destination(addr());\n+  assert(pd_call == x, \"fail in reloc\");\n+}\n+\n+address* Relocation::pd_address_in_code() {\n+  assert(NativeCall::is_load_pc_relative_at(addr()), \"Not the expected instruction sequence!\");\n+  return (address*)(MacroAssembler::target_addr_for_insn(addr()));\n+}\n+\n+address Relocation::pd_get_address_from_code() {\n+  return MacroAssembler::pd_call_destination(addr());\n+}\n+\n+void poll_Relocation::fix_relocation_after_move(const CodeBuffer* src, CodeBuffer* dest) {\n+  if (NativeInstruction::maybe_cpool_ref(addr())) {\n+    address old_addr = old_addr_for(addr(), src, dest);\n+    MacroAssembler::pd_patch_instruction_size(addr(), MacroAssembler::target_addr_for_insn(old_addr));\n+  }\n+}\n+\n+void metadata_Relocation::pd_fix_value(address x) {\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/relocInfo_riscv.cpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_RELOCINFO_RISCV_HPP\n+#define CPU_RISCV_RELOCINFO_RISCV_HPP\n+\n+  \/\/ machine-dependent parts of class relocInfo\n+ private:\n+  enum {\n+    \/\/ Relocations are byte-aligned.\n+    offset_unit        =  1,\n+    \/\/ Must be at least 1 for RelocInfo::narrow_oop_in_const.\n+    format_width       =  1\n+  };\n+\n+ public:\n+\n+  \/\/ This platform has no oops in the code that are not also\n+  \/\/ listed in the oop section.\n+  static bool mustIterateImmediateOopsInCode() { return false; }\n+\n+#endif \/\/ CPU_RISCV_RELOCINFO_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/relocInfo_riscv.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -0,0 +1,10250 @@\n+\/\/\n+\/\/ Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+\/\/ Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+\/\/ RISCV Architecture Description File\n+\n+\/\/----------REGISTER DEFINITION BLOCK------------------------------------------\n+\/\/ This information is used by the matcher and the register allocator to\n+\/\/ describe individual registers and classes of registers within the target\n+\/\/ archtecture.\n+\n+register %{\n+\/\/----------Architecture Description Register Definitions----------------------\n+\/\/ General Registers\n+\/\/ \"reg_def\"  name ( register save type, C convention save type,\n+\/\/                   ideal register type, encoding );\n+\/\/ Register Save Types:\n+\/\/\n+\/\/ NS  = No-Save:       The register allocator assumes that these registers\n+\/\/                      can be used without saving upon entry to the method, &\n+\/\/                      that they do not need to be saved at call sites.\n+\/\/\n+\/\/ SOC = Save-On-Call:  The register allocator assumes that these registers\n+\/\/                      can be used without saving upon entry to the method,\n+\/\/                      but that they must be saved at call sites.\n+\/\/\n+\/\/ SOE = Save-On-Entry: The register allocator assumes that these registers\n+\/\/                      must be saved before using them upon entry to the\n+\/\/                      method, but they do not need to be saved at call\n+\/\/                      sites.\n+\/\/\n+\/\/ AS  = Always-Save:   The register allocator assumes that these registers\n+\/\/                      must be saved before using them upon entry to the\n+\/\/                      method, & that they must be saved at call sites.\n+\/\/\n+\/\/ Ideal Register Type is used to determine how to save & restore a\n+\/\/ register.  Op_RegI will get spilled with LoadI\/StoreI, Op_RegP will get\n+\/\/ spilled with LoadP\/StoreP.  If the register supports both, use Op_RegI.\n+\/\/\n+\/\/ The encoding number is the actual bit-pattern placed into the opcodes.\n+\n+\/\/ We must define the 64 bit int registers in two 32 bit halves, the\n+\/\/ real lower register and a virtual upper half register. upper halves\n+\/\/ are used by the register allocator but are not actually supplied as\n+\/\/ operands to memory ops.\n+\/\/\n+\/\/ follow the C1 compiler in making registers\n+\/\/\n+\/\/   x7, x9-x17, x27-x31 volatile (caller save)\n+\/\/   x0-x4, x8, x23 system (no save, no allocate)\n+\/\/   x5-x6 non-allocatable (so we can use them as temporary regs)\n+\n+\/\/\n+\/\/ as regards Java usage. we don't use any callee save registers\n+\/\/ because this makes it difficult to de-optimise a frame (see comment\n+\/\/ in x86 implementation of Deoptimization::unwind_callee_save_values)\n+\/\/\n+\n+\/\/ General Registers\n+\n+reg_def R0      ( NS,  NS,  Op_RegI, 0,  x0->as_VMReg()         ); \/\/ zr\n+reg_def R0_H    ( NS,  NS,  Op_RegI, 0,  x0->as_VMReg()->next() );\n+reg_def R1      ( NS,  SOC, Op_RegI, 1,  x1->as_VMReg()         ); \/\/ ra\n+reg_def R1_H    ( NS,  SOC, Op_RegI, 1,  x1->as_VMReg()->next() );\n+reg_def R2      ( NS,  SOE, Op_RegI, 2,  x2->as_VMReg()         ); \/\/ sp\n+reg_def R2_H    ( NS,  SOE, Op_RegI, 2,  x2->as_VMReg()->next() );\n+reg_def R3      ( NS,  NS,  Op_RegI, 3,  x3->as_VMReg()         ); \/\/ gp\n+reg_def R3_H    ( NS,  NS,  Op_RegI, 3,  x3->as_VMReg()->next() );\n+reg_def R4      ( NS,  NS,  Op_RegI, 4,  x4->as_VMReg()         ); \/\/ tp\n+reg_def R4_H    ( NS,  NS,  Op_RegI, 4,  x4->as_VMReg()->next() );\n+reg_def R7      ( SOC, SOC, Op_RegI, 7,  x7->as_VMReg()         );\n+reg_def R7_H    ( SOC, SOC, Op_RegI, 7,  x7->as_VMReg()->next() );\n+reg_def R8      ( NS,  SOE, Op_RegI, 8,  x8->as_VMReg()         ); \/\/ fp\n+reg_def R8_H    ( NS,  SOE, Op_RegI, 8,  x8->as_VMReg()->next() );\n+reg_def R9      ( SOC, SOE, Op_RegI, 9,  x9->as_VMReg()         );\n+reg_def R9_H    ( SOC, SOE, Op_RegI, 9,  x9->as_VMReg()->next() );\n+reg_def R10     ( SOC, SOC, Op_RegI, 10, x10->as_VMReg()        );\n+reg_def R10_H   ( SOC, SOC, Op_RegI, 10, x10->as_VMReg()->next());\n+reg_def R11     ( SOC, SOC, Op_RegI, 11, x11->as_VMReg()        );\n+reg_def R11_H   ( SOC, SOC, Op_RegI, 11, x11->as_VMReg()->next());\n+reg_def R12     ( SOC, SOC, Op_RegI, 12, x12->as_VMReg()        );\n+reg_def R12_H   ( SOC, SOC, Op_RegI, 12, x12->as_VMReg()->next());\n+reg_def R13     ( SOC, SOC, Op_RegI, 13, x13->as_VMReg()        );\n+reg_def R13_H   ( SOC, SOC, Op_RegI, 13, x13->as_VMReg()->next());\n+reg_def R14     ( SOC, SOC, Op_RegI, 14, x14->as_VMReg()        );\n+reg_def R14_H   ( SOC, SOC, Op_RegI, 14, x14->as_VMReg()->next());\n+reg_def R15     ( SOC, SOC, Op_RegI, 15, x15->as_VMReg()        );\n+reg_def R15_H   ( SOC, SOC, Op_RegI, 15, x15->as_VMReg()->next());\n+reg_def R16     ( SOC, SOC, Op_RegI, 16, x16->as_VMReg()        );\n+reg_def R16_H   ( SOC, SOC, Op_RegI, 16, x16->as_VMReg()->next());\n+reg_def R17     ( SOC, SOC, Op_RegI, 17, x17->as_VMReg()        );\n+reg_def R17_H   ( SOC, SOC, Op_RegI, 17, x17->as_VMReg()->next());\n+reg_def R18     ( SOC, SOE, Op_RegI, 18, x18->as_VMReg()        );\n+reg_def R18_H   ( SOC, SOE, Op_RegI, 18, x18->as_VMReg()->next());\n+reg_def R19     ( SOC, SOE, Op_RegI, 19, x19->as_VMReg()        );\n+reg_def R19_H   ( SOC, SOE, Op_RegI, 19, x19->as_VMReg()->next());\n+reg_def R20     ( SOC, SOE, Op_RegI, 20, x20->as_VMReg()        ); \/\/ caller esp\n+reg_def R20_H   ( SOC, SOE, Op_RegI, 20, x20->as_VMReg()->next());\n+reg_def R21     ( SOC, SOE, Op_RegI, 21, x21->as_VMReg()        );\n+reg_def R21_H   ( SOC, SOE, Op_RegI, 21, x21->as_VMReg()->next());\n+reg_def R22     ( SOC, SOE, Op_RegI, 22, x22->as_VMReg()        );\n+reg_def R22_H   ( SOC, SOE, Op_RegI, 22, x22->as_VMReg()->next());\n+reg_def R23     ( NS,  SOE, Op_RegI, 23, x23->as_VMReg()        ); \/\/ java thread\n+reg_def R23_H   ( NS,  SOE, Op_RegI, 23, x23->as_VMReg()->next());\n+reg_def R24     ( SOC, SOE, Op_RegI, 24, x24->as_VMReg()        );\n+reg_def R24_H   ( SOC, SOE, Op_RegI, 24, x24->as_VMReg()->next());\n+reg_def R25     ( SOC, SOE, Op_RegI, 25, x25->as_VMReg()        );\n+reg_def R25_H   ( SOC, SOE, Op_RegI, 25, x25->as_VMReg()->next());\n+reg_def R26     ( SOC, SOE, Op_RegI, 26, x26->as_VMReg()        );\n+reg_def R26_H   ( SOC, SOE, Op_RegI, 26, x26->as_VMReg()->next());\n+reg_def R27     ( SOC, SOE, Op_RegI, 27, x27->as_VMReg()        ); \/\/ heapbase\n+reg_def R27_H   ( SOC, SOE, Op_RegI, 27, x27->as_VMReg()->next());\n+reg_def R28     ( SOC, SOC, Op_RegI, 28, x28->as_VMReg()        );\n+reg_def R28_H   ( SOC, SOC, Op_RegI, 28, x28->as_VMReg()->next());\n+reg_def R29     ( SOC, SOC, Op_RegI, 29, x29->as_VMReg()        );\n+reg_def R29_H   ( SOC, SOC, Op_RegI, 29, x29->as_VMReg()->next());\n+reg_def R30     ( SOC, SOC, Op_RegI, 30, x30->as_VMReg()        );\n+reg_def R30_H   ( SOC, SOC, Op_RegI, 30, x30->as_VMReg()->next());\n+reg_def R31     ( SOC, SOC, Op_RegI, 31, x31->as_VMReg()        );\n+reg_def R31_H   ( SOC, SOC, Op_RegI, 31, x31->as_VMReg()->next());\n+\n+\/\/ ----------------------------\n+\/\/ Float\/Double Registers\n+\/\/ ----------------------------\n+\n+\/\/ Double Registers\n+\n+\/\/ The rules of ADL require that double registers be defined in pairs.\n+\/\/ Each pair must be two 32-bit values, but not necessarily a pair of\n+\/\/ single float registers. In each pair, ADLC-assigned register numbers\n+\/\/ must be adjacent, with the lower number even. Finally, when the\n+\/\/ CPU stores such a register pair to memory, the word associated with\n+\/\/ the lower ADLC-assigned number must be stored to the lower address.\n+\n+\/\/ RISCV has 32 floating-point registers. Each can store a single\n+\/\/ or double precision floating-point value.\n+\n+\/\/ for Java use float registers f0-f31 are always save on call whereas\n+\/\/ the platform ABI treats f8-f9 and f18-f27 as callee save). Other\n+\/\/ float registers are SOC as per the platform spec\n+\n+reg_def F0    ( SOC, SOC, Op_RegF,  0,  f0->as_VMReg()          );\n+reg_def F0_H  ( SOC, SOC, Op_RegF,  0,  f0->as_VMReg()->next()  );\n+reg_def F1    ( SOC, SOC, Op_RegF,  1,  f1->as_VMReg()          );\n+reg_def F1_H  ( SOC, SOC, Op_RegF,  1,  f1->as_VMReg()->next()  );\n+reg_def F2    ( SOC, SOC, Op_RegF,  2,  f2->as_VMReg()          );\n+reg_def F2_H  ( SOC, SOC, Op_RegF,  2,  f2->as_VMReg()->next()  );\n+reg_def F3    ( SOC, SOC, Op_RegF,  3,  f3->as_VMReg()          );\n+reg_def F3_H  ( SOC, SOC, Op_RegF,  3,  f3->as_VMReg()->next()  );\n+reg_def F4    ( SOC, SOC, Op_RegF,  4,  f4->as_VMReg()          );\n+reg_def F4_H  ( SOC, SOC, Op_RegF,  4,  f4->as_VMReg()->next()  );\n+reg_def F5    ( SOC, SOC, Op_RegF,  5,  f5->as_VMReg()          );\n+reg_def F5_H  ( SOC, SOC, Op_RegF,  5,  f5->as_VMReg()->next()  );\n+reg_def F6    ( SOC, SOC, Op_RegF,  6,  f6->as_VMReg()          );\n+reg_def F6_H  ( SOC, SOC, Op_RegF,  6,  f6->as_VMReg()->next()  );\n+reg_def F7    ( SOC, SOC, Op_RegF,  7,  f7->as_VMReg()          );\n+reg_def F7_H  ( SOC, SOC, Op_RegF,  7,  f7->as_VMReg()->next()  );\n+reg_def F8    ( SOC, SOE, Op_RegF,  8,  f8->as_VMReg()          );\n+reg_def F8_H  ( SOC, SOE, Op_RegF,  8,  f8->as_VMReg()->next()  );\n+reg_def F9    ( SOC, SOE, Op_RegF,  9,  f9->as_VMReg()          );\n+reg_def F9_H  ( SOC, SOE, Op_RegF,  9,  f9->as_VMReg()->next()  );\n+reg_def F10   ( SOC, SOC, Op_RegF,  10, f10->as_VMReg()         );\n+reg_def F10_H ( SOC, SOC, Op_RegF,  10, f10->as_VMReg()->next() );\n+reg_def F11   ( SOC, SOC, Op_RegF,  11, f11->as_VMReg()         );\n+reg_def F11_H ( SOC, SOC, Op_RegF,  11, f11->as_VMReg()->next() );\n+reg_def F12   ( SOC, SOC, Op_RegF,  12, f12->as_VMReg()         );\n+reg_def F12_H ( SOC, SOC, Op_RegF,  12, f12->as_VMReg()->next() );\n+reg_def F13   ( SOC, SOC, Op_RegF,  13, f13->as_VMReg()         );\n+reg_def F13_H ( SOC, SOC, Op_RegF,  13, f13->as_VMReg()->next() );\n+reg_def F14   ( SOC, SOC, Op_RegF,  14, f14->as_VMReg()         );\n+reg_def F14_H ( SOC, SOC, Op_RegF,  14, f14->as_VMReg()->next() );\n+reg_def F15   ( SOC, SOC, Op_RegF,  15, f15->as_VMReg()         );\n+reg_def F15_H ( SOC, SOC, Op_RegF,  15, f15->as_VMReg()->next() );\n+reg_def F16   ( SOC, SOC, Op_RegF,  16, f16->as_VMReg()         );\n+reg_def F16_H ( SOC, SOC, Op_RegF,  16, f16->as_VMReg()->next() );\n+reg_def F17   ( SOC, SOC, Op_RegF,  17, f17->as_VMReg()         );\n+reg_def F17_H ( SOC, SOC, Op_RegF,  17, f17->as_VMReg()->next() );\n+reg_def F18   ( SOC, SOE, Op_RegF,  18, f18->as_VMReg()         );\n+reg_def F18_H ( SOC, SOE, Op_RegF,  18, f18->as_VMReg()->next() );\n+reg_def F19   ( SOC, SOE, Op_RegF,  19, f19->as_VMReg()         );\n+reg_def F19_H ( SOC, SOE, Op_RegF,  19, f19->as_VMReg()->next() );\n+reg_def F20   ( SOC, SOE, Op_RegF,  20, f20->as_VMReg()         );\n+reg_def F20_H ( SOC, SOE, Op_RegF,  20, f20->as_VMReg()->next() );\n+reg_def F21   ( SOC, SOE, Op_RegF,  21, f21->as_VMReg()         );\n+reg_def F21_H ( SOC, SOE, Op_RegF,  21, f21->as_VMReg()->next() );\n+reg_def F22   ( SOC, SOE, Op_RegF,  22, f22->as_VMReg()         );\n+reg_def F22_H ( SOC, SOE, Op_RegF,  22, f22->as_VMReg()->next() );\n+reg_def F23   ( SOC, SOE, Op_RegF,  23, f23->as_VMReg()         );\n+reg_def F23_H ( SOC, SOE, Op_RegF,  23, f23->as_VMReg()->next() );\n+reg_def F24   ( SOC, SOE, Op_RegF,  24, f24->as_VMReg()         );\n+reg_def F24_H ( SOC, SOE, Op_RegF,  24, f24->as_VMReg()->next() );\n+reg_def F25   ( SOC, SOE, Op_RegF,  25, f25->as_VMReg()         );\n+reg_def F25_H ( SOC, SOE, Op_RegF,  25, f25->as_VMReg()->next() );\n+reg_def F26   ( SOC, SOE, Op_RegF,  26, f26->as_VMReg()         );\n+reg_def F26_H ( SOC, SOE, Op_RegF,  26, f26->as_VMReg()->next() );\n+reg_def F27   ( SOC, SOE, Op_RegF,  27, f27->as_VMReg()         );\n+reg_def F27_H ( SOC, SOE, Op_RegF,  27, f27->as_VMReg()->next() );\n+reg_def F28   ( SOC, SOC, Op_RegF,  28, f28->as_VMReg()         );\n+reg_def F28_H ( SOC, SOC, Op_RegF,  28, f28->as_VMReg()->next() );\n+reg_def F29   ( SOC, SOC, Op_RegF,  29, f29->as_VMReg()         );\n+reg_def F29_H ( SOC, SOC, Op_RegF,  29, f29->as_VMReg()->next() );\n+reg_def F30   ( SOC, SOC, Op_RegF,  30, f30->as_VMReg()         );\n+reg_def F30_H ( SOC, SOC, Op_RegF,  30, f30->as_VMReg()->next() );\n+reg_def F31   ( SOC, SOC, Op_RegF,  31, f31->as_VMReg()         );\n+reg_def F31_H ( SOC, SOC, Op_RegF,  31, f31->as_VMReg()->next() );\n+\n+\/\/ ----------------------------\n+\/\/ Special Registers\n+\/\/ ----------------------------\n+\n+\/\/ On riscv, the physical flag register is missing, so we use t1 instead,\n+\/\/ to bridge the RegFlag semantics in share\/opto\n+\n+reg_def RFLAGS   (SOC, SOC, Op_RegFlags, 6, x6->as_VMReg()        );\n+\n+\/\/ Specify priority of register selection within phases of register\n+\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n+\/\/ give registers a low priority when they are required by machine\n+\/\/ instructions, like EAX and EDX on I486, and choose no-save registers\n+\/\/ before save-on-call, & save-on-call before save-on-entry.  Registers\n+\/\/ which participate in fixed calling sequences should come last.\n+\/\/ Registers which are used as pairs must fall on an even boundary.\n+\n+alloc_class chunk0(\n+    \/\/ volatiles\n+    R7,  R7_H,\n+    R28, R28_H,\n+    R29, R29_H,\n+    R30, R30_H,\n+    R31, R31_H,\n+\n+    \/\/ arg registers\n+    R10, R10_H,\n+    R11, R11_H,\n+    R12, R12_H,\n+    R13, R13_H,\n+    R14, R14_H,\n+    R15, R15_H,\n+    R16, R16_H,\n+    R17, R17_H,\n+\n+    \/\/ non-volatiles\n+    R9,  R9_H,\n+    R18, R18_H,\n+    R19, R19_H,\n+    R20, R20_H,\n+    R21, R21_H,\n+    R22, R22_H,\n+    R24, R24_H,\n+    R25, R25_H,\n+    R26, R26_H,\n+\n+    \/\/ non-allocatable registers\n+    R23, R23_H, \/\/ java thread\n+    R27, R27_H, \/\/ heapbase\n+    R4,  R4_H,  \/\/ thread\n+    R8,  R8_H,  \/\/ fp\n+    R0,  R0_H,  \/\/ zero\n+    R1,  R1_H,  \/\/ ra\n+    R2,  R2_H,  \/\/ sp\n+    R3,  R3_H,  \/\/ gp\n+);\n+\n+alloc_class chunk1(\n+\n+    \/\/ no save\n+    F0,  F0_H,\n+    F1,  F1_H,\n+    F2,  F2_H,\n+    F3,  F3_H,\n+    F4,  F4_H,\n+    F5,  F5_H,\n+    F6,  F6_H,\n+    F7,  F7_H,\n+    F28, F28_H,\n+    F29, F29_H,\n+    F30, F30_H,\n+    F31, F31_H,\n+\n+    \/\/ arg registers\n+    F10, F10_H,\n+    F11, F11_H,\n+    F12, F12_H,\n+    F13, F13_H,\n+    F14, F14_H,\n+    F15, F15_H,\n+    F16, F16_H,\n+    F17, F17_H,\n+\n+    \/\/ non-volatiles\n+    F8,  F8_H,\n+    F9,  F9_H,\n+    F18, F18_H,\n+    F19, F19_H,\n+    F20, F20_H,\n+    F21, F21_H,\n+    F22, F22_H,\n+    F23, F23_H,\n+    F24, F24_H,\n+    F25, F25_H,\n+    F26, F26_H,\n+    F27, F27_H,\n+);\n+\n+alloc_class chunk2(RFLAGS);\n+\n+\/\/----------Architecture Description Register Classes--------------------------\n+\/\/ Several register classes are automatically defined based upon information in\n+\/\/ this architecture description.\n+\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n+\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n+\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n+\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/\n+\n+\/\/ Class for all 32 bit general purpose registers\n+reg_class all_reg32(\n+    R0,\n+    R1,\n+    R2,\n+    R3,\n+    R4,\n+    R7,\n+    R8,\n+    R9,\n+    R10,\n+    R11,\n+    R12,\n+    R13,\n+    R14,\n+    R15,\n+    R16,\n+    R17,\n+    R18,\n+    R19,\n+    R20,\n+    R21,\n+    R22,\n+    R23,\n+    R24,\n+    R25,\n+    R26,\n+    R27,\n+    R28,\n+    R29,\n+    R30,\n+    R31\n+);\n+\n+\/\/ Class for any 32 bit integer registers (excluding zr)\n+reg_class any_reg32 %{\n+  return _ANY_REG32_mask;\n+%}\n+\n+\/\/ Singleton class for R10 int register\n+reg_class int_r10_reg(R10);\n+\n+\/\/ Singleton class for R12 int register\n+reg_class int_r12_reg(R12);\n+\n+\/\/ Singleton class for R13 int register\n+reg_class int_r13_reg(R13);\n+\n+\/\/ Singleton class for R14 int register\n+reg_class int_r14_reg(R14);\n+\n+\/\/ Class for all long integer registers\n+reg_class all_reg(\n+    R0,  R0_H,\n+    R1,  R1_H,\n+    R2,  R2_H,\n+    R3,  R3_H,\n+    R4,  R4_H,\n+    R7,  R7_H,\n+    R8,  R8_H,\n+    R9,  R9_H,\n+    R10, R10_H,\n+    R11, R11_H,\n+    R12, R12_H,\n+    R13, R13_H,\n+    R14, R14_H,\n+    R15, R15_H,\n+    R16, R16_H,\n+    R17, R17_H,\n+    R18, R18_H,\n+    R19, R19_H,\n+    R20, R20_H,\n+    R21, R21_H,\n+    R22, R22_H,\n+    R23, R23_H,\n+    R24, R24_H,\n+    R25, R25_H,\n+    R26, R26_H,\n+    R27, R27_H,\n+    R28, R28_H,\n+    R29, R29_H,\n+    R30, R30_H,\n+    R31, R31_H\n+);\n+\n+\/\/ Class for all long integer registers (excluding zr)\n+reg_class any_reg %{\n+  return _ANY_REG_mask;\n+%}\n+\n+\/\/ Class for non-allocatable 32 bit registers\n+reg_class non_allocatable_reg32(\n+    R0,                       \/\/ zr\n+    R1,                       \/\/ ra\n+    R2,                       \/\/ sp\n+    R3,                       \/\/ gp\n+    R4,                       \/\/ tp\n+    R23                       \/\/ java thread\n+);\n+\n+\/\/ Class for non-allocatable 64 bit registers\n+reg_class non_allocatable_reg(\n+    R0,  R0_H,                \/\/ zr\n+    R1,  R1_H,                \/\/ ra\n+    R2,  R2_H,                \/\/ sp\n+    R3,  R3_H,                \/\/ gp\n+    R4,  R4_H,                \/\/ tp\n+    R23, R23_H                \/\/ java thread\n+);\n+\n+reg_class no_special_reg32 %{\n+  return _NO_SPECIAL_REG32_mask;\n+%}\n+\n+reg_class no_special_reg %{\n+  return _NO_SPECIAL_REG_mask;\n+%}\n+\n+reg_class ptr_reg %{\n+  return _PTR_REG_mask;\n+%}\n+\n+reg_class no_special_ptr_reg %{\n+  return _NO_SPECIAL_PTR_REG_mask;\n+%}\n+\n+\/\/ Class for 64 bit register r10\n+reg_class r10_reg(\n+    R10, R10_H\n+);\n+\n+\/\/ Class for 64 bit register r11\n+reg_class r11_reg(\n+    R11, R11_H\n+);\n+\n+\/\/ Class for 64 bit register r12\n+reg_class r12_reg(\n+    R12, R12_H\n+);\n+\n+\/\/ Class for 64 bit register r13\n+reg_class r13_reg(\n+    R13, R13_H\n+);\n+\n+\/\/ Class for 64 bit register r14\n+reg_class r14_reg(\n+    R14, R14_H\n+);\n+\n+\/\/ Class for 64 bit register r15\n+reg_class r15_reg(\n+    R15, R15_H\n+);\n+\n+\/\/ Class for 64 bit register r16\n+reg_class r16_reg(\n+    R16, R16_H\n+);\n+\n+\/\/ Class for method register\n+reg_class method_reg(\n+    R31, R31_H\n+);\n+\n+\/\/ Class for heapbase register\n+reg_class heapbase_reg(\n+    R27, R27_H\n+);\n+\n+\/\/ Class for java thread register\n+reg_class java_thread_reg(\n+    R23, R23_H\n+);\n+\n+reg_class r28_reg(\n+    R28, R28_H\n+);\n+\n+reg_class r29_reg(\n+    R29, R29_H\n+);\n+\n+reg_class r30_reg(\n+    R30, R30_H\n+);\n+\n+\/\/ Class for zero registesr\n+reg_class zr_reg(\n+    R0, R0_H\n+);\n+\n+\/\/ Class for thread register\n+reg_class thread_reg(\n+    R4, R4_H\n+);\n+\n+\/\/ Class for frame pointer register\n+reg_class fp_reg(\n+    R8, R8_H\n+);\n+\n+\/\/ Class for link register\n+reg_class ra_reg(\n+    R1, R1_H\n+);\n+\n+\/\/ Class for long sp register\n+reg_class sp_reg(\n+    R2, R2_H\n+);\n+\n+\/\/ Class for all float registers\n+reg_class float_reg(\n+    F0,\n+    F1,\n+    F2,\n+    F3,\n+    F4,\n+    F5,\n+    F6,\n+    F7,\n+    F8,\n+    F9,\n+    F10,\n+    F11,\n+    F12,\n+    F13,\n+    F14,\n+    F15,\n+    F16,\n+    F17,\n+    F18,\n+    F19,\n+    F20,\n+    F21,\n+    F22,\n+    F23,\n+    F24,\n+    F25,\n+    F26,\n+    F27,\n+    F28,\n+    F29,\n+    F30,\n+    F31\n+);\n+\n+\/\/ Double precision float registers have virtual `high halves' that\n+\/\/ are needed by the allocator.\n+\/\/ Class for all double registers\n+reg_class double_reg(\n+    F0,  F0_H,\n+    F1,  F1_H,\n+    F2,  F2_H,\n+    F3,  F3_H,\n+    F4,  F4_H,\n+    F5,  F5_H,\n+    F6,  F6_H,\n+    F7,  F7_H,\n+    F8,  F8_H,\n+    F9,  F9_H,\n+    F10, F10_H,\n+    F11, F11_H,\n+    F12, F12_H,\n+    F13, F13_H,\n+    F14, F14_H,\n+    F15, F15_H,\n+    F16, F16_H,\n+    F17, F17_H,\n+    F18, F18_H,\n+    F19, F19_H,\n+    F20, F20_H,\n+    F21, F21_H,\n+    F22, F22_H,\n+    F23, F23_H,\n+    F24, F24_H,\n+    F25, F25_H,\n+    F26, F26_H,\n+    F27, F27_H,\n+    F28, F28_H,\n+    F29, F29_H,\n+    F30, F30_H,\n+    F31, F31_H\n+);\n+\n+\/\/ Class for 64 bit register f0\n+reg_class f0_reg(\n+    F0, F0_H\n+);\n+\n+\/\/ Class for 64 bit register f1\n+reg_class f1_reg(\n+    F1, F1_H\n+);\n+\n+\/\/ Class for 64 bit register f2\n+reg_class f2_reg(\n+    F2, F2_H\n+);\n+\n+\/\/ Class for 64 bit register f3\n+reg_class f3_reg(\n+    F3, F3_H\n+);\n+\n+\/\/ class for condition codes\n+reg_class reg_flags(RFLAGS);\n+%}\n+\n+\/\/----------DEFINITION BLOCK---------------------------------------------------\n+\/\/ Define name --> value mappings to inform the ADLC of an integer valued name\n+\/\/ Current support includes integer values in the range [0, 0x7FFFFFFF]\n+\/\/ Format:\n+\/\/        int_def  <name>         ( <int_value>, <expression>);\n+\/\/ Generated Code in ad_<arch>.hpp\n+\/\/        #define  <name>   (<expression>)\n+\/\/        \/\/ value == <int_value>\n+\/\/ Generated code in ad_<arch>.cpp adlc_verification()\n+\/\/        assert( <name> == <int_value>, \"Expect (<expression>) to equal <int_value>\");\n+\/\/\n+\n+\/\/ we follow the ppc-aix port in using a simple cost model which ranks\n+\/\/ register operations as cheap, memory ops as more expensive and\n+\/\/ branches as most expensive. the first two have a low as well as a\n+\/\/ normal cost. huge cost appears to be a way of saying don't do\n+\/\/ something\n+\n+definitions %{\n+  \/\/ The default cost (of a register move instruction).\n+  int_def DEFAULT_COST         (  100,               100);\n+  int_def ALU_COST             (  100,  1 * DEFAULT_COST);          \/\/ unknown, const, arith, shift, slt,\n+                                                                    \/\/ multi, auipc, nop, logical, move\n+  int_def LOAD_COST            (  300,  3 * DEFAULT_COST);          \/\/ load, fpload\n+  int_def STORE_COST           (  100,  1 * DEFAULT_COST);          \/\/ store, fpstore\n+  int_def XFER_COST            (  300,  3 * DEFAULT_COST);          \/\/ mfc, mtc, fcvt, fmove, fcmp\n+  int_def BRANCH_COST          (  100,  1 * DEFAULT_COST);          \/\/ branch, jmp, call\n+  int_def IMUL_COST            ( 1000, 10 * DEFAULT_COST);          \/\/ imul\n+  int_def IDIVSI_COST          ( 3400, 34 * DEFAULT_COST);          \/\/ idivdi\n+  int_def IDIVDI_COST          ( 6600, 66 * DEFAULT_COST);          \/\/ idivsi\n+  int_def FMUL_SINGLE_COST     (  500,  5 * DEFAULT_COST);          \/\/ fadd, fmul, fmadd\n+  int_def FMUL_DOUBLE_COST     (  700,  7 * DEFAULT_COST);          \/\/ fadd, fmul, fmadd\n+  int_def FDIV_COST            ( 2000, 20 * DEFAULT_COST);          \/\/ fdiv\n+  int_def FSQRT_COST           ( 2500, 25 * DEFAULT_COST);          \/\/ fsqrt\n+  int_def VOLATILE_REF_COST    ( 1000, 10 * DEFAULT_COST);\n+%}\n+\n+\n+\n+\/\/----------SOURCE BLOCK-------------------------------------------------------\n+\/\/ This is a block of C++ code which provides values, functions, and\n+\/\/ definitions necessary in the rest of the architecture description\n+\n+source_hpp %{\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/convertnode.hpp\"\n+\n+extern RegMask _ANY_REG32_mask;\n+extern RegMask _ANY_REG_mask;\n+extern RegMask _PTR_REG_mask;\n+extern RegMask _NO_SPECIAL_REG32_mask;\n+extern RegMask _NO_SPECIAL_REG_mask;\n+extern RegMask _NO_SPECIAL_PTR_REG_mask;\n+\n+class CallStubImpl {\n+\n+  \/\/--------------------------------------------------------------\n+  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n+  \/\/--------------------------------------------------------------\n+\n+ public:\n+  \/\/ Size of call trampoline stub.\n+  static uint size_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+\n+  \/\/ number of relocations needed by a call trampoline stub\n+  static uint reloc_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+};\n+\n+class HandlerImpl {\n+\n+ public:\n+\n+  static int emit_exception_handler(CodeBuffer &cbuf);\n+  static int emit_deopt_handler(CodeBuffer& cbuf);\n+\n+  static uint size_exception_handler() {\n+    return MacroAssembler::far_branch_size();\n+  }\n+\n+  static uint size_deopt_handler() {\n+    \/\/ count auipc + far branch\n+    return NativeInstruction::instruction_size + MacroAssembler::far_branch_size();\n+  }\n+};\n+\n+bool is_CAS(int opcode, bool maybe_volatile);\n+\n+\/\/ predicate controlling translation of CompareAndSwapX\n+bool needs_acquiring_load_reserved(const Node *load);\n+\n+\/\/ predicate controlling translation of StoreCM\n+bool unnecessary_storestore(const Node *storecm);\n+\n+\/\/ predicate controlling addressing modes\n+bool size_fits_all_mem_uses(AddPNode* addp, int shift);\n+%}\n+\n+source %{\n+\n+\/\/ Derived RegMask with conditionally allocatable registers\n+\n+RegMask _ANY_REG32_mask;\n+RegMask _ANY_REG_mask;\n+RegMask _PTR_REG_mask;\n+RegMask _NO_SPECIAL_REG32_mask;\n+RegMask _NO_SPECIAL_REG_mask;\n+RegMask _NO_SPECIAL_PTR_REG_mask;\n+\n+void reg_mask_init() {\n+\n+  _ANY_REG32_mask = _ALL_REG32_mask;\n+  _ANY_REG32_mask.Remove(OptoReg::as_OptoReg(x0->as_VMReg()));\n+\n+  _ANY_REG_mask = _ALL_REG_mask;\n+  _ANY_REG_mask.SUBTRACT(_ZR_REG_mask);\n+\n+  _PTR_REG_mask = _ALL_REG_mask;\n+  _PTR_REG_mask.SUBTRACT(_ZR_REG_mask);\n+\n+  _NO_SPECIAL_REG32_mask = _ALL_REG32_mask;\n+  _NO_SPECIAL_REG32_mask.SUBTRACT(_NON_ALLOCATABLE_REG32_mask);\n+\n+  _NO_SPECIAL_REG_mask = _ALL_REG_mask;\n+  _NO_SPECIAL_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+\n+  _NO_SPECIAL_PTR_REG_mask = _ALL_REG_mask;\n+  _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+\n+  \/\/ x27 is not allocatable when compressed oops is on\n+  if (UseCompressedOops) {\n+    _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n+    _NO_SPECIAL_REG_mask.SUBTRACT(_HEAPBASE_REG_mask);\n+    _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_HEAPBASE_REG_mask);\n+  }\n+\n+  \/\/ x8 is not allocatable when PreserveFramePointer is on\n+  if (PreserveFramePointer) {\n+    _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n+    _NO_SPECIAL_REG_mask.SUBTRACT(_FP_REG_mask);\n+    _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_FP_REG_mask);\n+  }\n+}\n+\n+\/\/ is_CAS(int opcode, bool maybe_volatile)\n+\/\/\n+\/\/ return true if opcode is one of the possible CompareAndSwapX\n+\/\/ values otherwise false.\n+bool is_CAS(int opcode, bool maybe_volatile)\n+{\n+  switch (opcode) {\n+    \/\/ We handle these\n+    case Op_CompareAndSwapI:\n+    case Op_CompareAndSwapL:\n+    case Op_CompareAndSwapP:\n+    case Op_CompareAndSwapN:\n+#if INCLUDE_SHENANDOAHGC\n+    case Op_ShenandoahCompareAndSwapP:\n+    case Op_ShenandoahCompareAndSwapN:\n+#endif\n+    case Op_CompareAndSwapB:\n+    case Op_CompareAndSwapS:\n+    case Op_GetAndSetI:\n+    case Op_GetAndSetL:\n+    case Op_GetAndSetP:\n+    case Op_GetAndSetN:\n+    case Op_GetAndAddI:\n+    case Op_GetAndAddL:\n+      return true;\n+    case Op_CompareAndExchangeI:\n+    case Op_CompareAndExchangeN:\n+    case Op_CompareAndExchangeB:\n+    case Op_CompareAndExchangeS:\n+    case Op_CompareAndExchangeL:\n+    case Op_CompareAndExchangeP:\n+    case Op_WeakCompareAndSwapB:\n+    case Op_WeakCompareAndSwapS:\n+    case Op_WeakCompareAndSwapI:\n+    case Op_WeakCompareAndSwapL:\n+    case Op_WeakCompareAndSwapP:\n+    case Op_WeakCompareAndSwapN:\n+      return maybe_volatile;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\/\/ predicate controlling translation of CAS\n+\/\/\n+\/\/ returns true if CAS needs to use an acquiring load otherwise false\n+bool needs_acquiring_load_reserved(const Node *n)\n+{\n+  assert(n != NULL && is_CAS(n->Opcode(), true), \"expecting a compare and swap\");\n+\n+  LoadStoreNode* ldst = n->as_LoadStore();\n+  if (n != NULL && is_CAS(n->Opcode(), false)) {\n+    assert(ldst != NULL && ldst->trailing_membar() != NULL, \"expected trailing membar\");\n+  } else {\n+    return ldst != NULL && ldst->trailing_membar() != NULL;\n+  }\n+  \/\/ so we can just return true here\n+  return true;\n+}\n+\n+\/\/ predicate controlling translation of StoreCM\n+\/\/\n+\/\/ returns true if a StoreStore must precede the card write otherwise\n+\/\/ false\n+\n+bool unnecessary_storestore(const Node *storecm)\n+{\n+  assert(storecm->Opcode()  == Op_StoreCM, \"expecting a StoreCM\");\n+\n+  \/\/ we need to generate a dmb ishst between an object put and the\n+  \/\/ associated card mark when we are using CMS without conditional\n+  \/\/ card marking\n+\n+  if (UseConcMarkSweepGC && !UseCondCardMark) {\n+    return false;\n+  }\n+\n+  \/\/ a storestore is unnecesary in all other cases\n+\n+  return true;\n+}\n+\n+#define __ _masm.\n+\n+\/\/ advance declarations for helper functions to convert register\n+\/\/ indices to register objects\n+\n+\/\/ the ad file has to provide implementations of certain methods\n+\/\/ expected by the generic code\n+\/\/\n+\/\/ REQUIRED FUNCTIONALITY\n+\n+\/\/=============================================================================\n+\n+\/\/ !!!!! Special hack to get all types of calls to specify the byte offset\n+\/\/       from the start of the call to the point where the return address\n+\/\/       will point.\n+\n+int MachCallStaticJavaNode::ret_addr_offset()\n+{\n+  \/\/ jal\n+  return 1 * NativeInstruction::instruction_size;\n+}\n+\n+int MachCallDynamicJavaNode::ret_addr_offset()\n+{\n+  return 7 * NativeInstruction::instruction_size; \/\/ movptr, jal\n+}\n+\n+int MachCallRuntimeNode::ret_addr_offset() {\n+  \/\/ for generated stubs the call will be\n+  \/\/   jal(addr)\n+  \/\/ or with far branches\n+  \/\/   jal(trampoline_stub)\n+  \/\/ for real runtime callouts it will be 11 instructions\n+  \/\/ see riscv_enc_java_to_runtime\n+  \/\/   la(t1, retaddr)                ->  auipc + addi\n+  \/\/   la(t0, RuntimeAddress(addr))   ->  lui + addi + slli + addi + slli + addi\n+  \/\/   addi(sp, sp, -2 * wordSize)    ->  addi\n+  \/\/   sd(t1, Address(sp, wordSize))  ->  sd\n+  \/\/   jalr(t0)                       ->  jalr\n+  CodeBlob *cb = CodeCache::find_blob(_entry_point);\n+  if (cb != NULL) {\n+    return 1 * NativeInstruction::instruction_size;\n+  } else {\n+    return 11 * NativeInstruction::instruction_size;\n+  }\n+}\n+\n+\/\/\n+\/\/ Compute padding required for nodes which need alignment\n+\/\/\n+\n+\/\/ With RVC a call instruction may get 2-byte aligned.\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  \/\/ to make sure the address of jal 4-byte aligned.\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+\/\/ With RVC a call instruction may get 2-byte aligned.\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  \/\/ skip the movptr in MacroAssembler::ic_call():\n+  \/\/ lui + addi + slli + addi + slli + addi\n+  \/\/ Though movptr() has already 4-byte aligned with or without RVC,\n+  \/\/ We need to prevent from further changes by explicitly calculating the size.\n+  const int movptr_size = 6 * NativeInstruction::instruction_size;\n+  current_offset += movptr_size;\n+  \/\/ to make sure the address of jal 4-byte aligned.\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+\/\/ Indicate if the safepoint node needs the polling page as an input\n+\n+\/\/ the shared code plants the oop data at the start of the generated\n+\/\/ code for the safepoint node and that needs ot be at the load\n+\/\/ instruction itself. so we cannot plant a mov of the safepoint poll\n+\/\/ address followed by a load. setting this to true means the mov is\n+\/\/ scheduled as a prior instruction. that's better for scheduling\n+\/\/ anyway.\n+\n+bool SafePointNode::needs_polling_address_input()\n+{\n+  return true;\n+}\n+\n+\/\/=============================================================================\n+\n+#ifndef PRODUCT\n+void MachBreakpointNode::format(PhaseRegAlloc *ra_, outputStream *st) const {\n+  assert_cond(st != NULL);\n+  st->print(\"BREAKPOINT\");\n+}\n+#endif\n+\n+void MachBreakpointNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+  MacroAssembler _masm(&cbuf);\n+  Assembler::CompressibleRegion cr(&_masm);\n+  __ ebreak();\n+}\n+\n+uint MachBreakpointNode::size(PhaseRegAlloc *ra_) const {\n+  return MachNode::size(ra_);\n+}\n+\n+\/\/=============================================================================\n+\n+#ifndef PRODUCT\n+  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n+  }\n+#endif\n+\n+  void MachNopNode::emit(CodeBuffer &cbuf, PhaseRegAlloc*) const {\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm); \/\/ nops shall be 2-byte under RVC for alignment purposes.\n+    for (int i = 0; i < _count; i++) {\n+      __ nop();\n+    }\n+  }\n+\n+  uint MachNopNode::size(PhaseRegAlloc*) const {\n+    return _count * (UseRVC ? NativeInstruction::compressed_instruction_size : NativeInstruction::instruction_size);\n+  }\n+\n+\/\/=============================================================================\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::Empty;\n+\n+int Compile::ConstantTable::calculate_table_base_offset() const {\n+  return 0;  \/\/ absolute addressing, no offset\n+}\n+\n+bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n+void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n+  ShouldNotReachHere();\n+}\n+\n+void MachConstantBaseNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const {\n+  \/\/ Empty encoding\n+}\n+\n+uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n+  return 0;\n+}\n+\n+#ifndef PRODUCT\n+void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  assert_cond(st != NULL);\n+  st->print(\"-- \\t\/\/ MachConstantBaseNode (empty encoding)\");\n+}\n+#endif\n+\n+#ifndef PRODUCT\n+void MachPrologNode::format(PhaseRegAlloc *ra_, outputStream *st) const {\n+  assert_cond(st != NULL && ra_ != NULL);\n+  Compile* C = ra_->C;\n+\n+  int framesize = C->frame_slots() << LogBytesPerInt;\n+\n+  if (C->need_stack_bang(framesize)) {\n+    st->print(\"# stack bang size=%d\\n\\t\", framesize);\n+  }\n+\n+  st->print(\"sd  fp, [sp, #%d]\\n\\t\", - 2 * wordSize);\n+  st->print(\"sd  ra, [sp, #%d]\\n\\t\", - wordSize);\n+  if (PreserveFramePointer) { st->print(\"sub  fp, sp, #%d\\n\\t\", 2 * wordSize); }\n+  st->print(\"sub sp, sp, #%d\\n\\t\", framesize);\n+}\n+#endif\n+\n+void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+  assert_cond(ra_ != NULL);\n+  Compile* C = ra_->C;\n+  MacroAssembler _masm(&cbuf);\n+\n+  \/\/ n.b. frame size includes space for return pc and fp\n+  const int framesize = C->frame_size_in_bytes();\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  __ nop();\n+\n+  assert_cond(C != NULL);\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize)) {\n+    __ generate_stack_overflow_check(bangsize);\n+  }\n+\n+  __ build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+\n+  C->set_frame_complete(cbuf.insts_size());\n+\n+  if (C->has_mach_constant_base_node()) {\n+    \/\/ NOTE: We set the table base offset here because users might be\n+    \/\/ emitted before MachConstantBaseNode.\n+    Compile::ConstantTable& constant_table = C->constant_table();\n+    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n+  }\n+}\n+\n+uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n+{\n+  assert_cond(ra_ != NULL);\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+int MachPrologNode::reloc() const\n+{\n+  return 0;\n+}\n+\n+\/\/=============================================================================\n+\n+#ifndef PRODUCT\n+void MachEpilogNode::format(PhaseRegAlloc *ra_, outputStream *st) const {\n+  assert_cond(st != NULL && ra_ != NULL);\n+  Compile* C = ra_->C;\n+  assert_cond(C != NULL);\n+  int framesize = C->frame_size_in_bytes();\n+\n+  st->print(\"# pop frame %d\\n\\t\", framesize);\n+\n+  if (framesize == 0) {\n+    st->print(\"ld  ra, [sp,#%d]\\n\\t\", (2 * wordSize));\n+    st->print(\"ld  fp, [sp,#%d]\\n\\t\", (3 * wordSize));\n+    st->print(\"add sp, sp, #%d\\n\\t\", (2 * wordSize));\n+  } else {\n+    st->print(\"add  sp, sp, #%d\\n\\t\", framesize);\n+    st->print(\"ld  ra, [sp,#%d]\\n\\t\", - 2 * wordSize);\n+    st->print(\"ld  fp, [sp,#%d]\\n\\t\", - wordSize);\n+  }\n+\n+  if (do_polling() && C->is_method_compilation()) {\n+    st->print(\"# touch polling page\\n\\t\");\n+    st->print(\"li  t0, #0x%lx\\n\\t\", p2i(os::get_polling_page()));\n+    st->print(\"ld  zr, [t0]\");\n+  }\n+}\n+#endif\n+\n+void MachEpilogNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+  assert_cond(ra_ != NULL);\n+  Compile* C = ra_->C;\n+  MacroAssembler _masm(&cbuf);\n+  assert_cond(C != NULL);\n+  int framesize = C->frame_size_in_bytes();\n+\n+  __ remove_frame(framesize);\n+\n+  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n+    __ reserved_stack_check();\n+  }\n+\n+  if (do_polling() && C->is_method_compilation()) {\n+    __ read_polling_page(t0, os::get_polling_page(), relocInfo::poll_return_type);\n+  }\n+}\n+\n+uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n+  assert_cond(ra_ != NULL);\n+  \/\/ Variable size. Determine dynamically.\n+  return MachNode::size(ra_);\n+}\n+\n+int MachEpilogNode::reloc() const {\n+  \/\/ Return number of relocatable values contained in this instruction.\n+  return 1; \/\/ 1 for polling page.\n+}\n+const Pipeline * MachEpilogNode::pipeline() const {\n+  return MachNode::pipeline_class();\n+}\n+\n+\/\/ This method seems to be obsolete. It is declared in machnode.hpp\n+\/\/ and defined in all *.ad files, but it is never called. Should we\n+\/\/ get rid of it?\n+int MachEpilogNode::safepoint_offset() const {\n+  assert(do_polling(), \"no return for this epilog node\");\n+  return 4;\n+}\n+\n+\/\/=============================================================================\n+\n+\/\/ Figure out which register class each belongs in: rc_int, rc_float or\n+\/\/ rc_stack.\n+enum RC { rc_bad, rc_int, rc_float, rc_stack };\n+\n+static enum RC rc_class(OptoReg::Name reg) {\n+\n+  if (reg == OptoReg::Bad) {\n+    return rc_bad;\n+  }\n+\n+  \/\/ we have 30 int registers * 2 halves\n+  \/\/ (t0 and t1 are omitted)\n+  int slots_of_int_registers = RegisterImpl::max_slots_per_register * (RegisterImpl::number_of_registers - 2);\n+  if (reg < slots_of_int_registers) {\n+    return rc_int;\n+  }\n+\n+  \/\/ we have 32 float register * 2 halves\n+  int slots_of_float_registers = FloatRegisterImpl::max_slots_per_register * FloatRegisterImpl::number_of_registers;\n+  if (reg < slots_of_int_registers + slots_of_float_registers) {\n+    return rc_float;\n+  }\n+\n+  \/\/ Between float regs & stack is the flags regs.\n+  assert(OptoReg::is_stack(reg), \"blow up if spilling flags\");\n+\n+  return rc_stack;\n+}\n+\n+uint MachSpillCopyNode::implementation(CodeBuffer *cbuf, PhaseRegAlloc *ra_, bool do_size, outputStream *st) const {\n+  assert_cond(ra_ != NULL);\n+  Compile* C = ra_->C;\n+\n+  \/\/ Get registers to move.\n+  OptoReg::Name src_hi = ra_->get_reg_second(in(1));\n+  OptoReg::Name src_lo = ra_->get_reg_first(in(1));\n+  OptoReg::Name dst_hi = ra_->get_reg_second(this);\n+  OptoReg::Name dst_lo = ra_->get_reg_first(this);\n+\n+  enum RC src_hi_rc = rc_class(src_hi);\n+  enum RC src_lo_rc = rc_class(src_lo);\n+  enum RC dst_hi_rc = rc_class(dst_hi);\n+  enum RC dst_lo_rc = rc_class(dst_lo);\n+\n+  assert(src_lo != OptoReg::Bad && dst_lo != OptoReg::Bad, \"must move at least 1 register\");\n+\n+  if (src_hi != OptoReg::Bad) {\n+    assert((src_lo & 1) == 0 && src_lo + 1 == src_hi &&\n+           (dst_lo & 1) == 0 && dst_lo + 1 == dst_hi,\n+           \"expected aligned-adjacent pairs\");\n+  }\n+\n+  if (src_lo == dst_lo && src_hi == dst_hi) {\n+    return 0;            \/\/ Self copy, no move.\n+  }\n+\n+  bool is64 = (src_lo & 1) == 0 && src_lo + 1 == src_hi &&\n+              (dst_lo & 1) == 0 && dst_lo + 1 == dst_hi;\n+  int src_offset = ra_->reg2offset(src_lo);\n+  int dst_offset = ra_->reg2offset(dst_lo);\n+\n+  if (cbuf != NULL) {\n+    MacroAssembler _masm(cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    switch (src_lo_rc) {\n+      case rc_int:\n+        if (dst_lo_rc == rc_int) {  \/\/ gpr --> gpr copy\n+          if (!is64 && this->ideal_reg() != Op_RegI) { \/\/ zero extended for narrow oop or klass\n+            __ zero_extend(as_Register(Matcher::_regEncode[dst_lo]), as_Register(Matcher::_regEncode[src_lo]), 32);\n+          } else {\n+            __ mv(as_Register(Matcher::_regEncode[dst_lo]), as_Register(Matcher::_regEncode[src_lo]));\n+          }\n+        } else if (dst_lo_rc == rc_float) { \/\/ gpr --> fpr copy\n+          if (is64) {\n+            __ fmv_d_x(as_FloatRegister(Matcher::_regEncode[dst_lo]),\n+                       as_Register(Matcher::_regEncode[src_lo]));\n+          } else {\n+            __ fmv_w_x(as_FloatRegister(Matcher::_regEncode[dst_lo]),\n+                       as_Register(Matcher::_regEncode[src_lo]));\n+          }\n+        } else {                    \/\/ gpr --> stack spill\n+          assert(dst_lo_rc == rc_stack, \"spill to bad register class\");\n+          __ spill(as_Register(Matcher::_regEncode[src_lo]), is64, dst_offset);\n+        }\n+        break;\n+      case rc_float:\n+        if (dst_lo_rc == rc_int) {  \/\/ fpr --> gpr copy\n+          if (is64) {\n+            __ fmv_x_d(as_Register(Matcher::_regEncode[dst_lo]),\n+                       as_FloatRegister(Matcher::_regEncode[src_lo]));\n+          } else {\n+            __ fmv_x_w(as_Register(Matcher::_regEncode[dst_lo]),\n+                       as_FloatRegister(Matcher::_regEncode[src_lo]));\n+          }\n+        } else if (dst_lo_rc == rc_float) { \/\/ fpr --> fpr copy\n+          if (is64) {\n+            __ fmv_d(as_FloatRegister(Matcher::_regEncode[dst_lo]),\n+                     as_FloatRegister(Matcher::_regEncode[src_lo]));\n+          } else {\n+            __ fmv_s(as_FloatRegister(Matcher::_regEncode[dst_lo]),\n+                     as_FloatRegister(Matcher::_regEncode[src_lo]));\n+          }\n+        } else {                    \/\/ fpr --> stack spill\n+          assert(dst_lo_rc == rc_stack, \"spill to bad register class\");\n+          __ spill(as_FloatRegister(Matcher::_regEncode[src_lo]),\n+                   is64, dst_offset);\n+        }\n+        break;\n+      case rc_stack:\n+        if (dst_lo_rc == rc_int) {  \/\/ stack --> gpr load\n+          if (this->ideal_reg() == Op_RegI) {\n+            __ unspill(as_Register(Matcher::_regEncode[dst_lo]), is64, src_offset);\n+          } else { \/\/ \/\/ zero extended for narrow oop or klass\n+            __ unspillu(as_Register(Matcher::_regEncode[dst_lo]), is64, src_offset);\n+          }\n+        } else if (dst_lo_rc == rc_float) { \/\/ stack --> fpr load\n+          __ unspill(as_FloatRegister(Matcher::_regEncode[dst_lo]),\n+                     is64, src_offset);\n+        } else {                    \/\/ stack --> stack copy\n+          assert(dst_lo_rc == rc_stack, \"spill to bad register class\");\n+          if (this->ideal_reg() == Op_RegI) {\n+            __ unspill(t0, is64, src_offset);\n+          } else { \/\/ zero extended for narrow oop or klass\n+            __ unspillu(t0, is64, src_offset);\n+          }\n+          __ spill(t0, is64, dst_offset);\n+        }\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  if (st != NULL) {\n+    st->print(\"spill \");\n+    if (src_lo_rc == rc_stack) {\n+      st->print(\"[sp, #%d] -> \", src_offset);\n+    } else {\n+      st->print(\"%s -> \", Matcher::regName[src_lo]);\n+    }\n+    if (dst_lo_rc == rc_stack) {\n+      st->print(\"[sp, #%d]\", dst_offset);\n+    } else {\n+      st->print(\"%s\", Matcher::regName[dst_lo]);\n+    }\n+    st->print(\"\\t# spill size = %d\", is64 ? 64 : 32);\n+  }\n+\n+  return 0;\n+}\n+\n+#ifndef PRODUCT\n+void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream *st) const {\n+  if (ra_ == NULL) {\n+    st->print(\"N%d = SpillCopy(N%d)\", _idx, in(1)->_idx);\n+  } else {\n+    implementation(NULL, ra_, false, st);\n+  }\n+}\n+#endif\n+\n+void MachSpillCopyNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+  implementation(&cbuf, ra_, false, NULL);\n+}\n+\n+uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n+  return MachNode::size(ra_);\n+}\n+\n+\/\/=============================================================================\n+\n+#ifndef PRODUCT\n+void BoxLockNode::format(PhaseRegAlloc *ra_, outputStream *st) const {\n+  assert_cond(ra_ != NULL && st != NULL);\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_reg_first(this);\n+  st->print(\"add %s, sp, #%d\\t# box lock\",\n+            Matcher::regName[reg], offset);\n+}\n+#endif\n+\n+void BoxLockNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+  MacroAssembler _masm(&cbuf);\n+\n+  assert_cond(ra_ != NULL);\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg    = ra_->get_encode(this);\n+\n+  if (is_imm_in_range(offset, 12, 0)) {\n+    __ addi(as_Register(reg), sp, offset);\n+  } else if (is_imm_in_range(offset, 32, 0)) {\n+    __ li32(t0, offset);\n+    __ add(as_Register(reg), sp, t0);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+uint BoxLockNode::size(PhaseRegAlloc *ra_) const {\n+  \/\/ BoxLockNode is not a MachNode, so we can't just call MachNode::size(ra_).\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+\n+  if (is_imm_in_range(offset, 12, 0)) {\n+    return NativeInstruction::instruction_size;\n+  } else {\n+    return 3 * NativeInstruction::instruction_size; \/\/ lui + addiw + add;\n+  }\n+}\n+\n+\/\/=============================================================================\n+\n+#ifndef PRODUCT\n+void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  assert_cond(st != NULL);\n+  st->print_cr(\"# MachUEPNode\");\n+  if (UseCompressedClassPointers) {\n+    st->print_cr(\"\\tlwu t0, [j_rarg0, oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    if (Universe::narrow_klass_shift() != 0) {\n+      st->print_cr(\"\\tdecode_klass_not_null t0, t0\");\n+    }\n+  } else {\n+    st->print_cr(\"\\tld t0, [j_rarg0, oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+  }\n+  st->print_cr(\"\\tbeq t0, t1, ic_hit\");\n+  st->print_cr(\"\\tj, SharedRuntime::_ic_miss_stub\\t # Inline cache check\");\n+  st->print_cr(\"\\tic_hit:\");\n+}\n+#endif\n+\n+void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  \/\/ This is the unverified entry point.\n+  MacroAssembler _masm(&cbuf);\n+\n+  Label skip;\n+  __ cmp_klass(j_rarg0, t1, t0, skip);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  __ bind(skip);\n+}\n+\n+uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  assert_cond(ra_ != NULL);\n+  return MachNode::size(ra_);\n+}\n+\n+\/\/ REQUIRED EMIT CODE\n+\n+\/\/=============================================================================\n+\n+\/\/ Emit exception handler code.\n+int HandlerImpl::emit_exception_handler(CodeBuffer& cbuf)\n+{\n+  \/\/ la_patchable t0, #exception_blob_entry_point\n+  \/\/ jr (offset)t0\n+  \/\/ or\n+  \/\/ j #exception_blob_entry_point\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  MacroAssembler _masm(&cbuf);\n+  address base = __ start_a_stub(size_exception_handler());\n+  if (base == NULL) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+  __ far_jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n+  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n+  __ end_a_stub();\n+  return offset;\n+}\n+\n+\/\/ Emit deopt handler code.\n+int HandlerImpl::emit_deopt_handler(CodeBuffer& cbuf)\n+{\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  MacroAssembler _masm(&cbuf);\n+  address base = __ start_a_stub(size_deopt_handler());\n+  if (base == NULL) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+\n+  __ auipc(ra, 0);\n+  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow\");\n+  __ end_a_stub();\n+  return offset;\n+\n+}\n+\/\/ REQUIRED MATCHER CODE\n+\n+\/\/=============================================================================\n+\n+const bool Matcher::match_rule_supported(int opcode) {\n+  if (!has_match_rule(opcode)) {\n+    return false;\n+  }\n+\n+  switch (opcode) {\n+    case Op_PopCountI:\n+    case Op_PopCountL:\n+      return UsePopCountInstruction;\n+\n+    case Op_CountLeadingZerosI:\n+    case Op_CountLeadingZerosL:\n+    case Op_CountTrailingZerosI:\n+    case Op_CountTrailingZerosL:\n+      return UseRVB;\n+  }\n+\n+  return true; \/\/ Per default match rules are supported.\n+}\n+\n+\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n+\/\/ other intrinsics guarded with vector length (vlen).\n+const bool Matcher::match_rule_supported_vector(int opcode, int vlen) {\n+  return false;\n+}\n+\n+const bool Matcher::has_predicated_vectors(void) {\n+  return false;\n+}\n+\n+const int Matcher::float_pressure(int default_pressure_threshold) {\n+  return default_pressure_threshold;\n+}\n+\n+int Matcher::regnum_to_fpu_offset(int regnum)\n+{\n+  Unimplemented();\n+  return 0;\n+}\n+\n+\/\/ Is this branch offset short enough that a short branch can be used?\n+\/\/\n+\/\/ NOTE: If the platform does not provide any short branch variants, then\n+\/\/       this method should return false for offset 0.\n+\/\/ |---label(L1)-----|\n+\/\/ |-----------------|\n+\/\/ |-----------------|----------eq: float-------------------\n+\/\/ |-----------------| \/\/ far_cmpD_branch   |   cmpD_branch\n+\/\/ |------- ---------|    feq;              |      feq;\n+\/\/ |-far_cmpD_branch-|    beqz done;        |      bnez L;\n+\/\/ |-----------------|    j L;              |\n+\/\/ |-----------------|    bind(done);       |\n+\/\/ |-----------------|--------------------------------------\n+\/\/ |-----------------| \/\/ so shortBrSize = br_size - 4;\n+\/\/ |-----------------| \/\/ so offs = offset - shortBrSize + 4;\n+\/\/ |---label(L2)-----|\n+bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n+  \/\/ The passed offset is relative to address of the branch.\n+  int shortBrSize = br_size - 4;\n+  int offs = offset - shortBrSize + 4;\n+  return (-4096 <= offs && offs < 4096);\n+}\n+\n+const bool Matcher::isSimpleConstant64(jlong value) {\n+  \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n+  \/\/ Probably always true, even if a temp register is required.\n+  return true;\n+}\n+\n+\/\/ true just means we have fast l2f conversion\n+const bool Matcher::convL2FSupported(void) {\n+  return true;\n+}\n+\n+\/\/ Vector width in bytes.\n+const int Matcher::vector_width_in_bytes(BasicType bt) {\n+  return 0;\n+}\n+\n+\/\/ Limits on vector size (number of elements) loaded into vector.\n+const int Matcher::max_vector_size(const BasicType bt) {\n+  return vector_width_in_bytes(bt) \/ type2aelembytes(bt);\n+}\n+const int Matcher::min_vector_size(const BasicType bt) {\n+  return max_vector_size(bt);\n+}\n+\n+\/\/ Vector ideal reg.\n+const uint Matcher::vector_ideal_reg(int len) {\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+const uint Matcher::vector_shift_count_ideal_reg(int size) {\n+  fatal(\"vector shift is not supported\");\n+  return Node::NotAMachineReg;\n+}\n+\n+\/\/ AES support not yet implemented\n+const bool Matcher::pass_original_key_for_aes() {\n+  return false;\n+}\n+\n+\/\/ RISC-V supports misaligned vectors store\/load.\n+const bool Matcher::misaligned_vectors_ok() {\n+  return true;\n+}\n+\n+\/\/ false => size gets scaled to BytesPerLong, ok.\n+const bool Matcher::init_array_count_is_in_bytes = false;\n+\n+\/\/ Use conditional move (CMOVL)\n+const int Matcher::long_cmove_cost() {\n+  \/\/ long cmoves are no more expensive than int cmoves\n+  return 0;\n+}\n+\n+const int Matcher::float_cmove_cost() {\n+  \/\/ float cmoves are no more expensive than int cmoves\n+  return 0;\n+}\n+\n+\/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n+const bool Matcher::require_postalloc_expand = false;\n+\n+\/\/ Do we need to mask the count passed to shift instructions or does\n+\/\/ the cpu only look at the lower 5\/6 bits anyway?\n+const bool Matcher::need_masked_shift_count = false;\n+\n+\/\/ This affects two different things:\n+\/\/  - how Decode nodes are matched\n+\/\/  - how ImplicitNullCheck opportunities are recognized\n+\/\/ If true, the matcher will try to remove all Decodes and match them\n+\/\/ (as operands) into nodes. NullChecks are not prepared to deal with\n+\/\/ Decodes by final_graph_reshaping().\n+\/\/ If false, final_graph_reshaping() forces the decode behind the Cmp\n+\/\/ for a NullCheck. The matcher matches the Decode node into a register.\n+\/\/ Implicit_null_check optimization moves the Decode along with the\n+\/\/ memory operation back up before the NullCheck.\n+bool Matcher::narrow_oop_use_complex_address() {\n+  return Universe::narrow_oop_shift() == 0;\n+}\n+\n+bool Matcher::narrow_klass_use_complex_address() {\n+\/\/ TODO\n+\/\/ decide whether we need to set this to true\n+  return false;\n+}\n+\n+bool Matcher::const_oop_prefer_decode() {\n+  \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n+  return Universe::narrow_oop_base() == NULL;\n+}\n+\n+bool Matcher::const_klass_prefer_decode() {\n+  \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n+  return Universe::narrow_klass_base() == NULL;\n+}\n+\n+\/\/ Is it better to copy float constants, or load them directly from\n+\/\/ memory?  Intel can load a float constant from a direct address,\n+\/\/ requiring no extra registers.  Most RISCs will have to materialize\n+\/\/ an address into a register first, so they would do better to copy\n+\/\/ the constant from stack.\n+const bool Matcher::rematerialize_float_constants = false;\n+\n+\/\/ If CPU can load and store mis-aligned doubles directly then no\n+\/\/ fixup is needed.  Else we split the double into 2 integer pieces\n+\/\/ and move it piece-by-piece.  Only happens when passing doubles into\n+\/\/ C code as the Java calling convention forces doubles to be aligned.\n+const bool Matcher::misaligned_doubles_ok = true;\n+\n+\/\/ No-op on amd64\n+void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n+  Unimplemented();\n+}\n+\n+\/\/ Advertise here if the CPU requires explicit rounding operations to\n+\/\/ implement the UseStrictFP mode.\n+const bool Matcher::strict_fp_requires_explicit_rounding = false;\n+\n+\/\/ Are floats converted to double when stored to stack during\n+\/\/ deoptimization?\n+bool Matcher::float_in_double() { return false; }\n+\n+\/\/ Do ints take an entire long register or just half?\n+\/\/ The relevant question is how the int is callee-saved:\n+\/\/ the whole long is written but de-opt'ing will have to extract\n+\/\/ the relevant 32 bits.\n+const bool Matcher::int_in_long = true;\n+\n+\/\/ Return whether or not this register is ever used as an argument.\n+\/\/ This function is used on startup to build the trampoline stubs in\n+\/\/ generateOptoStub.  Registers not mentioned will be killed by the VM\n+\/\/ call in the trampoline, and arguments in those registers not be\n+\/\/ available to the callee.\n+bool Matcher::can_be_java_arg(int reg)\n+{\n+  return\n+    reg ==  R10_num || reg == R10_H_num ||\n+    reg ==  R11_num || reg == R11_H_num ||\n+    reg ==  R12_num || reg == R12_H_num ||\n+    reg ==  R13_num || reg == R13_H_num ||\n+    reg ==  R14_num || reg == R14_H_num ||\n+    reg ==  R15_num || reg == R15_H_num ||\n+    reg ==  R16_num || reg == R16_H_num ||\n+    reg ==  R17_num || reg == R17_H_num ||\n+    reg ==  F10_num || reg == F10_H_num ||\n+    reg ==  F11_num || reg == F11_H_num ||\n+    reg ==  F12_num || reg == F12_H_num ||\n+    reg ==  F13_num || reg == F13_H_num ||\n+    reg ==  F14_num || reg == F14_H_num ||\n+    reg ==  F15_num || reg == F15_H_num ||\n+    reg ==  F16_num || reg == F16_H_num ||\n+    reg ==  F17_num || reg == F17_H_num;\n+}\n+\n+bool Matcher::is_spillable_arg(int reg)\n+{\n+  return can_be_java_arg(reg);\n+}\n+\n+bool Matcher::use_asm_for_ldiv_by_con(jlong divisor) {\n+  return false;\n+}\n+\n+RegMask Matcher::divI_proj_mask() {\n+  ShouldNotReachHere();\n+  return RegMask();\n+}\n+\n+\/\/ Register for MODI projection of divmodI.\n+RegMask Matcher::modI_proj_mask() {\n+  ShouldNotReachHere();\n+  return RegMask();\n+}\n+\n+\/\/ Register for DIVL projection of divmodL.\n+RegMask Matcher::divL_proj_mask() {\n+  ShouldNotReachHere();\n+  return RegMask();\n+}\n+\n+\/\/ Register for MODL projection of divmodL.\n+RegMask Matcher::modL_proj_mask() {\n+  ShouldNotReachHere();\n+  return RegMask();\n+}\n+\n+const RegMask Matcher::method_handle_invoke_SP_save_mask() {\n+  return FP_REG_mask();\n+}\n+\n+bool size_fits_all_mem_uses(AddPNode* addp, int shift) {\n+  assert_cond(addp != NULL);\n+  for (DUIterator_Fast imax, i = addp->fast_outs(imax); i < imax; i++) {\n+    Node* u = addp->fast_out(i);\n+    if (u != NULL && u->is_Mem()) {\n+      int opsize = u->as_Mem()->memory_size();\n+      assert(opsize > 0, \"unexpected memory operand size\");\n+      if (u->as_Mem()->memory_size() != (1 << shift)) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n+const bool Matcher::convi2l_type_required = false;\n+\n+\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n+\/\/ to be subsumed into complex addressing expressions or compute them\n+\/\/ into registers?\n+bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  return clone_base_plus_offset_address(m, mstack, address_visited);\n+}\n+\n+void Compile::reshape_address(AddPNode* addp) {\n+}\n+\n+%}\n+\n+\n+\n+\/\/----------ENCODING BLOCK-----------------------------------------------------\n+\/\/ This block specifies the encoding classes used by the compiler to\n+\/\/ output byte streams.  Encoding classes are parameterized macros\n+\/\/ used by Machine Instruction Nodes in order to generate the bit\n+\/\/ encoding of the instruction.  Operands specify their base encoding\n+\/\/ interface with the interface keyword.  There are currently\n+\/\/ supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &\n+\/\/ COND_INTER.  REG_INTER causes an operand to generate a function\n+\/\/ which returns its register number when queried.  CONST_INTER causes\n+\/\/ an operand to generate a function which returns the value of the\n+\/\/ constant when queried.  MEMORY_INTER causes an operand to generate\n+\/\/ four functions which return the Base Register, the Index Register,\n+\/\/ the Scale Value, and the Offset Value of the operand when queried.\n+\/\/ COND_INTER causes an operand to generate six functions which return\n+\/\/ the encoding code (ie - encoding bits for the instruction)\n+\/\/ associated with each basic boolean condition for a conditional\n+\/\/ instruction.\n+\/\/\n+\/\/ Instructions specify two basic values for encoding.  Again, a\n+\/\/ function is available to check if the constant displacement is an\n+\/\/ oop. They use the ins_encode keyword to specify their encoding\n+\/\/ classes (which must be a sequence of enc_class names, and their\n+\/\/ parameters, specified in the encoding block), and they use the\n+\/\/ opcode keyword to specify, in order, their primary, secondary, and\n+\/\/ tertiary opcode.  Only the opcode sections which a particular\n+\/\/ instruction needs for encoding need to be specified.\n+encode %{\n+  \/\/ BEGIN Non-volatile memory access\n+\n+  enc_class riscv_enc_li_imm(iRegIorL dst, immIorL src) %{\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    int64_t con = (int64_t)$src$$constant;\n+    Register dst_reg = as_Register($dst$$reg);\n+    __ li(dst_reg, con);\n+  %}\n+\n+  enc_class riscv_enc_mov_p(iRegP dst, immP src) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    address con = (address)$src$$constant;\n+    if (con == NULL || con == (address)1) {\n+      ShouldNotReachHere();\n+    } else {\n+      relocInfo::relocType rtype = $src->constant_reloc();\n+      if (rtype == relocInfo::oop_type) {\n+        __ movoop(dst_reg, (jobject)con, \/*immediate*\/true);\n+      } else if (rtype == relocInfo::metadata_type) {\n+        __ mov_metadata(dst_reg, (Metadata*)con);\n+      } else {\n+        assert(rtype == relocInfo::none, \"unexpected reloc type\");\n+        __ li(dst_reg, $src$$constant);\n+      }\n+    }\n+  %}\n+\n+  enc_class riscv_enc_mov_p1(iRegP dst) %{\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    Register dst_reg = as_Register($dst$$reg);\n+    __ li(dst_reg, 1);\n+  %}\n+\n+  enc_class riscv_enc_mov_poll_page(iRegP dst, immPollPage src) %{\n+    MacroAssembler _masm(&cbuf);\n+    int32_t offset = 0;\n+    address page = (address)$src$$constant;\n+    unsigned long align = (unsigned long)page & 0xfff;\n+    assert(align == 0, \"polling page must be page aligned\");\n+    Register dst_reg = as_Register($dst$$reg);\n+    __ la_patchable(dst_reg, Address(page, relocInfo::poll_type), offset);\n+    __ addi(dst_reg, dst_reg, offset);\n+  %}\n+\n+  enc_class riscv_enc_mov_byte_map_base(iRegP dst) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ load_byte_map_base($dst$$Register);\n+  %}\n+\n+  enc_class riscv_enc_mov_n(iRegN dst, immN src) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    address con = (address)$src$$constant;\n+    if (con == NULL) {\n+      ShouldNotReachHere();\n+    } else {\n+      relocInfo::relocType rtype = $src->constant_reloc();\n+      assert(rtype == relocInfo::oop_type, \"unexpected reloc type\");\n+      __ set_narrow_oop(dst_reg, (jobject)con);\n+    }\n+  %}\n+\n+  enc_class riscv_enc_mov_zero(iRegNorP dst) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    __ mv(dst_reg, zr);\n+  %}\n+\n+  enc_class riscv_enc_mov_nk(iRegN dst, immNKlass src) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    address con = (address)$src$$constant;\n+    if (con == NULL) {\n+      ShouldNotReachHere();\n+    } else {\n+      relocInfo::relocType rtype = $src->constant_reloc();\n+      assert(rtype == relocInfo::metadata_type, \"unexpected reloc type\");\n+      __ set_narrow_klass(dst_reg, (Klass *)con);\n+    }\n+  %}\n+\n+  enc_class riscv_enc_cmpxchgw(iRegINoSp res, memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  enc_class riscv_enc_cmpxchgn(iRegINoSp res, memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  enc_class riscv_enc_cmpxchg(iRegINoSp res, memory mem, iRegLNoSp oldval, iRegLNoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  enc_class riscv_enc_cmpxchgw_acq(iRegINoSp res, memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  enc_class riscv_enc_cmpxchgn_acq(iRegINoSp res, memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  enc_class riscv_enc_cmpxchg_acq(iRegINoSp res, memory mem, iRegLNoSp oldval, iRegLNoSp newval) %{\n+    MacroAssembler _masm(&cbuf);\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+  %}\n+\n+  \/\/ compare and branch instruction encodings\n+\n+  enc_class riscv_enc_j(label lbl) %{\n+    MacroAssembler _masm(&cbuf);\n+    Label* L = $lbl$$label;\n+    __ j(*L);\n+  %}\n+\n+  enc_class riscv_enc_far_cmpULtGe_imm0_branch(cmpOpULtGe cmp, iRegIorL op1, label lbl) %{\n+    MacroAssembler _masm(&cbuf);\n+    Label* L = $lbl$$label;\n+    switch ($cmp$$cmpcode) {\n+      case(BoolTest::ge):\n+        __ j(*L);\n+        break;\n+      case(BoolTest::lt):\n+        break;\n+      default:\n+        Unimplemented();\n+    }\n+  %}\n+\n+  \/\/ call instruction encodings\n+\n+  enc_class riscv_enc_partial_subtype_check(iRegP sub, iRegP super, iRegP temp, iRegP result) %{\n+    Register sub_reg = as_Register($sub$$reg);\n+    Register super_reg = as_Register($super$$reg);\n+    Register temp_reg = as_Register($temp$$reg);\n+    Register result_reg = as_Register($result$$reg);\n+    Register cr_reg = t1;\n+\n+    Label miss;\n+    Label done;\n+    MacroAssembler _masm(&cbuf);\n+    __ check_klass_subtype_slow_path(sub_reg, super_reg, temp_reg, result_reg,\n+                                     NULL, &miss);\n+    if ($primary) {\n+      __ mv(result_reg, zr);\n+    } else {\n+      __ mv(cr_reg, zr);\n+      __ j(done);\n+    }\n+\n+    __ bind(miss);\n+    if (!$primary) {\n+      __ li(cr_reg, 1);\n+    }\n+\n+    __ bind(done);\n+  %}\n+\n+  enc_class riscv_enc_java_static_call(method meth) %{\n+    MacroAssembler _masm(&cbuf);\n+\n+    address addr = (address)$meth$$method;\n+    address call = NULL;\n+    assert_cond(addr != NULL);\n+    if (!_method) {\n+      \/\/ A call to a runtime wrapper, e.g. new, new_typeArray_Java, uncommon_trap.\n+      call = __ trampoline_call(Address(addr, relocInfo::runtime_call_type), &cbuf);\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n+    } else {\n+      int method_index = resolved_method_index(cbuf);\n+      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n+                                                  : static_call_Relocation::spec(method_index);\n+      call = __ trampoline_call(Address(addr, rspec), &cbuf);\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n+\n+      \/\/ Emit stub for static call\n+      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+      if (stub == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n+    }\n+  %}\n+\n+  enc_class riscv_enc_java_dynamic_call(method meth) %{\n+    MacroAssembler _masm(&cbuf);\n+    int method_index = resolved_method_index(cbuf);\n+    address call = __ ic_call((address)$meth$$method, method_index);\n+    if (call == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n+\n+  enc_class riscv_enc_call_epilog() %{\n+    MacroAssembler _masm(&cbuf);\n+    if (VerifyStackAtCalls) {\n+      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n+      __ call_Unimplemented();\n+    }\n+  %}\n+\n+  enc_class riscv_enc_java_to_runtime(method meth) %{\n+    MacroAssembler _masm(&cbuf);\n+\n+    \/\/ some calls to generated routines (arraycopy code) are scheduled\n+    \/\/ by C2 as runtime calls. if so we can call them using a jr (they\n+    \/\/ will be in a reachable segment) otherwise we have to use a jalr\n+    \/\/ which loads the absolute address into a register.\n+    address entry = (address)$meth$$method;\n+    CodeBlob *cb = CodeCache::find_blob(entry);\n+    if (cb != NULL) {\n+      address call = __ trampoline_call(Address(entry, relocInfo::runtime_call_type));\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n+    } else {\n+      Label retaddr;\n+      __ la(t1, retaddr);\n+      __ la(t0, RuntimeAddress(entry));\n+      \/\/ Leave a breadcrumb for JavaFrameAnchor::capture_last_Java_pc()\n+      __ addi(sp, sp, -2 * wordSize);\n+      __ sd(t1, Address(sp, wordSize));\n+      __ jalr(t0);\n+      __ bind(retaddr);\n+      __ addi(sp, sp, 2 * wordSize);\n+    }\n+  %}\n+\n+  \/\/ using the cr register as the bool result: 0 for success; others failed.\n+  enc_class riscv_enc_fast_lock(iRegP object, iRegP box, iRegP tmp1, iRegP tmp2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register flag = t1;\n+    Register oop = as_Register($object$$reg);\n+    Register box = as_Register($box$$reg);\n+    Register disp_hdr = as_Register($tmp1$$reg);\n+    Register tmp = as_Register($tmp2$$reg);\n+    Label cont;\n+    Label object_has_monitor;\n+\n+    assert_different_registers(oop, box, tmp, disp_hdr, t0);\n+\n+    \/\/ Load markWord from object into displaced_header.\n+    __ ld(disp_hdr, Address(oop, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Always do locking in runtime.\n+    if (EmitSync & 0x01) {\n+      __ mv(flag, 1);\n+      return;\n+    }\n+\n+    if (UseBiasedLocking && !UseOptoBiasInlining) {\n+      __ biased_locking_enter(box, oop, disp_hdr, tmp, true, cont, \/*slow_case*\/NULL, NULL, flag);\n+    }\n+\n+    \/\/ Check for existing monitor\n+    if ((EmitSync & 0x02) == 0) {\n+      __ andi(t0, disp_hdr, markOopDesc::monitor_value);\n+      __ bnez(t0, object_has_monitor);\n+    }\n+\n+    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+    __ ori(tmp, disp_hdr, markOopDesc::unlocked_value);\n+\n+    \/\/ Initialize the box. (Must happen before we update the object mark!)\n+    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+    \/\/ Compare object markWord with an unlocked value (tmp) and if\n+    \/\/ equal exchange the stack address of our box with object markWord.\n+    \/\/ On failure disp_hdr contains the possibly locked markWord.\n+    __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+               Assembler::rl, \/*result*\/disp_hdr);\n+    __ mv(flag, zr);\n+    __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+    \/\/ object, will have now locked it will continue at label cont\n+    \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+    \/\/ Check if the owner is self by comparing the value in the\n+    \/\/ markWord of object (disp_hdr) with the stack pointer.\n+    __ sub(disp_hdr, disp_hdr, sp);\n+    __ li(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markOopDesc::lock_mask_in_place));\n+    \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+    \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+    \/\/ recursive lock.\n+    __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+    __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+\n+    if ((EmitSync & 0x02) == 0) {\n+      __ j(cont);\n+\n+      \/\/ Handle existing monitor.\n+      __ bind(object_has_monitor);\n+      \/\/ The object's monitor m is unlocked iff m->owner == NULL,\n+      \/\/ otherwise m->owner may contain a thread or a stack address.\n+      \/\/\n+      \/\/ Try to CAS m->owner from NULL to current thread.\n+      __ add(tmp, disp_hdr, (ObjectMonitor::owner_offset_in_bytes() - markOopDesc::monitor_value));\n+      __ cmpxchg(\/*memory address*\/tmp, \/*expected value*\/zr, \/*new value*\/xthread, Assembler::int64, Assembler::aq,\n+                 Assembler::rl, \/*result*\/flag); \/\/ cas succeeds if flag == zr(expected)\n+\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markOopDesc::monitor_value so use markOopDesc::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markOopDesc::unused_mark());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n+\n+    __ bind(cont);\n+  %}\n+\n+  \/\/ using cr flag to indicate the fast_unlock result: 0 for success; others failed.\n+  enc_class riscv_enc_fast_unlock(iRegP object, iRegP box, iRegP tmp1, iRegP tmp2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register flag = t1;\n+    Register oop = as_Register($object$$reg);\n+    Register box = as_Register($box$$reg);\n+    Register disp_hdr = as_Register($tmp1$$reg);\n+    Register tmp = as_Register($tmp2$$reg);\n+    Label cont;\n+    Label object_has_monitor;\n+\n+    assert_different_registers(oop, box, tmp, disp_hdr, flag);\n+\n+    \/\/ Always do locking in runtime.\n+    if (EmitSync & 0x01) {\n+      __ mv(flag, 1);\n+      return;\n+    }\n+\n+    if (UseBiasedLocking && !UseOptoBiasInlining) {\n+      __ biased_locking_exit(oop, tmp, cont, flag);\n+    }\n+\n+    \/\/ Find the lock address and load the displaced header from the stack.\n+    __ ld(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+    \/\/ If the displaced header is 0, we have a recursive unlock.\n+    __ mv(flag, disp_hdr);\n+    __ beqz(disp_hdr, cont);\n+\n+    \/\/ Handle existing monitor.\n+    if ((EmitSync & 0x02) == 0) {\n+      __ ld(tmp, Address(oop, oopDesc::mark_offset_in_bytes()));\n+      __ andi(t0, disp_hdr, markOopDesc::monitor_value);\n+      __ bnez(t0, object_has_monitor);\n+    }\n+\n+    \/\/ Check if it is still a light weight lock, this is true if we\n+    \/\/ see the stack address of the basicLock in the markWord of the\n+    \/\/ object.\n+\n+    __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+               Assembler::rl, \/*result*\/tmp);\n+    __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+    __ j(cont);\n+\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+    \/\/ Handle existing monitor.\n+    if ((EmitSync & 0x02) == 0) {\n+      __ bind(object_has_monitor);\n+      STATIC_ASSERT(markOopDesc::monitor_value <= INT_MAX);\n+      __ add(tmp, tmp, -(int)markOopDesc::monitor_value); \/\/ monitor\n+      __ ld(flag, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+      __ xorr(flag, flag, xthread); \/\/ Will be 0 if we are the owner.\n+      __ orr(flag, flag, disp_hdr); \/\/ Will be 0 if there are 0 recursions\n+      __ bnez(flag, cont);\n+\n+      __ ld(flag, Address(tmp, ObjectMonitor::EntryList_offset_in_bytes()));\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset_in_bytes()));\n+      __ orr(flag, flag, disp_hdr); \/\/ Will be 0 if both are 0.\n+      __ bnez(flag, cont);\n+      \/\/ need a release store here\n+      __ la(tmp, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+      __ sd(zr, Address(tmp)); \/\/ set unowned\n+    }\n+\n+    __ bind(cont);\n+  %}\n+\n+  \/\/ arithmetic encodings\n+\n+  enc_class riscv_enc_divw(iRegI dst, iRegI src1, iRegI src2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    Register src1_reg = as_Register($src1$$reg);\n+    Register src2_reg = as_Register($src2$$reg);\n+    __ corrected_idivl(dst_reg, src1_reg, src2_reg, false);\n+  %}\n+\n+  enc_class riscv_enc_div(iRegI dst, iRegI src1, iRegI src2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    Register src1_reg = as_Register($src1$$reg);\n+    Register src2_reg = as_Register($src2$$reg);\n+    __ corrected_idivq(dst_reg, src1_reg, src2_reg, false);\n+  %}\n+\n+  enc_class riscv_enc_modw(iRegI dst, iRegI src1, iRegI src2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    Register src1_reg = as_Register($src1$$reg);\n+    Register src2_reg = as_Register($src2$$reg);\n+    __ corrected_idivl(dst_reg, src1_reg, src2_reg, true);\n+  %}\n+\n+  enc_class riscv_enc_mod(iRegI dst, iRegI src1, iRegI src2) %{\n+    MacroAssembler _masm(&cbuf);\n+    Register dst_reg = as_Register($dst$$reg);\n+    Register src1_reg = as_Register($src1$$reg);\n+    Register src2_reg = as_Register($src2$$reg);\n+    __ corrected_idivq(dst_reg, src1_reg, src2_reg, true);\n+  %}\n+\n+  enc_class riscv_enc_tail_call(iRegP jump_target) %{\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    Register target_reg = as_Register($jump_target$$reg);\n+    __ jr(target_reg);\n+  %}\n+\n+  enc_class riscv_enc_tail_jmp(iRegP jump_target) %{\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    Register target_reg = as_Register($jump_target$$reg);\n+    \/\/ exception oop should be in x10\n+    \/\/ ret addr has been popped into ra\n+    \/\/ callee expects it in x13\n+    __ mv(x13, ra);\n+    __ jr(target_reg);\n+  %}\n+\n+  enc_class riscv_enc_rethrow() %{\n+    MacroAssembler _masm(&cbuf);\n+    __ far_jump(RuntimeAddress(OptoRuntime::rethrow_stub()));\n+  %}\n+\n+  enc_class riscv_enc_ret() %{\n+    MacroAssembler _masm(&cbuf);\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ ret();\n+  %}\n+\n+%}\n+\n+\/\/----------FRAME--------------------------------------------------------------\n+\/\/ Definition of frame structure and management information.\n+\/\/\n+\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n+\/\/                             |   (to get allocators register number\n+\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n+\/\/  r   CALLER     |        |\n+\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n+\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n+\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n+\/\/  h     ^        |   in   |  5\n+\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n+\/\/  |     |        |        |  3\n+\/\/  |     |        +--------+\n+\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n+\/\/        |    old |preserve|      Must be even aligned.\n+\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n+\/\/        |        |   in   |  3   area for Intel ret address\n+\/\/     Owned by    |preserve|      Empty on Sparc.\n+\/\/       SELF      +--------+\n+\/\/        |        |  pad2  |  2   pad to align old SP\n+\/\/        |        +--------+  1\n+\/\/        |        | locks  |  0\n+\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n+\/\/        |        |  pad1  | 11   pad to align new SP\n+\/\/        |        +--------+\n+\/\/        |        |        | 10\n+\/\/        |        | spills |  9   spills\n+\/\/        V        |        |  8   (pad0 slot for callee)\n+\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n+\/\/        ^        |  out   |  7\n+\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n+\/\/     Owned by    +--------+\n+\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n+\/\/        |    new |preserve|      Must be even-aligned.\n+\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n+\/\/        |        |        |\n+\/\/\n+\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n+\/\/         known from SELF's arguments and the Java calling convention.\n+\/\/         Region 6-7 is determined per call site.\n+\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n+\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n+\/\/         are owned by the CALLEE.  Holes should not be nessecary in the\n+\/\/         incoming area, as the Java calling convention is completely under\n+\/\/         the control of the AD file.  Doubles can be sorted and packed to\n+\/\/         avoid holes.  Holes in the outgoing arguments may be nessecary for\n+\/\/         varargs C calling conventions.\n+\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n+\/\/         even aligned with pad0 as needed.\n+\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n+\/\/           (the latter is true on Intel but is it false on RISCV?)\n+\/\/         region 6-11 is even aligned; it may be padded out more so that\n+\/\/         the region from SP to FP meets the minimum stack alignment.\n+\/\/ Note 4: For I2C adapters, the incoming FP may not meet the minimum stack\n+\/\/         alignment.  Region 11, pad1, may be dynamically extended so that\n+\/\/         SP meets the minimum alignment.\n+\n+frame %{\n+  \/\/ What direction does stack grow in (assumed to be same for C & Java)\n+  stack_direction(TOWARDS_LOW);\n+\n+  \/\/ These three registers define part of the calling convention\n+  \/\/ between compiled code and the interpreter.\n+\n+  \/\/ Inline Cache Register or methodOop for I2C.\n+  inline_cache_reg(R31);\n+\n+  \/\/ Method Oop Register when calling interpreter.\n+  interpreter_method_oop_reg(R31);\n+\n+  \/\/ Optional: name the operand used by cisc-spilling to access [stack_pointer + offset]\n+  cisc_spilling_operand_name(indOffset);\n+\n+  \/\/ Number of stack slots consumed by locking an object\n+  \/\/ generate Compile::sync_stack_slots\n+  \/\/ VMRegImpl::slots_per_word = wordSize \/ stack_slot_size = 8 \/ 4 = 2\n+  sync_stack_slots(1 * VMRegImpl::slots_per_word);\n+\n+  \/\/ Compiled code's Frame Pointer\n+  frame_pointer(R2);\n+\n+  \/\/ Interpreter stores its frame pointer in a register which is\n+  \/\/ stored to the stack by I2CAdaptors.\n+  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n+  interpreter_frame_pointer(R8);\n+\n+  \/\/ Stack alignment requirement\n+  stack_alignment(StackAlignmentInBytes); \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n+\n+  \/\/ Number of stack slots between incoming argument block and the start of\n+  \/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n+  \/\/ EPILOG must remove this many slots. RISC-V needs two slots for\n+  \/\/ return address and fp.\n+  in_preserve_stack_slots(2 * VMRegImpl::slots_per_word);\n+\n+  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n+  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n+  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes \/ BytesPerInt);\n+\n+  \/\/ The after-PROLOG location of the return address.  Location of\n+  \/\/ return address specifies a type (REG or STACK) and a number\n+  \/\/ representing the register number (i.e. - use a register name) or\n+  \/\/ stack slot.\n+  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n+  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n+  \/\/ TODO this may well be correct but need to check why that - 2 is there\n+  \/\/ ppc port uses 0 but we definitely need to allow for fixed_slots\n+  \/\/ which folds in the space used for monitors\n+  return_addr(STACK - 2 +\n+              align_up((Compile::current()->in_preserve_stack_slots() +\n+                        Compile::current()->fixed_slots()),\n+                       stack_alignment_in_slots()));\n+\n+  \/\/ Body of function which returns an integer array locating\n+  \/\/ arguments either in registers or in stack slots.  Passed an array\n+  \/\/ of ideal registers called \"sig\" and a \"length\" count.  Stack-slot\n+  \/\/ offsets are based on outgoing arguments, i.e. a CALLER setting up\n+  \/\/ arguments for a CALLEE.  Incoming stack arguments are\n+  \/\/ automatically biased by the preserve_stack_slots field above.\n+\n+  calling_convention\n+  %{\n+    \/\/ No difference between ingoing\/outgoing just pass false\n+    SharedRuntime::java_calling_convention(sig_bt, regs, length, false);\n+  %}\n+\n+  c_calling_convention\n+  %{\n+    \/\/ This is obviously always outgoing\n+    (void) SharedRuntime::c_calling_convention(sig_bt, regs, NULL, length);\n+  %}\n+\n+  \/\/ Location of compiled Java return values.  Same as C for now.\n+  return_value\n+  %{\n+    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,\n+           \"only return normal values\");\n+\n+    static const int lo[Op_RegL + 1] = { \/\/ enum name\n+      0,                                 \/\/ Op_Node\n+      0,                                 \/\/ Op_Set\n+      R10_num,                           \/\/ Op_RegN\n+      R10_num,                           \/\/ Op_RegI\n+      R10_num,                           \/\/ Op_RegP\n+      F10_num,                           \/\/ Op_RegF\n+      F10_num,                           \/\/ Op_RegD\n+      R10_num                            \/\/ Op_RegL\n+    };\n+\n+    static const int hi[Op_RegL + 1] = { \/\/ enum name\n+      0,                                 \/\/ Op_Node\n+      0,                                 \/\/ Op_Set\n+      OptoReg::Bad,                      \/\/ Op_RegN\n+      OptoReg::Bad,                      \/\/ Op_RegI\n+      R10_H_num,                         \/\/ Op_RegP\n+      OptoReg::Bad,                      \/\/ Op_RegF\n+      F10_H_num,                         \/\/ Op_RegD\n+      R10_H_num                          \/\/ Op_RegL\n+    };\n+\n+    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);\n+  %}\n+%}\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+\/\/----------Operand Attributes-------------------------------------------------\n+op_attrib op_cost(1);        \/\/ Required cost attribute\n+\n+\/\/----------Instruction Attributes---------------------------------------------\n+ins_attrib ins_cost(DEFAULT_COST); \/\/ Required cost attribute\n+ins_attrib ins_size(32);        \/\/ Required size attribute (in bits)\n+ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction\n+                                \/\/ a non-matching short branch variant\n+                                \/\/ of some long branch?\n+ins_attrib ins_alignment(4);    \/\/ Required alignment attribute (must\n+                                \/\/ be a power of 2) specifies the\n+                                \/\/ alignment that some part of the\n+                                \/\/ instruction (not necessarily the\n+                                \/\/ start) requires.  If > 1, a\n+                                \/\/ compute_padding() function must be\n+                                \/\/ provided for the instruction\n+\n+\/\/----------OPERANDS-----------------------------------------------------------\n+\/\/ Operand definitions must precede instruction definitions for correct parsing\n+\/\/ in the ADLC because operands constitute user defined types which are used in\n+\/\/ instruction definitions.\n+\n+\/\/----------Simple Operands----------------------------------------------------\n+\n+\/\/ Integer operands 32 bit\n+\/\/ 32 bit immediate\n+operand immI()\n+%{\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 32 bit zero\n+operand immI0()\n+%{\n+  predicate(n->get_int() == 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 32 bit unit increment\n+operand immI_1()\n+%{\n+  predicate(n->get_int() == 1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 32 bit unit decrement\n+operand immI_M1()\n+%{\n+  predicate(n->get_int() == -1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Unsigned Integer Immediate:  6-bit int, greater than 32\n+operand uimmI6_ge32() %{\n+  predicate(((unsigned int)(n->get_int()) < 64) && (n->get_int() >= 32));\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_le_4()\n+%{\n+  predicate(n->get_int() <= 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_16()\n+%{\n+  predicate(n->get_int() == 16);\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_24()\n+%{\n+  predicate(n->get_int() == 24);\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_31()\n+%{\n+  predicate(n->get_int() == 31);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_63()\n+%{\n+  predicate(n->get_int() == 63);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 32 bit integer valid for add immediate\n+operand immIAdd()\n+%{\n+  predicate(Assembler::operand_valid_for_add_immediate((int64_t)n->get_int()));\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 32 bit integer valid for sub immediate\n+operand immISub()\n+%{\n+  predicate(Assembler::operand_valid_for_add_immediate(-(int64_t)n->get_int()));\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 5 bit signed value.\n+operand immI5()\n+%{\n+  predicate(n->get_int() <= 15 && n->get_int() >= -16);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 5 bit signed value (simm5)\n+operand immL5()\n+%{\n+  predicate(n->get_long() <= 15 && n->get_long() >= -16);\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Integer operands 64 bit\n+\/\/ 64 bit immediate\n+operand immL()\n+%{\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 64 bit zero\n+operand immL0()\n+%{\n+  predicate(n->get_long() == 0);\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer operands\n+\/\/ Pointer Immediate\n+operand immP()\n+%{\n+  match(ConP);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ NULL Pointer Immediate\n+operand immP0()\n+%{\n+  predicate(n->get_ptr() == 0);\n+  match(ConP);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate One\n+\/\/ this is used in object initialization (initial object header)\n+operand immP_1()\n+%{\n+  predicate(n->get_ptr() == 1);\n+  match(ConP);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Polling Page Pointer Immediate\n+operand immPollPage()\n+%{\n+  predicate((address)n->get_ptr() == os::get_polling_page());\n+  match(ConP);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Card Table Byte Map Base\n+operand immByteMapBase()\n+%{\n+  \/\/ Get base of card map\n+  predicate(BarrierSet::barrier_set()->is_a(BarrierSet::CardTableBarrierSet) &&\n+            (jbyte*)n->get_ptr() == ((CardTableBarrierSet*)(BarrierSet::barrier_set()))->card_table()->byte_map_base());\n+  match(ConP);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate: low 16-bit mask\n+operand immI_16bits()\n+%{\n+  predicate(n->get_int() == 0xFFFF);\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate: low 32-bit mask\n+operand immL_32bits()\n+%{\n+  predicate(n->get_long() == 0xFFFFFFFFL);\n+  match(ConL);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 64 bit unit decrement\n+operand immL_M1()\n+%{\n+  predicate(n->get_long() == -1);\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\n+\/\/ 32 bit offset of pc in thread anchor\n+\n+operand immL_pc_off()\n+%{\n+  predicate(n->get_long() == in_bytes(JavaThread::frame_anchor_offset()) +\n+                             in_bytes(JavaFrameAnchor::last_Java_pc_offset()));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 64 bit integer valid for add immediate\n+operand immLAdd()\n+%{\n+  predicate(Assembler::operand_valid_for_add_immediate(n->get_long()));\n+  match(ConL);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 64 bit integer valid for sub immediate\n+operand immLSub()\n+%{\n+  predicate(Assembler::operand_valid_for_add_immediate(-(n->get_long())));\n+  match(ConL);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Narrow pointer operands\n+\/\/ Narrow Pointer Immediate\n+operand immN()\n+%{\n+  match(ConN);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Narrow NULL Pointer Immediate\n+operand immN0()\n+%{\n+  predicate(n->get_narrowcon() == 0);\n+  match(ConN);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immNKlass()\n+%{\n+  match(ConNKlass);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float and Double operands\n+\/\/ Double Immediate\n+operand immD()\n+%{\n+  match(ConD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate: +0.0d\n+operand immD0()\n+%{\n+  predicate(jlong_cast(n->getd()) == 0);\n+  match(ConD);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate\n+operand immF()\n+%{\n+  match(ConF);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate: +0.0f.\n+operand immF0()\n+%{\n+  predicate(jint_cast(n->getf()) == 0);\n+  match(ConF);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immIOffset()\n+%{\n+  predicate(is_imm_in_range(n->get_int(), 12, 0));\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immLOffset()\n+%{\n+  predicate(is_imm_in_range(n->get_long(), 12, 0));\n+  match(ConL);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Scale values\n+operand immIScale()\n+%{\n+  predicate(1 <= n->get_int() && (n->get_int() <= 3));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Integer 32 bit Register Operands\n+operand iRegI()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg32));\n+  match(RegI);\n+  match(iRegINoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Integer 32 bit Register not Special\n+operand iRegINoSp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_reg32));\n+  match(RegI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register R10 only\n+operand iRegI_R10()\n+%{\n+  constraint(ALLOC_IN_RC(int_r10_reg));\n+  match(RegI);\n+  match(iRegINoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register R12 only\n+operand iRegI_R12()\n+%{\n+  constraint(ALLOC_IN_RC(int_r12_reg));\n+  match(RegI);\n+  match(iRegINoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register R13 only\n+operand iRegI_R13()\n+%{\n+  constraint(ALLOC_IN_RC(int_r13_reg));\n+  match(RegI);\n+  match(iRegINoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register R14 only\n+operand iRegI_R14()\n+%{\n+  constraint(ALLOC_IN_RC(int_r14_reg));\n+  match(RegI);\n+  match(iRegINoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Integer 64 bit Register Operands\n+operand iRegL()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg));\n+  match(RegL);\n+  match(iRegLNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Integer 64 bit Register not Special\n+operand iRegLNoSp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_reg));\n+  match(RegL);\n+  match(iRegL_R10);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Long 64 bit Register R28 only\n+operand iRegL_R28()\n+%{\n+  constraint(ALLOC_IN_RC(r28_reg));\n+  match(RegL);\n+  match(iRegLNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Long 64 bit Register R29 only\n+operand iRegL_R29()\n+%{\n+  constraint(ALLOC_IN_RC(r29_reg));\n+  match(RegL);\n+  match(iRegLNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Long 64 bit Register R30 only\n+operand iRegL_R30()\n+%{\n+  constraint(ALLOC_IN_RC(r30_reg));\n+  match(RegL);\n+  match(iRegLNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer Register Operands\n+\/\/ Pointer Register\n+operand iRegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  match(iRegP_R10);\n+  match(javaThread_RegP);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer 64 bit Register not Special\n+operand iRegPNoSp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_ptr_reg));\n+  match(RegP);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand iRegP_R10()\n+%{\n+  constraint(ALLOC_IN_RC(r10_reg));\n+  match(RegP);\n+  \/\/ match(iRegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer 64 bit Register R11 only\n+operand iRegP_R11()\n+%{\n+  constraint(ALLOC_IN_RC(r11_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand iRegP_R12()\n+%{\n+  constraint(ALLOC_IN_RC(r12_reg));\n+  match(RegP);\n+  \/\/ match(iRegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer 64 bit Register R13 only\n+operand iRegP_R13()\n+%{\n+  constraint(ALLOC_IN_RC(r13_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand iRegP_R14()\n+%{\n+  constraint(ALLOC_IN_RC(r14_reg));\n+  match(RegP);\n+  \/\/ match(iRegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand iRegP_R15()\n+%{\n+  constraint(ALLOC_IN_RC(r15_reg));\n+  match(RegP);\n+  \/\/ match(iRegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand iRegP_R16()\n+%{\n+  constraint(ALLOC_IN_RC(r16_reg));\n+  match(RegP);\n+  \/\/ match(iRegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer 64 bit Register R28 only\n+operand iRegP_R28()\n+%{\n+  constraint(ALLOC_IN_RC(r28_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer Register Operands\n+\/\/ Narrow Pointer Register\n+operand iRegN()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg32));\n+  match(RegN);\n+  match(iRegNNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Integer 64 bit Register not Special\n+operand iRegNNoSp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_reg32));\n+  match(RegN);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ heap base register -- used for encoding immN0\n+operand iRegIHeapbase()\n+%{\n+  constraint(ALLOC_IN_RC(heapbase_reg));\n+  match(RegI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Long 64 bit Register R10 only\n+operand iRegL_R10()\n+%{\n+  constraint(ALLOC_IN_RC(r10_reg));\n+  match(RegL);\n+  match(iRegLNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Float Register\n+\/\/ Float register operands\n+operand fRegF()\n+%{\n+  constraint(ALLOC_IN_RC(float_reg));\n+  match(RegF);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Double Register\n+\/\/ Double register operands\n+operand fRegD()\n+%{\n+  constraint(ALLOC_IN_RC(double_reg));\n+  match(RegD);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Java Thread Register\n+operand javaThread_RegP(iRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(java_thread_reg)); \/\/ java_thread_reg\n+  match(reg);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/----------Memory Operands----------------------------------------------------\n+\/\/ RISCV has only base_plus_offset and literal address mode, so no need to use\n+\/\/ index and scale. Here set index as 0xffffffff and scale as 0x0.\n+operand indirect(iRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(reg);\n+  op_cost(0);\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+operand indOffI(iRegP reg, immIOffset off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+operand indOffL(iRegP reg, immLOffset off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+operand indirectN(iRegN reg)\n+%{\n+  predicate(Universe::narrow_oop_shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+  op_cost(0);\n+  format %{ \"[$reg]\\t# narrow\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+operand indOffIN(iRegN reg, immIOffset off)\n+%{\n+  predicate(Universe::narrow_oop_shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\\t# narrow\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+operand indOffLN(iRegN reg, immLOffset off)\n+%{\n+  predicate(Universe::narrow_oop_shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\\t# narrow\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ RISCV opto stubs need to write to the pc slot in the thread anchor\n+operand thread_anchor_pc(javaThread_RegP reg, immL_pc_off off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\n+\/\/----------Special Memory Operands--------------------------------------------\n+\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n+\/\/                      values on the stack where a match requires a value to\n+\/\/                      flow through memory.\n+operand stackSlotI(sRegI reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+  \/\/ match(RegI);\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x02);  \/\/ RSP\n+    index(0xffffffff);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotF(sRegF reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+  \/\/ match(RegF);\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x02);  \/\/ RSP\n+    index(0xffffffff);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotD(sRegD reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+  \/\/ match(RegD);\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x02);  \/\/ RSP\n+    index(0xffffffff);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotL(sRegL reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+  \/\/ match(RegL);\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x02);  \/\/ RSP\n+    index(0xffffffff);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+\/\/ Special operand allowing long args to int ops to be truncated for free\n+\n+operand iRegL2I(iRegL reg) %{\n+\n+  op_cost(0);\n+\n+  match(ConvL2I reg);\n+\n+  format %{ \"l2i($reg)\" %}\n+\n+  interface(REG_INTER)\n+%}\n+\n+\n+\/\/ Comparison Operands\n+\/\/ NOTE: Label is a predefined operand which should not be redefined in\n+\/\/       the AD file. It is generically handled within the ADLC.\n+\n+\/\/----------Conditional Branch Operands----------------------------------------\n+\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n+\/\/                  the following set of codes:\n+\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n+\/\/\n+\/\/ Other attributes of the comparison, such as unsignedness, are specified\n+\/\/ by the comparison instruction that sets a condition code flags register.\n+\/\/ That result is represented by a flags operand whose subtype is appropriate\n+\/\/ to the unsignedness (etc.) of the comparison.\n+\/\/\n+\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n+\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n+\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n+\n+\n+\/\/ used for signed integral comparisons and fp comparisons\n+operand cmpOp()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+\n+  \/\/ the values in interface derives from struct BoolTest::mask\n+  interface(COND_INTER) %{\n+    equal(0x0, \"eq\");\n+    greater(0x1, \"gt\");\n+    overflow(0x2, \"overflow\");\n+    less(0x3, \"lt\");\n+    not_equal(0x4, \"ne\");\n+    less_equal(0x5, \"le\");\n+    no_overflow(0x6, \"no_overflow\");\n+    greater_equal(0x7, \"ge\");\n+  %}\n+%}\n+\n+\/\/ used for unsigned integral comparisons\n+operand cmpOpU()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  \/\/ the values in interface derives from struct BoolTest::mask\n+  interface(COND_INTER) %{\n+    equal(0x0, \"eq\");\n+    greater(0x1, \"gtu\");\n+    overflow(0x2, \"overflow\");\n+    less(0x3, \"ltu\");\n+    not_equal(0x4, \"ne\");\n+    less_equal(0x5, \"leu\");\n+    no_overflow(0x6, \"no_overflow\");\n+    greater_equal(0x7, \"geu\");\n+  %}\n+%}\n+\n+\/\/ used for certain integral comparisons which can be\n+\/\/ converted to bxx instructions\n+operand cmpOpEqNe()\n+%{\n+  match(Bool);\n+  op_cost(0);\n+  predicate(n->as_Bool()->_test._test == BoolTest::ne ||\n+            n->as_Bool()->_test._test == BoolTest::eq);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x0, \"eq\");\n+    greater(0x1, \"gt\");\n+    overflow(0x2, \"overflow\");\n+    less(0x3, \"lt\");\n+    not_equal(0x4, \"ne\");\n+    less_equal(0x5, \"le\");\n+    no_overflow(0x6, \"no_overflow\");\n+    greater_equal(0x7, \"ge\");\n+  %}\n+%}\n+\n+operand cmpOpULtGe()\n+%{\n+  match(Bool);\n+  op_cost(0);\n+  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n+            n->as_Bool()->_test._test == BoolTest::ge);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x0, \"eq\");\n+    greater(0x1, \"gt\");\n+    overflow(0x2, \"overflow\");\n+    less(0x3, \"lt\");\n+    not_equal(0x4, \"ne\");\n+    less_equal(0x5, \"le\");\n+    no_overflow(0x6, \"no_overflow\");\n+    greater_equal(0x7, \"ge\");\n+  %}\n+%}\n+\n+operand cmpOpUEqNeLeGt()\n+%{\n+  match(Bool);\n+  op_cost(0);\n+  predicate(n->as_Bool()->_test._test == BoolTest::ne ||\n+            n->as_Bool()->_test._test == BoolTest::eq ||\n+            n->as_Bool()->_test._test == BoolTest::le ||\n+            n->as_Bool()->_test._test == BoolTest::gt);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x0, \"eq\");\n+    greater(0x1, \"gt\");\n+    overflow(0x2, \"overflow\");\n+    less(0x3, \"lt\");\n+    not_equal(0x4, \"ne\");\n+    less_equal(0x5, \"le\");\n+    no_overflow(0x6, \"no_overflow\");\n+    greater_equal(0x7, \"ge\");\n+  %}\n+%}\n+\n+\n+\/\/ Flags register, used as output of compare logic\n+operand rFlagsReg()\n+%{\n+  constraint(ALLOC_IN_RC(reg_flags));\n+  match(RegFlags);\n+\n+  op_cost(0);\n+  format %{ \"RFLAGS\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\n+\/\/ Method Register\n+operand inline_cache_RegP(iRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(method_reg)); \/\/ inline_cache_reg\n+  match(reg);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/----------OPERAND CLASSES----------------------------------------------------\n+\/\/ Operand Classes are groups of operands that are used as to simplify\n+\/\/ instruction definitions by not requiring the AD writer to specify\n+\/\/ separate instructions for every form of operand when the\n+\/\/ instruction accepts multiple operand types with the same basic\n+\/\/ encoding and format. The classic case of this is memory operands.\n+\n+\/\/ memory is used to define read\/write location for load\/store\n+\/\/ instruction defs. we can turn a memory op into an Address\n+\n+opclass memory(indirect, indOffI, indOffL, indirectN, indOffIN, indOffLN);\n+\n+\/\/ iRegIorL2I is used for src inputs in rules for 32 bit int (I)\n+\/\/ operations. it allows the src to be either an iRegI or a (ConvL2I\n+\/\/ iRegL). in the latter case the l2i normally planted for a ConvL2I\n+\/\/ can be elided because the 32-bit instruction will just employ the\n+\/\/ lower 32 bits anyway.\n+\/\/\n+\/\/ n.b. this does not elide all L2I conversions. if the truncated\n+\/\/ value is consumed by more than one operation then the ConvL2I\n+\/\/ cannot be bundled into the consuming nodes so an l2i gets planted\n+\/\/ (actually a mvw $dst $src) and the downstream instructions consume\n+\/\/ the result of the l2i as an iRegI input. That's a shame since the\n+\/\/ mvw is actually redundant but its not too costly.\n+\n+opclass iRegIorL2I(iRegI, iRegL2I);\n+opclass iRegIorL(iRegI, iRegL);\n+opclass iRegNorP(iRegN, iRegP);\n+opclass iRegILNP(iRegI, iRegL, iRegN, iRegP);\n+opclass iRegILNPNoSp(iRegINoSp, iRegLNoSp, iRegNNoSp, iRegPNoSp);\n+opclass immIorL(immI, immL);\n+\n+\/\/----------PIPELINE-----------------------------------------------------------\n+\/\/ Rules which define the behavior of the target architectures pipeline.\n+\n+\/\/ For specific pipelines, e.g. generic RISC-V, define the stages of that pipeline\n+\/\/pipe_desc(ID, EX, MEM, WR);\n+#define ID   S0\n+#define EX   S1\n+#define MEM  S2\n+#define WR   S3\n+\n+\/\/ Integer ALU reg operation\n+pipeline %{\n+\n+attributes %{\n+  \/\/ RISC-V instructions are of fixed length\n+  fixed_size_instructions;           \/\/ Fixed size instructions TODO does\n+  max_instructions_per_bundle = 2;   \/\/ Generic RISC-V 1, Sifive Series 7 2\n+  \/\/ RISC-V instructions come in 32-bit word units\n+  instruction_unit_size = 4;         \/\/ An instruction is 4 bytes long\n+  instruction_fetch_unit_size = 64;  \/\/ The processor fetches one line\n+  instruction_fetch_units = 1;       \/\/ of 64 bytes\n+\n+  \/\/ List of nop instructions\n+  nops( MachNop );\n+%}\n+\n+\/\/ We don't use an actual pipeline model so don't care about resources\n+\/\/ or description. we do use pipeline classes to introduce fixed\n+\/\/ latencies\n+\n+\/\/----------RESOURCES----------------------------------------------------------\n+\/\/ Resources are the functional units available to the machine\n+\n+\/\/ Generic RISC-V pipeline\n+\/\/ 1 decoder\n+\/\/ 1 instruction decoded per cycle\n+\/\/ 1 load\/store ops per cycle, 1 branch, 1 FPU\n+\/\/ 1 mul, 1 div\n+\n+resources ( DECODE,\n+            ALU,\n+            MUL,\n+            DIV,\n+            BRANCH,\n+            LDST,\n+            FPU);\n+\n+\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n+\/\/ Pipeline Description specifies the stages in the machine's pipeline\n+\n+\/\/ Define the pipeline as a generic 6 stage pipeline\n+pipe_desc(S0, S1, S2, S3, S4, S5);\n+\n+\/\/----------PIPELINE CLASSES---------------------------------------------------\n+\/\/ Pipeline Classes describe the stages in which input and output are\n+\/\/ referenced by the hardware pipeline.\n+\n+pipe_class fp_dop_reg_reg_s(fRegF dst, fRegF src1, fRegF src2)\n+%{\n+  single_instruction;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_dop_reg_reg_d(fRegD dst, fRegD src1, fRegD src2)\n+%{\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_uop_s(fRegF dst, fRegF src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_uop_d(fRegD dst, fRegD src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_d2f(fRegF dst, fRegD src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_f2d(fRegD dst, fRegF src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_f2i(iRegINoSp dst, fRegF src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_f2l(iRegLNoSp dst, fRegF src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_i2f(fRegF dst, iRegIorL2I src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_l2f(fRegF dst, iRegL src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_d2i(iRegINoSp dst, fRegD src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_d2l(iRegLNoSp dst, fRegD src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_i2d(fRegD dst, iRegIorL2I src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_l2d(fRegD dst, iRegIorL2I src)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_div_s(fRegF dst, fRegF src1, fRegF src2)\n+%{\n+  single_instruction;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_div_d(fRegD dst, fRegD src1, fRegD src2)\n+%{\n+  single_instruction;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_sqrt_s(fRegF dst, fRegF src1, fRegF src2)\n+%{\n+  single_instruction;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_sqrt_d(fRegD dst, fRegD src1, fRegD src2)\n+%{\n+  single_instruction;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_load_constant_s(fRegF dst)\n+%{\n+  single_instruction;\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_load_constant_d(fRegD dst)\n+%{\n+  single_instruction;\n+  dst    : S5(write);\n+  DECODE : ID;\n+  FPU    : S5;\n+%}\n+\n+pipe_class fp_load_mem_s(fRegF dst, memory mem)\n+%{\n+  single_instruction;\n+  mem    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+pipe_class fp_load_mem_d(fRegD dst, memory mem)\n+%{\n+  single_instruction;\n+  mem    : S1(read);\n+  dst    : S5(write);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+pipe_class fp_store_reg_s(fRegF src, memory mem)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  mem    : S5(write);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+pipe_class fp_store_reg_d(fRegD src, memory mem)\n+%{\n+  single_instruction;\n+  src    : S1(read);\n+  mem    : S5(write);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/------- Integer ALU operations --------------------------\n+\n+\/\/ Integer ALU reg-reg operation\n+\/\/ Operands needs in ID, result generated in EX\n+\/\/ E.g.  ADD   Rd, Rs1, Rs2\n+pipe_class ialu_reg_reg(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/ Integer ALU reg operation with constant shift\n+\/\/ E.g. SLLI    Rd, Rs1, #shift\n+pipe_class ialu_reg_shift(iRegI dst, iRegI src1)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  src1   : ID(read);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/ Integer ALU reg-reg operation with variable shift\n+\/\/ both operands must be available in ID\n+\/\/ E.g. SLL   Rd, Rs1, Rs2\n+pipe_class ialu_reg_reg_vshift(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/ Integer ALU reg operation\n+\/\/ E.g. NEG   Rd, Rs2\n+pipe_class ialu_reg(iRegI dst, iRegI src)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  src    : ID(read);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/ Integer ALU reg immediate operation\n+\/\/ E.g. ADDI   Rd, Rs1, #imm\n+pipe_class ialu_reg_imm(iRegI dst, iRegI src1)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  src1   : ID(read);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/ Integer ALU immediate operation (no source operands)\n+\/\/ E.g. LI    Rd, #imm\n+pipe_class ialu_imm(iRegI dst)\n+%{\n+  single_instruction;\n+  dst    : EX(write);\n+  DECODE : ID;\n+  ALU    : EX;\n+%}\n+\n+\/\/------- Multiply pipeline operations --------------------\n+\n+\/\/ Multiply reg-reg\n+\/\/ E.g. MULW   Rd, Rs1, Rs2\n+pipe_class imul_reg_reg(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  dst    : WR(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  MUL    : WR;\n+%}\n+\n+\/\/ E.g. MUL   RD, Rs1, Rs2\n+pipe_class lmul_reg_reg(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  fixed_latency(3); \/\/ Maximum latency for 64 bit mul\n+  dst    : WR(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  MUL    : WR;\n+%}\n+\n+\/\/------- Divide pipeline operations --------------------\n+\n+\/\/ E.g. DIVW   Rd, Rs1, Rs2\n+pipe_class idiv_reg_reg(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  fixed_latency(8); \/\/ Maximum latency for 32 bit divide\n+  dst    : WR(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  DIV    : WR;\n+%}\n+\n+\/\/ E.g. DIV   RD, Rs1, Rs2\n+pipe_class ldiv_reg_reg(iRegI dst, iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  fixed_latency(16); \/\/ Maximum latency for 64 bit divide\n+  dst    : WR(write);\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  DIV    : WR;\n+%}\n+\n+\/\/------- Load pipeline operations ------------------------\n+\n+\/\/ Load - reg, mem\n+\/\/ E.g. LA    Rd, mem\n+pipe_class iload_reg_mem(iRegI dst, memory mem)\n+%{\n+  single_instruction;\n+  dst    : WR(write);\n+  mem    : ID(read);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/ Load - reg, reg\n+\/\/ E.g. LD    Rd, Rs\n+pipe_class iload_reg_reg(iRegI dst, iRegI src)\n+%{\n+  single_instruction;\n+  dst    : WR(write);\n+  src    : ID(read);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/------- Store pipeline operations -----------------------\n+\n+\/\/ Store - zr, mem\n+\/\/ E.g. SD    zr, mem\n+pipe_class istore_mem(memory mem)\n+%{\n+  single_instruction;\n+  mem    : ID(read);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/ Store - reg, mem\n+\/\/ E.g. SD    Rs, mem\n+pipe_class istore_reg_mem(iRegI src, memory mem)\n+%{\n+  single_instruction;\n+  mem    : ID(read);\n+  src    : EX(read);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/ Store - reg, reg\n+\/\/ E.g. SD    Rs2, Rs1\n+pipe_class istore_reg_reg(iRegI dst, iRegI src)\n+%{\n+  single_instruction;\n+  dst    : ID(read);\n+  src    : EX(read);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/------- Store pipeline operations -----------------------\n+\n+\/\/ Branch\n+pipe_class pipe_branch()\n+%{\n+  single_instruction;\n+  DECODE : ID;\n+  BRANCH : EX;\n+%}\n+\n+\/\/ Branch\n+pipe_class pipe_branch_reg(iRegI src)\n+%{\n+  single_instruction;\n+  src    : ID(read);\n+  DECODE : ID;\n+  BRANCH : EX;\n+%}\n+\n+\/\/ Compare & Branch\n+\/\/ E.g. BEQ   Rs1, Rs2, L\n+pipe_class pipe_cmp_branch(iRegI src1, iRegI src2)\n+%{\n+  single_instruction;\n+  src1   : ID(read);\n+  src2   : ID(read);\n+  DECODE : ID;\n+  BRANCH : EX;\n+%}\n+\n+\/\/ E.g. BEQZ Rs, L\n+pipe_class pipe_cmpz_branch(iRegI src)\n+%{\n+  single_instruction;\n+  src    : ID(read);\n+  DECODE : ID;\n+  BRANCH : EX;\n+%}\n+\n+\/\/------- Synchronisation operations ----------------------\n+\/\/ Any operation requiring serialization\n+\/\/ E.g. FENCE\/Atomic Ops\/Load Acquire\/Store Release\n+pipe_class pipe_serial()\n+%{\n+  single_instruction;\n+  force_serialization;\n+  fixed_latency(16);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+pipe_class pipe_slow()\n+%{\n+  instruction_count(10);\n+  multiple_bundles;\n+  force_serialization;\n+  fixed_latency(16);\n+  DECODE : ID;\n+  LDST   : MEM;\n+%}\n+\n+\/\/ Empty pipeline class\n+pipe_class pipe_class_empty()\n+%{\n+  single_instruction;\n+  fixed_latency(0);\n+%}\n+\n+\/\/ Default pipeline class.\n+pipe_class pipe_class_default()\n+%{\n+  single_instruction;\n+  fixed_latency(2);\n+%}\n+\n+\/\/ Pipeline class for compares.\n+pipe_class pipe_class_compare()\n+%{\n+  single_instruction;\n+  fixed_latency(16);\n+%}\n+\n+\/\/ Pipeline class for memory operations.\n+pipe_class pipe_class_memory()\n+%{\n+  single_instruction;\n+  fixed_latency(16);\n+%}\n+\n+\/\/ Pipeline class for call.\n+pipe_class pipe_class_call()\n+%{\n+  single_instruction;\n+  fixed_latency(100);\n+%}\n+\n+\/\/ Define the class for the Nop node.\n+define %{\n+   MachNop = pipe_class_empty;\n+%}\n+%}\n+\/\/----------INSTRUCTIONS-------------------------------------------------------\n+\/\/\n+\/\/ match      -- States which machine-independent subtree may be replaced\n+\/\/               by this instruction.\n+\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n+\/\/               selection to identify a minimum cost tree of machine\n+\/\/               instructions that matches a tree of machine-independent\n+\/\/               instructions.\n+\/\/ format     -- A string providing the disassembly for this instruction.\n+\/\/               The value of an instruction's operand may be inserted\n+\/\/               by referring to it with a '$' prefix.\n+\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n+\/\/               to within an encode class as $primary, $secondary, and $tertiary\n+\/\/               rrspectively.  The primary opcode is commonly used to\n+\/\/               indicate the type of machine instruction, while secondary\n+\/\/               and tertiary are often used for prefix options or addressing\n+\/\/               modes.\n+\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n+\/\/               name must have been defined in an 'enc_class' specification\n+\/\/               in the encode section of the architecture description.\n+\n+\/\/ ============================================================================\n+\/\/ Memory (Load\/Store) Instructions\n+\n+\/\/ Load Instructions\n+\n+\/\/ Load Byte (8 bit signed)\n+instruct loadB(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadB mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lb  $dst, $mem\\t# byte, #@loadB\" %}\n+\n+  ins_encode %{\n+    __ lb(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit signed) into long\n+instruct loadB2L(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadB mem)));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lb  $dst, $mem\\t# byte, #@loadB2L\" %}\n+\n+  ins_encode %{\n+    __ lb(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit unsigned)\n+instruct loadUB(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadUB mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lbu  $dst, $mem\\t# byte, #@loadUB\" %}\n+\n+  ins_encode %{\n+    __ lbu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit unsigned) into long\n+instruct loadUB2L(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUB mem)));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lbu  $dst, $mem\\t# byte, #@loadUB2L\" %}\n+\n+  ins_encode %{\n+    __ lbu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed)\n+instruct loadS(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadS mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lh  $dst, $mem\\t# short, #@loadS\" %}\n+\n+  ins_encode %{\n+    __ lh(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) into long\n+instruct loadS2L(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadS mem)));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lh  $dst, $mem\\t# short, #@loadS2L\" %}\n+\n+  ins_encode %{\n+    __ lh(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Char (16 bit unsigned)\n+instruct loadUS(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadUS mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lhu  $dst, $mem\\t# short, #@loadUS\" %}\n+\n+  ins_encode %{\n+    __ lhu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Short\/Char (16 bit unsigned) into long\n+instruct loadUS2L(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUS mem)));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lhu  $dst, $mem\\t# short, #@loadUS2L\" %}\n+\n+  ins_encode %{\n+    __ lhu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed)\n+instruct loadI(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadI mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lw  $dst, $mem\\t# int, #@loadI\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ lw(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) into long\n+instruct loadI2L(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadI mem)));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lw  $dst, $mem\\t# int, #@loadI2L\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ lw(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit unsigned) into long\n+instruct loadUI2L(iRegLNoSp dst, memory mem, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lwu  $dst, $mem\\t# int, #@loadUI2L\" %}\n+\n+  ins_encode %{\n+    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Long (64 bit signed)\n+instruct loadL(iRegLNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadL mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"ld  $dst, $mem\\t# int, #@loadL\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ ld(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Range\n+instruct loadRange(iRegINoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadRange mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lwu  $dst, $mem\\t# range, #@loadRange\" %}\n+\n+  ins_encode %{\n+    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Pointer\n+instruct loadP(iRegPNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadP mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"ld  $dst, $mem\\t# ptr, #@loadP\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ ld(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Compressed Pointer\n+instruct loadN(iRegNNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadN mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lwu  $dst, $mem\\t# loadN, compressed ptr, #@loadN\" %}\n+\n+  ins_encode %{\n+    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Klass Pointer\n+instruct loadKlass(iRegPNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadKlass mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"ld  $dst, $mem\\t# class, #@loadKlass\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ ld(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Narrow Klass Pointer\n+instruct loadNKlass(iRegNNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lwu  $dst, $mem\\t# loadNKlass, compressed class ptr, #@loadNKlass\" %}\n+\n+  ins_encode %{\n+    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Float\n+instruct loadF(fRegF dst, memory mem)\n+%{\n+  match(Set dst (LoadF mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"flw  $dst, $mem\\t# float, #@loadF\" %}\n+\n+  ins_encode %{\n+    __ flw(as_FloatRegister($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(fp_load_mem_s);\n+%}\n+\n+\/\/ Load Double\n+instruct loadD(fRegD dst, memory mem)\n+%{\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"fld  $dst, $mem\\t# double, #@loadD\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ fld(as_FloatRegister($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(fp_load_mem_d);\n+%}\n+\n+\/\/ Load Int Constant\n+instruct loadConI(iRegINoSp dst, immI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"li $dst, $src\\t# int, #@loadConI\" %}\n+\n+  ins_encode(riscv_enc_li_imm(dst, src));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Long Constant\n+instruct loadConL(iRegLNoSp dst, immL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"li $dst, $src\\t# long, #@loadConL\" %}\n+\n+  ins_encode(riscv_enc_li_imm(dst, src));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Pointer Constant\n+instruct loadConP(iRegPNoSp dst, immP con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $con\\t# ptr, #@loadConP\" %}\n+\n+  ins_encode(riscv_enc_mov_p(dst, con));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Null Pointer Constant\n+instruct loadConP0(iRegPNoSp dst, immP0 con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $con\\t# NULL ptr, #@loadConP0\" %}\n+\n+  ins_encode(riscv_enc_mov_zero(dst));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Pointer Constant One\n+instruct loadConP1(iRegPNoSp dst, immP_1 con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $con\\t# load ptr constant one, #@loadConP1\" %}\n+\n+  ins_encode(riscv_enc_mov_p1(dst));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Poll Page Constant\n+instruct loadConPollPage(iRegPNoSp dst, immPollPage con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST * 6);\n+  format %{ \"movptr  $dst, $con\\t# Poll Page Ptr, #@loadConPollPage\" %}\n+\n+  ins_encode(riscv_enc_mov_poll_page(dst, con));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Byte Map Base Constant\n+instruct loadByteMapBase(iRegPNoSp dst, immByteMapBase con)\n+%{\n+  match(Set dst con);\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $con\\t# Byte Map Base, #@loadByteMapBase\" %}\n+\n+  ins_encode(riscv_enc_mov_byte_map_base(dst));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Narrow Pointer Constant\n+instruct loadConN(iRegNNoSp dst, immN con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST * 4);\n+  format %{ \"mv  $dst, $con\\t# compressed ptr, #@loadConN\" %}\n+\n+  ins_encode(riscv_enc_mov_n(dst, con));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Narrow Null Pointer Constant\n+instruct loadConN0(iRegNNoSp dst, immN0 con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $con\\t# compressed NULL ptr, #@loadConN0\" %}\n+\n+  ins_encode(riscv_enc_mov_zero(dst));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Narrow Klass Constant\n+instruct loadConNKlass(iRegNNoSp dst, immNKlass con)\n+%{\n+  match(Set dst con);\n+\n+  ins_cost(ALU_COST * 6);\n+  format %{ \"mv  $dst, $con\\t# compressed klass ptr, #@loadConNKlass\" %}\n+\n+  ins_encode(riscv_enc_mov_nk(dst, con));\n+\n+  ins_pipe(ialu_imm);\n+%}\n+\n+\/\/ Load Float Constant\n+instruct loadConF(fRegF dst, immF con) %{\n+  match(Set dst con);\n+\n+  ins_cost(LOAD_COST);\n+  format %{\n+    \"flw $dst, [$constantaddress]\\t# load from constant table: float=$con, #@loadConF\"\n+  %}\n+\n+  ins_encode %{\n+    __ flw(as_FloatRegister($dst$$reg), $constantaddress($con));\n+  %}\n+\n+  ins_pipe(fp_load_constant_s);\n+%}\n+\n+instruct loadConF0(fRegF dst, immF0 con) %{\n+  match(Set dst con);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.w.x $dst, zr\\t# float, #@loadConF0\" %}\n+\n+  ins_encode %{\n+    __ fmv_w_x(as_FloatRegister($dst$$reg), zr);\n+  %}\n+\n+  ins_pipe(fp_load_constant_s);\n+%}\n+\n+\/\/ Load Double Constant\n+instruct loadConD(fRegD dst, immD con) %{\n+  match(Set dst con);\n+\n+  ins_cost(LOAD_COST);\n+  format %{\n+    \"fld $dst, [$constantaddress]\\t# load from constant table: double=$con, #@loadConD\"\n+  %}\n+\n+  ins_encode %{\n+    __ fld(as_FloatRegister($dst$$reg), $constantaddress($con));\n+  %}\n+\n+  ins_pipe(fp_load_constant_d);\n+%}\n+\n+instruct loadConD0(fRegD dst, immD0 con) %{\n+  match(Set dst con);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.d.x $dst, zr\\t# double, #@loadConD0\" %}\n+\n+  ins_encode %{\n+    __ fmv_d_x(as_FloatRegister($dst$$reg), zr);\n+  %}\n+\n+  ins_pipe(fp_load_constant_d);\n+%}\n+\n+\/\/ Store Instructions\n+\/\/ Store CMS card-mark Immediate\n+instruct storeimmCM0(immI0 zero, memory mem)\n+%{\n+  match(Set mem (StoreCM mem zero));\n+  predicate(unnecessary_storestore(n));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"storestore (elided)\\n\\t\"\n+            \"sb zr, $mem\\t# byte, #@storeimmCM0\" %}\n+\n+  ins_encode %{\n+    __ sb(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store CMS card-mark Immediate with intervening StoreStore\n+\/\/ needed when using CMS with no conditional card marking\n+instruct storeimmCM0_ordered(immI0 zero, memory mem)\n+%{\n+  match(Set mem (StoreCM mem zero));\n+\n+  ins_cost(ALU_COST + STORE_COST);\n+  format %{ \"membar(StoreStore)\\n\\t\"\n+            \"sb zr, $mem\\t# byte, #@storeimmCM0_ordered\" %}\n+\n+  ins_encode %{\n+    __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+    __ sb(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Byte\n+instruct storeB(iRegIorL2I src, memory mem)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sb  $src, $mem\\t# byte, #@storeB\" %}\n+\n+  ins_encode %{\n+    __ sb(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct storeimmB0(immI0 zero, memory mem)\n+%{\n+  match(Set mem (StoreB mem zero));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sb zr, $mem\\t# byte, #@storeimmB0\" %}\n+\n+  ins_encode %{\n+    __ sb(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Char\/Short\n+instruct storeC(iRegIorL2I src, memory mem)\n+%{\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sh  $src, $mem\\t# short, #@storeC\" %}\n+\n+  ins_encode %{\n+    __ sh(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct storeimmC0(immI0 zero, memory mem)\n+%{\n+  match(Set mem (StoreC mem zero));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sh  zr, $mem\\t# short, #@storeimmC0\" %}\n+\n+  ins_encode %{\n+    __ sh(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Integer\n+instruct storeI(iRegIorL2I src, memory mem)\n+%{\n+  match(Set mem(StoreI mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  $src, $mem\\t# int, #@storeI\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sw(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct storeimmI0(immI0 zero, memory mem)\n+%{\n+  match(Set mem(StoreI mem zero));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  zr, $mem\\t# int, #@storeimmI0\" %}\n+\n+  ins_encode %{\n+    __ sw(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Long (64 bit signed)\n+instruct storeL(iRegL src, memory mem)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sd  $src, $mem\\t# long, #@storeL\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sd(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Store Long (64 bit signed)\n+instruct storeimmL0(immL0 zero, memory mem)\n+%{\n+  match(Set mem (StoreL mem zero));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sd  zr, $mem\\t# long, #@storeimmL0\" %}\n+\n+  ins_encode %{\n+    __ sd(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Pointer\n+instruct storeP(iRegP src, memory mem)\n+%{\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sd  $src, $mem\\t# ptr, #@storeP\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sd(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Store Pointer\n+instruct storeimmP0(immP0 zero, memory mem)\n+%{\n+  match(Set mem (StoreP mem zero));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sd zr, $mem\\t# ptr, #@storeimmP0\" %}\n+\n+  ins_encode %{\n+    __ sd(zr, Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_mem);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct storeN(iRegN src, memory mem)\n+%{\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  $src, $mem\\t# compressed ptr, #@storeN\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sw(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct storeImmN0(iRegIHeapbase heapbase, immN0 zero, memory mem)\n+%{\n+  match(Set mem (StoreN mem zero));\n+  predicate(Universe::narrow_oop_base() == NULL &&\n+            Universe::narrow_klass_base() == NULL);\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  rheapbase, $mem\\t# compressed ptr (rheapbase==0), #@storeImmN0\" %}\n+\n+  ins_encode %{\n+    __ sw(as_Register($heapbase$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Store Float\n+instruct storeF(fRegF src, memory mem)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"fsw  $src, $mem\\t# float, #@storeF\" %}\n+\n+  ins_encode %{\n+    __ fsw(as_FloatRegister($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(fp_store_reg_s);\n+%}\n+\n+\/\/ Store Double\n+instruct storeD(fRegD src, memory mem)\n+%{\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"fsd  $src, $mem\\t# double, #@storeD\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ fsd(as_FloatRegister($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(fp_store_reg_d);\n+%}\n+\n+\/\/ Store Compressed Klass Pointer\n+instruct storeNKlass(iRegN src, memory mem)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  $src, $mem\\t# compressed klass ptr, #@storeNKlass\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sw(as_Register($src$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Atomic operation instructions\n+\/\/\n+\/\/ Intel and SPARC both implement Ideal Node LoadPLocked and\n+\/\/ Store{PIL}Conditional instructions using a normal load for the\n+\/\/ LoadPLocked and a CAS for the Store{PIL}Conditional.\n+\/\/\n+\/\/ The ideal code appears only to use LoadPLocked\/storePConditional as a\n+\/\/ pair to lock object allocations from Eden space when not using\n+\/\/ TLABs.\n+\/\/\n+\/\/ There does not appear to be a Load{IL}Locked Ideal Node and the\n+\/\/ Ideal code appears to use Store{IL}Conditional as an alias for CAS\n+\/\/ and to use StoreIConditional only for 32-bit and StoreLConditional\n+\/\/ only for 64-bit.\n+\/\/\n+\/\/ We implement LoadPLocked and storePConditional instructions using,\n+\/\/ respectively the RISCV hw load-reserve and store-conditional\n+\/\/ instructions. Whereas we must implement each of\n+\/\/ Store{IL}Conditional using a CAS which employs a pair of\n+\/\/ instructions comprising a load-reserve followed by a\n+\/\/ store-conditional.\n+\n+\n+\/\/ Locked-load (load reserved) of the current heap-top\n+\/\/ used when updating the eden heap top\n+\/\/ implemented using lr_d on RISCV64\n+instruct loadPLocked(iRegPNoSp dst, indirect mem)\n+%{\n+  match(Set dst (LoadPLocked mem));\n+\n+  ins_cost(ALU_COST * 2 + LOAD_COST);\n+\n+  format %{ \"lr.d $dst, $mem\\t# ptr load reserved, #@loadPLocked\" %}\n+\n+  ins_encode %{\n+    __ la(t0, Address(as_Register($mem$$base), $mem$$disp));\n+    __ lr_d($dst$$Register, t0, Assembler::aq);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ Conditional-store of the updated heap-top.\n+\/\/ Used during allocation of the shared heap.\n+\/\/ implemented using sc_d on RISCV64.\n+instruct storePConditional(memory heap_top_ptr, iRegP oldval, iRegP newval, rFlagsReg cr)\n+%{\n+  match(Set cr (StorePConditional heap_top_ptr (Binary oldval newval)));\n+\n+  ins_cost(ALU_COST * 2 + STORE_COST);\n+\n+  format %{\n+    \"sc_d t1, $newval $heap_top_ptr,\\t# ptr store conditional, #@storePConditional\"\n+  %}\n+\n+  ins_encode %{\n+    __ la(t0, Address(as_Register($heap_top_ptr$$base), $heap_top_ptr$$disp));\n+    __ sc_d($cr$$Register, $newval$$Register, t0, Assembler::rl);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ storeLConditional is used by PhaseMacroExpand::expand_lock_node\n+\/\/ when attempting to rebias a lock towards the current thread.  We\n+\/\/ must use the acquire form of cmpxchg in order to guarantee acquire\n+\/\/ semantics in this case.\n+instruct storeLConditional(indirect mem, iRegLNoSp oldval, iRegLNoSp newval, rFlagsReg cr)\n+%{\n+  match(Set cr (StoreLConditional mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + 2 * BRANCH_COST);\n+\n+  format %{\n+    \"cmpxchg t1, $mem, $oldval, $newval, $mem\\t# if $mem == $oldval then $mem <-- $newval\"\n+    \"xorr $cr, $cr, $oldval\\t# $cr == 0 on successful write, #@storeLConditional\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $cr$$Register);\n+    __ xorr($cr$$Register,$cr$$Register, $oldval$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ storeIConditional also has acquire semantics, for no better reason\n+\/\/ than matching storeLConditional.\n+instruct storeIConditional(indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr)\n+%{\n+  match(Set cr (StoreIConditional mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2);\n+\n+  format %{\n+    \"cmpxchgw t1, $mem, $oldval, $newval, $mem\\t# if $mem == $oldval then $mem <-- $newval\"\n+    \"xorr $cr, $cr, $oldval\\t# $cr == 0 on successful write, #@storeIConditional\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $cr$$Register);\n+    __ xorr($cr$$Register,$cr$$Register, $oldval$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ standard CompareAndSwapX when we are using barriers\n+\/\/ these have higher priority than the rules selected by a predicate\n+instruct compareAndSwapB(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                         iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 10 + BRANCH_COST * 4);\n+\n+  effect(TEMP_DEF res, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (byte) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapB\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                            Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                            true \/* result as bool *\/, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapS(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                         iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 11 + BRANCH_COST * 4);\n+\n+  effect(TEMP_DEF res, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (short) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapS\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                            Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                            true \/* result as bool *\/, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapI(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval)\n+%{\n+  match(Set res (CompareAndSwapI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (int) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapI\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchgw(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapL(iRegINoSp res, indirect mem, iRegLNoSp oldval, iRegLNoSp newval)\n+%{\n+  match(Set res (CompareAndSwapL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (long) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapL\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchg(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapP\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchg(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapN(iRegINoSp res, indirect mem, iRegNNoSp oldval, iRegNNoSp newval)\n+%{\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 8 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapN\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchgn(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ alternative CompareAndSwapX when we are eliding barriers\n+instruct compareAndSwapBAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                            iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 10 + BRANCH_COST * 4);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (byte) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapBAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                            Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                            true \/* result as bool *\/, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapSAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                            iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 11 + BRANCH_COST * 4);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (short) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapSAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                            Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                            true \/* result as bool *\/, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapIAcq(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (int) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapIAcq\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchgw_acq(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapLAcq(iRegINoSp res, indirect mem, iRegLNoSp oldval, iRegLNoSp newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (long) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapLAcq\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchg_acq(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 6 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapPAcq\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchg_acq(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndSwapNAcq(iRegINoSp res, indirect mem, iRegNNoSp oldval, iRegNNoSp newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + ALU_COST * 8 + BRANCH_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"mv $res, $res == $oldval\\t# $res <-- ($res == $oldval ? 1 : 0), #@compareAndSwapNAcq\"\n+  %}\n+\n+  ins_encode(riscv_enc_cmpxchgn_acq(res, mem, oldval, newval));\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Sundry CAS operations.  Note that release is always true,\n+\/\/ regardless of the memory ordering of the CAS.  This is because we\n+\/\/ need the volatile case to be sequentially consistent but there is\n+\/\/ no trailing StoreLoad barrier emitted by C2.  Unfortunately we\n+\/\/ can't check the type of memory ordering here, so we always emit a\n+\/\/ sc_d(w) with rl bit set.\n+instruct compareAndExchangeB(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                             iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndExchangeB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST * 5);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (byte, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeB\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                            \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+                            \/*result_as_bool*\/ false, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeS(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                             iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndExchangeS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST * 6);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (short, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeS\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                            \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+                            \/*result_as_bool*\/ false, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeI(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval)\n+%{\n+  match(Set res (CompareAndExchangeI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (int, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeI\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeL(iRegLNoSp res, indirect mem, iRegL oldval, iRegL newval)\n+%{\n+  match(Set res (CompareAndExchangeL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (long, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeL\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval)\n+%{\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST * 3);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeN\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg $res = $mem, $oldval, $newval\\t# (ptr, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeP\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeBAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                                iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST * 5);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (byte, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeBAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                            \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+                            \/*result_as_bool*\/ false, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeSAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                                iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST * 6);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (short, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeSAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                            \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+                            \/*result_as_bool*\/ false, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeIAcq(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (int, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeIAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeLAcq(iRegLNoSp res, indirect mem, iRegL oldval, iRegL newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (long, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeLAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangeNAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 3 + ALU_COST);\n+\n+  effect(TEMP_DEF res);\n+\n+  format %{\n+    \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# (ptr, weak) if $mem == $oldval then $mem <-- $newval, #@compareAndExchangePAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapB(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                             iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (WeakCompareAndSwapB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 6);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (byte, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapB\"\n+  %}\n+\n+  ins_encode %{\n+    __ weak_cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                                 \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapS(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                             iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  match(Set res (WeakCompareAndSwapS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 7);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (short, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapS\"\n+  %}\n+\n+  ins_encode %{\n+    __ weak_cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                                 \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapI(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval)\n+%{\n+  match(Set res (WeakCompareAndSwapI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (int, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapI\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+                    \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapL(iRegINoSp res, indirect mem, iRegL oldval, iRegL newval)\n+%{\n+  match(Set res (WeakCompareAndSwapL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (long, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapL\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+                    \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapN(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval)\n+%{\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapN\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+                    \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak $mem, $oldval, $newval\\t# (ptr, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapP\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+                    \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapBAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                                iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapB mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 6);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (byte, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapBAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ weak_cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int8,\n+                                 \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapSAcq(iRegINoSp res, indirect mem, iRegI_R12 oldval, iRegI_R13 newval,\n+                                iRegI tmp1, iRegI tmp2, iRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapS mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 7);\n+\n+  effect(TEMP_DEF res, KILL cr, USE_KILL oldval, USE_KILL newval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (short, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapSAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ weak_cmpxchg_narrow_value(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int16,\n+                                 \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapIAcq(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapI mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (int, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapIAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int32,\n+                    \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapLAcq(iRegINoSp res, indirect mem, iRegL oldval, iRegL newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapL mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (long, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapLAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+                    \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 4);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (narrow oop, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapNAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::uint32,\n+                    \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct weakCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+\n+  ins_cost(LOAD_COST + STORE_COST + BRANCH_COST * 2 + ALU_COST * 2);\n+\n+  format %{\n+    \"cmpxchg_weak_acq $mem, $oldval, $newval\\t# (ptr, weak) if $mem == $oldval then $mem <-- $newval\\n\\t\"\n+    \"xori $res, $res, 1\\t# $res == 1 when success, #@weakCompareAndSwapPAcq\"\n+  %}\n+\n+  ins_encode %{\n+    __ cmpxchg_weak(as_Register($mem$$base), $oldval$$Register, $newval$$Register, Assembler::int64,\n+                    \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ xori($res$$Register, $res$$Register, 1);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct get_and_setI(indirect mem, iRegI newv, iRegINoSp prev)\n+%{\n+  match(Set prev (GetAndSetI mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchgw  $prev, $newv, [$mem]\\t#@get_and_setI\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgw($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setL(indirect mem, iRegL newv, iRegLNoSp prev)\n+%{\n+  match(Set prev (GetAndSetL mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchg  $prev, $newv, [$mem]\\t#@get_and_setL\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchg($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setN(indirect mem, iRegN newv, iRegINoSp prev)\n+%{\n+  match(Set prev (GetAndSetN mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchgwu $prev, $newv, [$mem]\\t#@get_and_setN\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgwu($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setP(indirect mem, iRegP newv, iRegPNoSp prev)\n+%{\n+  match(Set prev (GetAndSetP mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchg  $prev, $newv, [$mem]\\t#@get_and_setP\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchg($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setIAcq(indirect mem, iRegI newv, iRegINoSp prev)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set prev (GetAndSetI mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchgw_acq  $prev, $newv, [$mem]\\t#@get_and_setIAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgalw($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setLAcq(indirect mem, iRegL newv, iRegLNoSp prev)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set prev (GetAndSetL mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem]\\t#@get_and_setLAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgal($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setNAcq(indirect mem, iRegN newv, iRegINoSp prev)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set prev (GetAndSetN mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchgwu_acq $prev, $newv, [$mem]\\t#@get_and_setNAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgalwu($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_setPAcq(indirect mem, iRegP newv, iRegPNoSp prev)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set prev (GetAndSetP mem newv));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem]\\t#@get_and_setPAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgal($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addL(indirect mem, iRegLNoSp newval, iRegL incr)\n+%{\n+  match(Set newval (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL $newval, [$mem], $incr\\t#@get_and_addL\" %}\n+\n+  ins_encode %{\n+    __ atomic_add($newval$$Register, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addL_no_res(indirect mem, Universe dummy, iRegL incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used());\n+\n+  match(Set dummy (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL [$mem], $incr\\t#@get_and_addL_no_res\" %}\n+\n+  ins_encode %{\n+    __ atomic_add(noreg, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addLi(indirect mem, iRegLNoSp newval, immLAdd incr)\n+%{\n+  match(Set newval (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL $newval, [$mem], $incr\\t#@get_and_addLi\" %}\n+\n+  ins_encode %{\n+    __ atomic_add($newval$$Register, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addLi_no_res(indirect mem, Universe dummy, immLAdd incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used());\n+\n+  match(Set dummy (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL [$mem], $incr\\t#@get_and_addLi_no_res\" %}\n+\n+  ins_encode %{\n+    __ atomic_add(noreg, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addI(indirect mem, iRegINoSp newval, iRegIorL2I incr)\n+%{\n+  match(Set newval (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI $newval, [$mem], $incr\\t#@get_and_addI\" %}\n+\n+  ins_encode %{\n+    __ atomic_addw($newval$$Register, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addI_no_res(indirect mem, Universe dummy, iRegIorL2I incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used());\n+\n+  match(Set dummy (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI [$mem], $incr\\t#@get_and_addI_no_res\" %}\n+\n+  ins_encode %{\n+    __ atomic_addw(noreg, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addIi(indirect mem, iRegINoSp newval, immIAdd incr)\n+%{\n+  match(Set newval (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI $newval, [$mem], $incr\\t#@get_and_addIi\" %}\n+\n+  ins_encode %{\n+    __ atomic_addw($newval$$Register, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addIi_no_res(indirect mem, Universe dummy, immIAdd incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used());\n+\n+  match(Set dummy (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI [$mem], $incr\\t#@get_and_addIi_no_res\" %}\n+\n+  ins_encode %{\n+    __ atomic_addw(noreg, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addLAcq(indirect mem, iRegLNoSp newval, iRegL incr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set newval (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL_acq $newval, [$mem], $incr\\t#@get_and_addLAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addal($newval$$Register, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addL_no_resAcq(indirect mem, Universe dummy, iRegL incr) %{\n+  predicate(n->as_LoadStore()->result_not_used() && needs_acquiring_load_reserved(n));\n+\n+  match(Set dummy (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL_acq [$mem], $incr\\t#@get_and_addL_no_resAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addal(noreg, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addLiAcq(indirect mem, iRegLNoSp newval, immLAdd incr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set newval (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL_acq $newval, [$mem], $incr\\t#@get_and_addLiAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addal($newval$$Register, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addLi_no_resAcq(indirect mem, Universe dummy, immLAdd incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used() && needs_acquiring_load_reserved(n));\n+\n+  match(Set dummy (GetAndAddL mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addL_acq [$mem], $incr\\t#@get_and_addLi_no_resAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addal(noreg, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addIAcq(indirect mem, iRegINoSp newval, iRegIorL2I incr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set newval (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI_acq $newval, [$mem], $incr\\t#@get_and_addIAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addalw($newval$$Register, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addI_no_resAcq(indirect mem, Universe dummy, iRegIorL2I incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used() && needs_acquiring_load_reserved(n));\n+\n+  match(Set dummy (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI_acq [$mem], $incr\\t#@get_and_addI_no_resAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addalw(noreg, $incr$$Register, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addIiAcq(indirect mem, iRegINoSp newval, immIAdd incr)\n+%{\n+  predicate(needs_acquiring_load_reserved(n));\n+\n+  match(Set newval (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI_acq $newval, [$mem], $incr\\t#@get_and_addIiAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addalw($newval$$Register, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct get_and_addIi_no_resAcq(indirect mem, Universe dummy, immIAdd incr)\n+%{\n+  predicate(n->as_LoadStore()->result_not_used() && needs_acquiring_load_reserved(n));\n+\n+  match(Set dummy (GetAndAddI mem incr));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"get_and_addI_acq [$mem], $incr\\t#@get_and_addIi_no_resAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_addalw(noreg, $incr$$constant, as_Register($mem$$base));\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Arithmetic Instructions\n+\/\/\n+\n+\/\/ Integer Addition\n+\n+\/\/ TODO\n+\/\/ these currently employ operations which do not set CR and hence are\n+\/\/ not flagged as killing CR but we would like to isolate the cases\n+\/\/ where we want to set flags from those where we don't. need to work\n+\/\/ out how to do that.\n+instruct addI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (AddI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addw  $dst, $src1, $src2\\t#@addI_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ addw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_reg_imm(iRegINoSp dst, iRegIorL2I src1, immIAdd src2) %{\n+  match(Set dst (AddI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addiw  $dst, $src1, $src2\\t#@addI_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    int32_t con = (int32_t)$src2$$constant;\n+    __ addiw(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+instruct addI_reg_imm_l2i(iRegINoSp dst, iRegL src1, immIAdd src2) %{\n+  match(Set dst (AddI (ConvL2I src1) src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addiw  $dst, $src1, $src2\\t#@addI_reg_imm_l2i\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ addiw(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Pointer Addition\n+instruct addP_reg_reg(iRegPNoSp dst, iRegP src1, iRegL src2) %{\n+  match(Set dst (AddP src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"add $dst, $src1, $src2\\t# ptr, #@addP_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ add(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ If we shift more than 32 bits, we need not convert I2L.\n+instruct lShiftL_regI_immGE32(iRegLNoSp dst, iRegI src, uimmI6_ge32 scale) %{\n+  match(Set dst (LShiftL (ConvI2L src) scale));\n+  ins_cost(ALU_COST);\n+  format %{ \"slli  $dst, $src, $scale & 63\\t#@lShiftL_regI_immGE32\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ slli(as_Register($dst$$reg), as_Register($src$$reg), $scale$$constant & 63);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Pointer Immediate Addition\n+\/\/ n.b. this needs to be more expensive than using an indirect memory\n+\/\/ operand\n+instruct addP_reg_imm(iRegPNoSp dst, iRegP src1, immLAdd src2) %{\n+  match(Set dst (AddP src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"addi  $dst, $src1, $src2\\t# ptr, #@addP_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ src2 is imm, so actually call the addi\n+    __ add(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Long Addition\n+instruct addL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (AddL src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"add  $dst, $src1, $src2\\t#@addL_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ add(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ No constant pool entries requiredLong Immediate Addition.\n+instruct addL_reg_imm(iRegLNoSp dst, iRegL src1, immLAdd src2) %{\n+  match(Set dst (AddL src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"addi  $dst, $src1, $src2\\t#@addL_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ src2 is imm, so actually call the addi\n+    __ add(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Integer Subtraction\n+instruct subI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (SubI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"subw  $dst, $src1, $src2\\t#@subI_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ subw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate Subtraction\n+instruct subI_reg_imm(iRegINoSp dst, iRegIorL2I src1, immISub src2) %{\n+  match(Set dst (SubI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addiw  $dst, $src1, -$src2\\t#@subI_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ src2 is imm, so actually call the addiw\n+    __ subw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Long Subtraction\n+instruct subL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (SubL src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"sub  $dst, $src1, $src2\\t#@subL_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sub(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ No constant pool entries requiredLong Immediate Subtraction.\n+instruct subL_reg_imm(iRegLNoSp dst, iRegL src1, immLSub src2) %{\n+  match(Set dst (SubL src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"addi  $dst, $src1, -$src2\\t#@subL_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ src2 is imm, so actually call the addi\n+    __ sub(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           $src2$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Integer Negation (special case for sub)\n+\n+instruct negI_reg(iRegINoSp dst, iRegIorL2I src, immI0 zero) %{\n+  match(Set dst (SubI zero src));\n+  ins_cost(ALU_COST);\n+  format %{ \"subw  $dst, x0, $src\\t# int, #@negI_reg\" %}\n+\n+  ins_encode %{\n+    \/\/ actually call the subw\n+    __ negw(as_Register($dst$$reg),\n+            as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Long Negation\n+\n+instruct negL_reg(iRegLNoSp dst, iRegL src, immL0 zero) %{\n+  match(Set dst (SubL zero src));\n+  ins_cost(ALU_COST);\n+  format %{ \"sub  $dst, x0, $src\\t# long, #@negL_reg\" %}\n+\n+  ins_encode %{\n+    \/\/ actually call the sub\n+    __ neg(as_Register($dst$$reg),\n+           as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Integer Multiply\n+\n+instruct mulI(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (MulI src1 src2));\n+  ins_cost(IMUL_COST);\n+  format %{ \"mulw  $dst, $src1, $src2\\t#@mulI\" %}\n+\n+  \/\/this means 2 word multi, and no sign extend to 64 bits\n+  ins_encode %{\n+    \/\/ riscv64 mulw will sign-extension to high 32 bits in dst reg\n+    __ mulw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(imul_reg_reg);\n+%}\n+\n+\/\/ Long Multiply\n+\n+instruct mulL(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (MulL src1 src2));\n+  ins_cost(IMUL_COST);\n+  format %{ \"mul  $dst, $src1, $src2\\t#@mulL\" %}\n+\n+  ins_encode %{\n+    __ mul(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(lmul_reg_reg);\n+%}\n+\n+instruct mulHiL_rReg(iRegLNoSp dst, iRegL src1, iRegL src2)\n+%{\n+  match(Set dst (MulHiL src1 src2));\n+  ins_cost(IMUL_COST);\n+  format %{ \"mulh  $dst, $src1, $src2\\t# mulhi, #@mulHiL_rReg\" %}\n+\n+  ins_encode %{\n+    __ mulh(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(lmul_reg_reg);\n+%}\n+\n+\/\/ Integer Divide\n+\n+instruct divI(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (DivI src1 src2));\n+  ins_cost(IDIVSI_COST);\n+  format %{ \"divw  $dst, $src1, $src2\\t#@divI\"%}\n+\n+  ins_encode(riscv_enc_divw(dst, src1, src2));\n+  ins_pipe(idiv_reg_reg);\n+%}\n+\n+instruct signExtract(iRegINoSp dst, iRegIorL2I src1, immI_31 div1, immI_31 div2) %{\n+  match(Set dst (URShiftI (RShiftI src1 div1) div2));\n+  ins_cost(ALU_COST);\n+  format %{ \"srliw $dst, $src1, $div1\\t# int signExtract, #@signExtract\" %}\n+\n+  ins_encode %{\n+    __ srliw(as_Register($dst$$reg), as_Register($src1$$reg), 31);\n+  %}\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Long Divide\n+\n+instruct divL(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (DivL src1 src2));\n+  ins_cost(IDIVDI_COST);\n+  format %{ \"div  $dst, $src1, $src2\\t#@divL\" %}\n+\n+  ins_encode(riscv_enc_div(dst, src1, src2));\n+  ins_pipe(ldiv_reg_reg);\n+%}\n+\n+instruct signExtractL(iRegLNoSp dst, iRegL src1, immI_63 div1, immI_63 div2) %{\n+  match(Set dst (URShiftL (RShiftL src1 div1) div2));\n+  ins_cost(ALU_COST);\n+  format %{ \"srli $dst, $src1, $div1\\t# long signExtract, #@signExtractL\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ srli(as_Register($dst$$reg), as_Register($src1$$reg), 63);\n+  %}\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Integer Remainder\n+\n+instruct modI(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (ModI src1 src2));\n+  ins_cost(IDIVSI_COST);\n+  format %{ \"remw  $dst, $src1, $src2\\t#@modI\" %}\n+\n+  ins_encode(riscv_enc_modw(dst, src1, src2));\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Long Remainder\n+\n+instruct modL(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (ModL src1 src2));\n+  ins_cost(IDIVDI_COST);\n+  format %{ \"rem  $dst, $src1, $src2\\t#@modL\" %}\n+\n+  ins_encode(riscv_enc_mod(dst, src1, src2));\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Integer Shifts\n+\n+\/\/ Shift Left Register\n+\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+instruct lShiftI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (LShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"sllw  $dst, $src1, $src2\\t#@lShiftI_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ sllw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Left Immediate\n+instruct lShiftI_reg_imm(iRegINoSp dst, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (LShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"slliw  $dst, $src1, ($src2 & 0x1f)\\t#@lShiftI_reg_imm\" %}\n+\n+  ins_encode %{\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 5 bits of the I-immediate field for RV32I\n+    __ slliw(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             (unsigned) $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Shift Right Logical Register\n+\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+instruct urShiftI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (URShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"srlw  $dst, $src1, $src2\\t#@urShiftI_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ srlw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Right Logical Immediate\n+instruct urShiftI_reg_imm(iRegINoSp dst, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (URShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"srliw  $dst, $src1, ($src2 & 0x1f)\\t#@urShiftI_reg_imm\" %}\n+\n+  ins_encode %{\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 6 bits of the I-immediate field for RV64I\n+    __ srliw(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             (unsigned) $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Shift Right Arithmetic Register\n+\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+instruct rShiftI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (RShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"sraw  $dst, $src1, $src2\\t#@rShiftI_reg_reg\" %}\n+\n+  ins_encode %{\n+    \/\/ riscv will sign-ext dst high 32 bits\n+    __ sraw(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Right Arithmetic Immediate\n+instruct rShiftI_reg_imm(iRegINoSp dst, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (RShiftI src1 src2));\n+  ins_cost(ALU_COST);\n+  format %{ \"sraiw  $dst, $src1, ($src2 & 0x1f)\\t#@rShiftI_reg_imm\" %}\n+\n+  ins_encode %{\n+    \/\/ riscv will sign-ext dst high 32 bits\n+    __ sraiw(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             (unsigned) $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Long Shifts\n+\n+\/\/ Shift Left Register\n+\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+instruct lShiftL_reg_reg(iRegLNoSp dst, iRegL src1, iRegIorL2I src2) %{\n+  match(Set dst (LShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"sll  $dst, $src1, $src2\\t#@lShiftL_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ sll(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Left Immediate\n+instruct lShiftL_reg_imm(iRegLNoSp dst, iRegL src1, immI src2) %{\n+  match(Set dst (LShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"slli  $dst, $src1, ($src2 & 0x3f)\\t#@lShiftL_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 6 bits of the I-immediate field for RV64I\n+    __ slli(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (unsigned) $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Shift Right Logical Register\n+\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+instruct urShiftL_reg_reg(iRegLNoSp dst, iRegL src1, iRegIorL2I src2) %{\n+  match(Set dst (URShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"srl  $dst, $src1, $src2\\t#@urShiftL_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ srl(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Right Logical Immediate\n+instruct urShiftL_reg_imm(iRegLNoSp dst, iRegL src1, immI src2) %{\n+  match(Set dst (URShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"srli  $dst, $src1, ($src2 & 0x3f)\\t#@urShiftL_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 6 bits of the I-immediate field for RV64I\n+    __ srli(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (unsigned) $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ A special-case pattern for card table stores.\n+instruct urShiftP_reg_imm(iRegLNoSp dst, iRegP src1, immI src2) %{\n+  match(Set dst (URShiftL (CastP2X src1) src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"srli  $dst, p2x($src1), ($src2 & 0x3f)\\t#@urShiftP_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 6 bits of the I-immediate field for RV64I\n+    __ srli(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (unsigned) $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ Shift Right Arithmetic Register\n+\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+instruct rShiftL_reg_reg(iRegLNoSp dst, iRegL src1, iRegIorL2I src2) %{\n+  match(Set dst (RShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"sra  $dst, $src1, $src2\\t#@rShiftL_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ sra(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_vshift);\n+%}\n+\n+\/\/ Shift Right Arithmetic Immediate\n+instruct rShiftL_reg_imm(iRegLNoSp dst, iRegL src1, immI src2) %{\n+  match(Set dst (RShiftL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"srai  $dst, $src1, ($src2 & 0x3f)\\t#@rShiftL_reg_imm\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    \/\/ the shift amount is encoded in the lower\n+    \/\/ 6 bits of the I-immediate field for RV64I\n+    __ srai(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (unsigned) $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+instruct regI_not_reg(iRegINoSp dst, iRegI src1, immI_M1 m1) %{\n+  match(Set dst (XorI src1 m1));\n+  ins_cost(ALU_COST);\n+  format %{ \"xori  $dst, $src1, -1\\t#@regI_not_reg\" %}\n+\n+  ins_encode %{\n+    __ xori(as_Register($dst$$reg), as_Register($src1$$reg), -1);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct regL_not_reg(iRegLNoSp dst, iRegL src1, immL_M1 m1) %{\n+  match(Set dst (XorL src1 m1));\n+  ins_cost(ALU_COST);\n+  format %{ \"xori  $dst, $src1, -1\\t#@regL_not_reg\" %}\n+\n+  ins_encode %{\n+    __ xori(as_Register($dst$$reg), as_Register($src1$$reg), -1);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n+\/\/ ============================================================================\n+\/\/ Floating Point Arithmetic Instructions\n+\n+instruct addF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (AddF src1 src2));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fadd.s  $dst, $src1, $src2\\t#@addF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fadd_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+instruct addD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (AddD src1 src2));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fadd.d  $dst, $src1, $src2\\t#@addD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fadd_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_d);\n+%}\n+\n+instruct subF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (SubF src1 src2));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fsub.s  $dst, $src1, $src2\\t#@subF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fsub_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+instruct subD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (SubD src1 src2));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fsub.d  $dst, $src1, $src2\\t#@subD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fsub_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_d);\n+%}\n+\n+instruct mulF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (MulF src1 src2));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fmul.s  $dst, $src1, $src2\\t#@mulF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmul_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+instruct mulD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (MulD src1 src2));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fmul.d  $dst, $src1, $src2\\t#@mulD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmul_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_d);\n+%}\n+\n+\/\/ src1 * src2 + src3\n+instruct maddF_reg_reg(fRegF dst, fRegF src1, fRegF src2, fRegF src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaF src3 (Binary src1 src2)));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fmadd.s  $dst, $src1, $src2, $src3\\t#@maddF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmadd_s(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ src1 * src2 + src3\n+instruct maddD_reg_reg(fRegD dst, fRegD src1, fRegD src2, fRegD src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaD src3 (Binary src1 src2)));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fmadd.d  $dst, $src1, $src2, $src3\\t#@maddD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmadd_d(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ src1 * src2 - src3\n+instruct msubF_reg_reg(fRegF dst, fRegF src1, fRegF src2, fRegF src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaF (NegF src3) (Binary src1 src2)));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fmsub.s  $dst, $src1, $src2, $src3\\t#@msubF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmsub_s(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ src1 * src2 - src3\n+instruct msubD_reg_reg(fRegD dst, fRegD src1, fRegD src2, fRegD src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaD (NegD src3) (Binary src1 src2)));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fmsub.d  $dst, $src1, $src2, $src3\\t#@msubD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmsub_d(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ -src1 * src2 + src3\n+instruct nmsubF_reg_reg(fRegF dst, fRegF src1, fRegF src2, fRegF src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaF src3 (Binary (NegF src1) src2)));\n+  match(Set dst (FmaF src3 (Binary src1 (NegF src2))));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fnmsub.s  $dst, $src1, $src2, $src3\\t#@nmsubF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fnmsub_s(as_FloatRegister($dst$$reg),\n+                as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg),\n+                as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ -src1 * src2 + src3\n+instruct nmsubD_reg_reg(fRegD dst, fRegD src1, fRegD src2, fRegD src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaD src3 (Binary (NegD src1) src2)));\n+  match(Set dst (FmaD src3 (Binary src1 (NegD src2))));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fnmsub.d  $dst, $src1, $src2, $src3\\t#@nmsubD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fnmsub_d(as_FloatRegister($dst$$reg),\n+                as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg),\n+                as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ -src1 * src2 - src3\n+instruct nmaddF_reg_reg(fRegF dst, fRegF src1, fRegF src2, fRegF src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaF (NegF src3) (Binary (NegF src1) src2)));\n+  match(Set dst (FmaF (NegF src3) (Binary src1 (NegF src2))));\n+\n+  ins_cost(FMUL_SINGLE_COST);\n+  format %{ \"fnmadd.s  $dst, $src1, $src2, $src3\\t#@nmaddF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fnmadd_s(as_FloatRegister($dst$$reg),\n+                as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg),\n+                as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ -src1 * src2 - src3\n+instruct nmaddD_reg_reg(fRegD dst, fRegD src1, fRegD src2, fRegD src3) %{\n+  predicate(UseFMA);\n+  match(Set dst (FmaD (NegD src3) (Binary (NegD src1) src2)));\n+  match(Set dst (FmaD (NegD src3) (Binary src1 (NegD src2))));\n+\n+  ins_cost(FMUL_DOUBLE_COST);\n+  format %{ \"fnmadd.d  $dst, $src1, $src2, $src3\\t#@nmaddD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fnmadd_d(as_FloatRegister($dst$$reg),\n+                as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg),\n+                as_FloatRegister($src3$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ Math.max(FF)F\n+instruct maxF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (MaxF src1 src2));\n+  effect(TEMP_DEF dst);\n+\n+  format %{ \"maxF $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ minmax_FD(as_FloatRegister($dst$$reg),\n+                 as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg),\n+                 false \/* is_double *\/, false \/* is_min *\/);\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+\/\/ Math.min(FF)F\n+instruct minF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (MinF src1 src2));\n+  effect(TEMP_DEF dst);\n+\n+  format %{ \"minF $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ minmax_FD(as_FloatRegister($dst$$reg),\n+                 as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg),\n+                 false \/* is_double *\/, true \/* is_min *\/);\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+\/\/ Math.max(DD)D\n+instruct maxD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (MaxD src1 src2));\n+  effect(TEMP_DEF dst);\n+\n+  format %{ \"maxD $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ minmax_FD(as_FloatRegister($dst$$reg),\n+                 as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg),\n+                 true \/* is_double *\/, false \/* is_min *\/);\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_d);\n+%}\n+\n+\/\/ Math.min(DD)D\n+instruct minD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (MinD src1 src2));\n+  effect(TEMP_DEF dst);\n+\n+  format %{ \"minD $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ minmax_FD(as_FloatRegister($dst$$reg),\n+                 as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg),\n+                 true \/* is_double *\/, true \/* is_min *\/);\n+  %}\n+\n+  ins_pipe(fp_dop_reg_reg_d);\n+%}\n+\n+instruct divF_reg_reg(fRegF dst, fRegF src1, fRegF src2) %{\n+  match(Set dst (DivF src1  src2));\n+\n+  ins_cost(FDIV_COST);\n+  format %{ \"fdiv.s  $dst, $src1, $src2\\t#@divF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fdiv_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_div_s);\n+%}\n+\n+instruct divD_reg_reg(fRegD dst, fRegD src1, fRegD src2) %{\n+  match(Set dst (DivD src1  src2));\n+\n+  ins_cost(FDIV_COST);\n+  format %{ \"fdiv.d  $dst, $src1, $src2\\t#@divD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fdiv_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src1$$reg),\n+              as_FloatRegister($src2$$reg));\n+  %}\n+\n+  ins_pipe(fp_div_d);\n+%}\n+\n+instruct negF_reg_reg(fRegF dst, fRegF src) %{\n+  match(Set dst (NegF src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fsgnjn.s  $dst, $src, $src\\t#@negF_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fneg_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_uop_s);\n+%}\n+\n+instruct negD_reg_reg(fRegD dst, fRegD src) %{\n+  match(Set dst (NegD src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fsgnjn.d  $dst, $src, $src\\t#@negD_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fneg_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_uop_d);\n+%}\n+\n+instruct absI_reg(iRegINoSp dst, iRegIorL2I src) %{\n+  match(Set dst (AbsI src));\n+\n+  ins_cost(ALU_COST * 3);\n+  format %{\n+    \"sraiw  t0, $src, 0x1f\\n\\t\"\n+    \"addw  $dst, $src, t0\\n\\t\"\n+    \"xorr  $dst, $dst, t0\\t#@absI_reg\"\n+  %}\n+\n+  ins_encode %{\n+    __ sraiw(t0, as_Register($src$$reg), 0x1f);\n+    __ addw(as_Register($dst$$reg), as_Register($src$$reg), t0);\n+    __ xorr(as_Register($dst$$reg), as_Register($dst$$reg), t0);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct absL_reg(iRegLNoSp dst, iRegL src) %{\n+  match(Set dst (AbsL src));\n+\n+  ins_cost(ALU_COST * 3);\n+  format %{\n+    \"srai  t0, $src, 0x3f\\n\\t\"\n+    \"add  $dst, $src, t0\\n\\t\"\n+    \"xorr  $dst, $dst, t0\\t#@absL_reg\"\n+  %}\n+\n+  ins_encode %{\n+    __ srai(t0, as_Register($src$$reg), 0x3f);\n+    __ add(as_Register($dst$$reg), as_Register($src$$reg), t0);\n+    __ xorr(as_Register($dst$$reg), as_Register($dst$$reg), t0);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct absF_reg(fRegF dst, fRegF src) %{\n+  match(Set dst (AbsF src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fsgnjx.s  $dst, $src, $src\\t#@absF_reg\" %}\n+  ins_encode %{\n+    __ fabs_s(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_uop_s);\n+%}\n+\n+instruct absD_reg(fRegD dst, fRegD src) %{\n+  match(Set dst (AbsD src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fsgnjx.d  $dst, $src, $src\\t#@absD_reg\" %}\n+  ins_encode %{\n+    __ fabs_d(as_FloatRegister($dst$$reg),\n+              as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_uop_d);\n+%}\n+\n+instruct sqrtF_reg(fRegF dst, fRegF src) %{\n+  match(Set dst (SqrtF src));\n+\n+  ins_cost(FSQRT_COST);\n+  format %{ \"fsqrt.s  $dst, $src\\t#@sqrtF_reg\" %}\n+  ins_encode %{\n+    __ fsqrt_s(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_sqrt_s);\n+%}\n+\n+instruct sqrtD_reg(fRegD dst, fRegD src) %{\n+  match(Set dst (SqrtD src));\n+\n+  ins_cost(FSQRT_COST);\n+  format %{ \"fsqrt.d  $dst, $src\\t#@sqrtD_reg\" %}\n+  ins_encode %{\n+    __ fsqrt_d(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_sqrt_d);\n+%}\n+\n+\/\/ Arithmetic Instructions End\n+\n+\/\/ ============================================================================\n+\/\/ Logical Instructions\n+\n+\/\/ Register And\n+instruct andI_reg_reg(iRegINoSp dst, iRegI src1, iRegI src2) %{\n+  match(Set dst (AndI src1 src2));\n+\n+  format %{ \"andr  $dst, $src1, $src2\\t#@andI_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ andr(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate And\n+instruct andI_reg_imm(iRegINoSp dst, iRegI src1, immIAdd src2) %{\n+  match(Set dst (AndI src1 src2));\n+\n+  format %{ \"andi  $dst, $src1, $src2\\t#@andI_reg_imm\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ andi(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Register Or\n+instruct orI_reg_reg(iRegINoSp dst, iRegI src1, iRegI src2) %{\n+  match(Set dst (OrI src1 src2));\n+\n+  format %{ \"orr  $dst, $src1, $src2\\t#@orI_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ orr(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate Or\n+instruct orI_reg_imm(iRegINoSp dst, iRegI src1, immIAdd src2) %{\n+  match(Set dst (OrI src1 src2));\n+\n+  format %{ \"ori  $dst, $src1, $src2\\t#@orI_reg_imm\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ ori(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Register Xor\n+instruct xorI_reg_reg(iRegINoSp dst, iRegI src1, iRegI src2) %{\n+  match(Set dst (XorI src1 src2));\n+\n+  format %{ \"xorr  $dst, $src1, $src2\\t#@xorI_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ xorr(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate Xor\n+instruct xorI_reg_imm(iRegINoSp dst, iRegI src1, immIAdd src2) %{\n+  match(Set dst (XorI src1 src2));\n+\n+  format %{ \"xori  $dst, $src1, $src2\\t#@xorI_reg_imm\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ xori(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Register And Long\n+instruct andL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (AndL src1 src2));\n+\n+  format %{ \"andr  $dst, $src1, $src2\\t#@andL_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ andr(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate And Long\n+instruct andL_reg_imm(iRegLNoSp dst, iRegL src1, immLAdd src2) %{\n+  match(Set dst (AndL src1 src2));\n+\n+  format %{ \"andi  $dst, $src1, $src2\\t#@andL_reg_imm\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ andi(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Register Or Long\n+instruct orL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (OrL src1 src2));\n+\n+  format %{ \"orr  $dst, $src1, $src2\\t#@orL_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ orr(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate Or Long\n+instruct orL_reg_imm(iRegLNoSp dst, iRegL src1, immLAdd src2) %{\n+  match(Set dst (OrL src1 src2));\n+\n+  format %{ \"ori  $dst, $src1, $src2\\t#@orL_reg_imm\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ ori(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ Register Xor Long\n+instruct xorL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (XorL src1 src2));\n+\n+  format %{ \"xorr  $dst, $src1, $src2\\t#@xorL_reg_reg\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ xorr(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Immediate Xor Long\n+instruct xorL_reg_imm(iRegLNoSp dst, iRegL src1, immLAdd src2) %{\n+  match(Set dst (XorL src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"xori  $dst, $src1, $src2\\t#@xorL_reg_imm\" %}\n+\n+  ins_encode %{\n+    __ xori(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            (int32_t)($src2$$constant));\n+  %}\n+\n+  ins_pipe(ialu_reg_imm);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ BSWAP Instructions\n+\n+instruct bytes_reverse_int(iRegINoSp dst, iRegIorL2I src, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesI src));\n+  effect(TEMP cr);\n+\n+  ins_cost(ALU_COST * 13);\n+  format %{ \"revb_w_w  $dst, $src\\t#@bytes_reverse_int\" %}\n+\n+  ins_encode %{\n+    __ revb_w_w(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_long(iRegLNoSp dst, iRegL src, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesL src));\n+  effect(TEMP cr);\n+\n+  ins_cost(ALU_COST * 29);\n+  format %{ \"revb  $dst, $src\\t#@bytes_reverse_long\" %}\n+\n+  ins_encode %{\n+    __ revb(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short(iRegINoSp dst, iRegIorL2I src) %{\n+  match(Set dst (ReverseBytesUS src));\n+\n+  ins_cost(ALU_COST * 5);\n+  format %{ \"revb_h_h_u  $dst, $src\\t#@bytes_reverse_unsigned_short\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h_u(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_short(iRegINoSp dst, iRegIorL2I src) %{\n+  match(Set dst (ReverseBytesS src));\n+\n+  ins_cost(ALU_COST * 5);\n+  format %{ \"revb_h_h  $dst, $src\\t#@bytes_reverse_short\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ MemBar Instruction\n+\n+instruct load_fence() %{\n+  match(LoadFence);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"#@load_fence\" %}\n+\n+  ins_encode %{\n+    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_acquire() %{\n+  match(MemBarAcquire);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"#@membar_acquire\\n\\t\"\n+            \"fence ir iorw\" %}\n+\n+  ins_encode %{\n+    __ block_comment(\"membar_acquire\");\n+    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_acquire_lock() %{\n+  match(MemBarAcquireLock);\n+  ins_cost(0);\n+\n+  format %{ \"#@membar_acquire_lock (elided)\" %}\n+\n+  ins_encode %{\n+    __ block_comment(\"membar_acquire_lock (elided)\");\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct store_fence() %{\n+  match(StoreFence);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"#@store_fence\" %}\n+\n+  ins_encode %{\n+    __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_release() %{\n+  match(MemBarRelease);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"#@membar_release\\n\\t\"\n+            \"fence iorw ow\" %}\n+\n+  ins_encode %{\n+    __ block_comment(\"membar_release\");\n+    __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_storestore() %{\n+  match(MemBarStoreStore);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"MEMBAR-store-store\\t#@membar_storestore\" %}\n+\n+  ins_encode %{\n+    __ membar(MacroAssembler::StoreStore);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_release_lock() %{\n+  match(MemBarReleaseLock);\n+  ins_cost(0);\n+\n+  format %{ \"#@membar_release_lock (elided)\" %}\n+\n+  ins_encode %{\n+    __ block_comment(\"membar_release_lock (elided)\");\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct membar_volatile() %{\n+  match(MemBarVolatile);\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"#@membar_volatile\\n\\t\"\n+             \"fence iorw iorw\"%}\n+\n+  ins_encode %{\n+    __ block_comment(\"membar_volatile\");\n+    __ membar(MacroAssembler::StoreLoad);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Cast Instructions (Java-level type cast)\n+\n+instruct castX2P(iRegPNoSp dst, iRegL src) %{\n+  match(Set dst (CastX2P src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $src\\t# long -> ptr, #@castX2P\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    if ($dst$$reg != $src$$reg) {\n+      __ mv(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castP2X(iRegLNoSp dst, iRegP src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $src\\t# ptr -> long, #@castP2X\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    if ($dst$$reg != $src$$reg) {\n+      __ mv(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castPP(iRegPNoSp dst)\n+%{\n+  match(Set dst (CastPP dst));\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"# castPP of $dst, #@castPP\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct castII(iRegI dst)\n+%{\n+  match(Set dst (CastII dst));\n+\n+  size(0);\n+  format %{ \"# castII of $dst, #@castII\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct checkCastPP(iRegPNoSp dst)\n+%{\n+  match(Set dst (CheckCastPP dst));\n+\n+  size(0);\n+  ins_cost(0);\n+  format %{ \"# checkcastPP of $dst, #@checkCastPP\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Convert Instructions\n+\n+\/\/ int to bool\n+instruct convI2Bool(iRegINoSp dst, iRegI src)\n+%{\n+  match(Set dst (Conv2B src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"snez  $dst, $src\\t#@convI2Bool\" %}\n+\n+  ins_encode %{\n+    __ snez(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ pointer to bool\n+instruct convP2Bool(iRegINoSp dst, iRegP src)\n+%{\n+  match(Set dst (Conv2B src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"snez  $dst, $src\\t#@convP2Bool\" %}\n+\n+  ins_encode %{\n+    __ snez(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ int <-> long\n+\n+instruct convI2L_reg_reg(iRegLNoSp dst, iRegIorL2I src)\n+%{\n+  match(Set dst (ConvI2L src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addw  $dst, $src, zr\\t#@convI2L_reg_reg\" %}\n+  ins_encode %{\n+    __ addw(as_Register($dst$$reg), as_Register($src$$reg), zr);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct convL2I_reg(iRegINoSp dst, iRegL src) %{\n+  match(Set dst (ConvL2I src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"addw  $dst, $src, zr\\t#@convL2I_reg\" %}\n+\n+  ins_encode %{\n+    __ addw(as_Register($dst$$reg), as_Register($src$$reg), zr);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ int to unsigned long (Zero-extend)\n+instruct convI2UL_reg_reg(iRegLNoSp dst, iRegIorL2I src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L src) mask));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"zero_extend $dst, $src, 32\\t# i2ul, #@convI2UL_reg_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ zero_extend(as_Register($dst$$reg), as_Register($src$$reg), 32);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ float <-> double\n+\n+instruct convF2D_reg(fRegD dst, fRegF src) %{\n+  match(Set dst (ConvF2D src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.d.s  $dst, $src\\t#@convF2D_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_d_s(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_f2d);\n+%}\n+\n+instruct convD2F_reg(fRegF dst, fRegD src) %{\n+  match(Set dst (ConvD2F src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.s.d  $dst, $src\\t#@convD2F_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_s_d(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_d2f);\n+%}\n+\n+\/\/ float <-> int\n+\n+instruct convF2I_reg_reg(iRegINoSp dst, fRegF src) %{\n+  match(Set dst (ConvF2I src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.w.s  $dst, $src\\t#@convF2I_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_w_s_safe($dst$$Register, $src$$FloatRegister);\n+  %}\n+\n+  ins_pipe(fp_f2i);\n+%}\n+\n+instruct convI2F_reg_reg(fRegF dst, iRegIorL2I src) %{\n+  match(Set dst (ConvI2F src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.s.w  $dst, $src\\t#@convI2F_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_s_w(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_i2f);\n+%}\n+\n+\/\/ float <-> long\n+\n+instruct convF2L_reg_reg(iRegLNoSp dst, fRegF src) %{\n+  match(Set dst (ConvF2L src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.l.s  $dst, $src\\t#@convF2L_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_l_s_safe($dst$$Register, $src$$FloatRegister);\n+  %}\n+\n+  ins_pipe(fp_f2l);\n+%}\n+\n+instruct convL2F_reg_reg(fRegF dst, iRegL src) %{\n+  match(Set dst (ConvL2F src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.s.l  $dst, $src\\t#@convL2F_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_s_l(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_l2f);\n+%}\n+\n+\/\/ double <-> int\n+\n+instruct convD2I_reg_reg(iRegINoSp dst, fRegD src) %{\n+  match(Set dst (ConvD2I src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.w.d  $dst, $src\\t#@convD2I_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_w_d_safe($dst$$Register, $src$$FloatRegister);\n+  %}\n+\n+  ins_pipe(fp_d2i);\n+%}\n+\n+instruct convI2D_reg_reg(fRegD dst, iRegIorL2I src) %{\n+  match(Set dst (ConvI2D src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.d.w  $dst, $src\\t#@convI2D_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_d_w(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_i2d);\n+%}\n+\n+\/\/ double <-> long\n+\n+instruct convD2L_reg_reg(iRegLNoSp dst, fRegD src) %{\n+  match(Set dst (ConvD2L src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.l.d  $dst, $src\\t#@convD2L_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_l_d_safe($dst$$Register, $src$$FloatRegister);\n+  %}\n+\n+  ins_pipe(fp_d2l);\n+%}\n+\n+instruct convL2D_reg_reg(fRegD dst, iRegL src) %{\n+  match(Set dst (ConvL2D src));\n+\n+  ins_cost(XFER_COST);\n+  format %{ \"fcvt.d.l  $dst, $src\\t#@convL2D_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fcvt_d_l(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_l2d);\n+%}\n+\n+\/\/ Convert oop into int for vectors alignment masking\n+instruct convP2I(iRegINoSp dst, iRegP src) %{\n+  match(Set dst (ConvL2I (CastP2X src)));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"zero_extend $dst, $src, 32\\t# ptr -> int, #@convP2I\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ zero_extend($dst$$Register, $src$$Register, 32);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Convert compressed oop into int for vectors alignment masking\n+\/\/ in case of 32bit oops (heap < 4Gb).\n+instruct convN2I(iRegINoSp dst, iRegN src)\n+%{\n+  predicate(Universe::narrow_oop_shift() == 0);\n+  match(Set dst (ConvL2I (CastP2X (DecodeN src))));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"mv  $dst, $src\\t# compressed ptr -> int, #@convN2I\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ mv($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Convert oop pointer into compressed form\n+instruct encodeHeapOop(iRegNNoSp dst, iRegP src) %{\n+  match(Set dst (EncodeP src));\n+  ins_cost(ALU_COST);\n+  format %{ \"encode_heap_oop  $dst, $src\\t#@encodeHeapOop\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    __ encode_heap_oop(d, s);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decodeHeapOop(iRegPNoSp dst, iRegN src) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&\n+            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+\n+  ins_cost(0);\n+  format %{ \"decode_heap_oop  $dst, $src\\t#@decodeHeapOop\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    __ decode_heap_oop(d, s);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decodeHeapOop_not_null(iRegPNoSp dst, iRegN src) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||\n+            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+\n+  ins_cost(0);\n+  format %{ \"decode_heap_oop_not_null $dst, $src\\t#@decodeHeapOop_not_null\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    __ decode_heap_oop_not_null(d, s);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Convert klass pointer into compressed form.\n+instruct encodeKlass_not_null(iRegNNoSp dst, iRegP src) %{\n+  match(Set dst (EncodePKlass src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"encode_klass_not_null  $dst, $src\\t#@encodeKlass_not_null\" %}\n+\n+  ins_encode %{\n+    Register src_reg = as_Register($src$$reg);\n+    Register dst_reg = as_Register($dst$$reg);\n+    __ encode_klass_not_null(dst_reg, src_reg, t0);\n+  %}\n+\n+   ins_pipe(ialu_reg);\n+%}\n+\n+instruct decodeKlass_not_null(iRegPNoSp dst, iRegN src, iRegPNoSp tmp) %{\n+  match(Set dst (DecodeNKlass src));\n+\n+  effect(TEMP tmp);\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"decode_klass_not_null  $dst, $src\\t#@decodeKlass_not_null\" %}\n+\n+  ins_encode %{\n+    Register src_reg = as_Register($src$$reg);\n+    Register dst_reg = as_Register($dst$$reg);\n+    Register tmp_reg = as_Register($tmp$$reg);\n+    __ decode_klass_not_null(dst_reg, src_reg, tmp_reg);\n+  %}\n+\n+   ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ stack <-> reg and reg <-> reg shuffles with no conversion\n+\n+instruct MoveF2I_stack_reg(iRegINoSp dst, stackSlotF src) %{\n+\n+  match(Set dst (MoveF2I src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(LOAD_COST);\n+\n+  format %{ \"lw  $dst, $src\\t#@MoveF2I_stack_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ lw(as_Register($dst$$reg), Address(sp, $src$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_reg);\n+\n+%}\n+\n+instruct MoveI2F_stack_reg(fRegF dst, stackSlotI src) %{\n+\n+  match(Set dst (MoveI2F src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(LOAD_COST);\n+\n+  format %{ \"flw  $dst, $src\\t#@MoveI2F_stack_reg\" %}\n+\n+  ins_encode %{\n+    __ flw(as_FloatRegister($dst$$reg), Address(sp, $src$$disp));\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+\n+%}\n+\n+instruct MoveD2L_stack_reg(iRegLNoSp dst, stackSlotD src) %{\n+\n+  match(Set dst (MoveD2L src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(LOAD_COST);\n+\n+  format %{ \"ld  $dst, $src\\t#@MoveD2L_stack_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ ld(as_Register($dst$$reg), Address(sp, $src$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_reg);\n+\n+%}\n+\n+instruct MoveL2D_stack_reg(fRegD dst, stackSlotL src) %{\n+\n+  match(Set dst (MoveL2D src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(LOAD_COST);\n+\n+  format %{ \"fld  $dst, $src\\t#@MoveL2D_stack_reg\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ fld(as_FloatRegister($dst$$reg), Address(sp, $src$$disp));\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+\n+%}\n+\n+instruct MoveF2I_reg_stack(stackSlotI dst, fRegF src) %{\n+\n+  match(Set dst (MoveF2I src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(STORE_COST);\n+\n+  format %{ \"fsw  $src, $dst\\t#@MoveF2I_reg_stack\" %}\n+\n+  ins_encode %{\n+    __ fsw(as_FloatRegister($src$$reg), Address(sp, $dst$$disp));\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+\n+%}\n+\n+instruct MoveI2F_reg_stack(stackSlotF dst, iRegI src) %{\n+\n+  match(Set dst (MoveI2F src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(STORE_COST);\n+\n+  format %{ \"sw  $src, $dst\\t#@MoveI2F_reg_stack\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sw(as_Register($src$$reg), Address(sp, $dst$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_reg);\n+\n+%}\n+\n+instruct MoveD2L_reg_stack(stackSlotL dst, fRegD src) %{\n+\n+  match(Set dst (MoveD2L src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(STORE_COST);\n+\n+  format %{ \"fsd  $dst, $src\\t#@MoveD2L_reg_stack\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ fsd(as_FloatRegister($src$$reg), Address(sp, $dst$$disp));\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+\n+%}\n+\n+instruct MoveL2D_reg_stack(stackSlotD dst, iRegL src) %{\n+\n+  match(Set dst (MoveL2D src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(STORE_COST);\n+\n+  format %{ \"sd  $src, $dst\\t#@MoveL2D_reg_stack\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    __ sd(as_Register($src$$reg), Address(sp, $dst$$disp));\n+  %}\n+\n+  ins_pipe(istore_reg_reg);\n+\n+%}\n+\n+instruct MoveF2I_reg_reg(iRegINoSp dst, fRegF src) %{\n+\n+  match(Set dst (MoveF2I src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.x.w  $dst, $src\\t#@MoveL2D_reg_stack\" %}\n+\n+  ins_encode %{\n+    __ fmv_x_w(as_Register($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_f2i);\n+\n+%}\n+\n+instruct MoveI2F_reg_reg(fRegF dst, iRegI src) %{\n+\n+  match(Set dst (MoveI2F src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.w.x  $dst, $src\\t#@MoveI2F_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmv_w_x(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_i2f);\n+\n+%}\n+\n+instruct MoveD2L_reg_reg(iRegLNoSp dst, fRegD src) %{\n+\n+  match(Set dst (MoveD2L src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.x.d $dst, $src\\t#@MoveD2L_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmv_x_d(as_Register($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_d2l);\n+\n+%}\n+\n+instruct MoveL2D_reg_reg(fRegD dst, iRegL src) %{\n+\n+  match(Set dst (MoveL2D src));\n+\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(XFER_COST);\n+\n+  format %{ \"fmv.d.x  $dst, $src\\t#@MoveD2L_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ fmv_d_x(as_FloatRegister($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(fp_l2d);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Compare Instructions which set the result float comparisons in dest register.\n+\n+instruct cmpF3_reg_reg(iRegINoSp dst, fRegF op1, fRegF op2)\n+%{\n+  match(Set dst (CmpF3 op1 op2));\n+\n+  ins_cost(XFER_COST * 2 + BRANCH_COST + ALU_COST);\n+  format %{ \"flt.s  $dst, $op2, $op1\\t#@cmpF3_reg_reg\\n\\t\"\n+            \"bgtz   $dst, done\\n\\t\"\n+            \"feq.s  $dst, $op1, $op2\\n\\t\"\n+            \"addi   $dst, $dst, -1\\t#@cmpF3_reg_reg\"\n+  %}\n+\n+  ins_encode %{\n+    \/\/ we want -1 for unordered or less than, 0 for equal and 1 for greater than.\n+    __ float_compare(as_Register($dst$$reg), as_FloatRegister($op1$$reg),\n+                     as_FloatRegister($op2$$reg), -1 \/*unordered_result < 0*\/);\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct cmpD3_reg_reg(iRegINoSp dst, fRegD op1, fRegD op2)\n+%{\n+  match(Set dst (CmpD3 op1 op2));\n+\n+  ins_cost(XFER_COST * 2 + BRANCH_COST + ALU_COST);\n+  format %{ \"flt.d  $dst, $op2, $op1\\t#@cmpD3_reg_reg\\n\\t\"\n+            \"bgtz   $dst, done\\n\\t\"\n+            \"feq.d  $dst, $op1, $op2\\n\\t\"\n+            \"addi   $dst, $dst, -1\\t#@cmpD3_reg_reg\"\n+  %}\n+\n+  ins_encode %{\n+    \/\/ we want -1 for unordered or less than, 0 for equal and 1 for greater than.\n+    __ double_compare(as_Register($dst$$reg), as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg), -1 \/*unordered_result < 0*\/);\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct cmpL3_reg_reg(iRegINoSp dst, iRegL op1, iRegL op2)\n+%{\n+  match(Set dst (CmpL3 op1 op2));\n+\n+  ins_cost(ALU_COST * 3 + BRANCH_COST);\n+  format %{ \"slt   $dst, $op2, $op1\\t#@cmpL3_reg_reg\\n\\t\"\n+            \"bnez  $dst, done\\n\\t\"\n+            \"slt  $dst, $op1, $op2\\n\\t\"\n+            \"neg   $dst, $dst\\t#@cmpL3_reg_reg\"\n+  %}\n+  ins_encode %{\n+    __ cmp_l2i(t0, as_Register($op1$$reg), as_Register($op2$$reg));\n+    __ mv(as_Register($dst$$reg), t0);\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct cmpLTMask_reg_reg(iRegINoSp dst, iRegI p, iRegI q)\n+%{\n+  match(Set dst (CmpLTMask p q));\n+\n+  ins_cost(2 * ALU_COST);\n+\n+  format %{ \"slt $dst, $p, $q\\t#@cmpLTMask_reg_reg\\n\\t\"\n+            \"subw $dst, zr, $dst\\t#@cmpLTMask_reg_reg\"\n+  %}\n+\n+  ins_encode %{\n+    __ slt(as_Register($dst$$reg), as_Register($p$$reg), as_Register($q$$reg));\n+    __ subw(as_Register($dst$$reg), zr, as_Register($dst$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct cmpLTMask_reg_zero(iRegINoSp dst, iRegIorL2I op, immI0 zero)\n+%{\n+  match(Set dst (CmpLTMask op zero));\n+\n+  ins_cost(ALU_COST);\n+\n+  format %{ \"sraiw $dst, $dst, 31\\t#@cmpLTMask_reg_reg\" %}\n+\n+  ins_encode %{\n+    __ sraiw(as_Register($dst$$reg), as_Register($op$$reg), 31);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\n+\/\/ ============================================================================\n+\/\/ Max and Min\n+\n+instruct minI_rReg(iRegINoSp dst, iRegI src1, iRegI src2)\n+%{\n+  match(Set dst (MinI src1 src2));\n+\n+  effect(DEF dst, USE src1, USE src2);\n+\n+  ins_cost(BRANCH_COST + ALU_COST * 2);\n+  format %{\n+    \"ble $src1, $src2, Lsrc1.\\t#@minI_rReg\\n\\t\"\n+    \"mv $dst, $src2\\n\\t\"\n+    \"j Ldone\\n\\t\"\n+    \"bind Lsrc1\\n\\t\"\n+    \"mv $dst, $src1\\n\\t\"\n+    \"bind\\t#@minI_rReg\"\n+  %}\n+\n+  ins_encode %{\n+    Label Lsrc1, Ldone;\n+    __ ble(as_Register($src1$$reg), as_Register($src2$$reg), Lsrc1);\n+    __ mv(as_Register($dst$$reg), as_Register($src2$$reg));\n+    __ j(Ldone);\n+    __ bind(Lsrc1);\n+    __ mv(as_Register($dst$$reg), as_Register($src1$$reg));\n+    __ bind(Ldone);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct maxI_rReg(iRegINoSp dst, iRegI src1, iRegI src2)\n+%{\n+  match(Set dst (MaxI src1 src2));\n+\n+  effect(DEF dst, USE src1, USE src2);\n+\n+  ins_cost(BRANCH_COST + ALU_COST * 2);\n+  format %{\n+    \"bge $src1, $src2, Lsrc1\\t#@maxI_rReg\\n\\t\"\n+    \"mv $dst, $src2\\n\\t\"\n+    \"j Ldone\\n\\t\"\n+    \"bind Lsrc1\\n\\t\"\n+    \"mv $dst, $src1\\n\\t\"\n+    \"bind\\t#@maxI_rReg\"\n+  %}\n+\n+  ins_encode %{\n+    Label Lsrc1, Ldone;\n+    __ bge(as_Register($src1$$reg), as_Register($src2$$reg), Lsrc1);\n+    __ mv(as_Register($dst$$reg), as_Register($src2$$reg));\n+    __ j(Ldone);\n+    __ bind(Lsrc1);\n+    __ mv(as_Register($dst$$reg), as_Register($src1$$reg));\n+    __ bind(Ldone);\n+\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Branch Instructions\n+\/\/ Direct Branch.\n+instruct branch(label lbl)\n+%{\n+  match(Goto);\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"j  $lbl\\t#@branch\" %}\n+\n+  ins_encode(riscv_enc_j(lbl));\n+\n+  ins_pipe(pipe_branch);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Compare and Branch Instructions\n+\n+\/\/ Patterns for short (< 12KiB) variants\n+\n+\/\/ Compare flags and branch near instructions.\n+instruct cmpFlag_branch(cmpOpEqNe cmp, rFlagsReg cr, label lbl) %{\n+  match(If cmp cr);\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp  $cr, zr, $lbl\\t#@cmpFlag_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($cr$$reg), *($lbl$$label));\n+  %}\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare signed int and branch near instructions\n+instruct cmpI_branch(cmpOp cmp, iRegI op1, iRegI op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpI_branch'.\n+  match(If cmp (CmpI op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpI_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpI_loop(cmpOp cmp, iRegI op1, iRegI op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpI_loop'.\n+  match(CountedLoopEnd cmp (CmpI op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpI_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare unsigned int and branch near instructions\n+instruct cmpU_branch(cmpOpU cmp, iRegI op1, iRegI op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpU_branch'.\n+  match(If cmp (CmpU op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpU_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpU_loop(cmpOpU cmp, iRegI op1, iRegI op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpU_loop'.\n+  match(CountedLoopEnd cmp (CmpU op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpU_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare signed long and branch near instructions\n+instruct cmpL_branch(cmpOp cmp, iRegL op1, iRegL op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpL_branch'.\n+  match(If cmp (CmpL op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpL_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpL_loop(cmpOp cmp, iRegL op1, iRegL op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpL_loop'.\n+  match(CountedLoopEnd cmp (CmpL op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpL_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare unsigned long and branch near instructions\n+instruct cmpUL_branch(cmpOpU cmp, iRegL op1, iRegL op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpUL_branch'.\n+  match(If cmp (CmpUL op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpUL_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpUL_loop(cmpOpU cmp, iRegL op1, iRegL op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpUL_loop'.\n+  match(CountedLoopEnd cmp (CmpUL op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpUL_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare pointer and branch near instructions\n+instruct cmpP_branch(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpP_branch'.\n+  match(If cmp (CmpP op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpP_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpP_loop(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpP_loop'.\n+  match(CountedLoopEnd cmp (CmpP op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpP_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare narrow pointer and branch near instructions\n+instruct cmpN_branch(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpN_branch'.\n+  match(If cmp (CmpN op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpN_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpN_loop(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpN_loop'.\n+  match(CountedLoopEnd cmp (CmpN op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpN_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare float and branch near instructions\n+instruct cmpF_branch(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpF_branch'.\n+  match(If cmp (CmpF op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST);\n+  format %{ \"float_b$cmp $op1, $op2, $lbl \\t#@cmpF_branch\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpF_loop(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpF_loop'.\n+  match(CountedLoopEnd cmp (CmpF op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST);\n+  format %{ \"float_b$cmp $op1, $op2, $lbl\\t#@cmpF_loop\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare double and branch near instructions\n+instruct cmpD_branch(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpD_branch'.\n+  match(If cmp (CmpD op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST);\n+  format %{ \"double_b$cmp $op1, $op2, $lbl\\t#@cmpD_branch\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode | MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n+                        as_FloatRegister($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpD_loop(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpD_loop'.\n+  match(CountedLoopEnd cmp (CmpD op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST);\n+  format %{ \"double_b$cmp $op1, $op2, $lbl\\t#@cmpD_loop\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode | MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n+                        as_FloatRegister($op2$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare signed int with zero and branch near instructions\n+instruct cmpI_reg_imm0_branch(cmpOp cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpI_reg_imm0_branch'.\n+  match(If cmp (CmpI op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpI_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpI_reg_imm0_loop(cmpOp cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpI_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpI op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpI_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare unsigned int with zero and branch near instructions\n+instruct cmpUEqNeLeGt_reg_imm0_branch(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpUEqNeLeGt_reg_imm0_branch'.\n+  match(If cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpUEqNeLeGt_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpUEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpUEqNeLeGt_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpUEqNeLeGt_reg_imm0_loop\" %}\n+\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare signed long with zero and branch near instructions\n+instruct cmpL_reg_imm0_branch(cmpOp cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpL_reg_imm0_branch'.\n+  match(If cmp (CmpL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpL_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpL_reg_imm0_loop(cmpOp cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpL_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpL_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare unsigned long with zero and branch near instructions\n+instruct cmpULEqNeLeGt_reg_imm0_branch(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpULEqNeLeGt_reg_imm0_branch'.\n+  match(If cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpULEqNeLeGt_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpULEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  \/\/ Same match rule as `far_cmpULEqNeLeGt_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpULEqNeLeGt_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare pointer with zero and branch near instructions\n+instruct cmpP_imm0_branch(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpP_reg_imm0_branch'.\n+  match(If cmp (CmpP op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpP_imm0_loop(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpP_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpP op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare narrow pointer with zero and branch near instructions\n+instruct cmpN_imm0_branch(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpN_reg_imm0_branch'.\n+  match(If cmp (CmpN op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpN_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpN_imm0_loop(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpN_reg_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpN op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpN_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Compare narrow pointer with pointer zero and branch near instructions\n+instruct cmpP_narrowOop_imm0_branch(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpP_narrowOop_imm0_branch'.\n+  match(If cmp (CmpP (DecodeN op1) zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_narrowOop_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+instruct cmpP_narrowOop_imm0_loop(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n+  \/\/ Same match rule as `far_cmpP_narrowOop_imm0_loop'.\n+  match(CountedLoopEnd cmp (CmpP (DecodeN op1) zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_narrowOop_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ Patterns for far (20KiB) variants\n+\n+instruct far_cmpFlag_branch(cmpOp cmp, rFlagsReg cr, label lbl) %{\n+  match(If cmp cr);\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"far_b$cmp $cr, zr, $lbl\\t#@far_cmpFlag_branch\"%}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($cr$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+\/\/ Compare signed int and branch far instructions\n+instruct far_cmpI_branch(cmpOp cmp, iRegI op1, iRegI op2, label lbl) %{\n+  match(If cmp (CmpI op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  \/\/ the format instruction [far_b$cmp] here is be used as two insructions\n+  \/\/ in macroassembler: b$not_cmp(op1, op2, done), j($lbl), bind(done)\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpI_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpI_loop(cmpOp cmp, iRegI op1, iRegI op2, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpI op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpI_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpU_branch(cmpOpU cmp, iRegI op1, iRegI op2, label lbl) %{\n+  match(If cmp (CmpU op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp $op1, $op2, $lbl\\t#@far_cmpU_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpU_loop(cmpOpU cmp, iRegI op1, iRegI op2, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpU op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp $op1, $op2, $lbl\\t#@far_cmpU_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpL_branch(cmpOp cmp, iRegL op1, iRegL op2, label lbl) %{\n+  match(If cmp (CmpL op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpL_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpLloop(cmpOp cmp, iRegL op1, iRegL op2, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpL op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpL_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpUL_branch(cmpOpU cmp, iRegL op1, iRegL op2, label lbl) %{\n+  match(If cmp (CmpUL op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpUL_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpUL_loop(cmpOpU cmp, iRegL op1, iRegL op2, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpUL op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpUL_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpP_branch(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n+%{\n+  match(If cmp (CmpP op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpP_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpP_loop(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpP op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpP_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpN_branch(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n+%{\n+  match(If cmp (CmpN op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpN_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+instruct far_cmpN_loop(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpN op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpN_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode | MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n+                  as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmp_branch);\n+%}\n+\n+\/\/ Float compare and branch instructions\n+instruct far_cmpF_branch(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n+%{\n+  match(If cmp (CmpF op1 op2));\n+\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST * 2);\n+  format %{ \"far_float_b$cmp $op1, $op2, $lbl\\t#@far_cmpF_branch\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct far_cmpF_loop(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpF op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST * 2);\n+  format %{ \"far_float_b$cmp $op1, $op2, $lbl\\t#@far_cmpF_loop\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+\/\/ Double compare and branch instructions\n+instruct far_cmpD_branch(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n+%{\n+  match(If cmp (CmpD op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST * 2);\n+  format %{ \"far_double_b$cmp $op1, $op2, $lbl\\t#@far_cmpD_branch\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode | MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n+                        as_FloatRegister($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct far_cmpD_loop(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpD op1 op2));\n+  effect(USE lbl);\n+\n+  ins_cost(XFER_COST + BRANCH_COST * 2);\n+  format %{ \"far_double_b$cmp $op1, $op2, $lbl\\t#@far_cmpD_loop\"%}\n+\n+  ins_encode %{\n+    __ float_cmp_branch($cmp$$cmpcode | MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n+                        as_FloatRegister($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct far_cmpI_reg_imm0_branch(cmpOp cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(If cmp (CmpI op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpI_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpI_reg_imm0_loop(cmpOp cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpI op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpI_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpUEqNeLeGt_imm0_branch(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(If cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpUEqNeLeGt_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpUEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpUEqNeLeGt_reg_imm0_loop\" %}\n+\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+\/\/ compare lt\/ge unsigned instructs has no short instruct with same match\n+instruct far_cmpULtGe_reg_imm0_branch(cmpOpULtGe cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(If cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULtGe_reg_imm0_branch\" %}\n+\n+  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpULtGe_reg_imm0_loop(cmpOpULtGe cmp, iRegI op1, immI0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpU op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULtGe_reg_imm0_loop\" %}\n+\n+  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpL_reg_imm0_branch(cmpOp cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(If cmp (CmpL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpL_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpL_reg_imm0_loop(cmpOp cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpL_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ cmp_branch($cmp$$cmpcode, as_Register($op1$$reg), zr, *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpULEqNeLeGt_reg_imm0_branch(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(If cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpULEqNeLeGt_reg_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpULEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpULEqNeLeGt_reg_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+\/\/ compare lt\/ge unsigned instructs has no short instruct with same match\n+instruct far_cmpULLtGe_reg_imm0_branch(cmpOpULtGe cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(If cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULLtGe_reg_imm0_branch\" %}\n+\n+  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpULLtGe_reg_imm0_loop(cmpOpULtGe cmp, iRegL op1, immL0 zero, label lbl)\n+%{\n+  match(CountedLoopEnd cmp (CmpUL op1 zero));\n+\n+  effect(USE op1, USE lbl);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULLtGe_reg_imm0_loop\" %}\n+\n+  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpP_imm0_branch(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n+  match(If cmp (CmpP op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpP_imm0_loop(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpP op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpN_imm0_branch(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n+  match(If cmp (CmpN op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpN_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpN_imm0_loop(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpN op1 zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+\n+  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpN_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpP_narrowOop_imm0_branch(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n+  match(If cmp (CmpP (DecodeN op1) zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_narrowOop_imm0_branch\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+instruct far_cmpP_narrowOop_imm0_loop(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n+  match(CountedLoopEnd cmp (CmpP (DecodeN op1) zero));\n+  effect(USE lbl);\n+\n+  ins_cost(BRANCH_COST * 2);\n+  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_narrowOop_imm0_loop\" %}\n+\n+  ins_encode %{\n+    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n+  %}\n+\n+  ins_pipe(pipe_cmpz_branch);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Conditional Move Instructions\n+instruct cmovI_cmpI(iRegINoSp dst, iRegI src, iRegI op1, iRegI op2, cmpOp cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpI op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+             \"bneg$cop $op1, $op2, skip\\t#@cmovI_cmpI\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovI_cmpU(iRegINoSp dst, iRegI src, iRegI op1, iRegI op2, cmpOpU cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpU op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+             \"bneg$cop $op1, $op2, skip\\t#@cmovI_cmpU\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode | MacroAssembler::unsigned_branch_mask,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovI_cmpL(iRegINoSp dst, iRegI src, iRegL op1, iRegL op2, cmpOp cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpL op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+             \"bneg$cop $op1, $op2, skip\\t#@cmovI_cmpL\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovL_cmpL(iRegLNoSp dst, iRegL src, iRegL op1, iRegL op2, cmpOp cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpL op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+             \"bneg$cop $op1, $op2, skip\\t#@cmovL_cmpL\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovL_cmpUL(iRegLNoSp dst, iRegL src, iRegL op1, iRegL op2, cmpOpU cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpUL op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+             \"bneg$cop $op1, $op2, skip\\t#@cmovL_cmpUL\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode | MacroAssembler::unsigned_branch_mask,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovI_cmpUL(iRegINoSp dst, iRegI src, iRegL op1, iRegL op2, cmpOpU cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpUL op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+  format %{\n+             \"bneg$cop $op1, $op2\\t#@cmovI_cmpUL\\n\\t\"\n+             \"mv $dst, $src\\n\\t\"\n+             \"skip:\"\n+         %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode | MacroAssembler::unsigned_branch_mask,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+\/\/ ============================================================================\n+\/\/ Procedure Call\/Return Instructions\n+\n+\/\/ Call Java Static Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallStaticJavaDirect(method meth)\n+%{\n+  match(CallStaticJava);\n+\n+  effect(USE meth);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"CALL,static $meth\\t#@CallStaticJavaDirect\" %}\n+\n+  ins_encode(riscv_enc_java_static_call(meth),\n+             riscv_enc_call_epilog);\n+\n+  ins_pipe(pipe_class_call);\n+  ins_alignment(4);\n+%}\n+\n+\/\/ TO HERE\n+\n+\/\/ Call Java Dynamic Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallDynamicJavaDirect(method meth, rFlagsReg cr)\n+%{\n+  match(CallDynamicJava);\n+\n+  effect(USE meth, KILL cr);\n+\n+  ins_cost(BRANCH_COST + ALU_COST * 6);\n+\n+  format %{ \"CALL,dynamic $meth\\t#@CallDynamicJavaDirect\" %}\n+\n+  ins_encode(riscv_enc_java_dynamic_call(meth),\n+             riscv_enc_call_epilog);\n+\n+  ins_pipe(pipe_class_call);\n+  ins_alignment(4);\n+%}\n+\n+\/\/ Call Runtime Instruction\n+\n+instruct CallRuntimeDirect(method meth, rFlagsReg cr)\n+%{\n+  match(CallRuntime);\n+\n+  effect(USE meth, KILL cr);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"CALL, runtime $meth\\t#@CallRuntimeDirect\" %}\n+\n+  ins_encode(riscv_enc_java_to_runtime(meth));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+\/\/ Call Runtime Instruction\n+\n+instruct CallLeafDirect(method meth, rFlagsReg cr)\n+%{\n+  match(CallLeaf);\n+\n+  effect(USE meth, KILL cr);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"CALL, runtime leaf $meth\\t#@CallLeafDirect\" %}\n+\n+  ins_encode(riscv_enc_java_to_runtime(meth));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+\/\/ Call Runtime Instruction\n+\n+instruct CallLeafNoFPDirect(method meth, rFlagsReg cr)\n+%{\n+  match(CallLeafNoFP);\n+\n+  effect(USE meth, KILL cr);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp $meth\\t#@CallLeafNoFPDirect\" %}\n+\n+  ins_encode(riscv_enc_java_to_runtime(meth));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Partial Subtype Check\n+\/\/\n+\/\/ superklass array for an instance of the superklass.  Set a hidden\n+\/\/ internal cache on a hit (cache is checked with exposed code in\n+\/\/ gen_subtype_check()).  Return zero for a hit.  The encoding\n+\/\/ ALSO sets flags.\n+\n+instruct partialSubtypeCheck(iRegP_R15 result, iRegP_R14 sub, iRegP_R10 super, iRegP_R12 tmp, rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  effect(KILL tmp, KILL cr);\n+\n+  ins_cost(2 * STORE_COST + 3 * LOAD_COST + 4 * ALU_COST + BRANCH_COST * 4);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\\t#@partialSubtypeCheck\" %}\n+\n+  ins_encode(riscv_enc_partial_subtype_check(sub, super, tmp, result));\n+\n+  opcode(0x1); \/\/ Force zero of result reg on hit\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct partialSubtypeCheckVsZero(iRegP_R15 result, iRegP_R14 sub, iRegP_R10 super, iRegP_R12 tmp,\n+                                   immP0 zero, rFlagsReg cr)\n+%{\n+  match(Set cr (CmpP (PartialSubtypeCheck sub super) zero));\n+  effect(KILL tmp, KILL result);\n+\n+  ins_cost(2 * STORE_COST + 3 * LOAD_COST + 4 * ALU_COST + BRANCH_COST * 4);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super == 0\\t#@partialSubtypeCheckVsZero\" %}\n+\n+  ins_encode(riscv_enc_partial_subtype_check(sub, super, tmp, result));\n+\n+  opcode(0x0); \/\/ Don't zero result reg on hit\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareU(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                         iRegI_R10 result, iRegP_R28 tmp1, iRegL_R29 tmp2, iRegL_R30 tmp3, rFlagsReg cr)\n+%{\n+  predicate(((StrCompNode *)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp(Binary str1 cnt1)(Binary str2 cnt2)));\n+  effect(KILL tmp1, KILL tmp2, KILL tmp3, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1, $cnt1, $str2, $cnt2 -> $result\\t#@string_compareU\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                      StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                         iRegI_R10 result, iRegP_R28 tmp1, iRegL_R29 tmp2, iRegL_R30 tmp3, rFlagsReg cr)\n+%{\n+  predicate(((StrCompNode *)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp(Binary str1 cnt1)(Binary str2 cnt2)));\n+  effect(KILL tmp1, KILL tmp2, KILL tmp3, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1, $cnt1, $str2, $cnt2 -> $result\\t#@string_compareL\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                      StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                          iRegI_R10 result, iRegP_R28 tmp1, iRegL_R29 tmp2, iRegL_R30 tmp3, rFlagsReg cr)\n+%{\n+  predicate(((StrCompNode *)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp(Binary str1 cnt1)(Binary str2 cnt2)));\n+  effect(KILL tmp1, KILL tmp2, KILL tmp3, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{\"String Compare $str1, $cnt1, $str2, $cnt2 -> $result\\t#@string_compareUL\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                      StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareLU(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                          iRegI_R10 result, iRegP_R28 tmp1, iRegL_R29 tmp2, iRegL_R30 tmp3,\n+                          rFlagsReg cr)\n+%{\n+  predicate(((StrCompNode *)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp(Binary str1 cnt1)(Binary str2 cnt2)));\n+  effect(KILL tmp1, KILL tmp2, KILL tmp3, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1, $cnt1, $str2, $cnt2 -> $result\\t#@string_compareLU\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                      StrIntrinsicNode::LU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexofUU(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                          iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n+\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UU)\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $tmp3$$Register, $tmp4$$Register,\n+                      $tmp5$$Register, $tmp6$$Register,\n+                      $result$$Register, StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexofLL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                          iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n+\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (LL)\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $tmp3$$Register, $tmp4$$Register,\n+                      $tmp5$$Register, $tmp6$$Register,\n+                      $result$$Register, StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexofUL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2, iRegI_R14 cnt2,\n+                          iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UL)\" %}\n+\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $tmp3$$Register, $tmp4$$Register,\n+                      $tmp5$$Register, $tmp6$$Register,\n+                      $result$$Register, StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexof_conUU(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2,\n+                              immI_le_4 int_cnt2, iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                              iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);\n+\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UU)\" %}\n+\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    __ string_indexof_linearscan($str1$$Register, $str2$$Register,\n+                                 $cnt1$$Register, zr,\n+                                 $tmp1$$Register, $tmp2$$Register,\n+                                 $tmp3$$Register, $tmp4$$Register,\n+                                 icnt2, $result$$Register, StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexof_conLL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2,\n+                              immI_le_4 int_cnt2, iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                              iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);\n+\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (LL)\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    __ string_indexof_linearscan($str1$$Register, $str2$$Register,\n+                                 $cnt1$$Register, zr,\n+                                 $tmp1$$Register, $tmp2$$Register,\n+                                 $tmp3$$Register, $tmp4$$Register,\n+                                 icnt2, $result$$Register, StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_indexof_conUL(iRegP_R11 str1, iRegI_R12 cnt1, iRegP_R13 str2,\n+                              immI_1 int_cnt2, iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                              iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+%{\n+  predicate(((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt1, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);\n+\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UL)\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    __ string_indexof_linearscan($str1$$Register, $str2$$Register,\n+                                 $cnt1$$Register, zr,\n+                                 $tmp1$$Register, $tmp2$$Register,\n+                                 $tmp3$$Register, $tmp4$$Register,\n+                                 icnt2, $result$$Register, StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct stringU_indexof_char(iRegP_R11 str1, iRegI_R12 cnt1, iRegI_R13 ch,\n+                              iRegI_R10 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                              iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+%{\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP_DEF result,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);\n+\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result\" %}\n+  ins_encode %{\n+    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                           $result$$Register, $tmp1$$Register, $tmp2$$Register,\n+                           $tmp3$$Register, $tmp4$$Register, false \/* isU *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\n+\/\/ clearing of an array\n+instruct clearArray_reg_reg(iRegL_R29 cnt, iRegP_R28 base, Universe dummy)\n+%{\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base);\n+\n+  ins_cost(4 * DEFAULT_COST);\n+  format %{ \"ClearArray $cnt, $base\\t#@clearArray_reg_reg\" %}\n+\n+  ins_encode %{\n+    address tpc = __ zero_words($base$$Register, $cnt$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct clearArray_imm_reg(immL cnt, iRegP_R28 base, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate((uint64_t)n->in(2)->get_long() < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * DEFAULT_COST);\n+  format %{ \"ClearArray $cnt, $base\\t#@clearArray_imm_reg\" %}\n+\n+  ins_encode %{\n+    __ zero_words($base$$Register, (uint64_t)$cnt$$constant);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_equalsL(iRegP_R11 str1, iRegP_R13 str2, iRegI_R14 cnt,\n+                        iRegI_R10 result, rFlagsReg cr)\n+%{\n+  predicate(((StrEqualsNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL cr);\n+\n+  format %{ \"String Equals $str1, $str2, $cnt -> $result\\t#@string_equalsL\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_equals($str1$$Register, $str2$$Register,\n+                     $result$$Register, $cnt$$Register, 1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_equalsU(iRegP_R11 str1, iRegP_R13 str2, iRegI_R14 cnt,\n+                        iRegI_R10 result, rFlagsReg cr)\n+%{\n+  predicate(((StrEqualsNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL cr);\n+\n+  format %{ \"String Equals $str1, $str2, $cnt -> $result\\t#@string_equalsU\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_equals($str1$$Register, $str2$$Register,\n+                     $result$$Register, $cnt$$Register, 2);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct array_equalsB(iRegP_R11 ary1, iRegP_R12 ary2, iRegI_R10 result,\n+                       iRegP_R13 tmp1, iRegP_R14 tmp2, iRegP_R15 tmp3,\n+                       iRegP_R16 tmp4, iRegP_R28 tmp5, rFlagsReg cr)\n+%{\n+  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL tmp5, KILL cr);\n+\n+  format %{ \"Array Equals $ary1, ary2 -> $result\\t#@array_equalsB \/\/ KILL $tmp5\" %}\n+  ins_encode %{\n+    __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register, $tmp4$$Register,\n+                     $result$$Register, $tmp5$$Register, 1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct array_equalsC(iRegP_R11 ary1, iRegP_R12 ary2, iRegI_R10 result,\n+                       iRegP_R13 tmp1, iRegP_R14 tmp2, iRegP_R15 tmp3,\n+                       iRegP_R16 tmp4, iRegP_R28 tmp5, rFlagsReg cr)\n+%{\n+  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL tmp5, KILL cr);\n+\n+  format %{ \"Array Equals $ary1, ary2 -> $result\\t#@array_equalsC \/\/ KILL $tmp5\" %}\n+  ins_encode %{\n+    __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register, $tmp4$$Register,\n+                     $result$$Register, $tmp5$$Register, 2);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Safepoint Instructions\n+\n+instruct safePoint(iRegP poll)\n+%{\n+  match(SafePoint poll);\n+\n+  ins_cost(2 * LOAD_COST);\n+  format %{\n+    \"lwu zr, [$poll]\\t# Safepoint: poll for GC, #@safePoint\"\n+  %}\n+  ins_encode %{\n+    __ read_polling_page(as_Register($poll$$reg), 0, relocInfo::poll_type);\n+  %}\n+  ins_pipe(pipe_serial); \/\/ ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ This name is KNOWN by the ADLC and cannot be changed.\n+\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n+\/\/ for this guy.\n+instruct tlsLoadP(javaThread_RegP dst)\n+%{\n+  match(Set dst (ThreadLocal));\n+\n+  ins_cost(0);\n+\n+  format %{ \" -- \\t\/\/ $dst=Thread::current(), empty, #@tlsLoadP\" %}\n+\n+  size(0);\n+\n+  ins_encode( \/*empty*\/ );\n+\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+\/\/ inlined locking and unlocking\n+\/\/ using t1 as the 'flag' register to bridge the BoolNode producers and consumers\n+instruct cmpFastLock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+\n+  ins_cost(LOAD_COST * 2 + STORE_COST * 3 + ALU_COST * 6 + BRANCH_COST * 3);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp1,$tmp2, #@cmpFastLock\" %}\n+\n+  ins_encode(riscv_enc_fast_lock(object, box, tmp1, tmp2));\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ using t1 as the 'flag' register to bridge the BoolNode producers and consumers\n+instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+\n+  ins_cost(LOAD_COST * 2 + STORE_COST + ALU_COST * 2 + BRANCH_COST * 4);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp1, $tmp2, #@cmpFastUnlock\" %}\n+\n+  ins_encode(riscv_enc_fast_unlock(object, box, tmp1, tmp2));\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ Tail Call; Jump from runtime stub to Java code.\n+\/\/ Also known as an 'interprocedural jump'.\n+\/\/ Target of jump will eventually return to caller.\n+\/\/ TailJump below removes the return address.\n+instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_oop)\n+%{\n+  match(TailCall jump_target method_oop);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"jalr $jump_target\\t# $method_oop holds method oop, #@TailCalljmpInd.\" %}\n+\n+  ins_encode(riscv_enc_tail_call(jump_target));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+instruct TailjmpInd(iRegPNoSp jump_target, iRegP_R10 ex_oop)\n+%{\n+  match(TailJump jump_target ex_oop);\n+\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{ \"jalr $jump_target\\t# $ex_oop holds exception oop, #@TailjmpInd.\" %}\n+\n+  ins_encode(riscv_enc_tail_jmp(jump_target));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+\/\/ Create exception oop: created by stack-crawling runtime code.\n+\/\/ Created exception is now available to this handler, and is setup\n+\/\/ just prior to jumping to this handler. No code emitted.\n+instruct CreateException(iRegP_R10 ex_oop)\n+%{\n+  match(Set ex_oop (CreateEx));\n+\n+  ins_cost(0);\n+  format %{ \" -- \\t\/\/ exception oop; no code emitted, #@CreateException\" %}\n+\n+  size(0);\n+\n+  ins_encode( \/*empty*\/ );\n+\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+\/\/ Rethrow exception: The exception oop will come in the first\n+\/\/ argument position. Then JUMP (not call) to the rethrow stub code.\n+instruct RethrowException()\n+%{\n+  match(Rethrow);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"j rethrow_stub\\t#@RethrowException\" %}\n+\n+  ins_encode(riscv_enc_rethrow());\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n+\/\/ Return Instruction\n+\/\/ epilog node loads ret address into ra as part of frame pop\n+instruct Ret()\n+%{\n+  match(Return);\n+\n+  ins_cost(BRANCH_COST);\n+  format %{ \"ret\\t\/\/ return register, #@Ret\" %}\n+\n+  ins_encode(riscv_enc_ret());\n+\n+  ins_pipe(pipe_branch);\n+%}\n+\n+\/\/ Die now.\n+instruct ShouldNotReachHere() %{\n+  match(Halt);\n+\n+  ins_cost(BRANCH_COST);\n+\n+  format %{ \"#@ShouldNotReachHere\" %}\n+\n+  ins_encode %{\n+    Assembler::CompressibleRegion cr(&_masm);\n+    if (is_reachable()) {\n+      __ halt();\n+    }\n+  %}\n+\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\n+\/\/----------PEEPHOLE RULES-----------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n+\/\/\n+\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n+\/\/\n+\/\/ peepconstraint %{\n+\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n+\/\/  [, ...] );\n+\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n+\/\/\n+\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n+\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n+\/\/ \/\/ in the replacement instruction's match rule\n+\/\/\n+\/\/ ---------VM FLAGS---------------------------------------------------------\n+\/\/\n+\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n+\/\/\n+\/\/ Each peephole rule is given an identifying number starting with zero and\n+\/\/ increasing by one in the order seen by the parser.  An individual peephole\n+\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n+\/\/ on the command-line.\n+\/\/\n+\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n+\/\/\n+\/\/ Only match adjacent instructions in same basic block\n+\/\/ Only equality constraints\n+\/\/ Only constraints between operands, not (0.dest_reg == RAX_enc)\n+\/\/ Only one replacement instruction\n+\/\/\n+\/\/----------SMARTSPILL RULES---------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n+\n+\/\/ Local Variables:\n+\/\/ mode: c++\n+\/\/ End:\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":10250,"deletions":0,"binary":false,"changes":10250,"status":"added"},{"patch":"@@ -0,0 +1,451 @@\n+\/\/\n+\/\/ Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+\/\/ RISCV Bit-Manipulation Extension Architecture Description File\n+\n+\/\/ Convert oop into int for vectors alignment masking\n+instruct convP2I_rvb(iRegINoSp dst, iRegP src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ConvL2I (CastP2X src)));\n+\n+  format %{ \"zext.w  $dst, $src\\t# ptr -> int @convP2I_rvb\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ zext_w(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ byte to int\n+instruct convB2I_reg_reg_rvb(iRegINoSp dst, iRegIorL2I src, immI_24 lshift, immI_24 rshift) %{\n+  predicate(UseRVB);\n+  match(Set dst (RShiftI (LShiftI src lshift) rshift));\n+\n+  format %{ \"sext.b  $dst, $src\\t# b2i, #@convB2I_reg_reg_rvb\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ sext_b(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ int to short\n+instruct convI2S_reg_reg_rvb(iRegINoSp dst, iRegIorL2I src, immI_16 lshift, immI_16 rshift) %{\n+  predicate(UseRVB);\n+  match(Set dst (RShiftI (LShiftI src lshift) rshift));\n+\n+  format %{ \"sext.h  $dst, $src\\t# i2s, #@convI2S_reg_reg_rvb\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ sext_h(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ short to unsigned int\n+instruct convS2UI_reg_reg_rvb(iRegINoSp dst, iRegIorL2I src, immI_16bits mask) %{\n+  predicate(UseRVB);\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"zext.h  $dst, $src\\t# s2ui, #@convS2UI_reg_reg_rvb\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ zext_h(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ int to unsigned long (zero extend)\n+instruct convI2UL_reg_reg_rvb(iRegLNoSp dst, iRegIorL2I src, immL_32bits mask) %{\n+  predicate(UseRVB);\n+  match(Set dst (AndL (ConvI2L src) mask));\n+\n+  format %{ \"zext.w  $dst, $src\\t# i2ul, #@convI2UL_reg_reg_rvb\" %}\n+\n+  ins_cost(ALU_COST);\n+  ins_encode %{\n+    __ zext_w(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ BSWAP instructions\n+instruct bytes_reverse_int_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesI src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_w_w  $dst, $src\\t#@bytes_reverse_int_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_w_w(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_long_rvb(iRegLNoSp dst, iRegL src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesL src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"rev8  $dst, $src\\t#@bytes_reverse_long_rvb\" %}\n+\n+  ins_encode %{\n+    __ rev8(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesUS src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_h_h_u  $dst, $src\\t#@bytes_reverse_unsigned_short_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h_u(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_short_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesS src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_h_h  $dst, $src\\t#@bytes_reverse_short_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Add Pointer\n+instruct shaddP_reg_reg_rvb(iRegPNoSp dst, iRegP src1, iRegL src2, immIScale imm) %{\n+  predicate(UseRVB);\n+  match(Set dst (AddP src1 (LShiftL src2 imm)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"shadd  $dst, $src2, $src1, $imm\\t# ptr, #@shaddP_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ shadd(as_Register($dst$$reg),\n+             as_Register($src2$$reg),\n+             as_Register($src1$$reg),\n+             t0,\n+             $imm$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shaddP_reg_reg_ext_rvb(iRegPNoSp dst, iRegP src1, iRegI src2, immIScale imm) %{\n+  predicate(UseRVB);\n+  match(Set dst (AddP src1 (LShiftL (ConvI2L src2) imm)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"shadd  $dst, $src2, $src1, $imm\\t# ptr, #@shaddP_reg_reg_ext_rvb\" %}\n+\n+  ins_encode %{\n+    __ shadd(as_Register($dst$$reg),\n+             as_Register($src2$$reg),\n+             as_Register($src1$$reg),\n+             t0,\n+             $imm$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Add Long\n+instruct shaddL_reg_reg_rvb(iRegLNoSp dst, iRegL src1, iRegL src2, immIScale imm) %{\n+  predicate(UseRVB);\n+  match(Set dst (AddL src1 (LShiftL src2 imm)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"shadd  $dst, $src2, $src1, $imm\\t#@shaddL_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ shadd(as_Register($dst$$reg),\n+             as_Register($src2$$reg),\n+             as_Register($src1$$reg),\n+             t0,\n+             $imm$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shaddL_reg_reg_ext_rvb(iRegLNoSp dst, iRegL src1, iRegI src2, immIScale imm) %{\n+  predicate(UseRVB);\n+  match(Set dst (AddL src1 (LShiftL (ConvI2L src2) imm)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"shadd  $dst, $src2, $src1, $imm\\t#@shaddL_reg_reg_ext_rvb\" %}\n+\n+  ins_encode %{\n+    __ shadd(as_Register($dst$$reg),\n+             as_Register($src2$$reg),\n+             as_Register($src1$$reg),\n+             t0,\n+             $imm$$constant);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zeros Count instructions\n+instruct countLeadingZerosI_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (CountLeadingZerosI src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"clzw  $dst, $src\\t#@countLeadingZerosI_rvb\" %}\n+\n+  ins_encode %{\n+    __ clzw(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL_rvb(iRegINoSp dst, iRegL src) %{\n+  predicate(UseRVB);\n+  match(Set dst (CountLeadingZerosL src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"clz  $dst, $src\\t#@countLeadingZerosL_rvb\" %}\n+\n+  ins_encode %{\n+    __ clz(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (CountTrailingZerosI src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"ctzw  $dst, $src\\t#@countTrailingZerosI_rvb\" %}\n+\n+  ins_encode %{\n+    __ ctzw(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL_rvb(iRegINoSp dst, iRegL src) %{\n+  predicate(UseRVB);\n+  match(Set dst (CountTrailingZerosL src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"ctz  $dst, $src\\t#@countTrailingZerosL_rvb\" %}\n+\n+  ins_encode %{\n+    __ ctz(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Population Count instructions\n+instruct popCountI_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"cpopw  $dst, $src\\t#@popCountI_rvb\" %}\n+\n+  ins_encode %{\n+    __ cpopw(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long\/bitCount(long) returns an int.\n+instruct popCountL_rvb(iRegINoSp dst, iRegL src) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"cpop  $dst, $src\\t#@popCountL_rvb\" %}\n+\n+  ins_encode %{\n+    __ cpop(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Max and Min\n+instruct minI_reg_rvb(iRegINoSp dst, iRegI src1, iRegI src2) %{\n+  predicate(UseRVB);\n+  match(Set dst (MinI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"min  $dst, $src1, $src2\\t#@minI_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ min(as_Register($dst$$reg), as_Register($src1$$reg), as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct maxI_reg_rvb(iRegINoSp dst, iRegI src1, iRegI src2) %{\n+  predicate(UseRVB);\n+  match(Set dst (MaxI src1 src2));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"max  $dst, $src1, $src2\\t#@maxI_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ max(as_Register($dst$$reg), as_Register($src1$$reg), as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Abs\n+instruct absI_reg_rvb(iRegINoSp dst, iRegI src) %{\n+  predicate(UseRVB);\n+  match(Set dst (AbsI src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{\n+    \"negw  t0, $src\\n\\t\"\n+    \"max  $dst, $src, t0\\t#@absI_reg_rvb\"\n+  %}\n+\n+  ins_encode %{\n+    __ negw(t0, as_Register($src$$reg));\n+    __ max(as_Register($dst$$reg), as_Register($src$$reg), t0);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct absL_reg_rvb(iRegLNoSp dst, iRegL src) %{\n+  predicate(UseRVB);\n+  match(Set dst (AbsL src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{\n+    \"neg  t0, $src\\n\\t\"\n+    \"max $dst, $src, t0\\t#@absL_reg_rvb\"\n+  %}\n+\n+  ins_encode %{\n+    __ neg(t0, as_Register($src$$reg));\n+    __ max(as_Register($dst$$reg), as_Register($src$$reg), t0);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Not\n+instruct andnI_reg_reg_rvb(iRegINoSp dst, iRegI src1, iRegI src2, immI_M1 m1) %{\n+  predicate(UseRVB);\n+  match(Set dst (AndI src1 (XorI src2 m1)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"andn  $dst, $src1, $src2\\t#@andnI_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ andn(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct andnL_reg_reg_rvb(iRegLNoSp dst, iRegL src1, iRegL src2, immL_M1 m1) %{\n+  predicate(UseRVB);\n+  match(Set dst (AndL src1 (XorL src2 m1)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"andn  $dst, $src1, $src2\\t#@andnL_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ andn(as_Register($dst$$reg),\n+            as_Register($src1$$reg),\n+            as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Not\n+instruct ornI_reg_reg_rvb(iRegINoSp dst, iRegI src1, iRegI src2, immI_M1 m1) %{\n+  predicate(UseRVB);\n+  match(Set dst (OrI src1 (XorI src2 m1)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"orn  $dst, $src1, $src2\\t#@ornI_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ orn(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct ornL_reg_reg_rvb(iRegLNoSp dst, iRegL src1, iRegL src2, immL_M1 m1) %{\n+  predicate(UseRVB);\n+  match(Set dst (OrL src1 (XorL src2 m1)));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"orn  $dst, $src1, $src2\\t#@ornL_reg_reg_rvb\" %}\n+\n+  ins_encode %{\n+    __ orn(as_Register($dst$$reg),\n+           as_Register($src1$$reg),\n+           as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_b.ad","additions":451,"deletions":0,"binary":false,"changes":451,"status":"added"},{"patch":"@@ -0,0 +1,2670 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/debugInfoRec.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"code\/vtableStubs.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/compiledICHolder.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n+#ifdef COMPILER2\n+#include \"adfiles\/ad_riscv.hpp\"\n+#include \"opto\/runtime.hpp\"\n+#endif\n+\n+#define __ masm->\n+\n+const int StackAlignmentInSlots = StackAlignmentInBytes \/ VMRegImpl::stack_slot_size;\n+\n+class SimpleRuntimeFrame {\n+public:\n+\n+  \/\/ Most of the runtime stubs have this simple frame layout.\n+  \/\/ This class exists to make the layout shared in one place.\n+  \/\/ Offsets are for compiler stack slots, which are jints.\n+  enum layout {\n+    \/\/ The frame sender code expects that fp will be in the \"natural\" place and\n+    \/\/ will override any oopMap setting for it. We must therefore force the layout\n+    \/\/ so that it agrees with the frame sender code.\n+    \/\/ we don't expect any arg reg save area so riscv asserts that\n+    \/\/ frame::arg_reg_save_area_bytes == 0\n+    fp_off = 0, fp_off2,\n+    return_off, return_off2,\n+    framesize\n+  };\n+};\n+\n+class RegisterSaver {\n+ public:\n+  RegisterSaver() {}\n+  ~RegisterSaver() {}\n+  OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words);\n+  void restore_live_registers(MacroAssembler* masm);\n+\n+  \/\/ Offsets into the register save area\n+  \/\/ Used by deoptimization when it is managing result register\n+  \/\/ values on its own\n+  \/\/ gregs:28, float_register:32; except: x1(ra) & x2(sp) & gp(x3) & tp(x4)\n+  \/\/ |---f0---|<---SP\n+  \/\/ |---f1---|\n+  \/\/ |   ..   |\n+  \/\/ |---f31--|\n+  \/\/ |---reserved slot for stack alignment---|\n+  \/\/ |---x5---|\n+  \/\/ |   x6   |\n+  \/\/ |---.. --|\n+  \/\/ |---x31--|\n+  \/\/ |---fp---|\n+  \/\/ |---ra---|\n+  int f0_offset_in_bytes(void) {\n+    return 0;\n+  }\n+  int reserved_slot_offset_in_bytes(void) {\n+    return f0_offset_in_bytes() +\n+           FloatRegisterImpl::max_slots_per_register *\n+           FloatRegisterImpl::number_of_registers *\n+           BytesPerInt;\n+  }\n+\n+  int reg_offset_in_bytes(Register r) {\n+    assert (r->encoding() > 4, \"ra, sp, gp and tp not saved\");\n+    return reserved_slot_offset_in_bytes() + (r->encoding() - 4 \/* x1, x2, x3, x4 *\/) * wordSize;\n+  }\n+\n+  int freg_offset_in_bytes(FloatRegister f) {\n+    return f0_offset_in_bytes() + f->encoding() * wordSize;\n+  }\n+\n+  int ra_offset_in_bytes(void) {\n+    return reserved_slot_offset_in_bytes() +\n+           (RegisterImpl::number_of_registers - 3) *\n+           RegisterImpl::max_slots_per_register *\n+           BytesPerInt;\n+  }\n+};\n+\n+OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words) {\n+  assert_cond(masm != NULL && total_frame_words != NULL);\n+  int frame_size_in_bytes = align_up(additional_frame_words * wordSize + ra_offset_in_bytes() + wordSize, 16);\n+  \/\/ OopMap frame size is in compiler stack slots (jint's) not bytes or words\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  \/\/ The caller will allocate additional_frame_words\n+  int additional_frame_slots = additional_frame_words * wordSize \/ BytesPerInt;\n+  \/\/ CodeBlob frame size is in words.\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+  *total_frame_words = frame_size_in_words;\n+\n+  \/\/ Save Integer and Float registers.\n+  __ enter();\n+  __ push_CPU_state();\n+\n+  \/\/ Set an oopmap for the call site.  This oopmap will map all\n+  \/\/ oop-registers and debug-info registers as callee-saved.  This\n+  \/\/ will allow deoptimization at this safepoint to find all possible\n+  \/\/ debug-info recordings, as well as let GC find all oops.\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* oop_map = new OopMap(frame_size_in_slots, 0);\n+  assert_cond(oop_maps != NULL && oop_map != NULL);\n+\n+  int sp_offset_in_slots = 0;\n+  int step_in_slots = 0;\n+\n+  step_in_slots = FloatRegisterImpl::max_slots_per_register;\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++, sp_offset_in_slots += step_in_slots) {\n+    FloatRegister r = as_FloatRegister(i);\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset_in_slots), r->as_VMReg());\n+  }\n+\n+  step_in_slots = RegisterImpl::max_slots_per_register;\n+  \/\/ skip the slot reserved for alignment, see MacroAssembler::push_reg;\n+  \/\/ also skip x5 ~ x6 on the stack because they are caller-saved registers.\n+  sp_offset_in_slots += RegisterImpl::max_slots_per_register * 3;\n+  \/\/ besides, we ignore x0 ~ x4 because push_CPU_state won't push them on the stack.\n+  for (int i = 7; i < RegisterImpl::number_of_registers; i++, sp_offset_in_slots += step_in_slots) {\n+    Register r = as_Register(i);\n+    if (r != xthread) {\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset_in_slots + additional_frame_slots), r->as_VMReg());\n+    }\n+  }\n+\n+  return oop_map;\n+}\n+\n+void RegisterSaver::restore_live_registers(MacroAssembler* masm) {\n+  assert_cond(masm != NULL);\n+  __ pop_CPU_state();\n+  __ leave();\n+}\n+\n+\/\/ Is vector's size (in bytes) bigger than a size saved by default?\n+bool SharedRuntime::is_wide_vector(int size) {\n+  return false;\n+}\n+\n+size_t SharedRuntime::trampoline_size() {\n+  return 6 * NativeInstruction::instruction_size;\n+}\n+\n+void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {\n+  int32_t offset = 0;\n+  __ movptr_with_offset(t0, destination, offset);\n+  __ jalr(x0, t0, offset);\n+}\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved fp and ra\n+  \/\/ This should really be in_preserve_stack_slots\n+  return r->reg2stack() * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ ---------------------------------------------------------------------------\n+\/\/ Read the array of BasicTypes from a signature, and compute where the\n+\/\/ arguments should go.  Values in the VMRegPair regs array refer to 4-byte\n+\/\/ quantities.  Values less than VMRegImpl::stack0 are registers, those above\n+\/\/ refer to 4-byte stack slots.  All stack slots are based off of the stack pointer\n+\/\/ as framesizes are fixed.\n+\/\/ VMRegImpl::stack0 refers to the first slot 0(sp).\n+\/\/ and VMRegImpl::stack0+1 refers to the memory word 4-byes higher.  Register\n+\/\/ up to RegisterImpl::number_of_registers) are the 64-bit\n+\/\/ integer registers.\n+\n+\/\/ Note: the INPUTS in sig_bt are in units of Java argument words,\n+\/\/ which are 64-bit.  The OUTPUTS are in 32-bit units.\n+\n+\/\/ The Java calling convention is a \"shifted\" version of the C ABI.\n+\/\/ By skipping the first C ABI register we can call non-static jni\n+\/\/ methods with small numbers of arguments without having to shuffle\n+\/\/ the arguments at all. Since we control the java ABI we ought to at\n+\/\/ least get some advantage out of it.\n+\n+int SharedRuntime::java_calling_convention(const BasicType *sig_bt,\n+                                           VMRegPair *regs,\n+                                           int total_args_passed,\n+                                           int is_outgoing) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[Argument::n_int_register_parameters_j] = {\n+    j_rarg0, j_rarg1, j_rarg2, j_rarg3,\n+    j_rarg4, j_rarg5, j_rarg6, j_rarg7\n+  };\n+  static const FloatRegister FP_ArgReg[Argument::n_float_register_parameters_j] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+  uint stk_args = 0; \/\/ inc by 2 each time\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+      case T_BOOLEAN: \/\/ fall through\n+      case T_CHAR:    \/\/ fall through\n+      case T_BYTE:    \/\/ fall through\n+      case T_SHORT:   \/\/ fall through\n+      case T_INT:\n+        if (int_args < Argument::n_int_register_parameters_j) {\n+          regs[i].set1(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set1(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_VOID:\n+        \/\/ halves of T_LONG or T_DOUBLE\n+        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+        regs[i].set_bad();\n+        break;\n+      case T_LONG:      \/\/ fall through\n+        assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      case T_OBJECT:    \/\/ fall through\n+      case T_ARRAY:     \/\/ fall through\n+      case T_ADDRESS:\n+        if (int_args < Argument::n_int_register_parameters_j) {\n+          regs[i].set2(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set2(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_FLOAT:\n+        if (fp_args < Argument::n_float_register_parameters_j) {\n+          regs[i].set1(FP_ArgReg[fp_args++]->as_VMReg());\n+        } else {\n+          regs[i].set1(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_DOUBLE:\n+        assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        if (fp_args < Argument::n_float_register_parameters_j) {\n+          regs[i].set2(FP_ArgReg[fp_args++]->as_VMReg());\n+        } else {\n+          regs[i].set2(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  return align_up(stk_args, 2);\n+}\n+\n+\/\/ Patch the callers callsite with entry to compiled code if it exists.\n+static void patch_callers_callsite(MacroAssembler *masm) {\n+  assert_cond(masm != NULL);\n+  Label L;\n+  __ ld(t0, Address(xmethod, in_bytes(Method::code_offset())));\n+  __ beqz(t0, L);\n+\n+  __ enter();\n+  __ push_CPU_state();\n+\n+  \/\/ VM needs caller's callsite\n+  \/\/ VM needs target method\n+  \/\/ This needs to be a long call since we will relocate this adapter to\n+  \/\/ the codeBuffer and it may not reach\n+\n+#ifndef PRODUCT\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+\n+  __ mv(c_rarg0, xmethod);\n+  __ mv(c_rarg1, ra);\n+  int32_t offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)), offset);\n+  __ jalr(x1, t0, offset);\n+\n+  \/\/ Explicit fence.i required because fixup_callers_callsite may change the code\n+  \/\/ stream.\n+  __ safepoint_ifence();\n+\n+  __ pop_CPU_state();\n+  \/\/ restore sp\n+  __ leave();\n+  __ bind(L);\n+}\n+\n+static void gen_c2i_adapter(MacroAssembler *masm,\n+                            int total_args_passed,\n+                            int comp_args_on_stack,\n+                            const BasicType *sig_bt,\n+                            const VMRegPair *regs,\n+                            Label& skip_fixup) {\n+  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n+  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n+  \/\/ interpreter, which means the caller made a static call to get here\n+  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n+  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n+  patch_callers_callsite(masm);\n+\n+  __ bind(skip_fixup);\n+\n+  int words_pushed = 0;\n+\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  __ mv(x30, sp);\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, 2 * wordSize);\n+\n+  if (extraspace) {\n+    __ sub(sp, sp, extraspace);\n+  }\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+  for (int i = 0; i < total_args_passed; i++) {\n+    if (sig_bt[i] == T_VOID) {\n+      assert(i > 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"missing half\");\n+      continue;\n+    }\n+\n+    \/\/ offset to start parameters\n+    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n+    int next_off = st_off - Interpreter::stackElementSize;\n+\n+    \/\/ Say 4 args:\n+    \/\/ i   st_off\n+    \/\/ 0   32 T_LONG\n+    \/\/ 1   24 T_VOID\n+    \/\/ 2   16 T_OBJECT\n+    \/\/ 3    8 T_BOOL\n+    \/\/ -    0 return address\n+    \/\/\n+    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+    VMReg r_1 = regs[i].first();\n+    VMReg r_2 = regs[i].second();\n+    if (!r_1->is_valid()) {\n+      assert(!r_2->is_valid(), \"\");\n+      continue;\n+    }\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use t0\n+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n+                    + extraspace\n+                    + words_pushed * wordSize);\n+      if (!r_2->is_valid()) {\n+        __ lwu(t0, Address(sp, ld_off));\n+        __ sd(t0, Address(sp, st_off), \/*temp register*\/esp);\n+      } else {\n+        __ ld(t0, Address(sp, ld_off), \/*temp register*\/esp);\n+\n+        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n+        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n+        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n+          \/\/ st_off == MSW, next_off == LSW\n+          __ sd(t0, Address(sp, next_off), \/*temp register*\/esp);\n+#ifdef ASSERT\n+          \/\/ Overwrite the unused slot with known junk\n+          __ li(t0, 0xdeadffffdeadaaaaul);\n+          __ sd(t0, Address(sp, st_off), \/*temp register*\/esp);\n+#endif \/* ASSERT *\/\n+        } else {\n+          __ sd(t0, Address(sp, st_off), \/*temp register*\/esp);\n+        }\n+      }\n+    } else if (r_1->is_Register()) {\n+      Register r = r_1->as_Register();\n+      if (!r_2->is_valid()) {\n+        \/\/ must be only an int (or less ) so move only 32bits to slot\n+        __ sd(r, Address(sp, st_off));\n+      } else {\n+        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n+        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n+        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+          \/\/ long\/double in gpr\n+#ifdef ASSERT\n+          \/\/ Overwrite the unused slot with known junk\n+          __ li(t0, 0xdeadffffdeadaaabul);\n+          __ sd(t0, Address(sp, st_off), \/*temp register*\/esp);\n+#endif \/* ASSERT *\/\n+          __ sd(r, Address(sp, next_off));\n+        } else {\n+          __ sd(r, Address(sp, st_off));\n+        }\n+      }\n+    } else {\n+      assert(r_1->is_FloatRegister(), \"\");\n+      if (!r_2->is_valid()) {\n+        \/\/ only a float use just part of the slot\n+        __ fsw(r_1->as_FloatRegister(), Address(sp, st_off));\n+      } else {\n+#ifdef ASSERT\n+        \/\/ Overwrite the unused slot with known junk\n+        __ li(t0, 0xdeadffffdeadaaacul);\n+        __ sd(t0, Address(sp, st_off), \/*temp register*\/esp);\n+#endif \/* ASSERT *\/\n+        __ fsd(r_1->as_FloatRegister(), Address(sp, next_off));\n+      }\n+    }\n+  }\n+\n+  __ mv(esp, sp); \/\/ Interp expects args on caller's expression stack\n+\n+  __ ld(t0, Address(xmethod, in_bytes(Method::interpreter_entry_offset())));\n+  __ jr(t0);\n+}\n+\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n+                                    int total_args_passed,\n+                                    int comp_args_on_stack,\n+                                    const BasicType *sig_bt,\n+                                    const VMRegPair *regs) {\n+  \/\/ Cut-out for having no stack args.\n+  int comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+  if (comp_args_on_stack != 0) {\n+    __ sub(t0, sp, comp_words_on_stack * wordSize);\n+    __ andi(sp, t0, -16);\n+  }\n+\n+  \/\/ Will jump to the compiled code just as if compiled code was doing it.\n+  \/\/ Pre-load the register-jump target early, to schedule it better.\n+  __ ld(t1, Address(xmethod, in_bytes(Method::from_compiled_offset())));\n+\n+  \/\/ Now generate the shuffle code.\n+  for (int i = 0; i < total_args_passed; i++) {\n+    if (sig_bt[i] == T_VOID) {\n+      assert(i > 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"missing half\");\n+      continue;\n+    }\n+\n+    \/\/ Pick up 0, 1 or 2 words from SP+offset.\n+\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n+           \"scrambled load targets?\");\n+    \/\/ Load in argument order going down.\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n+    \/\/ Point to interpreter value (vs. tag)\n+    int next_off = ld_off - Interpreter::stackElementSize;\n+\n+    VMReg r_1 = regs[i].first();\n+    VMReg r_2 = regs[i].second();\n+    if (!r_1->is_valid()) {\n+      assert(!r_2->is_valid(), \"\");\n+      continue;\n+    }\n+    if (r_1->is_stack()) {\n+      \/\/ Convert stack slot to an SP offset (+ wordSize to account for return address )\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n+      if (!r_2->is_valid()) {\n+        __ lw(t0, Address(esp, ld_off));\n+        __ sd(t0, Address(sp, st_off), \/*temp register*\/t2);\n+      } else {\n+        \/\/\n+        \/\/ We are using two optoregs. This can be either T_OBJECT,\n+        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+        \/\/ So we must adjust where to pick up the data to match the\n+        \/\/ interpreter.\n+        \/\/\n+        \/\/ Interpreter local[n] == MSW, local[n+1] == LSW however locals\n+        \/\/ are accessed as negative so LSW is at LOW address\n+\n+        \/\/ ld_off is MSW so get LSW\n+        const int offset = (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) ?\n+                           next_off : ld_off;\n+        __ ld(t0, Address(esp, offset));\n+        \/\/ st_off is LSW (i.e. reg.first())\n+        __ sd(t0, Address(sp, st_off), \/*temp register*\/t2);\n+      }\n+    } else if (r_1->is_Register()) {  \/\/ Register argument\n+      Register r = r_1->as_Register();\n+      if (r_2->is_valid()) {\n+        \/\/\n+        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+        \/\/ So we must adjust where to pick up the data to match the\n+        \/\/ interpreter.\n+\n+        const int offset = (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) ?\n+                           next_off : ld_off;\n+\n+        \/\/ this can be a misaligned move\n+        __ ld(r, Address(esp, offset));\n+      } else {\n+        \/\/ sign extend and use a full word?\n+        __ lw(r, Address(esp, ld_off));\n+      }\n+    } else {\n+      if (!r_2->is_valid()) {\n+        __ flw(r_1->as_FloatRegister(), Address(esp, ld_off));\n+      } else {\n+        __ fld(r_1->as_FloatRegister(), Address(esp, next_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ 6243940 We might end up in handle_wrong_method if\n+  \/\/ the callee is deoptimized as we race thru here. If that\n+  \/\/ happens we don't want to take a safepoint because the\n+  \/\/ caller frame will look interpreted and arguments are now\n+  \/\/ \"compiled\" so it is much better to make this transition\n+  \/\/ invisible to the stack walking code. Unfortunately if\n+  \/\/ we try and find the callee by normal means a safepoint\n+  \/\/ is possible. So we stash the desired callee in the thread\n+  \/\/ and the vm will find there should this case occur.\n+\n+  __ sd(xmethod, Address(xthread, JavaThread::callee_target_offset()));\n+\n+  __ jr(t1);\n+}\n+\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                            int total_args_passed,\n+                                                            int comp_args_on_stack,\n+                                                            const BasicType *sig_bt,\n+                                                            const VMRegPair *regs,\n+                                                            AdapterFingerPrint* fingerprint) {\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+\n+  address c2i_unverified_entry = __ pc();\n+  Label skip_fixup;\n+\n+  Label ok;\n+\n+  const Register holder = t1;\n+  const Register receiver = j_rarg0;\n+  const Register tmp = t2;  \/\/ A call-clobbered register not used for arg passing\n+\n+  \/\/ -------------------------------------------------------------------------\n+  \/\/ Generate a C2I adapter.  On entry we know xmethod holds the Method* during calls\n+  \/\/ to the interpreter.  The args start out packed in the compiled layout.  They\n+  \/\/ need to be unpacked into the interpreter layout.  This will almost always\n+  \/\/ require some stack space.  We grow the current (compiled) stack, then repack\n+  \/\/ the args.  We  finally end in a jump to the generic interpreter entry point.\n+  \/\/ On exit from the interpreter, the interpreter will restore our SP (lest the\n+  \/\/ compiled code, which relys solely on SP and not FP, get sick).\n+\n+  {\n+    __ block_comment(\"c2i_unverified_entry {\");\n+    __ load_klass(t0, receiver);\n+    __ ld(tmp, Address(holder, CompiledICHolder::holder_klass_offset()));\n+    __ ld(xmethod, Address(holder, CompiledICHolder::holder_metadata_offset()));\n+    __ beq(t0, tmp, ok);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+    __ bind(ok);\n+    \/\/ Method might have been compiled since the call site was patched to\n+    \/\/ interpreted; if that is the case treat it as a miss so we can get\n+    \/\/ the call site corrected.\n+    __ ld(t0, Address(xmethod, in_bytes(Method::code_offset())));\n+    __ beqz(t0, skip_fixup);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ block_comment(\"} c2i_unverified_entry\");\n+  }\n+\n+  address c2i_entry = __ pc();\n+\n+  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+\n+  __ flush();\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);\n+}\n+\n+int SharedRuntime::c_calling_convention(const BasicType *sig_bt,\n+                                         VMRegPair *regs,\n+                                         VMRegPair *regs2,\n+                                         int total_args_passed) {\n+  assert(regs2 == NULL, \"not needed on riscv\");\n+\n+  \/\/ We return the amount of VMRegImpl stack slots we need to reserve for all\n+  \/\/ the arguments NOT counting out_preserve_stack_slots.\n+\n+  static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {\n+    c_rarg0, c_rarg1, c_rarg2, c_rarg3,\n+    c_rarg4, c_rarg5,  c_rarg6,  c_rarg7\n+  };\n+  static const FloatRegister FP_ArgReg[Argument::n_float_register_parameters_c] = {\n+    c_farg0, c_farg1, c_farg2, c_farg3,\n+    c_farg4, c_farg5, c_farg6, c_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+  uint stk_args = 0; \/\/ inc by 2 each time\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+      case T_BOOLEAN:  \/\/ fall through\n+      case T_CHAR:     \/\/ fall through\n+      case T_BYTE:     \/\/ fall through\n+      case T_SHORT:    \/\/ fall through\n+      case T_INT:\n+        if (int_args < Argument::n_int_register_parameters_c) {\n+          regs[i].set1(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set1(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_LONG:      \/\/ fall through\n+        assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      case T_OBJECT:    \/\/ fall through\n+      case T_ARRAY:     \/\/ fall through\n+      case T_ADDRESS:   \/\/ fall through\n+      case T_METADATA:\n+        if (int_args < Argument::n_int_register_parameters_c) {\n+          regs[i].set2(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set2(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_FLOAT:\n+        if (fp_args < Argument::n_float_register_parameters_c) {\n+          regs[i].set1(FP_ArgReg[fp_args++]->as_VMReg());\n+        } else if (int_args < Argument::n_int_register_parameters_c) {\n+          regs[i].set1(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set1(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_DOUBLE:\n+        assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        if (fp_args < Argument::n_float_register_parameters_c) {\n+          regs[i].set2(FP_ArgReg[fp_args++]->as_VMReg());\n+        } else if (int_args < Argument::n_int_register_parameters_c) {\n+          regs[i].set2(INT_ArgReg[int_args++]->as_VMReg());\n+        } else {\n+          regs[i].set2(VMRegImpl::stack2reg(stk_args));\n+          stk_args += 2;\n+        }\n+        break;\n+      case T_VOID: \/\/ Halves of longs and doubles\n+        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+        regs[i].set_bad();\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  return stk_args;\n+}\n+\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (riscv64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+  assert_cond(masm != NULL);\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      __ ld(t0, Address(fp, reg2offset_in(src.first())));\n+      __ sd(t0, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      __ lw(dst.first()->as_Register(), Address(fp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    __ sd(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      \/\/ 32bits extend sign\n+      __ addw(dst.first()->as_Register(), src.first()->as_Register(), zr);\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+static void object_move(MacroAssembler* masm,\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+  assert_cond(masm != NULL && map != NULL && receiver_offset != NULL);\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+  Register rHandle = dst.first()->is_stack() ? t1 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    __ ld(t0, Address(fp, reg2offset_in(src.first())));\n+    __ la(rHandle, Address(fp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    Label notZero1;\n+    __ bnez(t0, notZero1);\n+    __ mv(rHandle, zr);\n+    __ bind(notZero1);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot = -1;\n+    if (rOop == j_rarg0) {\n+      oop_slot = 0;\n+    } else if (rOop == j_rarg1) {\n+      oop_slot = 1;\n+    } else if (rOop == j_rarg2) {\n+      oop_slot = 2;\n+    } else if (rOop == j_rarg3) {\n+      oop_slot = 3;\n+    } else if (rOop == j_rarg4) {\n+      oop_slot = 4;\n+    } else if (rOop == j_rarg5) {\n+      oop_slot = 5;\n+    } else if (rOop == j_rarg6) {\n+      oop_slot = 6;\n+    } else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot * VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    __ sd(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    \/\/rOop maybe the same as rHandle\n+    if (rOop == rHandle) {\n+      Label isZero;\n+      __ beqz(rOop, isZero);\n+      __ la(rHandle, Address(sp, offset));\n+      __ bind(isZero);\n+    } else {\n+      Label notZero2;\n+      __ la(rHandle, Address(sp, offset));\n+      __ bnez(rOop, notZero2);\n+      __ mv(rHandle, zr);\n+      __ bind(notZero2);\n+    }\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    __ sd(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n+         src.first()->is_reg() && dst.first()->is_reg() || src.first()->is_stack() && dst.first()->is_reg(), \"Unexpected error\");\n+  assert_cond(masm != NULL);\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      __ lwu(t0, Address(fp, reg2offset_in(src.first())));\n+      __ sw(t0, Address(sp, reg2offset_out(dst.first())));\n+    } else if (dst.first()->is_Register()) {\n+      __ lwu(dst.first()->as_Register(), Address(fp, reg2offset_in(src.first())));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg()) {\n+      __ fmv_s(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+}\n+\n+\/\/ A long move\n+static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+  assert_cond(masm != NULL);\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      __ ld(t0, Address(fp, reg2offset_in(src.first())));\n+      __ sd(t0, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      __ ld(dst.first()->as_Register(), Address(fp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    __ sd(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      __ mv(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ A double move\n+static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n+         src.first()->is_reg() && dst.first()->is_reg() || src.first()->is_stack() && dst.first()->is_reg(), \"Unexpected error\");\n+  assert_cond(masm != NULL);\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      __ ld(t0, Address(fp, reg2offset_in(src.first())));\n+      __ sd(t0, Address(sp, reg2offset_out(dst.first())));\n+    } else if (dst.first()-> is_Register()) {\n+      __ ld(dst.first()->as_Register(), Address(fp, reg2offset_in(src.first())));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg()) {\n+      __ fmv_d(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+}\n+\n+void SharedRuntime::save_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {\n+  assert_cond(masm != NULL);\n+  \/\/ We always ignore the frame_slots arg and just use the space just below frame pointer\n+  \/\/ which by this time is free to use\n+  switch (ret_type) {\n+    case T_FLOAT:\n+      __ fsw(f10, Address(fp, -3 * wordSize));\n+      break;\n+    case T_DOUBLE:\n+      __ fsd(f10, Address(fp, -3 * wordSize));\n+      break;\n+    case T_VOID:  break;\n+    default: {\n+      __ sd(x10, Address(fp, -3 * wordSize));\n+    }\n+  }\n+}\n+\n+void SharedRuntime::restore_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {\n+  assert_cond(masm != NULL);\n+  \/\/ We always ignore the frame_slots arg and just use the space just below frame pointer\n+  \/\/ which by this time is free to use\n+  switch (ret_type) {\n+    case T_FLOAT:\n+      __ flw(f10, Address(fp, -3 * wordSize));\n+      break;\n+    case T_DOUBLE:\n+      __ fld(f10, Address(fp, -3 * wordSize));\n+      break;\n+    case T_VOID:  break;\n+    default: {\n+      __ ld(x10, Address(fp, -3 * wordSize));\n+    }\n+  }\n+}\n+\n+static void save_args(MacroAssembler *masm, int arg_count, int first_arg, VMRegPair *args) {\n+  assert_cond(masm != NULL && args != NULL);\n+  RegSet x;\n+  for ( int i = first_arg ; i < arg_count ; i++ ) {\n+    if (args[i].first()->is_Register()) {\n+      x = x + args[i].first()->as_Register();\n+    } else if (args[i].first()->is_FloatRegister()) {\n+      __ addi(sp, sp, -2 * wordSize);\n+      __ fsd(args[i].first()->as_FloatRegister(), Address(sp, 0));\n+    }\n+  }\n+  __ push_reg(x, sp);\n+}\n+\n+static void restore_args(MacroAssembler *masm, int arg_count, int first_arg, VMRegPair *args) {\n+  assert_cond(masm != NULL && args != NULL);\n+  RegSet x;\n+  for ( int i = first_arg ; i < arg_count ; i++ ) {\n+    if (args[i].first()->is_Register()) {\n+      x = x + args[i].first()->as_Register();\n+    } else {\n+      ;\n+    }\n+  }\n+  __ pop_reg(x, sp);\n+  for ( int i = arg_count - 1 ; i >= first_arg ; i-- ) {\n+    if (args[i].first()->is_Register()) {\n+      ;\n+    } else if (args[i].first()->is_FloatRegister()) {\n+      __ fld(args[i].first()->as_FloatRegister(), Address(sp, 0));\n+      __ add(sp, sp, 2 * wordSize);\n+    }\n+  }\n+}\n+\n+static void rt_call(MacroAssembler* masm, address dest) {\n+  assert_cond(masm != NULL);\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    __ far_call(RuntimeAddress(dest));\n+  } else {\n+    int32_t offset = 0;\n+    __ la_patchable(t0, RuntimeAddress(dest), offset);\n+    __ jalr(x1, t0, offset);\n+  }\n+}\n+\n+static void verify_oop_args(MacroAssembler* masm,\n+                            const methodHandle& method,\n+                            const BasicType* sig_bt,\n+                            const VMRegPair* regs) {\n+  const Register temp_reg = x9;  \/\/ not part of any compiled calling seq\n+  if (VerifyOops) {\n+    for (int i = 0; i < method->size_of_parameters(); i++) {\n+      if (sig_bt[i] == T_OBJECT ||\n+          sig_bt[i] == T_ARRAY) {\n+        VMReg r = regs[i].first();\n+        assert(r->is_valid(), \"bad oop arg\");\n+        if (r->is_stack()) {\n+          __ ld(temp_reg, Address(sp, r->reg2stack() * VMRegImpl::stack_slot_size));\n+          __ verify_oop(temp_reg);\n+        } else {\n+          __ verify_oop(r->as_Register());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+static void gen_special_dispatch(MacroAssembler* masm,\n+                                 const methodHandle& method,\n+                                 const BasicType* sig_bt,\n+                                 const VMRegPair* regs) {\n+  verify_oop_args(masm, method, sig_bt, regs);\n+  vmIntrinsics::ID iid = method->intrinsic_id();\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+  bool     has_receiver   = false;\n+  Register receiver_reg   = noreg;\n+  int      member_arg_pos = -1;\n+  Register member_reg     = noreg;\n+  int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);\n+  if (ref_kind != 0) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing MemberName argument\n+    member_reg = x9;  \/\/ known to be free at this point\n+    has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n+    has_receiver = true;\n+  } else {\n+    fatal(\"unexpected intrinsic id %d\", iid);\n+  }\n+\n+  if (member_reg != noreg) {\n+    \/\/ Load the member_arg into register, if necessary.\n+    SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);\n+    VMReg r = regs[member_arg_pos].first();\n+    if (r->is_stack()) {\n+      __ ld(member_reg, Address(sp, r->reg2stack() * VMRegImpl::stack_slot_size));\n+    } else {\n+      \/\/ no data motion is needed\n+      member_reg = r->as_Register();\n+    }\n+  }\n+\n+  if (has_receiver) {\n+    \/\/ Make sure the receiver is loaded into a register.\n+    assert(method->size_of_parameters() > 0, \"oob\");\n+    assert(sig_bt[0] == T_OBJECT, \"receiver argument must be an object\");\n+    VMReg r = regs[0].first();\n+    assert(r->is_valid(), \"bad receiver arg\");\n+    if (r->is_stack()) {\n+      \/\/ Porting note:  This assumes that compiled calling conventions always\n+      \/\/ pass the receiver oop in a register.  If this is not true on some\n+      \/\/ platform, pick a temp and load the receiver from stack.\n+      fatal(\"receiver always in a register\");\n+      receiver_reg = x12;  \/\/ known to be free at this point\n+      __ ld(receiver_reg, Address(sp, r->reg2stack() * VMRegImpl::stack_slot_size));\n+    } else {\n+      \/\/ no data motion is needed\n+      receiver_reg = r->as_Register();\n+    }\n+  }\n+\n+  \/\/ Figure out which address we are really jumping to:\n+  MethodHandles::generate_method_handle_dispatch(masm, iid,\n+                                                 receiver_reg, member_reg, \/*for_compiler_entry:*\/ true);\n+}\n+\n+\/\/ ---------------------------------------------------------------------------\n+\/\/ Generate a native wrapper for a given method.  The method takes arguments\n+\/\/ in the Java compiled code convention, marshals them to the native\n+\/\/ convention (handlizes oops, etc), transitions to native, makes the call,\n+\/\/ returns to java state (possibly blocking), unhandlizes any result and\n+\/\/ returns.\n+\/\/\n+\/\/ Critical native functions are a shorthand for the use of\n+\/\/ GetPrimtiveArrayCritical and disallow the use of any other JNI\n+\/\/ functions.  The wrapper is expected to unpack the arguments before\n+\/\/ passing them to the callee and perform checks before and after the\n+\/\/ native call to ensure that they GCLocker\n+\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n+\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ block and the check for pending exceptions it's impossible for them\n+\/\/ to be thrown.\n+\/\/\n+\/\/ They are roughly structured like this:\n+\/\/    if (GCLocker::needs_gc()) SharedRuntime::block_for_jni_critical()\n+\/\/    tranistion to thread_in_native\n+\/\/    unpack arrray arguments and call native entry point\n+\/\/    check for safepoint in progress\n+\/\/    check if any thread suspend flags are set\n+\/\/      call into JVM and possible unlock the JNI critical\n+\/\/      if a GC was suppressed while in the critical native.\n+\/\/    transition back to thread_in_Java\n+\/\/    return to caller\n+\/\/\n+nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,\n+                                                const methodHandle& method,\n+                                                int compile_id,\n+                                                BasicType* in_sig_bt,\n+                                                VMRegPair* in_regs,\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n+  if (method->is_method_handle_intrinsic()) {\n+    vmIntrinsics::ID iid = method->intrinsic_id();\n+    intptr_t start = (intptr_t)__ pc();\n+    int vep_offset = ((intptr_t)__ pc()) - start;\n+\n+    \/\/ First instruction must be a nop as it may need to be patched on deoptimisation\n+    __ nop();\n+    gen_special_dispatch(masm,\n+                         method,\n+                         in_sig_bt,\n+                         in_regs);\n+    int frame_complete = ((intptr_t)__ pc()) - start;  \/\/ not complete, period\n+    __ flush();\n+    int stack_slots = SharedRuntime::out_preserve_stack_slots();  \/\/ no out slots at all, actually\n+    return nmethod::new_native_nmethod(method,\n+                                       compile_id,\n+                                       masm->code(),\n+                                       vep_offset,\n+                                       frame_complete,\n+                                       stack_slots \/ VMRegImpl::slots_per_word,\n+                                       in_ByteSize(-1),\n+                                       in_ByteSize(-1),\n+                                       (OopMapSet*)NULL);\n+  }\n+  address native_func = method->native_function();\n+  assert(native_func != NULL, \"must have function\");\n+\n+  \/\/ An OopMap for lock (and class if static)\n+  OopMapSet *oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  intptr_t start = (intptr_t)__ pc();\n+\n+  \/\/ We have received a description of where all the java arg are located\n+  \/\/ on entry to the wrapper. We need to convert these args to where\n+  \/\/ the jni function will expect them. To figure out where they go\n+  \/\/ we convert the java signature to a C signature by inserting\n+  \/\/ the hidden arguments as arg[0] and possibly arg[1] (static method)\n+\n+  const int total_in_args = method->size_of_parameters();\n+  int total_c_args = total_in_args + (method->is_static() ? 2 : 1);\n+\n+  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);\n+  VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);\n+  BasicType* in_elem_bt = NULL;\n+\n+  int argc = 0;\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n+\n+  for (int i = 0; i < total_in_args ; i++) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n+  }\n+\n+  \/\/ Now figure out where the args must be stored and how much stack space\n+  \/\/ they require.\n+  int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);\n+\n+  \/\/ Compute framesize for the wrapper.  We need to handlize all oops in\n+  \/\/ incoming registers\n+\n+  \/\/ Calculate the total number of stack slots we will need.\n+\n+  \/\/ First count the abi requirement plus all of the outgoing args\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;\n+\n+  \/\/ Now the space for the inbound oop handle area\n+  int total_save_slots = 8 * VMRegImpl::slots_per_word;  \/\/ 8 arguments passed in registers\n+\n+  int oop_handle_offset = stack_slots;\n+  stack_slots += total_save_slots;\n+\n+  \/\/ Now any space we need for handlizing a klass if static method\n+\n+  int klass_slot_offset = 0;\n+  int klass_offset = -1;\n+  int lock_slot_offset = 0;\n+  bool is_static = false;\n+\n+  if (method->is_static()) {\n+    klass_slot_offset = stack_slots;\n+    stack_slots += VMRegImpl::slots_per_word;\n+    klass_offset = klass_slot_offset * VMRegImpl::stack_slot_size;\n+    is_static = true;\n+  }\n+\n+  \/\/ Plus a lock if needed\n+\n+  if (method->is_synchronized()) {\n+    lock_slot_offset = stack_slots;\n+    stack_slots += VMRegImpl::slots_per_word;\n+  }\n+\n+  \/\/ Now a place (+2) to save return values or temp during shuffling\n+  \/\/ + 4 for return address (which we own) and saved fp\n+  stack_slots += 6;\n+\n+  \/\/ Ok The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      | 2 slots (ra)        |\n+  \/\/      | 2 slots (fp)        |\n+  \/\/      |---------------------|\n+  \/\/      | 2 slots for moves   |\n+  \/\/      |---------------------|\n+  \/\/      | lock box (if sync)  |\n+  \/\/      |---------------------| <- lock_slot_offset\n+  \/\/      | klass (if static)   |\n+  \/\/      |---------------------| <- klass_slot_offset\n+  \/\/      | oopHandle area      |\n+  \/\/      |---------------------| <- oop_handle_offset (8 java arg registers)\n+  \/\/      | outbound memory     |\n+  \/\/      | based arguments     |\n+  \/\/      |                     |\n+  \/\/      |---------------------|\n+  \/\/      |                     |\n+  \/\/ SP-> | out_preserved_slots |\n+  \/\/\n+  \/\/\n+\n+\n+  \/\/ Now compute actual number of stack words we need rounding to make\n+  \/\/ stack properly aligned.\n+  stack_slots = align_up(stack_slots, StackAlignmentInSlots);\n+\n+  int stack_size = stack_slots * VMRegImpl::stack_slot_size;\n+\n+  \/\/ First thing make an ic check to see if we should even be here\n+\n+  \/\/ We are free to use all registers as temps without saving them and\n+  \/\/ restoring them except fp. fp is the only callee save register\n+  \/\/ as far as the interpreter and the compiler(s) are concerned.\n+\n+\n+  const Register ic_reg = t1;\n+  const Register receiver = j_rarg0;\n+\n+  Label hit;\n+  Label exception_pending;\n+\n+  assert_different_registers(ic_reg, receiver, t0);\n+  __ verify_oop(receiver);\n+  __ cmp_klass(receiver, ic_reg, t0, hit);\n+\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+\n+  \/\/ Verified entry point must be aligned\n+  __ align(8);\n+\n+  __ bind(hit);\n+\n+  int vep_offset = ((intptr_t)__ pc()) - start;\n+\n+  \/\/ If we have to make this method not-entrant we'll overwrite its\n+  \/\/ first instruction with a jump.\n+  __ nop();\n+\n+  \/\/ Generate stack overflow check\n+  __ bang_stack_with_offset((int)JavaThread::stack_shadow_zone_size());\n+\n+  \/\/ Generate a new frame for the wrapper.\n+  __ enter();\n+  \/\/ -2 because return address is already present and so is saved fp\n+  __ sub(sp, sp, stack_size - 2 * wordSize);\n+\n+  \/\/ Frame is now completed as far as size and linkage.\n+  int frame_complete = ((intptr_t)__ pc()) - start;\n+\n+  \/\/ We use x18 as the oop handle for the receiver\/klass\n+  \/\/ It is callee save so it survives the call to native\n+\n+  const Register oop_handle_reg = x18;\n+\n+  \/\/\n+  \/\/ We immediately shuffle the arguments so that any vm call we have to\n+  \/\/ make from here on out (sync slow path, jvmti, etc.) we will have\n+  \/\/ captured the oops from our caller and have a valid oopMap for\n+  \/\/ them.\n+\n+  \/\/ -----------------\n+  \/\/ The Grand Shuffle\n+\n+  \/\/ The Java calling convention is either equal (linux) or denser (win64) than the\n+  \/\/ c calling convention. However the because of the jni_env argument the c calling\n+  \/\/ convention always has at least one more (and two for static) arguments than Java.\n+  \/\/ Therefore if we move the args from java -> c backwards then we will never have\n+  \/\/ a register->register conflict and we don't have to build a dependency graph\n+  \/\/ and figure out how to break any cycles.\n+  \/\/\n+\n+  \/\/ Record esp-based slot for receiver on stack for non-static methods\n+  int receiver_offset = -1;\n+\n+  \/\/ This is a trick. We double the stack slots so we can claim\n+  \/\/ the oops in the caller's frame. Since we are sure to have\n+  \/\/ more args than the caller doubling is enough to make\n+  \/\/ sure we can capture all the incoming oop args from the\n+  \/\/ caller.\n+  \/\/\n+  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n+  assert_cond(map != NULL);\n+\n+  int float_args = 0;\n+  int int_args = 0;\n+\n+#ifdef ASSERT\n+  bool reg_destroyed[RegisterImpl::number_of_registers];\n+  bool freg_destroyed[FloatRegisterImpl::number_of_registers];\n+  for ( int r = 0 ; r < RegisterImpl::number_of_registers ; r++ ) {\n+    reg_destroyed[r] = false;\n+  }\n+  for ( int f = 0 ; f < FloatRegisterImpl::number_of_registers ; f++ ) {\n+    freg_destroyed[f] = false;\n+  }\n+\n+#endif \/* ASSERT *\/\n+\n+  \/\/ For JNI natives the incoming and outgoing registers are offset upwards.\n+  GrowableArray<int> arg_order(2 * total_in_args);\n+  VMRegPair tmp_vmreg;\n+  tmp_vmreg.set2(x9->as_VMReg());\n+\n+  for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+    arg_order.push(i);\n+    arg_order.push(c_arg);\n+  }\n+\n+  int temploc = -1;\n+  for (int ai = 0; ai < arg_order.length(); ai += 2) {\n+    int i = arg_order.at(ai);\n+    int c_arg = arg_order.at(ai + 1);\n+    __ block_comment(err_msg(\"mv %d -> %d\", i, c_arg));\n+    assert(c_arg != -1 && i != -1, \"wrong order\");\n+#ifdef ASSERT\n+    if (in_regs[i].first()->is_Register()) {\n+      assert(!reg_destroyed[in_regs[i].first()->as_Register()->encoding()], \"destroyed reg!\");\n+    } else if (in_regs[i].first()->is_FloatRegister()) {\n+      assert(!freg_destroyed[in_regs[i].first()->as_FloatRegister()->encoding()], \"destroyed reg!\");\n+    }\n+    if (out_regs[c_arg].first()->is_Register()) {\n+      reg_destroyed[out_regs[c_arg].first()->as_Register()->encoding()] = true;\n+    } else if (out_regs[c_arg].first()->is_FloatRegister()) {\n+      freg_destroyed[out_regs[c_arg].first()->as_FloatRegister()->encoding()] = true;\n+    }\n+#endif \/* ASSERT *\/\n+    switch (in_sig_bt[i]) {\n+      case T_ARRAY:\n+      case T_OBJECT:\n+        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+                    ((i == 0) && (!is_static)),\n+                    &receiver_offset);\n+        int_args++;\n+        break;\n+      case T_VOID:\n+        break;\n+\n+      case T_FLOAT:\n+        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        float_args++;\n+        break;\n+\n+      case T_DOUBLE:\n+        assert( i + 1 < total_in_args &&\n+                in_sig_bt[i + 1] == T_VOID &&\n+                out_sig_bt[c_arg + 1] == T_VOID, \"bad arg list\");\n+        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        float_args++;\n+        break;\n+\n+      case T_LONG :\n+        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        int_args++;\n+        break;\n+\n+      case T_ADDRESS:\n+        assert(false, \"found T_ADDRESS in java args\");\n+        break;\n+\n+      default:\n+        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        int_args++;\n+    }\n+  }\n+\n+  \/\/ point c_arg at the first arg that is already loaded in case we\n+  \/\/ need to spill before we call out\n+  int c_arg = total_c_args - total_in_args;\n+\n+  \/\/ Pre-load a static method's oop into c_rarg1.\n+  if (method->is_static()) {\n+\n+    \/\/  load oop into a register\n+    __ movoop(c_rarg1,\n+              JNIHandles::make_local(method->method_holder()->java_mirror()),\n+              \/*immediate*\/true);\n+\n+    \/\/ Now handlize the static class mirror it's known not-null.\n+    __ sd(c_rarg1, Address(sp, klass_offset));\n+    map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n+\n+    \/\/ Now get the handle\n+    __ la(c_rarg1, Address(sp, klass_offset));\n+    \/\/ and protect the arg if we must spill\n+    c_arg--;\n+  }\n+\n+  \/\/ Change state to native (we save the return address in the thread, since it might not\n+  \/\/ be pushed on the stack when we do a stack traversal).\n+  \/\/ We use the same pc\/oopMap repeatedly when we call out\n+\n+  Label native_return;\n+  __ set_last_Java_frame(sp, noreg, native_return, t0);\n+\n+  Label dtrace_method_entry, dtrace_method_entry_done;\n+  {\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address)&DTraceMethodProbes), offset);\n+    __ lbu(t0, Address(t0, offset));\n+    __ addw(t0, t0, zr);\n+    __ bnez(t0, dtrace_method_entry);\n+    __ bind(dtrace_method_entry_done);\n+  }\n+\n+  \/\/ RedefineClasses() tracing support for obsolete method entry\n+  if (log_is_enabled(Trace, redefine, class, obsolete)) {\n+    \/\/ protect the args we've loaded\n+    save_args(masm, total_c_args, c_arg, out_regs);\n+    __ mov_metadata(c_rarg1, method());\n+    __ call_VM_leaf(\n+      CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry),\n+      xthread, c_rarg1);\n+    restore_args(masm, total_c_args, c_arg, out_regs);\n+  }\n+\n+  \/\/ Lock a synchronized method\n+\n+  \/\/ Register definitions used by locking and unlocking\n+\n+  const Register swap_reg = x10;\n+  const Register obj_reg  = x9;  \/\/ Will contain the oop\n+  const Register lock_reg = x30;  \/\/ Address of compiler lock object (BasicLock)\n+  const Register old_hdr  = x30;  \/\/ value of old header at unlock time\n+  const Register tmp      = ra;\n+\n+  Label slow_path_lock;\n+  Label lock_done;\n+\n+  if (method->is_synchronized()) {\n+\n+    const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();\n+\n+    \/\/ Get the handle (the 2nd argument)\n+    __ mv(oop_handle_reg, c_rarg1);\n+\n+    \/\/ Get address of the box\n+\n+    __ la(lock_reg, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+\n+    \/\/ Load the oop from the handle\n+    __ ld(obj_reg, Address(oop_handle_reg, 0));\n+\n+    if (UseBiasedLocking) {\n+      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n+    }\n+\n+    \/\/ Load (object->mark() | 1) into swap_reg % x10\n+    __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+    __ ori(swap_reg, t0, 1);\n+\n+    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+    __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+    \/\/ src -> dest if dest == x10 else x10 <- dest\n+    {\n+      Label here;\n+      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, lock_done, \/*fallthrough*\/NULL);\n+    }\n+\n+    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+    \/\/  1) (mark & 3) == 0, and\n+    \/\/  2) sp <= mark < mark + os::pagesize()\n+    \/\/ These 3 tests can be done by evaluating the following\n+    \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+    \/\/ assuming both stack pointer and pagesize have their\n+    \/\/ least significant 2 bits clear.\n+    \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+    __ sub(swap_reg, swap_reg, sp);\n+    __ andi(swap_reg, swap_reg, 3 - os::vm_page_size());\n+\n+    \/\/ Save the test result, for recursive case, the result is zero\n+    __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+    __ bnez(swap_reg, slow_path_lock);\n+\n+    \/\/ Slow path will re-enter here\n+    __ bind(lock_done);\n+  }\n+\n+\n+  \/\/ Finally just about ready to make the JNI call\n+\n+  \/\/ get JNIEnv* which is first argument to native\n+  __ la(c_rarg0, Address(xthread, in_bytes(JavaThread::jni_environment_offset())));\n+\n+  \/\/ Now set thread in native\n+  __ la(t1, Address(xthread, JavaThread::thread_state_offset()));\n+  __ mv(t0, _thread_in_native);\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  __ sw(t0, Address(t1));\n+\n+  rt_call(masm, native_func);\n+\n+  __ bind(native_return);\n+\n+  intptr_t return_pc = (intptr_t) __ pc();\n+  oop_maps->add_gc_map(return_pc - start, map);\n+\n+  \/\/ Unpack native results.\n+  if (ret_type != T_OBJECT && ret_type != T_ARRAY) {\n+    __ cast_primitive_type(ret_type, x10);\n+  }\n+\n+  Label safepoint_in_progress, safepoint_in_progress_done;\n+  Label after_transition;\n+\n+  \/\/ Switch thread to \"native transition\" state before reading the synchronization state.\n+  \/\/ This additional state is necessary because reading and testing the synchronization\n+  \/\/ state is not atomic w.r.t. GC, as this scenario demonstrates:\n+  \/\/     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.\n+  \/\/     VM thread changes sync state to synchronizing and suspends threads for GC.\n+  \/\/     Thread A is resumed to finish this native method, but doesn't block here since it\n+  \/\/     didn't see any synchronization is progress, and escapes.\n+  __ mv(t0, _thread_in_native_trans);\n+\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(MacroAssembler::AnyAny);\n+\n+  \/\/ check for safepoint operation in progress and\/or pending suspend requests\n+  {\n+    __ safepoint_poll_acquire(safepoint_in_progress);\n+    __ lwu(t0, Address(xthread, JavaThread::suspend_flags_offset()));\n+    __ bnez(t0, safepoint_in_progress);\n+    __ bind(safepoint_in_progress_done);\n+  }\n+\n+  \/\/ change thread state\n+  __ la(t1, Address(xthread, JavaThread::thread_state_offset()));\n+  __ mv(t0, _thread_in_Java);\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  __ sw(t0, Address(t1));\n+  __ bind(after_transition);\n+\n+  Label reguard;\n+  Label reguard_done;\n+  __ lbu(t0, Address(xthread, JavaThread::stack_guard_state_offset()));\n+  __ mv(t1, JavaThread::stack_guard_yellow_reserved_disabled);\n+  __ beq(t0, t1, reguard);\n+  __ bind(reguard_done);\n+\n+  \/\/ native result if any is live\n+\n+  \/\/ Unlock\n+  Label unlock_done;\n+  Label slow_path_unlock;\n+  if (method->is_synchronized()) {\n+\n+    \/\/ Get locked oop from the handle we passed to jni\n+    __ ld(obj_reg, Address(oop_handle_reg, 0));\n+\n+    Label done;\n+\n+    if (UseBiasedLocking) {\n+      __ biased_locking_exit(obj_reg, old_hdr, done);\n+    }\n+\n+    \/\/ Simple recursive lock?\n+    __ ld(t0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+    __ beqz(t0, done);\n+\n+    \/\/ Must save x10 if if it is live now because cmpxchg must use it\n+    if (ret_type != T_FLOAT && ret_type != T_DOUBLE && ret_type != T_VOID) {\n+      save_native_result(masm, ret_type, stack_slots);\n+    }\n+\n+    \/\/ get address of the stack lock\n+    __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+    \/\/  get old displaced header\n+    __ ld(old_hdr, Address(x10, 0));\n+\n+    \/\/ Atomic swap old header if oop still contains the stack lock\n+    Label succeed;\n+    __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, succeed, &slow_path_unlock);\n+    __ bind(succeed);\n+\n+    \/\/ slow path re-enters here\n+    __ bind(unlock_done);\n+    if (ret_type != T_FLOAT && ret_type != T_DOUBLE && ret_type != T_VOID) {\n+      restore_native_result(masm, ret_type, stack_slots);\n+    }\n+\n+    __ bind(done);\n+  }\n+\n+  Label dtrace_method_exit, dtrace_method_exit_done;\n+  {\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address)&DTraceMethodProbes), offset);\n+    __ lbu(t0, Address(t0, offset));\n+    __ bnez(t0, dtrace_method_exit);\n+    __ bind(dtrace_method_exit_done);\n+  }\n+\n+  __ reset_last_Java_frame(false);\n+\n+  \/\/ Unbox oop result, e.g. JNIHandles::resolve result.\n+  if (is_reference_type(ret_type)) {\n+    __ resolve_jobject(x10, xthread, t1);\n+  }\n+\n+  if (CheckJNICalls) {\n+    \/\/ clear_pending_jni_exception_check\n+    __ sd(zr, Address(xthread, JavaThread::pending_jni_exception_check_fn_offset()));\n+  }\n+\n+  \/\/ reset handle block\n+  __ ld(x12, Address(xthread, JavaThread::active_handles_offset()));\n+  __ sd(zr, Address(x12, JNIHandleBlock::top_offset_in_bytes()));\n+\n+  __ leave();\n+\n+  \/\/ Any exception pending?\n+  __ ld(t0, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+  __ bnez(t0, exception_pending);\n+\n+  \/\/ We're done\n+  __ ret();\n+\n+  \/\/ Unexpected paths are out of line and go here\n+\n+  \/\/ forward the exception\n+  __ bind(exception_pending);\n+\n+  \/\/ and forward the exception\n+  __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ Slow path locking & unlocking\n+  if (method->is_synchronized()) {\n+\n+    __ block_comment(\"Slow path lock {\");\n+    __ bind(slow_path_lock);\n+\n+    \/\/ has last_Java_frame setup. No exceptions so do vanilla call not call_VM\n+    \/\/ args are (oop obj, BasicLock* lock, JavaThread* thread)\n+\n+    \/\/ protect the args we've loaded\n+    save_args(masm, total_c_args, c_arg, out_regs);\n+\n+    __ mv(c_rarg0, obj_reg);\n+    __ mv(c_rarg1, lock_reg);\n+    __ mv(c_rarg2, xthread);\n+\n+    \/\/ Not a leaf but we have last_Java_frame setup as we want\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C), 3);\n+    restore_args(masm, total_c_args, c_arg, out_regs);\n+\n+#ifdef ASSERT\n+    { Label L;\n+      __ ld(t0, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+      __ beqz(t0, L);\n+      __ stop(\"no pending exception allowed on exit from monitorenter\");\n+      __ bind(L);\n+    }\n+#endif\n+    __ j(lock_done);\n+\n+    __ block_comment(\"} Slow path lock\");\n+\n+    __ block_comment(\"Slow path unlock {\");\n+    __ bind(slow_path_unlock);\n+\n+    if (ret_type == T_FLOAT || ret_type == T_DOUBLE) {\n+      save_native_result(masm, ret_type, stack_slots);\n+    }\n+\n+    __ mv(c_rarg2, xthread);\n+    __ la(c_rarg1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+    __ mv(c_rarg0, obj_reg);\n+\n+    \/\/ Save pending exception around call to VM (which contains an EXCEPTION_MARK)\n+    \/\/ NOTE that obj_reg == x9 currently\n+    __ ld(x9, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+    __ sd(zr, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+\n+    rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n+\n+#ifdef ASSERT\n+    {\n+      Label L;\n+      __ ld(t0, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+      __ beqz(t0, L);\n+      __ stop(\"no pending exception allowed on exit complete_monitor_unlocking_C\");\n+      __ bind(L);\n+    }\n+#endif \/* ASSERT *\/\n+\n+    __ sd(x9, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+\n+    if (ret_type == T_FLOAT || ret_type == T_DOUBLE) {\n+      restore_native_result(masm, ret_type, stack_slots);\n+    }\n+    __ j(unlock_done);\n+\n+    __ block_comment(\"} Slow path unlock\");\n+\n+  } \/\/ synchronized\n+\n+  \/\/ SLOW PATH Reguard the stack if needed\n+\n+  __ bind(reguard);\n+  save_native_result(masm, ret_type, stack_slots);\n+  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+  restore_native_result(masm, ret_type, stack_slots);\n+  \/\/ and continue\n+  __ j(reguard_done);\n+\n+  \/\/ SLOW PATH safepoint\n+  {\n+    __ block_comment(\"safepoint {\");\n+    __ bind(safepoint_in_progress);\n+\n+    \/\/ Don't use call_VM as it will see a possible pending exception and forward it\n+    \/\/ and never return here preventing us from clearing _last_native_pc down below.\n+    \/\/\n+    save_native_result(masm, ret_type, stack_slots);\n+    __ mv(c_rarg0, xthread);\n+#ifndef PRODUCT\n+    assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+    int32_t offset = 0;\n+    __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)), offset);\n+    __ jalr(x1, t0, offset);\n+\n+    \/\/ Restore any method result value\n+    restore_native_result(masm, ret_type, stack_slots);\n+\n+    __ j(safepoint_in_progress_done);\n+    __ block_comment(\"} safepoint\");\n+  }\n+\n+  \/\/ SLOW PATH dtrace support\n+  {\n+    __ block_comment(\"dtrace entry {\");\n+    __ bind(dtrace_method_entry);\n+\n+    \/\/ We have all of the arguments setup at this point. We must not touch any register\n+    \/\/ argument registers at this point (what if we save\/restore them there are no oop?\n+\n+    save_args(masm, total_c_args, c_arg, out_regs);\n+    __ mov_metadata(c_rarg1, method());\n+    __ call_VM_leaf(\n+      CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry),\n+      xthread, c_rarg1);\n+    restore_args(masm, total_c_args, c_arg, out_regs);\n+    __ j(dtrace_method_entry_done);\n+    __ block_comment(\"} dtrace entry\");\n+  }\n+\n+  {\n+    __ block_comment(\"dtrace exit {\");\n+    __ bind(dtrace_method_exit);\n+    save_native_result(masm, ret_type, stack_slots);\n+    __ mov_metadata(c_rarg1, method());\n+    __ call_VM_leaf(\n+         CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),\n+         xthread, c_rarg1);\n+    restore_native_result(masm, ret_type, stack_slots);\n+    __ j(dtrace_method_exit_done);\n+    __ block_comment(\"} dtrace exit\");\n+  }\n+\n+  __ flush();\n+\n+  nmethod *nm = nmethod::new_native_nmethod(method,\n+                                            compile_id,\n+                                            masm->code(),\n+                                            vep_offset,\n+                                            frame_complete,\n+                                            stack_slots \/ VMRegImpl::slots_per_word,\n+                                            (is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),\n+                                            in_ByteSize(lock_slot_offset*VMRegImpl::stack_slot_size),\n+                                            oop_maps);\n+  assert(nm != NULL, \"create native nmethod fail!\");\n+  return nm;\n+}\n+\n+\/\/ this function returns the adjust size (in number of words) to a c2i adapter\n+\/\/ activation for use during deoptimization\n+int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {\n+  assert(callee_locals >= callee_parameters,\n+         \"test and remove; got more parms than locals\");\n+  if (callee_locals < callee_parameters) {\n+    return 0;                   \/\/ No adjustment for negative locals\n+  }\n+  int diff = (callee_locals - callee_parameters) * Interpreter::stackElementWords;\n+  \/\/ diff is counted in stack words\n+  return align_up(diff, 2);\n+}\n+\n+\/\/------------------------------generate_deopt_blob----------------------------\n+void SharedRuntime::generate_deopt_blob() {\n+  \/\/ Allocate space for the code\n+  ResourceMark rm;\n+  \/\/ Setup code generation tools\n+  int pad = 0;\n+  CodeBuffer buffer(\"deopt_blob\", 2048 + pad, 1024);\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+  int frame_size_in_words = -1;\n+  OopMap* map = NULL;\n+  OopMapSet *oop_maps = new OopMapSet();\n+  assert_cond(masm != NULL && oop_maps != NULL);\n+  RegisterSaver reg_saver;\n+\n+  \/\/ -------------\n+  \/\/ This code enters when returning to a de-optimized nmethod.  A return\n+  \/\/ address has been pushed on the the stack, and return values are in\n+  \/\/ registers.\n+  \/\/ If we are doing a normal deopt then we were called from the patched\n+  \/\/ nmethod from the point we returned to the nmethod. So the return\n+  \/\/ address on the stack is wrong by NativeCall::instruction_size\n+  \/\/ We will adjust the value so it looks like we have the original return\n+  \/\/ address on the stack (like when we eagerly deoptimized).\n+  \/\/ In the case of an exception pending when deoptimizing, we enter\n+  \/\/ with a return address on the stack that points after the call we patched\n+  \/\/ into the exception handler. We have the following register state from,\n+  \/\/ e.g., the forward exception stub (see stubGenerator_riscv.cpp).\n+  \/\/    x10: exception oop\n+  \/\/    x9: exception handler\n+  \/\/    x13: throwing pc\n+  \/\/ So in this case we simply jam x13 into the useless return address and\n+  \/\/ the stack looks just like we want.\n+  \/\/\n+  \/\/ At this point we need to de-opt.  We save the argument return\n+  \/\/ registers.  We call the first C routine, fetch_unroll_info().  This\n+  \/\/ routine captures the return values and returns a structure which\n+  \/\/ describes the current frame size and the sizes of all replacement frames.\n+  \/\/ The current frame is compiled code and may contain many inlined\n+  \/\/ functions, each with their own JVM state.  We pop the current frame, then\n+  \/\/ push all the new frames.  Then we call the C routine unpack_frames() to\n+  \/\/ populate these frames.  Finally unpack_frames() returns us the new target\n+  \/\/ address.  Notice that callee-save registers are BLOWN here; they have\n+  \/\/ already been captured in the vframeArray at the time the return PC was\n+  \/\/ patched.\n+  address start = __ pc();\n+  Label cont;\n+\n+  \/\/ Prolog for non exception case!\n+\n+  \/\/ Save everything in sight.\n+  map = reg_saver.save_live_registers(masm, 0, &frame_size_in_words);\n+\n+  \/\/ Normal deoptimization.  Save exec mode for unpack_frames.\n+  __ mvw(xcpool, Deoptimization::Unpack_deopt); \/\/ callee-saved\n+  __ j(cont);\n+\n+  int reexecute_offset = __ pc() - start;\n+\n+  \/\/ Reexecute case\n+  \/\/ return address is the pc describes what bci to do re-execute at\n+\n+  \/\/ No need to update map as each call to save_live_registers will produce identical oopmap\n+  (void) reg_saver.save_live_registers(masm, 0, &frame_size_in_words);\n+\n+  __ mvw(xcpool, Deoptimization::Unpack_reexecute); \/\/ callee-saved\n+  __ j(cont);\n+\n+  int exception_offset = __ pc() - start;\n+\n+  \/\/ Prolog for exception case\n+\n+  \/\/ all registers are dead at this entry point, except for x10, and\n+  \/\/ x13 which contain the exception oop and exception pc\n+  \/\/ respectively.  Set them in TLS and fall thru to the\n+  \/\/ unpack_with_exception_in_tls entry point.\n+\n+  __ sd(x13, Address(xthread, JavaThread::exception_pc_offset()));\n+  __ sd(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+\n+  int exception_in_tls_offset = __ pc() - start;\n+\n+  \/\/ new implementation because exception oop is now passed in JavaThread\n+\n+  \/\/ Prolog for exception case\n+  \/\/ All registers must be preserved because they might be used by LinearScan\n+  \/\/ Exceptiop oop and throwing PC are passed in JavaThread\n+  \/\/ tos: stack at point of call to method that threw the exception (i.e. only\n+  \/\/ args are on the stack, no return address)\n+\n+  \/\/ The return address pushed by save_live_registers will be patched\n+  \/\/ later with the throwing pc. The correct value is not available\n+  \/\/ now because loading it from memory would destroy registers.\n+\n+  \/\/ NB: The SP at this point must be the SP of the method that is\n+  \/\/ being deoptimized.  Deoptimization assumes that the frame created\n+  \/\/ here by save_live_registers is immediately below the method's SP.\n+  \/\/ This is a somewhat fragile mechanism.\n+\n+  \/\/ Save everything in sight.\n+  map = reg_saver.save_live_registers(masm, 0, &frame_size_in_words);\n+\n+  \/\/ Now it is safe to overwrite any register\n+\n+  \/\/ Deopt during an exception.  Save exec mode for unpack_frames.\n+  __ li(xcpool, Deoptimization::Unpack_exception); \/\/ callee-saved\n+\n+  \/\/ load throwing pc from JavaThread and patch it as the return address\n+  \/\/ of the current frame. Then clear the field in JavaThread\n+\n+  __ ld(x13, Address(xthread, JavaThread::exception_pc_offset()));\n+  __ sd(x13, Address(fp, frame::return_addr_offset * wordSize));\n+  __ sd(zr, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ verify that there is really an exception oop in JavaThread\n+  __ ld(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ verify_oop(x10);\n+\n+  \/\/ verify that there is no pending exception\n+  Label no_pending_exception;\n+  __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+  __ beqz(t0, no_pending_exception);\n+  __ stop(\"must not have pending exception here\");\n+  __ bind(no_pending_exception);\n+#endif\n+\n+  __ bind(cont);\n+\n+  \/\/ Call C code.  Need thread and this frame, but NOT official VM entry\n+  \/\/ crud.  We cannot block on this call, no GC can happen.\n+  \/\/\n+  \/\/ UnrollBlock* fetch_unroll_info(JavaThread* thread)\n+\n+  \/\/ fetch_unroll_info needs to call last_java_frame().\n+\n+  Label retaddr;\n+  __ set_last_Java_frame(sp, noreg, retaddr, t0);\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    __ ld(t0, Address(xthread,\n+                              JavaThread::last_Java_fp_offset()));\n+    __ beqz(t0, L);\n+    __ stop(\"SharedRuntime::generate_deopt_blob: last_Java_fp not cleared\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+  __ mv(c_rarg0, xthread);\n+  __ mv(c_rarg1, xcpool);\n+  int32_t offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info)), offset);\n+  __ jalr(x1, t0, offset);\n+  __ bind(retaddr);\n+\n+  \/\/ Need to have an oopmap that tells fetch_unroll_info where to\n+  \/\/ find any register it might need.\n+  oop_maps->add_gc_map(__ pc() - start, map);\n+\n+  __ reset_last_Java_frame(false);\n+\n+  \/\/ Load UnrollBlock* into x15\n+  __ mv(x15, x10);\n+\n+  __ lwu(xcpool, Address(x15, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+  Label noException;\n+  __ li(t0, Deoptimization::Unpack_exception);\n+  __ bne(xcpool, t0, noException); \/\/ Was exception pending?\n+  __ ld(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ ld(x13, Address(xthread, JavaThread::exception_pc_offset()));\n+  __ sd(zr, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ sd(zr, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+  __ verify_oop(x10);\n+\n+  \/\/ Overwrite the result registers with the exception results.\n+  __ sd(x10, Address(sp, reg_saver.reg_offset_in_bytes(x10)));\n+\n+  __ bind(noException);\n+\n+  \/\/ Only register save data is on the stack.\n+  \/\/ Now restore the result registers.  Everything else is either dead\n+  \/\/ or captured in the vframeArray.\n+\n+  \/\/ Restore fp result register\n+  __ fld(f10, Address(sp, reg_saver.freg_offset_in_bytes(f10)));\n+  \/\/ Restore integer result register\n+  __ ld(x10, Address(sp, reg_saver.reg_offset_in_bytes(x10)));\n+\n+  \/\/ Pop all of the register save area off the stack\n+  __ add(sp, sp, frame_size_in_words * wordSize);\n+\n+  \/\/ All of the register save area has been popped of the stack. Only the\n+  \/\/ return address remains.\n+\n+  \/\/ Pop all the frames we must move\/replace.\n+  \/\/\n+  \/\/ Frame picture (youngest to oldest)\n+  \/\/ 1: self-frame (no frame link)\n+  \/\/ 2: deopting frame  (no frame link)\n+  \/\/ 3: caller of deopting frame (could be compiled\/interpreted).\n+  \/\/\n+  \/\/ Note: by leaving the return address of self-frame on the stack\n+  \/\/ and using the size of frame 2 to adjust the stack\n+  \/\/ when we are done the return to frame 3 will still be on the stack.\n+\n+  \/\/ Pop deoptimized frame\n+  __ lwu(x12, Address(x15, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ sub(x12, x12, 2 * wordSize);\n+  __ add(sp, sp, x12);\n+  __ ld(fp, Address(sp, 0));\n+  __ ld(ra, Address(sp, wordSize));\n+  __ addi(sp, sp, 2 * wordSize);\n+  \/\/ RA should now be the return address to the caller (3)\n+\n+#ifdef ASSERT\n+  \/\/ Compilers generate code that bang the stack by as much as the\n+  \/\/ interpreter would need. So this stack banging should never\n+  \/\/ trigger a fault. Verify that it does not on non product builds.\n+  __ lwu(x9, Address(x15, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ bang_stack_size(x9, x12);\n+#endif\n+  \/\/ Load address of array of frame pcs into x12\n+  __ ld(x12, Address(x15, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+\n+  \/\/ Load address of array of frame sizes into x14\n+  __ ld(x14, Address(x15, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+\n+  \/\/ Load counter into x13\n+  __ lwu(x13, Address(x15, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+\n+  \/\/ Now adjust the caller's stack to make up for the extra locals\n+  \/\/ but record the original sp so that we can save it in the skeletal interpreter\n+  \/\/ frame and the stack walking of interpreter_sender will get the unextended sp\n+  \/\/ value and not the \"real\" sp value.\n+\n+  const Register sender_sp = x16;\n+\n+  __ mv(sender_sp, sp);\n+  __ lwu(x9, Address(x15,\n+                     Deoptimization::UnrollBlock::\n+                     caller_adjustment_offset_in_bytes()));\n+  __ sub(sp, sp, x9);\n+\n+  \/\/ Push interpreter frames in a loop\n+  __ li(t0, 0xDEADDEAD);               \/\/ Make a recognizable pattern\n+  __ mv(t1, t0);\n+  Label loop;\n+  __ bind(loop);\n+  __ ld(x9, Address(x14, 0));          \/\/ Load frame size\n+  __ addi(x14, x14, wordSize);\n+  __ sub(x9, x9, 2 * wordSize);        \/\/ We'll push pc and fp by hand\n+  __ ld(ra, Address(x12, 0));          \/\/ Load pc\n+  __ addi(x12, x12, wordSize);\n+  __ enter();                          \/\/ Save old & set new fp\n+  __ sub(sp, sp, x9);                  \/\/ Prolog\n+  \/\/ This value is corrected by layout_activation_impl\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ sd(sender_sp, Address(fp, frame::interpreter_frame_sender_sp_offset * wordSize)); \/\/ Make it walkable\n+  __ mv(sender_sp, sp);                \/\/ Pass sender_sp to next frame\n+  __ addi(x13, x13, -1);               \/\/ Decrement counter\n+  __ bnez(x13, loop);\n+\n+    \/\/ Re-push self-frame\n+  __ ld(ra, Address(x12));\n+  __ enter();\n+\n+  \/\/ Allocate a full sized register save area.  We subtract 2 because\n+  \/\/ enter() just pushed 2 words\n+  __ sub(sp, sp, (frame_size_in_words - 2) * wordSize);\n+\n+  \/\/ Restore frame locals after moving the frame\n+  __ fsd(f10, Address(sp, reg_saver.freg_offset_in_bytes(f10)));\n+  __ sd(x10, Address(sp, reg_saver.reg_offset_in_bytes(x10)));\n+\n+  \/\/ Call C code.  Need thread but NOT official VM entry\n+  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n+  \/\/ restore return values to their stack-slots with the new SP.\n+  \/\/\n+  \/\/ void Deoptimization::unpack_frames(JavaThread* thread, int exec_mode)\n+\n+  \/\/ Use fp because the frames look interpreted now\n+  \/\/ Don't need the precise return PC here, just precise enough to point into this code blob.\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, fp, the_pc, t0);\n+\n+  __ mv(c_rarg0, xthread);\n+  __ mv(c_rarg1, xcpool); \/\/ second arg: exec_mode\n+  offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), offset);\n+  __ jalr(x1, t0, offset);\n+\n+  \/\/ Set an oopmap for the call site\n+  \/\/ Use the same PC we used for the last java frame\n+  oop_maps->add_gc_map(the_pc - start,\n+                       new OopMap(frame_size_in_words, 0));\n+\n+  \/\/ Clear fp AND pc\n+  __ reset_last_Java_frame(true);\n+\n+  \/\/ Collect return values\n+  __ fld(f10, Address(sp, reg_saver.freg_offset_in_bytes(f10)));\n+  __ ld(x10, Address(sp, reg_saver.reg_offset_in_bytes(x10)));\n+\n+  \/\/ Pop self-frame.\n+  __ leave();                           \/\/ Epilog\n+\n+  \/\/ Jump to interpreter\n+  __ ret();\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  _deopt_blob = DeoptimizationBlob::create(&buffer, oop_maps, 0, exception_offset, reexecute_offset, frame_size_in_words);\n+  assert(_deopt_blob != NULL, \"create deoptimization blob fail!\");\n+  _deopt_blob->set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);\n+}\n+\n+uint SharedRuntime::out_preserve_stack_slots() {\n+  return 0;\n+}\n+\n+#ifdef COMPILER2\n+\/\/------------------------------generate_uncommon_trap_blob--------------------\n+void SharedRuntime::generate_uncommon_trap_blob() {\n+  \/\/ Allocate space for the code\n+  ResourceMark rm;\n+  \/\/ Setup code generation tools\n+  CodeBuffer buffer(\"uncommon_trap_blob\", 2048, 1024);\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+  assert_cond(masm != NULL);\n+\n+  assert(SimpleRuntimeFrame::framesize % 4 == 0, \"sp not 16-byte aligned\");\n+\n+  address start = __ pc();\n+\n+  \/\/ Push self-frame.  We get here with a return address in RA\n+  \/\/ and sp should be 16 byte aligned\n+  \/\/ push fp and retaddr by hand\n+  __ addi(sp, sp, -2 * wordSize);\n+  __ sd(ra, Address(sp, wordSize));\n+  __ sd(fp, Address(sp, 0));\n+  \/\/ we don't expect an arg reg save area\n+#ifndef PRODUCT\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+  \/\/ compiler left unloaded_class_index in j_rarg0 move to where the\n+  \/\/ runtime expects it.\n+  __ addiw(c_rarg1, j_rarg0, 0);\n+\n+  \/\/ we need to set the past SP to the stack pointer of the stub frame\n+  \/\/ and the pc to the address where this runtime call will return\n+  \/\/ although actually any pc in this code blob will do).\n+  Label retaddr;\n+  __ set_last_Java_frame(sp, noreg, retaddr, t0);\n+\n+  \/\/ Call C code.  Need thread but NOT official VM entry\n+  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n+  \/\/ capture callee-saved registers as well as return values.\n+  \/\/\n+  \/\/ UnrollBlock* uncommon_trap(JavaThread* thread, jint unloaded_class_index, jint exec_mode)\n+  \/\/\n+  \/\/ n.b. 3 gp args, 0 fp args, integral return type\n+\n+  __ mv(c_rarg0, xthread);\n+  __ mvw(c_rarg2, (unsigned)Deoptimization::Unpack_uncommon_trap);\n+  int32_t offset = 0;\n+  __ la_patchable(t0,\n+        RuntimeAddress(CAST_FROM_FN_PTR(address,\n+                                        Deoptimization::uncommon_trap)), offset);\n+  __ jalr(x1, t0, offset);\n+  __ bind(retaddr);\n+\n+  \/\/ Set an oopmap for the call site\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(SimpleRuntimeFrame::framesize, 0);\n+  assert_cond(oop_maps != NULL && map != NULL);\n+\n+  \/\/ location of fp is known implicitly by the frame sender code\n+\n+  oop_maps->add_gc_map(__ pc() - start, map);\n+\n+  __ reset_last_Java_frame(false);\n+\n+  \/\/ move UnrollBlock* into x14\n+  __ mv(x14, x10);\n+\n+#ifdef ASSERT\n+  { Label L;\n+    __ lwu(t0, Address(x14, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+    __ mvw(t1, Deoptimization::Unpack_uncommon_trap);\n+    __ beq(t0, t1, L);\n+    __ stop(\"SharedRuntime::generate_deopt_blob: last_Java_fp not cleared\");\n+    __ bind(L);\n+  }\n+#endif\n+\n+  \/\/ Pop all the frames we must move\/replace.\n+  \/\/\n+  \/\/ Frame picture (youngest to oldest)\n+  \/\/ 1: self-frame (no frame link)\n+  \/\/ 2: deopting frame  (no frame link)\n+  \/\/ 3: caller of deopting frame (could be compiled\/interpreted).\n+\n+  __ add(sp, sp, (SimpleRuntimeFrame::framesize) << LogBytesPerInt); \/\/ Epilog!\n+\n+  \/\/ Pop deoptimized frame (int)\n+  __ lwu(x12, Address(x14,\n+                      Deoptimization::UnrollBlock::\n+                      size_of_deoptimized_frame_offset_in_bytes()));\n+  __ sub(x12, x12, 2 * wordSize);\n+  __ add(sp, sp, x12);\n+  __ ld(fp, sp, 0);\n+  __ ld(ra, sp, wordSize);\n+  __ addi(sp, sp, 2 * wordSize);\n+  \/\/ RA should now be the return address to the caller (3) frame\n+\n+#ifdef ASSERT\n+  \/\/ Compilers generate code that bang the stack by as much as the\n+  \/\/ interpreter would need. So this stack banging should never\n+  \/\/ trigger a fault. Verify that it does not on non product builds.\n+  __ lwu(x11, Address(x14,\n+                      Deoptimization::UnrollBlock::\n+                      total_frame_sizes_offset_in_bytes()));\n+  __ bang_stack_size(x11, x12);\n+#endif\n+\n+  \/\/ Load address of array of frame pcs into x12 (address*)\n+  __ ld(x12, Address(x14,\n+                     Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+\n+  \/\/ Load address of array of frame sizes into x15 (intptr_t*)\n+  __ ld(x15, Address(x14,\n+                     Deoptimization::UnrollBlock::\n+                     frame_sizes_offset_in_bytes()));\n+\n+  \/\/ Counter\n+  __ lwu(x13, Address(x14,\n+                      Deoptimization::UnrollBlock::\n+                      number_of_frames_offset_in_bytes())); \/\/ (int)\n+\n+  \/\/ Now adjust the caller's stack to make up for the extra locals but\n+  \/\/ record the original sp so that we can save it in the skeletal\n+  \/\/ interpreter frame and the stack walking of interpreter_sender\n+  \/\/ will get the unextended sp value and not the \"real\" sp value.\n+\n+  const Register sender_sp = t1; \/\/ temporary register\n+\n+  __ lwu(x11, Address(x14,\n+                      Deoptimization::UnrollBlock::\n+                      caller_adjustment_offset_in_bytes())); \/\/ (int)\n+  __ mv(sender_sp, sp);\n+  __ sub(sp, sp, x11);\n+\n+  \/\/ Push interpreter frames in a loop\n+  Label loop;\n+  __ bind(loop);\n+  __ ld(x11, Address(x15, 0));       \/\/ Load frame size\n+  __ sub(x11, x11, 2 * wordSize);    \/\/ We'll push pc and fp by hand\n+  __ ld(ra, Address(x12, 0));        \/\/ Save return address\n+  __ enter();                        \/\/ and old fp & set new fp\n+  __ sub(sp, sp, x11);               \/\/ Prolog\n+  __ sd(sender_sp, Address(fp, frame::interpreter_frame_sender_sp_offset * wordSize)); \/\/ Make it walkable\n+  \/\/ This value is corrected by layout_activation_impl\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ mv(sender_sp, sp);              \/\/ Pass sender_sp to next frame\n+  __ add(x15, x15, wordSize);        \/\/ Bump array pointer (sizes)\n+  __ add(x12, x12, wordSize);        \/\/ Bump array pointer (pcs)\n+  __ subw(x13, x13, 1);              \/\/ Decrement counter\n+  __ bgtz(x13, loop);\n+  __ ld(ra, Address(x12, 0));        \/\/ save final return address\n+  \/\/ Re-push self-frame\n+  __ enter();                        \/\/ & old fp & set new fp\n+\n+  \/\/ Use fp because the frames look interpreted now\n+  \/\/ Save \"the_pc\" since it cannot easily be retrieved using the last_java_SP after we aligned SP.\n+  \/\/ Don't need the precise return PC here, just precise enough to point into this code blob.\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, fp, the_pc, t0);\n+\n+  \/\/ Call C code.  Need thread but NOT official VM entry\n+  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n+  \/\/ restore return values to their stack-slots with the new SP.\n+  \/\/\n+  \/\/ BasicType unpack_frames(JavaThread* thread, int exec_mode)\n+  \/\/\n+\n+  \/\/ n.b. 2 gp args, 0 fp args, integral return type\n+\n+  \/\/ sp should already be aligned\n+  __ mv(c_rarg0, xthread);\n+  __ mvw(c_rarg1, (unsigned)Deoptimization::Unpack_uncommon_trap);\n+  offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), offset);\n+  __ jalr(x1, t0, offset);\n+\n+  \/\/ Set an oopmap for the call site\n+  \/\/ Use the same PC we used for the last java frame\n+  oop_maps->add_gc_map(the_pc - start, new OopMap(SimpleRuntimeFrame::framesize, 0));\n+\n+  \/\/ Clear fp AND pc\n+  __ reset_last_Java_frame(true);\n+\n+  \/\/ Pop self-frame.\n+  __ leave();                 \/\/ Epilog\n+\n+  \/\/ Jump to interpreter\n+  __ ret();\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  _uncommon_trap_blob =  UncommonTrapBlob::create(&buffer, oop_maps,\n+                                                  SimpleRuntimeFrame::framesize >> 1);\n+}\n+#endif \/\/ COMPILER2\n+\n+\/\/------------------------------generate_handler_blob------\n+\/\/\n+\/\/ Generate a special Compile2Runtime blob that saves all registers,\n+\/\/ and setup oopmap.\n+\/\/\n+SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {\n+  ResourceMark rm;\n+  OopMapSet *oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  OopMap* map = NULL;\n+\n+  \/\/ Allocate space for the code.  Setup code generation tools.\n+  CodeBuffer buffer(\"handler_blob\", 2048, 1024);\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+  assert_cond(masm != NULL);\n+\n+  address start   = __ pc();\n+  address call_pc = NULL;\n+  int frame_size_in_words = -1;\n+  bool cause_return = (poll_type == POLL_AT_RETURN);\n+  RegisterSaver reg_saver;\n+\n+  \/\/ Save Integer and Float registers.\n+  map = reg_saver.save_live_registers(masm, 0, &frame_size_in_words);\n+\n+  \/\/ The following is basically a call_VM.  However, we need the precise\n+  \/\/ address of the call in order to generate an oopmap. Hence, we do all the\n+  \/\/ work outselves.\n+\n+  Label retaddr;\n+  __ set_last_Java_frame(sp, noreg, retaddr, t0);\n+\n+  \/\/ The return address must always be correct so that frame constructor never\n+  \/\/ sees an invalid pc.\n+\n+  if (!cause_return) {\n+    \/\/ overwrite the return address pushed by save_live_registers\n+    \/\/ Additionally, x18 is a callee-saved register so we can look at\n+    \/\/ it later to determine if someone changed the return address for\n+    \/\/ us!\n+    __ ld(x18, Address(xthread, JavaThread::saved_exception_pc_offset()));\n+    __ sd(x18, Address(fp, frame::return_addr_offset * wordSize));\n+  }\n+\n+  \/\/ Do the call\n+  __ mv(c_rarg0, xthread);\n+  int32_t offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(call_ptr), offset);\n+  __ jalr(x1, t0, offset);\n+  __ bind(retaddr);\n+\n+  \/\/ Set an oopmap for the call site.  This oopmap will map all\n+  \/\/ oop-registers and debug-info registers as callee-saved.  This\n+  \/\/ will allow deoptimization at this safepoint to find all possible\n+  \/\/ debug-info recordings, as well as let GC find all oops.\n+\n+  oop_maps->add_gc_map( __ pc() - start, map);\n+\n+  Label noException;\n+\n+  __ reset_last_Java_frame(false);\n+\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+\n+  __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+  __ beqz(t0, noException);\n+\n+  \/\/ Exception pending\n+\n+  reg_saver.restore_live_registers(masm);\n+\n+  __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ No exception case\n+  __ bind(noException);\n+\n+  Label no_adjust, bail;\n+  if (SafepointMechanism::uses_thread_local_poll() && !cause_return) {\n+    \/\/ If our stashed return pc was modified by the runtime we avoid touching it\n+    __ ld(t0, Address(fp, frame::return_addr_offset * wordSize));\n+    __ bne(x18, t0, no_adjust);\n+\n+#ifdef ASSERT\n+    \/\/ Verify the correct encoding of the poll we're about to skip.\n+    \/\/ See NativeInstruction::is_lwu_to_zr()\n+    __ lwu(t0, Address(x18));\n+    __ andi(t1, t0, 0b0000011);\n+    __ mv(t2, 0b0000011);\n+    __ bne(t1, t2, bail); \/\/ 0-6:0b0000011\n+    __ srli(t1, t0, 7);\n+    __ andi(t1, t1, 0b00000);\n+    __ bnez(t1, bail);    \/\/ 7-11:0b00000\n+    __ srli(t1, t0, 12);\n+    __ andi(t1, t1, 0b110);\n+    __ mv(t2, 0b110);\n+    __ bne(t1, t2, bail); \/\/ 12-14:0b110\n+#endif\n+    \/\/ Adjust return pc forward to step over the safepoint poll instruction\n+    __ add(x18, x18, NativeInstruction::instruction_size);\n+    __ sd(x18, Address(fp, frame::return_addr_offset * wordSize));\n+  }\n+\n+  __ bind(no_adjust);\n+  \/\/ Normal exit, restore registers and exit.\n+\n+  reg_saver.restore_live_registers(masm);\n+  __ ret();\n+\n+#ifdef ASSERT\n+  __ bind(bail);\n+  __ stop(\"Attempting to adjust pc to skip safepoint poll but the return point is not what we expected\");\n+#endif\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  \/\/ Fill-out other meta info\n+  return SafepointBlob::create(&buffer, oop_maps, frame_size_in_words);\n+}\n+\n+\/\/\n+\/\/ generate_resolve_blob - call resolution (static\/virtual\/opt-virtual\/ic-miss\n+\/\/\n+\/\/ Generate a stub that calls into vm to find out the proper destination\n+\/\/ of a java call. All the argument registers are live at this point\n+\/\/ but since this is generic code we don't know what they are and the caller\n+\/\/ must do any gc of the args.\n+\/\/\n+RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {\n+  assert (StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+\n+  \/\/ allocate space for the code\n+  ResourceMark rm;\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+  assert_cond(masm != NULL);\n+\n+  int frame_size_in_words = -1;\n+  RegisterSaver reg_saver;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+  OopMap* map = NULL;\n+\n+  int start = __ offset();\n+\n+  map = reg_saver.save_live_registers(masm, 0, &frame_size_in_words);\n+\n+  int frame_complete = __ offset();\n+\n+  {\n+    Label retaddr;\n+    __ set_last_Java_frame(sp, noreg, retaddr, t0);\n+\n+    __ mv(c_rarg0, xthread);\n+    int32_t offset = 0;\n+    __ la_patchable(t0, RuntimeAddress(destination), offset);\n+    __ jalr(x1, t0, offset);\n+    __ bind(retaddr);\n+  }\n+\n+  \/\/ Set an oopmap for the call site.\n+  \/\/ We need this not only for callee-saved registers, but also for volatile\n+  \/\/ registers that the compiler might be keeping live across a safepoint.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ x10 contains the address we are going to jump to assuming no exception got installed\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+  __ bnez(t0, pending);\n+\n+  \/\/ get the returned Method*\n+  __ get_vm_result_2(xmethod, xthread);\n+  __ sd(xmethod, Address(sp, reg_saver.reg_offset_in_bytes(xmethod)));\n+\n+  \/\/ x10 is where we want to jump, overwrite t0 which is saved and temporary\n+  __ sd(x10, Address(sp, reg_saver.reg_offset_in_bytes(t0)));\n+  reg_saver.restore_live_registers(masm);\n+\n+  \/\/ We are back the the original state on entry and ready to go.\n+\n+  __ jr(t0);\n+\n+  \/\/ Pending exception after the safepoint\n+\n+  __ bind(pending);\n+\n+  reg_saver.restore_live_registers(masm);\n+\n+  \/\/ exception pending => remove activation and forward to exception handler\n+\n+  __ sd(zr, Address(xthread, JavaThread::vm_result_offset()));\n+\n+  __ ld(x10, Address(xthread, Thread::pending_exception_offset()));\n+  __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  masm->flush();\n+\n+  \/\/ return the  blob\n+  return RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, true);\n+}\n+\n+#ifdef COMPILER2\n+\/\/------------------------------generate_exception_blob---------------------------\n+\/\/ creates exception blob at the end\n+\/\/ Using exception blob, this code is jumped from a compiled method.\n+\/\/ (see emit_exception_handler in riscv.ad file)\n+\/\/\n+\/\/ Given an exception pc at a call we call into the runtime for the\n+\/\/ handler in this method. This handler might merely restore state\n+\/\/ (i.e. callee save registers) unwind the frame and jump to the\n+\/\/ exception handler for the nmethod if there is no Java level handler\n+\/\/ for the nmethod.\n+\/\/\n+\/\/ This code is entered with a jmp.\n+\/\/\n+\/\/ Arguments:\n+\/\/   x10: exception oop\n+\/\/   x13: exception pc\n+\/\/\n+\/\/ Results:\n+\/\/   x10: exception oop\n+\/\/   x13: exception pc in caller\n+\/\/   destination: exception handler of caller\n+\/\/\n+\/\/ Note: the exception pc MUST be at a call (precise debug information)\n+\/\/       Registers x10, x13, x12, x14, x15, t0 are not callee saved.\n+\/\/\n+\n+void OptoRuntime::generate_exception_blob() {\n+  assert(!OptoRuntime::is_callee_saved_register(R13_num), \"\");\n+  assert(!OptoRuntime::is_callee_saved_register(R10_num), \"\");\n+  assert(!OptoRuntime::is_callee_saved_register(R12_num), \"\");\n+\n+  assert(SimpleRuntimeFrame::framesize % 4 == 0, \"sp not 16-byte aligned\");\n+\n+  \/\/ Allocate space for the code\n+  ResourceMark rm;\n+  \/\/ Setup code generation tools\n+  CodeBuffer buffer(\"exception_blob\", 2048, 1024);\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+  assert_cond(masm != NULL);\n+\n+  \/\/ TODO check various assumptions made here\n+  \/\/\n+  \/\/ make sure we do so before running this\n+\n+  address start = __ pc();\n+\n+  \/\/ push fp and retaddr by hand\n+  \/\/ Exception pc is 'return address' for stack walker\n+  __ addi(sp, sp, -2 * wordSize);\n+  __ sd(ra, Address(sp, wordSize));\n+  __ sd(fp, Address(sp));\n+  \/\/ there are no callee save registers and we don't expect an\n+  \/\/ arg reg save area\n+#ifndef PRODUCT\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+  \/\/ Store exception in Thread object. We cannot pass any arguments to the\n+  \/\/ handle_exception call, since we do not want to make any assumption\n+  \/\/ about the size of the frame where the exception happened in.\n+  __ sd(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+  __ sd(x13, Address(xthread, JavaThread::exception_pc_offset()));\n+\n+  \/\/ This call does all the hard work.  It checks if an exception handler\n+  \/\/ exists in the method.\n+  \/\/ If so, it returns the handler address.\n+  \/\/ If not, it prepares for stack-unwinding, restoring the callee-save\n+  \/\/ registers of the frame being removed.\n+  \/\/\n+  \/\/ address OptoRuntime::handle_exception_C(JavaThread* thread)\n+  \/\/\n+  \/\/ n.b. 1 gp arg, 0 fp args, integral return type\n+\n+  \/\/ the stack should always be aligned\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, noreg, the_pc, t0);\n+  __ mv(c_rarg0, xthread);\n+  int32_t offset = 0;\n+  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C)), offset);\n+  __ jalr(x1, t0, offset);\n+\n+\n+  \/\/ handle_exception_C is a special VM call which does not require an explicit\n+  \/\/ instruction sync afterwards.\n+\n+  \/\/ Set an oopmap for the call site.  This oopmap will only be used if we\n+  \/\/ are unwinding the stack.  Hence, all locations will be dead.\n+  \/\/ Callee-saved registers will be the same as the frame above (i.e.,\n+  \/\/ handle_exception_stub), since they were restored when we got the\n+  \/\/ exception.\n+\n+  OopMapSet* oop_maps = new OopMapSet();\n+  assert_cond(oop_maps != NULL);\n+\n+  oop_maps->add_gc_map(the_pc - start, new OopMap(SimpleRuntimeFrame::framesize, 0));\n+\n+  __ reset_last_Java_frame(false);\n+\n+  \/\/ Restore callee-saved registers\n+\n+  \/\/ fp is an implicitly saved callee saved register (i.e. the calling\n+  \/\/ convention will save restore it in prolog\/epilog) Other than that\n+  \/\/ there are no callee save registers now that adapter frames are gone.\n+  \/\/ and we dont' expect an arg reg save area\n+  __ ld(fp, Address(sp));\n+  __ ld(x13, Address(sp, wordSize));\n+  __ addi(sp, sp , 2 * wordSize);\n+\n+  \/\/ x10: exception handler\n+\n+  \/\/ We have a handler in x10 (could be deopt blob).\n+  __ mv(t0, x10);\n+\n+  \/\/ Get the exception oop\n+  __ ld(x10, Address(xthread, JavaThread::exception_oop_offset()));\n+  \/\/ Get the exception pc in case we are deoptimized\n+  __ ld(x14, Address(xthread, JavaThread::exception_pc_offset()));\n+#ifdef ASSERT\n+  __ sd(zr, Address(xthread, JavaThread::exception_handler_pc_offset()));\n+  __ sd(zr, Address(xthread, JavaThread::exception_pc_offset()));\n+#endif\n+  \/\/ Clear the exception oop so GC no longer processes it as a root.\n+  __ sd(zr, Address(xthread, JavaThread::exception_oop_offset()));\n+\n+  \/\/ x10: exception oop\n+  \/\/ t0:  exception handler\n+  \/\/ x14: exception pc\n+  \/\/ Jump to handler\n+\n+  __ jr(t0);\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  \/\/ Set exception blob\n+  _exception_blob =  ExceptionBlob::create(&buffer, oop_maps, SimpleRuntimeFrame::framesize >> 1);\n+}\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":2670,"deletions":0,"binary":false,"changes":2670,"status":"added"},{"patch":"@@ -0,0 +1,3743 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nativeInst_riscv.hpp\"\n+#include \"oops\/instanceOop.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/runtime.hpp\"\n+#endif\n+#if INCLUDE_ZGC\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+#endif\n+\n+\/\/ Declaration and definition of StubGenerator (no .hpp file).\n+\/\/ For a more detailed description of the stub routine structure\n+\/\/ see the comment in stubRoutines.hpp\n+\n+#undef __\n+#define __ _masm->\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Stub Code definitions\n+\n+class StubGenerator: public StubCodeGenerator {\n+ private:\n+\n+#ifdef PRODUCT\n+#define inc_counter_np(counter) ((void)0)\n+#else\n+  void inc_counter_np_(int& counter) {\n+    __ la(t1, ExternalAddress((address)&counter));\n+    __ lwu(t0, Address(t1, 0));\n+    __ addiw(t0, t0, 1);\n+    __ sw(t0, Address(t1, 0));\n+  }\n+#define inc_counter_np(counter) \\\n+  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n+  inc_counter_np_(counter);\n+#endif\n+\n+  \/\/ Call stubs are used to call Java from C\n+  \/\/\n+  \/\/ Arguments:\n+  \/\/    c_rarg0:   call wrapper address                   address\n+  \/\/    c_rarg1:   result                                 address\n+  \/\/    c_rarg2:   result type                            BasicType\n+  \/\/    c_rarg3:   method                                 Method*\n+  \/\/    c_rarg4:   (interpreter) entry point              address\n+  \/\/    c_rarg5:   parameters                             intptr_t*\n+  \/\/    c_rarg6:   parameter size (in words)              int\n+  \/\/    c_rarg7:   thread                                 Thread*\n+  \/\/\n+  \/\/ There is no return from the stub itself as any Java result\n+  \/\/ is written to result\n+  \/\/\n+  \/\/ we save x1 (ra) as the return PC at the base of the frame and\n+  \/\/ link x8 (fp) below it as the frame pointer installing sp (x2)\n+  \/\/ into fp.\n+  \/\/\n+  \/\/ we save x10-x17, which accounts for all the c arguments.\n+  \/\/\n+  \/\/ TODO: strictly do we need to save them all? they are treated as\n+  \/\/ volatile by C so could we omit saving the ones we are going to\n+  \/\/ place in global registers (thread? method?) or those we only use\n+  \/\/ during setup of the Java call?\n+  \/\/\n+  \/\/ we don't need to save x5 which C uses as an indirect result location\n+  \/\/ return register.\n+  \/\/\n+  \/\/ we don't need to save x6-x7 and x28-x31 which both C and Java treat as\n+  \/\/ volatile\n+  \/\/\n+  \/\/ we save x9, x18-x27, f8-f9, and f18-f27 which Java uses as temporary\n+  \/\/ registers and C expects to be callee-save\n+  \/\/\n+  \/\/ so the stub frame looks like this when we enter Java code\n+  \/\/\n+  \/\/     [ return_from_Java     ] <--- sp\n+  \/\/     [ argument word n      ]\n+  \/\/      ...\n+  \/\/ -34 [ argument word 1      ]\n+  \/\/ -33 [ saved f27            ] <--- sp_after_call\n+  \/\/ -32 [ saved f26            ]\n+  \/\/ -31 [ saved f25            ]\n+  \/\/ -30 [ saved f24            ]\n+  \/\/ -29 [ saved f23            ]\n+  \/\/ -28 [ saved f22            ]\n+  \/\/ -27 [ saved f21            ]\n+  \/\/ -26 [ saved f20            ]\n+  \/\/ -25 [ saved f19            ]\n+  \/\/ -24 [ saved f18            ]\n+  \/\/ -23 [ saved f9             ]\n+  \/\/ -22 [ saved f8             ]\n+  \/\/ -21 [ saved x27            ]\n+  \/\/ -20 [ saved x26            ]\n+  \/\/ -19 [ saved x25            ]\n+  \/\/ -18 [ saved x24            ]\n+  \/\/ -17 [ saved x23            ]\n+  \/\/ -16 [ saved x22            ]\n+  \/\/ -15 [ saved x21            ]\n+  \/\/ -14 [ saved x20            ]\n+  \/\/ -13 [ saved x19            ]\n+  \/\/ -12 [ saved x18            ]\n+  \/\/ -11 [ saved x9             ]\n+  \/\/ -10 [ call wrapper   (x10) ]\n+  \/\/  -9 [ result         (x11) ]\n+  \/\/  -8 [ result type    (x12) ]\n+  \/\/  -7 [ method         (x13) ]\n+  \/\/  -6 [ entry point    (x14) ]\n+  \/\/  -5 [ parameters     (x15) ]\n+  \/\/  -4 [ parameter size (x16) ]\n+  \/\/  -3 [ thread         (x17) ]\n+  \/\/  -2 [ saved fp       (x8)  ]\n+  \/\/  -1 [ saved ra       (x1)  ]\n+  \/\/   0 [                      ] <--- fp == saved sp (x2)\n+\n+  \/\/ Call stub stack layout word offsets from fp\n+  enum call_stub_layout {\n+    sp_after_call_off  = -33,\n+\n+    f27_off            = -33,\n+    f26_off            = -32,\n+    f25_off            = -31,\n+    f24_off            = -30,\n+    f23_off            = -29,\n+    f22_off            = -28,\n+    f21_off            = -27,\n+    f20_off            = -26,\n+    f19_off            = -25,\n+    f18_off            = -24,\n+    f9_off             = -23,\n+    f8_off             = -22,\n+\n+    x27_off            = -21,\n+    x26_off            = -20,\n+    x25_off            = -19,\n+    x24_off            = -18,\n+    x23_off            = -17,\n+    x22_off            = -16,\n+    x21_off            = -15,\n+    x20_off            = -14,\n+    x19_off            = -13,\n+    x18_off            = -12,\n+    x9_off             = -11,\n+\n+    call_wrapper_off   = -10,\n+    result_off         = -9,\n+    result_type_off    = -8,\n+    method_off         = -7,\n+    entry_point_off    = -6,\n+    parameters_off     = -5,\n+    parameter_size_off = -4,\n+    thread_off         = -3,\n+    fp_f               = -2,\n+    retaddr_off        = -1,\n+  };\n+\n+  address generate_call_stub(address& return_address) {\n+    assert((int)frame::entry_frame_after_call_words == -(int)sp_after_call_off + 1 &&\n+           (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n+           \"adjust this code\");\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n+    address start = __ pc();\n+\n+    const Address sp_after_call (fp, sp_after_call_off  * wordSize);\n+\n+    const Address call_wrapper  (fp, call_wrapper_off   * wordSize);\n+    const Address result        (fp, result_off         * wordSize);\n+    const Address result_type   (fp, result_type_off    * wordSize);\n+    const Address method        (fp, method_off         * wordSize);\n+    const Address entry_point   (fp, entry_point_off    * wordSize);\n+    const Address parameters    (fp, parameters_off     * wordSize);\n+    const Address parameter_size(fp, parameter_size_off * wordSize);\n+\n+    const Address thread        (fp, thread_off         * wordSize);\n+\n+    const Address f27_save      (fp, f27_off            * wordSize);\n+    const Address f26_save      (fp, f26_off            * wordSize);\n+    const Address f25_save      (fp, f25_off            * wordSize);\n+    const Address f24_save      (fp, f24_off            * wordSize);\n+    const Address f23_save      (fp, f23_off            * wordSize);\n+    const Address f22_save      (fp, f22_off            * wordSize);\n+    const Address f21_save      (fp, f21_off            * wordSize);\n+    const Address f20_save      (fp, f20_off            * wordSize);\n+    const Address f19_save      (fp, f19_off            * wordSize);\n+    const Address f18_save      (fp, f18_off            * wordSize);\n+    const Address f9_save       (fp, f9_off             * wordSize);\n+    const Address f8_save       (fp, f8_off             * wordSize);\n+\n+    const Address x27_save      (fp, x27_off            * wordSize);\n+    const Address x26_save      (fp, x26_off            * wordSize);\n+    const Address x25_save      (fp, x25_off            * wordSize);\n+    const Address x24_save      (fp, x24_off            * wordSize);\n+    const Address x23_save      (fp, x23_off            * wordSize);\n+    const Address x22_save      (fp, x22_off            * wordSize);\n+    const Address x21_save      (fp, x21_off            * wordSize);\n+    const Address x20_save      (fp, x20_off            * wordSize);\n+    const Address x19_save      (fp, x19_off            * wordSize);\n+    const Address x18_save      (fp, x18_off            * wordSize);\n+\n+    const Address x9_save       (fp, x9_off             * wordSize);\n+\n+    \/\/ stub code\n+\n+    address riscv_entry = __ pc();\n+\n+    \/\/ set up frame and move sp to end of save area\n+    __ enter();\n+    __ addi(sp, fp, sp_after_call_off * wordSize);\n+\n+    \/\/ save register parameters and Java temporary\/global registers\n+    \/\/ n.b. we save thread even though it gets installed in\n+    \/\/ xthread because we want to sanity check tp later\n+    __ sd(c_rarg7, thread);\n+    __ sw(c_rarg6, parameter_size);\n+    __ sd(c_rarg5, parameters);\n+    __ sd(c_rarg4, entry_point);\n+    __ sd(c_rarg3, method);\n+    __ sd(c_rarg2, result_type);\n+    __ sd(c_rarg1, result);\n+    __ sd(c_rarg0, call_wrapper);\n+\n+    __ sd(x9, x9_save);\n+\n+    __ sd(x18, x18_save);\n+    __ sd(x19, x19_save);\n+    __ sd(x20, x20_save);\n+    __ sd(x21, x21_save);\n+    __ sd(x22, x22_save);\n+    __ sd(x23, x23_save);\n+    __ sd(x24, x24_save);\n+    __ sd(x25, x25_save);\n+    __ sd(x26, x26_save);\n+    __ sd(x27, x27_save);\n+\n+    __ fsd(f8,  f8_save);\n+    __ fsd(f9,  f9_save);\n+    __ fsd(f18, f18_save);\n+    __ fsd(f19, f19_save);\n+    __ fsd(f20, f20_save);\n+    __ fsd(f21, f21_save);\n+    __ fsd(f22, f22_save);\n+    __ fsd(f23, f23_save);\n+    __ fsd(f24, f24_save);\n+    __ fsd(f25, f25_save);\n+    __ fsd(f26, f26_save);\n+    __ fsd(f27, f27_save);\n+\n+    \/\/ install Java thread in global register now we have saved\n+    \/\/ whatever value it held\n+    __ mv(xthread, c_rarg7);\n+\n+    \/\/ And method\n+    __ mv(xmethod, c_rarg3);\n+\n+    \/\/ set up the heapbase register\n+    __ reinit_heapbase();\n+\n+#ifdef ASSERT\n+    \/\/ make sure we have no pending exceptions\n+    {\n+      Label L;\n+      __ ld(t0, Address(xthread, in_bytes(Thread::pending_exception_offset())));\n+      __ beqz(t0, L);\n+      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n+      __ BIND(L);\n+    }\n+#endif\n+    \/\/ pass parameters if any\n+    __ mv(esp, sp);\n+    __ slli(t0, c_rarg6, LogBytesPerWord);\n+    __ sub(t0, sp, t0); \/\/ Move SP out of the way\n+    __ andi(sp, t0, -2 * wordSize);\n+\n+    BLOCK_COMMENT(\"pass parameters if any\");\n+    Label parameters_done;\n+    \/\/ parameter count is still in c_rarg6\n+    \/\/ and parameter pointer identifying param 1 is in c_rarg5\n+    __ beqz(c_rarg6, parameters_done);\n+\n+    address loop = __ pc();\n+    __ ld(t0, c_rarg5, 0);\n+    __ addi(c_rarg5, c_rarg5, wordSize);\n+    __ addi(c_rarg6, c_rarg6, -1);\n+    __ push_reg(t0);\n+    __ bgtz(c_rarg6, loop);\n+\n+    __ BIND(parameters_done);\n+\n+    \/\/ call Java entry -- passing methdoOop, and current sp\n+    \/\/      xmethod: Method*\n+    \/\/      x30: sender sp\n+    BLOCK_COMMENT(\"call Java function\");\n+    __ mv(x30, sp);\n+    __ jalr(c_rarg4);\n+\n+    \/\/ save current address for use by exception handling code\n+\n+    return_address = __ pc();\n+\n+    \/\/ store result depending on type (everything that is not\n+    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+    \/\/ n.b. this assumes Java returns an integral result in x10\n+    \/\/ and a floating result in j_farg0\n+    __ ld(j_rarg2, result);\n+    Label is_long, is_float, is_double, exit;\n+    __ ld(j_rarg1, result_type);\n+    __ li(t0, (u1)T_OBJECT);\n+    __ beq(j_rarg1, t0, is_long);\n+    __ li(t0, (u1)T_LONG);\n+    __ beq(j_rarg1, t0, is_long);\n+    __ li(t0, (u1)T_FLOAT);\n+    __ beq(j_rarg1, t0, is_float);\n+    __ li(t0, (u1)T_DOUBLE);\n+    __ beq(j_rarg1, t0, is_double);\n+\n+    \/\/ handle T_INT case\n+    __ sw(x10, Address(j_rarg2));\n+\n+    __ BIND(exit);\n+\n+    \/\/ pop parameters\n+    __ addi(esp, fp, sp_after_call_off * wordSize);\n+\n+#ifdef ASSERT\n+    \/\/ verify that threads correspond\n+    {\n+      Label L, S;\n+      __ ld(t0, thread);\n+      __ bne(xthread, t0, S);\n+      __ get_thread(t0);\n+      __ beq(xthread, t0, L);\n+      __ BIND(S);\n+      __ stop(\"StubRoutines::call_stub: threads must correspond\");\n+      __ BIND(L);\n+    }\n+#endif\n+\n+    \/\/ restore callee-save registers\n+    __ fld(f27, f27_save);\n+    __ fld(f26, f26_save);\n+    __ fld(f25, f25_save);\n+    __ fld(f24, f24_save);\n+    __ fld(f23, f23_save);\n+    __ fld(f22, f22_save);\n+    __ fld(f21, f21_save);\n+    __ fld(f20, f20_save);\n+    __ fld(f19, f19_save);\n+    __ fld(f18, f18_save);\n+    __ fld(f9,  f9_save);\n+    __ fld(f8,  f8_save);\n+\n+    __ ld(x27, x27_save);\n+    __ ld(x26, x26_save);\n+    __ ld(x25, x25_save);\n+    __ ld(x24, x24_save);\n+    __ ld(x23, x23_save);\n+    __ ld(x22, x22_save);\n+    __ ld(x21, x21_save);\n+    __ ld(x20, x20_save);\n+    __ ld(x19, x19_save);\n+    __ ld(x18, x18_save);\n+\n+    __ ld(x9, x9_save);\n+\n+    __ ld(c_rarg0, call_wrapper);\n+    __ ld(c_rarg1, result);\n+    __ ld(c_rarg2, result_type);\n+    __ ld(c_rarg3, method);\n+    __ ld(c_rarg4, entry_point);\n+    __ ld(c_rarg5, parameters);\n+    __ ld(c_rarg6, parameter_size);\n+    __ ld(c_rarg7, thread);\n+\n+    \/\/ leave frame and return to caller\n+    __ leave();\n+    __ ret();\n+\n+    \/\/ handle return types different from T_INT\n+\n+    __ BIND(is_long);\n+    __ sd(x10, Address(j_rarg2, 0));\n+    __ j(exit);\n+\n+    __ BIND(is_float);\n+    __ fsw(j_farg0, Address(j_rarg2, 0), t0);\n+    __ j(exit);\n+\n+    __ BIND(is_double);\n+    __ fsd(j_farg0, Address(j_rarg2, 0), t0);\n+    __ j(exit);\n+\n+    return start;\n+  }\n+\n+  \/\/ Return point for a Java call if there's an exception thrown in\n+  \/\/ Java code.  The exception is caught and transformed into a\n+  \/\/ pending exception stored in JavaThread that can be tested from\n+  \/\/ within the VM.\n+  \/\/\n+  \/\/ Note: Usually the parameters are removed by the callee. In case\n+  \/\/ of an exception crossing an activation frame boundary, that is\n+  \/\/ not the case if the callee is compiled code => need to setup the\n+  \/\/ sp.\n+  \/\/\n+  \/\/ x10: exception oop\n+\n+  address generate_catch_exception() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n+    address start = __ pc();\n+\n+    \/\/ same as in generate_call_stub():\n+    const Address thread(fp, thread_off * wordSize);\n+\n+#ifdef ASSERT\n+    \/\/ verify that threads correspond\n+    {\n+      Label L, S;\n+      __ ld(t0, thread);\n+      __ bne(xthread, t0, S);\n+      __ get_thread(t0);\n+      __ beq(xthread, t0, L);\n+      __ bind(S);\n+      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n+      __ bind(L);\n+    }\n+#endif\n+\n+    \/\/ set pending exception\n+    __ verify_oop(x10);\n+\n+    __ sd(x10, Address(xthread, Thread::pending_exception_offset()));\n+    __ mv(t0, (address)__FILE__);\n+    __ sd(t0, Address(xthread, Thread::exception_file_offset()));\n+    __ mv(t0, (int)__LINE__);\n+    __ sw(t0, Address(xthread, Thread::exception_line_offset()));\n+\n+    \/\/ complete return to VM\n+    assert(StubRoutines::_call_stub_return_address != NULL,\n+           \"_call_stub_return_address must have been generated before\");\n+    __ j(StubRoutines::_call_stub_return_address);\n+\n+    return start;\n+  }\n+\n+  \/\/ Continuation point for runtime calls returning with a pending\n+  \/\/ exception.  The pending exception check happened in the runtime\n+  \/\/ or native call stub.  The pending exception in Thread is\n+  \/\/ converted into a Java-level exception.\n+  \/\/\n+  \/\/ Contract with Java-level exception handlers:\n+  \/\/ x10: exception\n+  \/\/ x13: throwing pc\n+  \/\/\n+  \/\/ NOTE: At entry of this stub, exception-pc must be in RA !!\n+\n+  \/\/ NOTE: this is always used as a jump target within generated code\n+  \/\/ so it just needs to be generated code with no x86 prolog\n+\n+  address generate_forward_exception() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n+    address start = __ pc();\n+\n+    \/\/ Upon entry, RA points to the return address returning into\n+    \/\/ Java (interpreted or compiled) code; i.e., the return address\n+    \/\/ becomes the throwing pc.\n+    \/\/\n+    \/\/ Arguments pushed before the runtime call are still on the stack\n+    \/\/ but the exception handler will reset the stack pointer ->\n+    \/\/ ignore them.  A potential result in registers can be ignored as\n+    \/\/ well.\n+\n+#ifdef ASSERT\n+    \/\/ make sure this code is only executed if there is a pending exception\n+    {\n+      Label L;\n+      __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+      __ bnez(t0, L);\n+      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n+      __ bind(L);\n+    }\n+#endif\n+\n+    \/\/ compute exception handler into x9\n+\n+    \/\/ call the VM to find the handler address associated with the\n+    \/\/ caller address. pass thread in x10 and caller pc (ret address)\n+    \/\/ in x11. n.b. the caller pc is in ra, unlike x86 where it is on\n+    \/\/ the stack.\n+    __ mv(c_rarg1, ra);\n+    \/\/ ra will be trashed by the VM call so we move it to x9\n+    \/\/ (callee-saved) because we also need to pass it to the handler\n+    \/\/ returned by this call.\n+    __ mv(x9, ra);\n+    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                         SharedRuntime::exception_handler_for_return_address),\n+                    xthread, c_rarg1);\n+    \/\/ we should not really care that ra is no longer the callee\n+    \/\/ address. we saved the value the handler needs in x9 so we can\n+    \/\/ just copy it to x13. however, the C2 handler will push its own\n+    \/\/ frame and then calls into the VM and the VM code asserts that\n+    \/\/ the PC for the frame above the handler belongs to a compiled\n+    \/\/ Java method. So, we restore ra here to satisfy that assert.\n+    __ mv(ra, x9);\n+    \/\/ setup x10 & x13 & clear pending exception\n+    __ mv(x13, x9);\n+    __ mv(x9, x10);\n+    __ ld(x10, Address(xthread, Thread::pending_exception_offset()));\n+    __ sd(zr, Address(xthread, Thread::pending_exception_offset()));\n+\n+#ifdef ASSERT\n+    \/\/ make sure exception is set\n+    {\n+      Label L;\n+      __ bnez(x10, L);\n+      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n+      __ bind(L);\n+    }\n+#endif\n+\n+    \/\/ continue at exception handler\n+    \/\/ x10: exception\n+    \/\/ x13: throwing pc\n+    \/\/ x9: exception handler\n+    __ verify_oop(x10);\n+    __ jr(x9);\n+\n+    return start;\n+  }\n+\n+  \/\/ Non-destructive plausibility checks for oops\n+  \/\/\n+  \/\/ Arguments:\n+  \/\/    x10: oop to verify\n+  \/\/    t0: error message\n+  \/\/\n+  \/\/ Stack after saving c_rarg3:\n+  \/\/    [tos + 0]: saved c_rarg3\n+  \/\/    [tos + 1]: saved c_rarg2\n+  \/\/    [tos + 2]: saved ra\n+  \/\/    [tos + 3]: saved t1\n+  \/\/    [tos + 4]: saved x10\n+  \/\/    [tos + 5]: saved t0\n+  address generate_verify_oop() {\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n+    address start = __ pc();\n+\n+    Label exit, error;\n+\n+    __ push_reg(0x3000, sp);   \/\/ save c_rarg2 and c_rarg3\n+\n+    __ la(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));\n+    __ ld(c_rarg3, Address(c_rarg2));\n+    __ add(c_rarg3, c_rarg3, 1);\n+    __ sd(c_rarg3, Address(c_rarg2));\n+\n+    \/\/ object is in x10\n+    \/\/ make sure object is 'reasonable'\n+    __ beqz(x10, exit); \/\/ if obj is NULL it is OK\n+\n+    \/\/ Check if the oop is in the right area of memory\n+    __ mv(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n+    __ andr(c_rarg2, x10, c_rarg3);\n+    __ mv(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n+\n+    \/\/ Compare c_rarg2 and c_rarg3.\n+    __ bne(c_rarg2, c_rarg3, error);\n+\n+    \/\/ make sure klass is 'reasonable', which is not zero.\n+    __ load_klass(x10, x10);  \/\/ get klass\n+    __ beqz(x10, error);      \/\/ if klass is NULL it is broken\n+\n+    \/\/ return if everything seems ok\n+    __ bind(exit);\n+\n+    __ pop_reg(0x3000, sp);   \/\/ pop c_rarg2 and c_rarg3\n+    __ ret();\n+\n+    \/\/ handle errors\n+    __ bind(error);\n+    __ pop_reg(0x3000, sp);   \/\/ pop c_rarg2 and c_rarg3\n+\n+    __ pusha();\n+    \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n+    __ mv(c_rarg0, t0);             \/\/ pass address of error message\n+    __ mv(c_rarg1, ra);             \/\/ pass return address\n+    __ mv(c_rarg2, sp);             \/\/ pass address of regs on stack\n+#ifndef PRODUCT\n+    assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n+    int32_t offset = 0;\n+    __ movptr_with_offset(t0, CAST_FROM_FN_PTR(address, MacroAssembler::debug64), offset);\n+    __ jalr(x1, t0, offset);\n+    __ ebreak();\n+\n+    return start;\n+  }\n+\n+  \/\/ The inner part of zero_words().\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/ x28: the HeapWord-aligned base address of an array to zero.\n+  \/\/ x29: the count in HeapWords, x29 > 0.\n+  \/\/\n+  \/\/ Returns x28 and x29, adjusted for the caller to clear.\n+  \/\/ x28: the base address of the tail of words left to clear.\n+  \/\/ x29: the number of words in the tail.\n+  \/\/      x29 < MacroAssembler::zero_words_block_size.\n+\n+  address generate_zero_blocks() {\n+    Label done;\n+\n+    const Register base = x28, cnt = x29;\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"zero_blocks\");\n+    address start = __ pc();\n+\n+    {\n+      \/\/ Clear the remaining blocks.\n+      Label loop;\n+      __ sub(cnt, cnt, MacroAssembler::zero_words_block_size);\n+      __ bltz(cnt, done);\n+      __ bind(loop);\n+      for (int i = 0; i < MacroAssembler::zero_words_block_size; i++) {\n+        __ sd(zr, Address(base, 0));\n+        __ add(base, base, 8);\n+      }\n+      __ sub(cnt, cnt, MacroAssembler::zero_words_block_size);\n+      __ bgez(cnt, loop);\n+      __ bind(done);\n+      __ add(cnt, cnt, MacroAssembler::zero_words_block_size);\n+    }\n+\n+    __ ret();\n+\n+    return start;\n+  }\n+\n+  typedef enum {\n+    copy_forwards = 1,\n+    copy_backwards = -1\n+  } copy_direction;\n+\n+  \/\/ Bulk copy of blocks of 8 words.\n+  \/\/\n+  \/\/ count is a count of words.\n+  \/\/\n+  \/\/ Precondition: count >= 8\n+  \/\/\n+  \/\/ Postconditions:\n+  \/\/\n+  \/\/ The least significant bit of count contains the remaining count\n+  \/\/ of words to copy.  The rest of count is trash.\n+  \/\/\n+  \/\/ s and d are adjusted to point to the remaining words to copy\n+  \/\/\n+  void generate_copy_longs(Label &start, Register s, Register d, Register count,\n+                           copy_direction direction) {\n+    int unit = wordSize * direction;\n+    int bias = wordSize;\n+\n+    const Register tmp_reg0 = x13, tmp_reg1 = x14, tmp_reg2 = x15, tmp_reg3 = x16,\n+      tmp_reg4 = x17, tmp_reg5 = x7, tmp_reg6 = x28, tmp_reg7 = x29;\n+\n+    const Register stride = x30;\n+\n+    assert_different_registers(t0, tmp_reg0, tmp_reg1, tmp_reg2, tmp_reg3,\n+      tmp_reg4, tmp_reg5, tmp_reg6, tmp_reg7);\n+    assert_different_registers(s, d, count, t0);\n+\n+    Label again, drain;\n+    const char* stub_name = NULL;\n+    if (direction == copy_forwards) {\n+      stub_name = \"forward_copy_longs\";\n+    } else {\n+      stub_name = \"backward_copy_longs\";\n+    }\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    __ align(CodeEntryAlignment);\n+    __ bind(start);\n+\n+    if (direction == copy_forwards) {\n+      __ sub(s, s, bias);\n+      __ sub(d, d, bias);\n+    }\n+\n+#ifdef ASSERT\n+    \/\/ Make sure we are never given < 8 words\n+    {\n+      Label L;\n+\n+      __ li(t0, 8);\n+      __ bge(count, t0, L);\n+      __ stop(\"genrate_copy_longs called with < 8 words\");\n+      __ bind(L);\n+    }\n+#endif\n+\n+    __ ld(tmp_reg0, Address(s, 1 * unit));\n+    __ ld(tmp_reg1, Address(s, 2 * unit));\n+    __ ld(tmp_reg2, Address(s, 3 * unit));\n+    __ ld(tmp_reg3, Address(s, 4 * unit));\n+    __ ld(tmp_reg4, Address(s, 5 * unit));\n+    __ ld(tmp_reg5, Address(s, 6 * unit));\n+    __ ld(tmp_reg6, Address(s, 7 * unit));\n+    __ ld(tmp_reg7, Address(s, 8 * unit));\n+    __ addi(s, s, 8 * unit);\n+\n+    __ sub(count, count, 16);\n+    __ bltz(count, drain);\n+\n+    __ bind(again);\n+\n+    __ sd(tmp_reg0, Address(d, 1 * unit));\n+    __ sd(tmp_reg1, Address(d, 2 * unit));\n+    __ sd(tmp_reg2, Address(d, 3 * unit));\n+    __ sd(tmp_reg3, Address(d, 4 * unit));\n+    __ sd(tmp_reg4, Address(d, 5 * unit));\n+    __ sd(tmp_reg5, Address(d, 6 * unit));\n+    __ sd(tmp_reg6, Address(d, 7 * unit));\n+    __ sd(tmp_reg7, Address(d, 8 * unit));\n+\n+    __ ld(tmp_reg0, Address(s, 1 * unit));\n+    __ ld(tmp_reg1, Address(s, 2 * unit));\n+    __ ld(tmp_reg2, Address(s, 3 * unit));\n+    __ ld(tmp_reg3, Address(s, 4 * unit));\n+    __ ld(tmp_reg4, Address(s, 5 * unit));\n+    __ ld(tmp_reg5, Address(s, 6 * unit));\n+    __ ld(tmp_reg6, Address(s, 7 * unit));\n+    __ ld(tmp_reg7, Address(s, 8 * unit));\n+\n+    __ addi(s, s, 8 * unit);\n+    __ addi(d, d, 8 * unit);\n+\n+    __ sub(count, count, 8);\n+    __ bgez(count, again);\n+\n+    \/\/ Drain\n+    __ bind(drain);\n+\n+    __ sd(tmp_reg0, Address(d, 1 * unit));\n+    __ sd(tmp_reg1, Address(d, 2 * unit));\n+    __ sd(tmp_reg2, Address(d, 3 * unit));\n+    __ sd(tmp_reg3, Address(d, 4 * unit));\n+    __ sd(tmp_reg4, Address(d, 5 * unit));\n+    __ sd(tmp_reg5, Address(d, 6 * unit));\n+    __ sd(tmp_reg6, Address(d, 7 * unit));\n+    __ sd(tmp_reg7, Address(d, 8 * unit));\n+    __ addi(d, d, 8 * unit);\n+\n+    {\n+      Label L1, L2;\n+      __ andi(t0, count, 4);\n+      __ beqz(t0, L1);\n+\n+      __ ld(tmp_reg0, Address(s, 1 * unit));\n+      __ ld(tmp_reg1, Address(s, 2 * unit));\n+      __ ld(tmp_reg2, Address(s, 3 * unit));\n+      __ ld(tmp_reg3, Address(s, 4 * unit));\n+      __ addi(s, s, 4 * unit);\n+\n+      __ sd(tmp_reg0, Address(d, 1 * unit));\n+      __ sd(tmp_reg1, Address(d, 2 * unit));\n+      __ sd(tmp_reg2, Address(d, 3 * unit));\n+      __ sd(tmp_reg3, Address(d, 4 * unit));\n+      __ addi(d, d, 4 * unit);\n+\n+      __ bind(L1);\n+\n+      if (direction == copy_forwards) {\n+        __ addi(s, s, bias);\n+        __ addi(d, d, bias);\n+      }\n+\n+      __ andi(t0, count, 2);\n+      __ beqz(t0, L2);\n+      if (direction == copy_backwards) {\n+        __ addi(s, s, 2 * unit);\n+        __ ld(tmp_reg0, Address(s));\n+        __ ld(tmp_reg1, Address(s, wordSize));\n+        __ addi(d, d, 2 * unit);\n+        __ sd(tmp_reg0, Address(d));\n+        __ sd(tmp_reg1, Address(d, wordSize));\n+      } else {\n+        __ ld(tmp_reg0, Address(s));\n+        __ ld(tmp_reg1, Address(s, wordSize));\n+        __ addi(s, s, 2 * unit);\n+        __ sd(tmp_reg0, Address(d));\n+        __ sd(tmp_reg1, Address(d, wordSize));\n+        __ addi(d, d, 2 * unit);\n+      }\n+      __ bind(L2);\n+    }\n+\n+    __ ret();\n+  }\n+\n+  Label copy_f, copy_b;\n+\n+  \/\/ All-singing all-dancing memory copy.\n+  \/\/\n+  \/\/ Copy count units of memory from s to d.  The size of a unit is\n+  \/\/ step, which can be positive or negative depending on the direction\n+  \/\/ of copy.  If is_aligned is false, we align the source address.\n+  \/\/\n+  \/*\n+   * if (is_aligned) {\n+   *   goto copy_8_bytes;\n+   * }\n+   * bool is_backwards = step < 0;\n+   * int granularity = uabs(step);\n+   * count = count  *  granularity;   * count bytes\n+   *\n+   * if (is_backwards) {\n+   *   s += count;\n+   *   d += count;\n+   * }\n+   *\n+   * count limit maybe greater than 16, for better performance\n+   * if (count < 16) {\n+   *   goto copy_small;\n+   * }\n+   *\n+   * if ((dst % 8) == (src % 8)) {\n+   *   aligned;\n+   *   goto copy8;\n+   * }\n+   *\n+   * copy_small:\n+   *   load element one by one;\n+   * done;\n+   *\/\n+\n+  typedef void (MacroAssembler::*copy_insn)(Register Rd, const Address &adr, Register temp);\n+\n+  void copy_memory_v(Register s, Register d, Register count, Register tmp, int step) {\n+    bool is_backward = step < 0;\n+    int granularity = uabs(step);\n+\n+    const Register src = x30, dst = x31, vl = x14, cnt = x15, tmp1 = x16, tmp2 = x17;\n+    assert_different_registers(s, d, cnt, vl, tmp, tmp1, tmp2);\n+    Assembler::SEW sew = Assembler::elembytes_to_sew(granularity);\n+    Label loop_forward, loop_backward, done;\n+\n+    __ mv(dst, d);\n+    __ mv(src, s);\n+    __ mv(cnt, count);\n+\n+    __ bind(loop_forward);\n+    __ vsetvli(vl, cnt, sew, Assembler::m8);\n+    if (is_backward) {\n+      __ bne(vl, cnt, loop_backward);\n+    }\n+\n+    __ vlex_v(v0, src, sew);\n+    __ sub(cnt, cnt, vl);\n+    __ slli(vl, vl, (int)sew);\n+    __ add(src, src, vl);\n+\n+    __ vsex_v(v0, dst, sew);\n+    __ add(dst, dst, vl);\n+    __ bnez(cnt, loop_forward);\n+\n+    if (is_backward) {\n+      __ j(done);\n+\n+      __ bind(loop_backward);\n+      __ sub(tmp, cnt, vl);\n+      __ slli(tmp, tmp, sew);\n+      __ add(tmp1, s, tmp);\n+      __ vlex_v(v0, tmp1, sew);\n+      __ add(tmp2, d, tmp);\n+      __ vsex_v(v0, tmp2, sew);\n+      __ sub(cnt, cnt, vl);\n+      __ bnez(cnt, loop_forward);\n+      __ bind(done);\n+    }\n+  }\n+\n+  void copy_memory(bool is_aligned, Register s, Register d,\n+                   Register count, Register tmp, int step) {\n+    if (UseRVV) {\n+      return copy_memory_v(s, d, count, tmp, step);\n+    }\n+\n+    bool is_backwards = step < 0;\n+    int granularity = uabs(step);\n+\n+    const Register src = x30, dst = x31, cnt = x15, tmp3 = x16, tmp4 = x17;\n+\n+    Label same_aligned;\n+    Label copy8, copy_small, done;\n+\n+    copy_insn ld_arr = NULL, st_arr = NULL;\n+    switch (granularity) {\n+      case 1 :\n+        ld_arr = (copy_insn)&MacroAssembler::lbu;\n+        st_arr = (copy_insn)&MacroAssembler::sb;\n+        break;\n+      case 2 :\n+        ld_arr = (copy_insn)&MacroAssembler::lhu;\n+        st_arr = (copy_insn)&MacroAssembler::sh;\n+        break;\n+      case 4 :\n+        ld_arr = (copy_insn)&MacroAssembler::lwu;\n+        st_arr = (copy_insn)&MacroAssembler::sw;\n+        break;\n+      case 8 :\n+        ld_arr = (copy_insn)&MacroAssembler::ld;\n+        st_arr = (copy_insn)&MacroAssembler::sd;\n+        break;\n+      default :\n+        ShouldNotReachHere();\n+    }\n+\n+    __ beqz(count, done);\n+    __ slli(cnt, count, exact_log2(granularity));\n+    if (is_backwards) {\n+      __ add(src, s, cnt);\n+      __ add(dst, d, cnt);\n+    } else {\n+      __ mv(src, s);\n+      __ mv(dst, d);\n+    }\n+\n+    if (is_aligned) {\n+      __ addi(tmp, cnt, -8);\n+      __ bgez(tmp, copy8);\n+      __ j(copy_small);\n+    }\n+\n+    __ mv(tmp, 16);\n+    __ blt(cnt, tmp, copy_small);\n+\n+    __ xorr(tmp, src, dst);\n+    __ andi(tmp, tmp, 0b111);\n+    __ bnez(tmp, copy_small);\n+\n+    __ bind(same_aligned);\n+    __ andi(tmp, src, 0b111);\n+    __ beqz(tmp, copy8);\n+    if (is_backwards) {\n+      __ addi(src, src, step);\n+      __ addi(dst, dst, step);\n+    }\n+    (_masm->*ld_arr)(tmp3, Address(src), t0);\n+    (_masm->*st_arr)(tmp3, Address(dst), t0);\n+    if (!is_backwards) {\n+      __ addi(src, src, step);\n+      __ addi(dst, dst, step);\n+    }\n+    __ addi(cnt, cnt, -granularity);\n+    __ beqz(cnt, done);\n+    __ j(same_aligned);\n+\n+    __ bind(copy8);\n+    if (is_backwards) {\n+      __ addi(src, src, -wordSize);\n+      __ addi(dst, dst, -wordSize);\n+    }\n+    __ ld(tmp3, Address(src));\n+    __ sd(tmp3, Address(dst));\n+    if (!is_backwards) {\n+      __ addi(src, src, wordSize);\n+      __ addi(dst, dst, wordSize);\n+    }\n+    __ addi(cnt, cnt, -wordSize);\n+    __ addi(tmp4, cnt, -8);\n+    __ bgez(tmp4, copy8); \/\/ cnt >= 8, do next loop\n+\n+    __ beqz(cnt, done);\n+\n+    __ bind(copy_small);\n+    if (is_backwards) {\n+      __ addi(src, src, step);\n+      __ addi(dst, dst, step);\n+    }\n+    (_masm->*ld_arr)(tmp3, Address(src), t0);\n+    (_masm->*st_arr)(tmp3, Address(dst), t0);\n+    if (!is_backwards) {\n+      __ addi(src, src, step);\n+      __ addi(dst, dst, step);\n+    }\n+    __ addi(cnt, cnt, -granularity);\n+    __ bgtz(cnt, copy_small);\n+\n+    __ bind(done);\n+  }\n+\n+  \/\/ Scan over array at a for count oops, verifying each one.\n+  \/\/ Preserves a and count, clobbers t0 and t1.\n+  void verify_oop_array(size_t size, Register a, Register count, Register temp) {\n+    Label loop, end;\n+    __ mv(t1, zr);\n+    __ slli(t0, count, exact_log2(size));\n+    __ bind(loop);\n+    __ bgeu(t1, t0, end);\n+\n+    __ add(temp, a, t1);\n+    if (size == (size_t)wordSize) {\n+      __ ld(temp, Address(temp, 0));\n+      __ verify_oop(temp);\n+    } else {\n+      __ lwu(temp, Address(temp, 0));\n+      __ decode_heap_oop(temp); \/\/ calls verify_oop\n+    }\n+    __ add(t1, t1, size);\n+    __ j(loop);\n+    __ bind(end);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   is_oop  - true => oop array, so generate store check code\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+  \/\/ the hardware handle it.  The two dwords within qwords that span\n+  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_int_oop_copy().\n+  \/\/\n+  address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address* entry,\n+                                 const char* name, bool dest_uninitialized = false) {\n+    const Register s = c_rarg0, d = c_rarg1, count = c_rarg2;\n+    RegSet saved_reg = RegSet::of(s, d, count);\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+    __ enter();\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);\n+\n+    if (is_oop) {\n+      \/\/ save regs before copy_memory\n+      __ push_reg(RegSet::of(d, count), sp);\n+    }\n+\n+    copy_memory(aligned, s, d, count, t0, size);\n+\n+    if (is_oop) {\n+      __ pop_reg(RegSet::of(d, count), sp);\n+      if (VerifyOops) {\n+        verify_oop_array(size, d, count, t2);\n+      }\n+    }\n+\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, t0, RegSet());\n+\n+    __ leave();\n+    __ mv(x10, zr); \/\/ return 0\n+    __ ret();\n+    return start;\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   is_oop  - true => oop array, so generate store check code\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+  \/\/ the hardware handle it.  The two dwords within qwords that span\n+  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/\n+  address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,\n+                                 address* entry, const char* name,\n+                                 bool dest_uninitialized = false) {\n+    const Register s = c_rarg0, d = c_rarg1, count = c_rarg2;\n+    RegSet saved_regs = RegSet::of(s, d, count);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+    __ enter();\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    \/\/ use fwd copy when (d-s) above_equal (count*size)\n+    __ sub(t0, d, s);\n+    __ slli(t1, count, exact_log2(size));\n+    __ bgeu(t0, t1, nooverlap_target);\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);\n+\n+    if (is_oop) {\n+      \/\/ save regs before copy_memory\n+      __ push_reg(RegSet::of(d, count), sp);\n+    }\n+\n+    copy_memory(aligned, s, d, count, t0, -size);\n+\n+    if (is_oop) {\n+      __ pop_reg(RegSet::of(d, count), sp);\n+      if (VerifyOops) {\n+        verify_oop_array(size, d, count, t2);\n+      }\n+    }\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, t0, RegSet());\n+    __ leave();\n+    __ mv(x10, zr); \/\/ return 0\n+    __ ret();\n+    return start;\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n+  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n+  \/\/ and stored atomically.\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n+  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n+  \/\/ and stored atomically.\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_byte_copy().\n+  \/\/\n+  address generate_disjoint_byte_copy(bool aligned, address* entry, const char* name) {\n+    const bool not_oop = false;\n+    return generate_disjoint_copy(sizeof (jbyte), aligned, not_oop, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n+  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n+  \/\/ and stored atomically.\n+  \/\/\n+  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                      address* entry, const char* name) {\n+    const bool not_oop = false;\n+    return generate_conjoint_copy(sizeof (jbyte), aligned, not_oop, nooverlap_target, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+  \/\/ let the hardware handle it.  The two or four words within dwords\n+  \/\/ or qwords that span cache line boundaries will still be loaded\n+  \/\/ and stored atomically.\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_short_copy().\n+  \/\/\n+  address generate_disjoint_short_copy(bool aligned,\n+                                       address* entry, const char* name) {\n+    const bool not_oop = false;\n+    return generate_disjoint_copy(sizeof (jshort), aligned, not_oop, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+  \/\/ let the hardware handle it.  The two or four words within dwords\n+  \/\/ or qwords that span cache line boundaries will still be loaded\n+  \/\/ and stored atomically.\n+  \/\/\n+  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                       address* entry, const char* name) {\n+    const bool not_oop = false;\n+    return generate_conjoint_copy(sizeof (jshort), aligned, not_oop, nooverlap_target, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+  \/\/ the hardware handle it.  The two dwords within qwords that span\n+  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_int_oop_copy().\n+  \/\/\n+  address generate_disjoint_int_copy(bool aligned, address* entry,\n+                                     const char* name, bool dest_uninitialized = false) {\n+    const bool not_oop = false;\n+    return generate_disjoint_copy(sizeof (jint), aligned, not_oop, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+  \/\/ the hardware handle it.  The two dwords within qwords that span\n+  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/\n+  address generate_conjoint_int_copy(bool aligned, address nooverlap_target,\n+                                     address* entry, const char* name,\n+                                     bool dest_uninitialized = false) {\n+    const bool not_oop = false;\n+    return generate_conjoint_copy(sizeof (jint), aligned, not_oop, nooverlap_target, entry, name);\n+  }\n+\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as size_t, can be zero\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n+  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n+  \/\/\n+  address generate_disjoint_long_copy(bool aligned, address* entry,\n+                                      const char* name, bool dest_uninitialized = false) {\n+    const bool not_oop = false;\n+    return generate_disjoint_copy(sizeof (jlong), aligned, not_oop, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as size_t, can be zero\n+  \/\/\n+  address generate_conjoint_long_copy(bool aligned,\n+                                      address nooverlap_target, address* entry,\n+                                      const char* name, bool dest_uninitialized = false) {\n+    const bool not_oop = false;\n+    return generate_conjoint_copy(sizeof (jlong), aligned, not_oop, nooverlap_target, entry, name);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as size_t, can be zero\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n+  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n+  \/\/\n+  address generate_disjoint_oop_copy(bool aligned, address* entry,\n+                                     const char* name, bool dest_uninitialized) {\n+    const bool is_oop = true;\n+    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n+    return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+  \/\/             ignored\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as size_t, can be zero\n+  \/\/\n+  address generate_conjoint_oop_copy(bool aligned,\n+                                     address nooverlap_target, address* entry,\n+                                     const char* name, bool dest_uninitialized) {\n+    const bool is_oop = true;\n+    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n+    return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,\n+                                  name, dest_uninitialized);\n+  }\n+\n+  \/\/ Helper for generating a dynamic type check.\n+  \/\/ Smashes t0, t1.\n+  void generate_type_check(Register sub_klass,\n+                           Register super_check_offset,\n+                           Register super_klass,\n+                           Label& L_success) {\n+    assert_different_registers(sub_klass, super_check_offset, super_klass);\n+\n+    BLOCK_COMMENT(\"type_check:\");\n+\n+    Label L_miss;\n+\n+    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg, &L_success, &L_miss, NULL, super_check_offset);\n+    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n+\n+    \/\/ Fall through on failure!\n+    __ BIND(L_miss);\n+  }\n+\n+  \/\/\n+  \/\/  Generate checkcasting array copy stub\n+  \/\/\n+  \/\/  Input:\n+  \/\/    c_rarg0   - source array address\n+  \/\/    c_rarg1   - destination array address\n+  \/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/    c_rarg3   - size_t ckoff (super_check_offset)\n+  \/\/    c_rarg4   - oop ckval (super_klass)\n+  \/\/\n+  \/\/  Output:\n+  \/\/    x10 ==  0  -  success\n+  \/\/    x10 == -1^K - failure, where K is partial transfer count\n+  \/\/\n+  address generate_checkcast_copy(const char* name, address* entry,\n+                                  bool dest_uninitialized = false) {\n+    Label L_load_element, L_store_element, L_do_card_marks, L_done, L_done_pop;\n+\n+    \/\/ Input registers (after setup_arg_regs)\n+    const Register from        = c_rarg0;   \/\/ source array address\n+    const Register to          = c_rarg1;   \/\/ destination array address\n+    const Register count       = c_rarg2;   \/\/ elementscount\n+    const Register ckoff       = c_rarg3;   \/\/ super_check_offset\n+    const Register ckval       = c_rarg4;   \/\/ super_klass\n+\n+    RegSet wb_pre_saved_regs   = RegSet::range(c_rarg0, c_rarg4);\n+    RegSet wb_post_saved_regs  = RegSet::of(count);\n+\n+    \/\/ Registers used as temps (x7, x9, x18 are save-on-entry)\n+    const Register count_save  = x19;       \/\/ orig elementscount\n+    const Register start_to    = x18;       \/\/ destination array start address\n+    const Register copied_oop  = x7;        \/\/ actual oop copied\n+    const Register r9_klass    = x9;        \/\/ oop._klass\n+\n+    \/\/---------------------------------------------------------------\n+    \/\/ Assembler stub will be used for this call to arraycopy\n+    \/\/ if the two arrays are subtypes of Object[] but the\n+    \/\/ destination array type is not equal to or a supertype\n+    \/\/ of the source type.  Each element must be separately\n+    \/\/ checked.\n+\n+    assert_different_registers(from, to, count, ckoff, ckval, start_to,\n+                               copied_oop, r9_klass, count_save);\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+    \/\/ Caller of this entry point must set up the argument registers.\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    \/\/ Empty array:  Nothing to do\n+    __ beqz(count, L_done);\n+\n+    __ push_reg(RegSet::of(x7, x9, x18, x19), sp);\n+\n+#ifdef ASSERT\n+    BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n+    \/\/ The ckoff and ckval must be mutually consistent,\n+    \/\/ even though caller generates both.\n+    { Label L;\n+      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+      __ lwu(start_to, Address(ckval, sco_offset));\n+      __ beq(ckoff, start_to, L);\n+      __ stop(\"super_check_offset inconsistent\");\n+      __ bind(L);\n+    }\n+#endif \/\/ASSERT\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n+    bool is_oop = true;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);\n+\n+    \/\/ save the original count\n+    __ mv(count_save, count);\n+\n+    \/\/ Copy from low to high addresses\n+    __ mv(start_to, to);              \/\/ Save destination array start address\n+    __ j(L_load_element);\n+\n+    \/\/ ======== begin loop ========\n+    \/\/ (Loop is rotated; its entry is L_load_element.)\n+    \/\/ Loop control:\n+    \/\/   for count to 0 do\n+    \/\/     copied_oop = load_heap_oop(from++)\n+    \/\/     ... generate_type_check ...\n+    \/\/     store_heap_oop(to++, copied_oop)\n+    \/\/   end\n+\n+    __ align(OptoLoopAlignment);\n+\n+    __ BIND(L_store_element);\n+    __ store_heap_oop(Address(to, 0), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ add(to, to, UseCompressedOops ? 4 : 8);\n+    __ sub(count, count, 1);\n+    __ beqz(count, L_do_card_marks);\n+\n+    \/\/ ======== loop entry is here ========\n+    __ BIND(L_load_element);\n+    __ load_heap_oop(copied_oop, Address(from, 0), noreg, noreg, AS_RAW); \/\/ load the oop\n+    __ add(from, from, UseCompressedOops ? 4 : 8);\n+    __ beqz(copied_oop, L_store_element);\n+\n+    __ load_klass(r9_klass, copied_oop);\/\/ query the object klass\n+    generate_type_check(r9_klass, ckoff, ckval, L_store_element);\n+    \/\/ ======== end loop ========\n+\n+    \/\/ It was a real error; we must depend on the caller to finish the job.\n+    \/\/ Register count = remaining oops, count_orig = total oops.\n+    \/\/ Emit GC store barriers for the oops we have copied and report\n+    \/\/ their number to the caller.\n+\n+    __ sub(count, count_save, count);     \/\/ K = partially copied oop count\n+    __ xori(count, count, -1);                   \/\/ report (-1^K) to caller\n+    __ beqz(count, L_done_pop);\n+\n+    __ BIND(L_do_card_marks);\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, t0, wb_post_saved_regs);\n+\n+    __ bind(L_done_pop);\n+    __ pop_reg(RegSet::of(x7, x9, x18, x19), sp);\n+    inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);\n+\n+    __ bind(L_done);\n+    __ mv(x10, count);\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n+  \/\/ Perform range checks on the proposed arraycopy.\n+  \/\/ Kills temp, but nothing else.\n+  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n+  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                              Register src_pos, \/\/ source position (c_rarg1)\n+                              Register dst,     \/\/ destination array oo (c_rarg2)\n+                              Register dst_pos, \/\/ destination position (c_rarg3)\n+                              Register length,\n+                              Register temp,\n+                              Label& L_failed) {\n+    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n+\n+    assert_different_registers(t0, temp);\n+\n+    \/\/ if [src_pos + length > arrayOop(src)->length()] then FAIL\n+    __ lwu(t0, Address(src, arrayOopDesc::length_offset_in_bytes()));\n+    __ addw(temp, length, src_pos);\n+    __ bgtu(temp, t0, L_failed);\n+\n+    \/\/ if [dst_pos + length > arrayOop(dst)->length()] then FAIL\n+    __ lwu(t0, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n+    __ addw(temp, length, dst_pos);\n+    __ bgtu(temp, t0, L_failed);\n+\n+    \/\/ Have to clean up high 32 bits of 'src_pos' and 'dst_pos'.\n+    __ zero_extend(src_pos, src_pos, 32);\n+    __ zero_extend(dst_pos, dst_pos, 32);\n+\n+    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n+  }\n+\n+  \/\/\n+  \/\/  Generate 'unsafe' array copy stub\n+  \/\/  Though just as safe as the other stubs, it takes an unscaled\n+  \/\/  size_t argument instead of an element count.\n+  \/\/\n+  \/\/  Input:\n+  \/\/    c_rarg0   - source array address\n+  \/\/    c_rarg1   - destination array address\n+  \/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to a long, int, short, or byte copy loop.\n+  \/\/\n+  address generate_unsafe_copy(const char* name,\n+                               address byte_copy_entry,\n+                               address short_copy_entry,\n+                               address int_copy_entry,\n+                               address long_copy_entry) {\n+    assert_cond(byte_copy_entry != NULL && short_copy_entry != NULL &&\n+                int_copy_entry != NULL && long_copy_entry != NULL);\n+    Label L_long_aligned, L_int_aligned, L_short_aligned;\n+    const Register s = c_rarg0, d = c_rarg1, count = c_rarg2;\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+    \/\/ bump this on entry, not on exit:\n+    inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);\n+\n+    __ orr(t0, s, d);\n+    __ orr(t0, t0, count);\n+\n+    __ andi(t0, t0, BytesPerLong - 1);\n+    __ beqz(t0, L_long_aligned);\n+    __ andi(t0, t0, BytesPerInt - 1);\n+    __ beqz(t0, L_int_aligned);\n+    __ andi(t0, t0, 1);\n+    __ beqz(t0, L_short_aligned);\n+    __ j(RuntimeAddress(byte_copy_entry));\n+\n+    __ BIND(L_short_aligned);\n+    __ srli(count, count, LogBytesPerShort);  \/\/ size => short_count\n+    __ j(RuntimeAddress(short_copy_entry));\n+    __ BIND(L_int_aligned);\n+    __ srli(count, count, LogBytesPerInt);    \/\/ size => int_count\n+    __ j(RuntimeAddress(int_copy_entry));\n+    __ BIND(L_long_aligned);\n+    __ srli(count, count, LogBytesPerLong);   \/\/ size => long_count\n+    __ j(RuntimeAddress(long_copy_entry));\n+\n+    return start;\n+  }\n+\n+  \/\/\n+  \/\/  Generate generic array copy stubs\n+  \/\/\n+  \/\/  Input:\n+  \/\/    c_rarg0    -  src oop\n+  \/\/    c_rarg1    -  src_pos (32-bits)\n+  \/\/    c_rarg2    -  dst oop\n+  \/\/    c_rarg3    -  dst_pos (32-bits)\n+  \/\/    c_rarg4    -  element count (32-bits)\n+  \/\/\n+  \/\/  Output:\n+  \/\/    x10 ==  0  -  success\n+  \/\/    x10 == -1^K - failure, where K is partial transfer count\n+  \/\/\n+  address generate_generic_copy(const char* name,\n+                                address byte_copy_entry, address short_copy_entry,\n+                                address int_copy_entry, address oop_copy_entry,\n+                                address long_copy_entry, address checkcast_copy_entry) {\n+    assert_cond(byte_copy_entry != NULL && short_copy_entry != NULL &&\n+                int_copy_entry != NULL && oop_copy_entry != NULL &&\n+                long_copy_entry != NULL && checkcast_copy_entry != NULL);\n+    Label L_failed, L_failed_0, L_objArray;\n+    Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;\n+\n+    \/\/ Input registers\n+    const Register src        = c_rarg0;  \/\/ source array oop\n+    const Register src_pos    = c_rarg1;  \/\/ source position\n+    const Register dst        = c_rarg2;  \/\/ destination array oop\n+    const Register dst_pos    = c_rarg3;  \/\/ destination position\n+    const Register length     = c_rarg4;\n+\n+    \/\/ Registers used as temps\n+    const Register dst_klass = c_rarg5;\n+\n+    __ align(CodeEntryAlignment);\n+\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+    \/\/ bump this on entry, not on exit:\n+    inc_counter_np(SharedRuntime::_generic_array_copy_ctr);\n+\n+    \/\/-----------------------------------------------------------------------\n+    \/\/ Assembler stub will be used for this call to arraycopy\n+    \/\/ if the following conditions are met:\n+    \/\/\n+    \/\/ (1) src and dst must not be null.\n+    \/\/ (2) src_pos must not be negative.\n+    \/\/ (3) dst_pos must not be negative.\n+    \/\/ (4) length  must not be negative.\n+    \/\/ (5) src klass and dst klass should be the same and not NULL.\n+    \/\/ (6) src and dst should be arrays.\n+    \/\/ (7) src_pos + length must not exceed length of src.\n+    \/\/ (8) dst_pos + length must not exceed length of dst.\n+    \/\/\n+\n+    \/\/ if [src == NULL] then return -1\n+    __ beqz(src, L_failed);\n+\n+    \/\/ if [src_pos < 0] then return -1\n+    \/\/ i.e. sign bit set\n+    __ andi(t0, src_pos, 1UL << 31);\n+    __ bnez(t0, L_failed);\n+\n+    \/\/ if [dst == NULL] then return -1\n+    __ beqz(dst, L_failed);\n+\n+    \/\/ if [dst_pos < 0] then return -1\n+    \/\/ i.e. sign bit set\n+    __ andi(t0, dst_pos, 1UL << 31);\n+    __ bnez(t0, L_failed);\n+\n+    \/\/ registers used as temp\n+    const Register scratch_length    = x28; \/\/ elements count to copy\n+    const Register scratch_src_klass = x29; \/\/ array klass\n+    const Register lh                = x30; \/\/ layout helper\n+\n+    \/\/ if [length < 0] then return -1\n+    __ addw(scratch_length, length, zr);    \/\/ length (elements count, 32-bits value)\n+    \/\/ i.e. sign bit set\n+    __ andi(t0, scratch_length, 1UL << 31);\n+    __ bnez(t0, L_failed);\n+\n+    __ load_klass(scratch_src_klass, src);\n+#ifdef ASSERT\n+    {\n+      BLOCK_COMMENT(\"assert klasses not null {\");\n+      Label L1, L2;\n+      __ bnez(scratch_src_klass, L2);   \/\/ it is broken if klass is NULL\n+      __ bind(L1);\n+      __ stop(\"broken null klass\");\n+      __ bind(L2);\n+      __ load_klass(t0, dst);\n+      __ beqz(t0, L1);     \/\/ this would be broken also\n+      BLOCK_COMMENT(\"} assert klasses not null done\");\n+    }\n+#endif\n+\n+    \/\/ Load layout helper (32-bits)\n+    \/\/\n+    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n+    \/\/ 32        30    24            16              8     2                 0\n+    \/\/\n+    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n+    \/\/\n+\n+    const int lh_offset = in_bytes(Klass::layout_helper_offset());\n+\n+    \/\/ Handle objArrays completely differently...\n+    const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n+    __ lw(lh, Address(scratch_src_klass, lh_offset));\n+    __ mvw(t0, objArray_lh);\n+    __ beq(lh, t0, L_objArray);\n+\n+    \/\/ if [src->klass() != dst->klass()] then return -1\n+    __ load_klass(t1, dst);\n+    __ bne(t1, scratch_src_klass, L_failed);\n+\n+    \/\/ if [src->is_Array() != NULL] then return -1\n+    \/\/ i.e. (lh >= 0)\n+    __ andi(t0, lh, 1UL << 31);\n+    __ beqz(t0, L_failed);\n+\n+    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n+#ifdef ASSERT\n+    {\n+      BLOCK_COMMENT(\"assert primitive array {\");\n+      Label L;\n+      __ mvw(t1, Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift);\n+      __ bge(lh, t1, L);\n+      __ stop(\"must be a primitive array\");\n+      __ bind(L);\n+      BLOCK_COMMENT(\"} assert primitive array done\");\n+    }\n+#endif\n+\n+    arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,\n+                           t1, L_failed);\n+\n+    \/\/ TypeArrayKlass\n+    \/\/\n+    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize)\n+    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize)\n+    \/\/\n+\n+    const Register t0_offset = t0;    \/\/ array offset\n+    const Register x22_elsize = lh;   \/\/ element size\n+\n+    \/\/ Get array_header_in_bytes()\n+    int lh_header_size_width = exact_log2(Klass::_lh_header_size_mask + 1);\n+    int lh_header_size_msb = Klass::_lh_header_size_shift + lh_header_size_width;\n+    __ slli(t0_offset, lh, XLEN - lh_header_size_msb);          \/\/ left shift to remove 24 ~ 32;\n+    __ srli(t0_offset, t0_offset, XLEN - lh_header_size_width); \/\/ array_offset\n+\n+    __ add(src, src, t0_offset);           \/\/ src array offset\n+    __ add(dst, dst, t0_offset);           \/\/ dst array offset\n+    BLOCK_COMMENT(\"choose copy loop based on element size\");\n+\n+    \/\/ next registers should be set before the jump to corresponding stub\n+    const Register from     = c_rarg0;  \/\/ source array address\n+    const Register to       = c_rarg1;  \/\/ destination array address\n+    const Register count    = c_rarg2;  \/\/ elements count\n+\n+    \/\/ 'from', 'to', 'count' registers should be set in such order\n+    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n+\n+    assert(Klass::_lh_log2_element_size_shift == 0, \"fix this code\");\n+\n+    \/\/ The possible values of elsize are 0-3, i.e. exact_log2(element\n+    \/\/ size in bytes).  We do a simple bitwise binary search.\n+  __ BIND(L_copy_bytes);\n+    __ andi(t0, x22_elsize, 2);\n+    __ bnez(t0, L_copy_ints);\n+    __ andi(t0, x22_elsize, 1);\n+    __ bnez(t0, L_copy_shorts);\n+    __ add(from, src, src_pos); \/\/ src_addr\n+    __ add(to, dst, dst_pos); \/\/ dst_addr\n+    __ addw(count, scratch_length, zr); \/\/ length\n+    __ j(RuntimeAddress(byte_copy_entry));\n+\n+  __ BIND(L_copy_shorts);\n+    __ shadd(from, src_pos, src, t0, 1); \/\/ src_addr\n+    __ shadd(to, dst_pos, dst, t0, 1); \/\/ dst_addr\n+    __ addw(count, scratch_length, zr); \/\/ length\n+    __ j(RuntimeAddress(short_copy_entry));\n+\n+  __ BIND(L_copy_ints);\n+    __ andi(t0, x22_elsize, 1);\n+    __ bnez(t0, L_copy_longs);\n+    __ shadd(from, src_pos, src, t0, 2); \/\/ src_addr\n+    __ shadd(to, dst_pos, dst, t0, 2); \/\/ dst_addr\n+    __ addw(count, scratch_length, zr); \/\/ length\n+    __ j(RuntimeAddress(int_copy_entry));\n+\n+  __ BIND(L_copy_longs);\n+#ifdef ASSERT\n+    {\n+      BLOCK_COMMENT(\"assert long copy {\");\n+      Label L;\n+      __ andi(lh, lh, Klass::_lh_log2_element_size_mask); \/\/ lh -> x22_elsize\n+      __ addw(lh, lh, zr);\n+      __ mvw(t0, LogBytesPerLong);\n+      __ beq(x22_elsize, t0, L);\n+      __ stop(\"must be long copy, but elsize is wrong\");\n+      __ bind(L);\n+      BLOCK_COMMENT(\"} assert long copy done\");\n+    }\n+#endif\n+    __ shadd(from, src_pos, src, t0, 3); \/\/ src_addr\n+    __ shadd(to, dst_pos, dst, t0, 3); \/\/ dst_addr\n+    __ addw(count, scratch_length, zr); \/\/ length\n+    __ j(RuntimeAddress(long_copy_entry));\n+\n+    \/\/ ObjArrayKlass\n+  __ BIND(L_objArray);\n+    \/\/ live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]\n+\n+    Label L_plain_copy, L_checkcast_copy;\n+    \/\/ test array classes for subtyping\n+    __ load_klass(t2, dst);\n+    __ bne(scratch_src_klass, t2, L_checkcast_copy); \/\/ usual case is exact equality\n+\n+    \/\/ Identically typed arrays can be copied without element-wise checks.\n+    arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,\n+                           t1, L_failed);\n+\n+    __ shadd(from, src_pos, src, t0, LogBytesPerHeapOop);\n+    __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+    __ shadd(to, dst_pos, dst, t0, LogBytesPerHeapOop);\n+    __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+    __ addw(count, scratch_length, zr); \/\/ length\n+  __ BIND(L_plain_copy);\n+    __ j(RuntimeAddress(oop_copy_entry));\n+\n+  __ BIND(L_checkcast_copy);\n+    \/\/ live at this point:  scratch_src_klass, scratch_length, t2 (dst_klass)\n+    {\n+      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n+      __ lwu(t0, Address(t2, lh_offset));\n+      __ mvw(t1, objArray_lh);\n+      __ bne(t0, t1, L_failed);\n+\n+      \/\/ It is safe to examine both src.length and dst.length.\n+      arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,\n+                             t2, L_failed);\n+\n+      __ load_klass(dst_klass, dst); \/\/ reload\n+\n+      \/\/ Marshal the base address arguments now, freeing registers.\n+      __ shadd(from, src_pos, src, t0, LogBytesPerHeapOop);\n+      __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+      __ shadd(to, dst_pos, dst, t0, LogBytesPerHeapOop);\n+      __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+      __ addw(count, length, zr);           \/\/ length (reloaded)\n+      const Register sco_temp = c_rarg3;      \/\/ this register is free now\n+      assert_different_registers(from, to, count, sco_temp,\n+                                 dst_klass, scratch_src_klass);\n+\n+      \/\/ Generate the type check.\n+      const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+      __ lwu(sco_temp, Address(dst_klass, sco_offset));\n+\n+      \/\/ Smashes t0, t1\n+      generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);\n+\n+      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n+      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+      __ ld(dst_klass, Address(dst_klass, ek_offset));\n+      __ lwu(sco_temp, Address(dst_klass, sco_offset));\n+\n+      \/\/ the checkcast_copy loop needs two extra arguments:\n+      assert(c_rarg3 == sco_temp, \"#3 already in place\");\n+      \/\/ Set up arguments for checkcast_copy_entry.\n+      __ mv(c_rarg4, dst_klass);  \/\/ dst.klass.element_klass\n+      __ j(RuntimeAddress(checkcast_copy_entry));\n+    }\n+\n+  __ BIND(L_failed);\n+    __ li(x10, -1);\n+    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return start;\n+  }\n+\n+  \/\/\n+  \/\/ Generate stub for array fill. If \"aligned\" is true, the\n+  \/\/ \"to\" address is assumed to be heapword aligned.\n+  \/\/\n+  \/\/ Arguments for generated stub:\n+  \/\/   to:    c_rarg0\n+  \/\/   value: c_rarg1\n+  \/\/   count: c_rarg2 treated as signed\n+  \/\/\n+  address generate_fill(BasicType t, bool aligned, const char* name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    BLOCK_COMMENT(\"Entry:\");\n+\n+    const Register to        = c_rarg0;  \/\/ source array address\n+    const Register value     = c_rarg1;  \/\/ value\n+    const Register count     = c_rarg2;  \/\/ elements count\n+\n+    const Register bz_base   = x28;      \/\/ base for block_zero routine\n+    const Register cnt_words = x29;      \/\/ temp register\n+    const Register tmp_reg   = t1;\n+\n+    __ enter();\n+\n+    Label L_fill_elements, L_exit1;\n+\n+    int shift = -1;\n+    switch (t) {\n+      case T_BYTE:\n+        shift = 0;\n+\n+        \/\/ Zero extend value\n+        \/\/ 8 bit -> 16 bit\n+        __ andi(value, value, 0xff);\n+        __ mv(tmp_reg, value);\n+        __ slli(tmp_reg, tmp_reg, 8);\n+        __ orr(value, value, tmp_reg);\n+\n+        \/\/ 16 bit -> 32 bit\n+        __ mv(tmp_reg, value);\n+        __ slli(tmp_reg, tmp_reg, 16);\n+        __ orr(value, value, tmp_reg);\n+\n+        __ mv(tmp_reg, 8 >> shift); \/\/ Short arrays (< 8 bytes) fill by element\n+        __ bltu(count, tmp_reg, L_fill_elements);\n+        break;\n+      case T_SHORT:\n+        shift = 1;\n+        \/\/ Zero extend value\n+        \/\/ 16 bit -> 32 bit\n+        __ andi(value, value, 0xffff);\n+        __ mv(tmp_reg, value);\n+        __ slli(tmp_reg, tmp_reg, 16);\n+        __ orr(value, value, tmp_reg);\n+\n+        \/\/ Short arrays (< 8 bytes) fill by element\n+        __ mv(tmp_reg, 8 >> shift);\n+        __ bltu(count, tmp_reg, L_fill_elements);\n+        break;\n+      case T_INT:\n+        shift = 2;\n+\n+        \/\/ Short arrays (< 8 bytes) fill by element\n+        __ mv(tmp_reg, 8 >> shift);\n+        __ bltu(count, tmp_reg, L_fill_elements);\n+        break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    \/\/ Align source address at 8 bytes address boundary.\n+    Label L_skip_align1, L_skip_align2, L_skip_align4;\n+    if (!aligned) {\n+      switch (t) {\n+        case T_BYTE:\n+          \/\/ One byte misalignment happens only for byte arrays.\n+          __ andi(t0, to, 1);\n+          __ beqz(t0, L_skip_align1);\n+          __ sb(value, Address(to, 0));\n+          __ addi(to, to, 1);\n+          __ addiw(count, count, -1);\n+          __ bind(L_skip_align1);\n+          \/\/ Fallthrough\n+        case T_SHORT:\n+          \/\/ Two bytes misalignment happens only for byte and short (char) arrays.\n+          __ andi(t0, to, 2);\n+          __ beqz(t0, L_skip_align2);\n+          __ sh(value, Address(to, 0));\n+          __ addi(to, to, 2);\n+          __ addiw(count, count, -(2 >> shift));\n+          __ bind(L_skip_align2);\n+          \/\/ Fallthrough\n+        case T_INT:\n+          \/\/ Align to 8 bytes, we know we are 4 byte aligned to start.\n+          __ andi(t0, to, 4);\n+          __ beqz(t0, L_skip_align4);\n+          __ sw(value, Address(to, 0));\n+          __ addi(to, to, 4);\n+          __ addiw(count, count, -(4 >> shift));\n+          __ bind(L_skip_align4);\n+          break;\n+        default: ShouldNotReachHere();\n+      }\n+    }\n+\n+    \/\/\n+    \/\/  Fill large chunks\n+    \/\/\n+    __ srliw(cnt_words, count, 3 - shift); \/\/ number of words\n+\n+    \/\/ 32 bit -> 64 bit\n+    __ andi(value, value, 0xffffffff);\n+    __ mv(tmp_reg, value);\n+    __ slli(tmp_reg, tmp_reg, 32);\n+    __ orr(value, value, tmp_reg);\n+\n+    __ slli(tmp_reg, cnt_words, 3 - shift);\n+    __ subw(count, count, tmp_reg);\n+    {\n+      __ fill_words(to, cnt_words, value);\n+    }\n+\n+    \/\/ Remaining count is less than 8 bytes. Fill it by a single store.\n+    \/\/ Note that the total length is no less than 8 bytes.\n+    if (t == T_BYTE || t == T_SHORT) {\n+      __ beqz(count, L_exit1);\n+      __ shadd(to, count, to, tmp_reg, shift); \/\/ points to the end\n+      __ sd(value, Address(to, -8)); \/\/ overwrite some elements\n+      __ bind(L_exit1);\n+      __ leave();\n+      __ ret();\n+    }\n+\n+    \/\/ Handle copies less than 8 bytes.\n+    Label L_fill_2, L_fill_4, L_exit2;\n+    __ bind(L_fill_elements);\n+    switch (t) {\n+      case T_BYTE:\n+        __ andi(t0, count, 1);\n+        __ beqz(t0, L_fill_2);\n+        __ sb(value, Address(to, 0));\n+        __ addi(to, to, 1);\n+        __ bind(L_fill_2);\n+        __ andi(t0, count, 2);\n+        __ beqz(t0, L_fill_4);\n+        __ sh(value, Address(to, 0));\n+        __ addi(to, to, 2);\n+        __ bind(L_fill_4);\n+        __ andi(t0, count, 4);\n+        __ beqz(t0, L_exit2);\n+        __ sw(value, Address(to, 0));\n+        break;\n+      case T_SHORT:\n+        __ andi(t0, count, 1);\n+        __ beqz(t0, L_fill_4);\n+        __ sh(value, Address(to, 0));\n+        __ addi(to, to, 2);\n+        __ bind(L_fill_4);\n+        __ andi(t0, count, 2);\n+        __ beqz(t0, L_exit2);\n+        __ sw(value, Address(to, 0));\n+        break;\n+      case T_INT:\n+        __ beqz(count, L_exit2);\n+        __ sw(value, Address(to, 0));\n+        break;\n+      default: ShouldNotReachHere();\n+    }\n+    __ bind(L_exit2);\n+    __ leave();\n+    __ ret();\n+    return start;\n+  }\n+\n+  void generate_arraycopy_stubs() {\n+    address entry                     = NULL;\n+    address entry_jbyte_arraycopy     = NULL;\n+    address entry_jshort_arraycopy    = NULL;\n+    address entry_jint_arraycopy      = NULL;\n+    address entry_oop_arraycopy       = NULL;\n+    address entry_jlong_arraycopy     = NULL;\n+    address entry_checkcast_arraycopy = NULL;\n+\n+    generate_copy_longs(copy_f, c_rarg0, c_rarg1, t1, copy_forwards);\n+    generate_copy_longs(copy_b, c_rarg0, c_rarg1, t1, copy_backwards);\n+\n+    StubRoutines::riscv::_zero_blocks = generate_zero_blocks();\n+\n+    \/\/*** jbyte\n+    \/\/ Always need aligned and unaligned versions\n+    StubRoutines::_jbyte_disjoint_arraycopy          = generate_disjoint_byte_copy(false, &entry,\n+                                                                                   \"jbyte_disjoint_arraycopy\");\n+    StubRoutines::_jbyte_arraycopy                   = generate_conjoint_byte_copy(false, entry,\n+                                                                                   &entry_jbyte_arraycopy,\n+                                                                                   \"jbyte_arraycopy\");\n+    StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(true, &entry,\n+                                                                                   \"arrayof_jbyte_disjoint_arraycopy\");\n+    StubRoutines::_arrayof_jbyte_arraycopy           = generate_conjoint_byte_copy(true, entry, NULL,\n+                                                                                   \"arrayof_jbyte_arraycopy\");\n+\n+    \/\/*** jshort\n+    \/\/ Always need aligned and unaligned versions\n+    StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &entry,\n+                                                                                    \"jshort_disjoint_arraycopy\");\n+    StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,\n+                                                                                    &entry_jshort_arraycopy,\n+                                                                                    \"jshort_arraycopy\");\n+    StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &entry,\n+                                                                                    \"arrayof_jshort_disjoint_arraycopy\");\n+    StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,\n+                                                                                    \"arrayof_jshort_arraycopy\");\n+\n+    \/\/*** jint\n+    \/\/ Aligned versions\n+    StubRoutines::_arrayof_jint_disjoint_arraycopy   = generate_disjoint_int_copy(true, &entry,\n+                                                                                  \"arrayof_jint_disjoint_arraycopy\");\n+    StubRoutines::_arrayof_jint_arraycopy            = generate_conjoint_int_copy(true, entry, &entry_jint_arraycopy,\n+                                                                                  \"arrayof_jint_arraycopy\");\n+    \/\/ In 64 bit we need both aligned and unaligned versions of jint arraycopy.\n+    \/\/ entry_jint_arraycopy always points to the unaligned version\n+    StubRoutines::_jint_disjoint_arraycopy           = generate_disjoint_int_copy(false, &entry,\n+                                                                                  \"jint_disjoint_arraycopy\");\n+    StubRoutines::_jint_arraycopy                    = generate_conjoint_int_copy(false, entry,\n+                                                                                  &entry_jint_arraycopy,\n+                                                                                  \"jint_arraycopy\");\n+\n+    \/\/*** jlong\n+    \/\/ It is always aligned\n+    StubRoutines::_arrayof_jlong_disjoint_arraycopy  = generate_disjoint_long_copy(true, &entry,\n+                                                                                   \"arrayof_jlong_disjoint_arraycopy\");\n+    StubRoutines::_arrayof_jlong_arraycopy           = generate_conjoint_long_copy(true, entry, &entry_jlong_arraycopy,\n+                                                                                   \"arrayof_jlong_arraycopy\");\n+    StubRoutines::_jlong_disjoint_arraycopy          = StubRoutines::_arrayof_jlong_disjoint_arraycopy;\n+    StubRoutines::_jlong_arraycopy                   = StubRoutines::_arrayof_jlong_arraycopy;\n+\n+    \/\/*** oops\n+    {\n+      \/\/ With compressed oops we need unaligned versions; notice that\n+      \/\/ we overwrite entry_oop_arraycopy.\n+      bool aligned = !UseCompressedOops;\n+\n+      StubRoutines::_arrayof_oop_disjoint_arraycopy\n+        = generate_disjoint_oop_copy(aligned, &entry, \"arrayof_oop_disjoint_arraycopy\",\n+                                     \/*dest_uninitialized*\/false);\n+      StubRoutines::_arrayof_oop_arraycopy\n+        = generate_conjoint_oop_copy(aligned, entry, &entry_oop_arraycopy, \"arrayof_oop_arraycopy\",\n+                                     \/*dest_uninitialized*\/false);\n+      \/\/ Aligned versions without pre-barriers\n+      StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit\n+        = generate_disjoint_oop_copy(aligned, &entry, \"arrayof_oop_disjoint_arraycopy_uninit\",\n+                                     \/*dest_uninitialized*\/true);\n+      StubRoutines::_arrayof_oop_arraycopy_uninit\n+        = generate_conjoint_oop_copy(aligned, entry, NULL, \"arrayof_oop_arraycopy_uninit\",\n+                                     \/*dest_uninitialized*\/true);\n+    }\n+\n+    StubRoutines::_oop_disjoint_arraycopy            = StubRoutines::_arrayof_oop_disjoint_arraycopy;\n+    StubRoutines::_oop_arraycopy                     = StubRoutines::_arrayof_oop_arraycopy;\n+    StubRoutines::_oop_disjoint_arraycopy_uninit     = StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit;\n+    StubRoutines::_oop_arraycopy_uninit              = StubRoutines::_arrayof_oop_arraycopy_uninit;\n+\n+    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n+    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n+                                                                        \/*dest_uninitialized*\/true);\n+\n+\n+    StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n+                                                              entry_jbyte_arraycopy,\n+                                                              entry_jshort_arraycopy,\n+                                                              entry_jint_arraycopy,\n+                                                              entry_jlong_arraycopy);\n+\n+    StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n+                                                               entry_jbyte_arraycopy,\n+                                                               entry_jshort_arraycopy,\n+                                                               entry_jint_arraycopy,\n+                                                               entry_oop_arraycopy,\n+                                                               entry_jlong_arraycopy,\n+                                                               entry_checkcast_arraycopy);\n+\n+    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n+    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n+    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n+    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n+    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n+    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n+  }\n+\n+  \/\/ Safefetch stubs.\n+  void generate_safefetch(const char* name, int size, address* entry,\n+                          address* fault_pc, address* continuation_pc) {\n+    \/\/ safefetch signatures:\n+    \/\/   int      SafeFetch32(int*      adr, int      errValue)\n+    \/\/   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue)\n+    \/\/\n+    \/\/ arguments:\n+    \/\/   c_rarg0 = adr\n+    \/\/   c_rarg1 = errValue\n+    \/\/\n+    \/\/ result:\n+    \/\/   PPC_RET  = *adr or errValue\n+    assert_cond(entry != NULL && fault_pc != NULL && continuation_pc != NULL);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+\n+    \/\/ Entry point, pc or function descriptor.\n+    *entry = __ pc();\n+\n+    \/\/ Load *adr into c_rarg1, may fault.\n+    *fault_pc = __ pc();\n+    switch (size) {\n+      case 4:\n+        \/\/ int32_t\n+        __ lw(c_rarg1, Address(c_rarg0, 0));\n+        break;\n+      case 8:\n+        \/\/ int64_t\n+        __ ld(c_rarg1, Address(c_rarg0, 0));\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+\n+    \/\/ return errValue or *adr\n+    *continuation_pc = __ pc();\n+    __ mv(x10, c_rarg1);\n+    __ ret();\n+  }\n+\n+  \/\/ code for comparing 16 bytes of strings with same encoding\n+  void compare_string_16_bytes_same(Label &DIFF1, Label &DIFF2) {\n+    const Register result = x10, str1 = x11, cnt1 = x12, str2 = x13, tmp1 = x28, tmp2 = x29, tmp4 = x7, tmp5 = x31;\n+    __ ld(tmp5, Address(str1));\n+    __ addi(str1, str1, 8);\n+    __ xorr(tmp4, tmp1, tmp2);\n+    __ ld(cnt1, Address(str2));\n+    __ addi(str2, str2, 8);\n+    __ bnez(tmp4, DIFF1);\n+    __ ld(tmp1, Address(str1));\n+    __ addi(str1, str1, 8);\n+    __ xorr(tmp4, tmp5, cnt1);\n+    __ ld(tmp2, Address(str2));\n+    __ addi(str2, str2, 8);\n+    __ bnez(tmp4, DIFF2);\n+  }\n+\n+  \/\/ code for comparing 8 characters of strings with Latin1 and Utf16 encoding\n+  void compare_string_8_x_LU(Register tmpL, Register tmpU, Label &DIFF1,\n+                              Label &DIFF2) {\n+    const Register strU = x12, curU = x7, strL = x29, tmp = x30;\n+    __ ld(tmpL, Address(strL));\n+    __ addi(strL, strL, 8);\n+    __ ld(tmpU, Address(strU));\n+    __ addi(strU, strU, 8);\n+    __ inflate_lo32(tmp, tmpL);\n+    __ mv(t0, tmp);\n+    __ xorr(tmp, curU, t0);\n+    __ bnez(tmp, DIFF2);\n+\n+    __ ld(curU, Address(strU));\n+    __ addi(strU, strU, 8);\n+    __ inflate_hi32(tmp, tmpL);\n+    __ mv(t0, tmp);\n+    __ xorr(tmp, tmpU, t0);\n+    __ bnez(tmp, DIFF1);\n+  }\n+\n+  \/\/ x10  = result\n+  \/\/ x11  = str1\n+  \/\/ x12  = cnt1\n+  \/\/ x13  = str2\n+  \/\/ x14  = cnt2\n+  \/\/ x28  = tmp1\n+  \/\/ x29  = tmp2\n+  \/\/ x30  = tmp3\n+  address generate_compare_long_string_different_encoding(bool isLU) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", isLU ? \"compare_long_string_different_encoding LU\" : \"compare_long_string_different_encoding UL\");\n+    address entry = __ pc();\n+    Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,\n+          DONE, CALCULATE_DIFFERENCE;\n+    const Register result = x10, str1 = x11, cnt1 = x12, str2 = x13, cnt2 = x14,\n+                   tmp1 = x28, tmp2 = x29, tmp3 = x30, tmp4 = x7, tmp5 = x31;\n+    RegSet spilled_regs = RegSet::of(tmp4, tmp5);\n+\n+    \/\/ cnt2 == amount of characters left to compare\n+    \/\/ Check already loaded first 4 symbols\n+    __ inflate_lo32(tmp3, isLU ? tmp1 : tmp2);\n+    __ mv(isLU ? tmp1 : tmp2, tmp3);\n+    __ addi(str1, str1, isLU ? wordSize \/ 2 : wordSize);\n+    __ addi(str2, str2, isLU ? wordSize : wordSize \/ 2);\n+    __ sub(cnt2, cnt2, 8); \/\/ Already loaded 4 symbols. Last 4 is special case.\n+    __ push_reg(spilled_regs, sp);\n+\n+    if (isLU) {\n+      __ add(str1, str1, cnt2);\n+      __ shadd(str2, cnt2, str2, t0, 1);\n+    } else {\n+      __ shadd(str1, cnt2, str1, t0, 1);\n+      __ add(str2, str2, cnt2);\n+    }\n+    __ xorr(tmp3, tmp1, tmp2);\n+    __ mv(tmp5, tmp2);\n+    __ bnez(tmp3, CALCULATE_DIFFERENCE);\n+\n+    Register strU = isLU ? str2 : str1,\n+             strL = isLU ? str1 : str2,\n+             tmpU = isLU ? tmp5 : tmp1, \/\/ where to keep U for comparison\n+             tmpL = isLU ? tmp1 : tmp5; \/\/ where to keep L for comparison\n+\n+    __ sub(tmp2, strL, cnt2); \/\/ strL pointer to load from\n+    __ slli(t0, cnt2, 1);\n+    __ sub(cnt1, strU, t0); \/\/ strU pointer to load from\n+\n+    __ ld(tmp4, Address(cnt1));\n+    __ addi(cnt1, cnt1, 8);\n+    __ beqz(cnt2, LOAD_LAST); \/\/ no characters left except last load\n+    __ sub(cnt2, cnt2, 16);\n+    __ bltz(cnt2, TAIL);\n+    __ bind(SMALL_LOOP); \/\/ smaller loop\n+      __ sub(cnt2, cnt2, 16);\n+      compare_string_8_x_LU(tmpL, tmpU, DIFF1, DIFF2);\n+      compare_string_8_x_LU(tmpL, tmpU, DIFF1, DIFF2);\n+      __ bgez(cnt2, SMALL_LOOP);\n+      __ addi(t0, cnt2, 16);\n+      __ beqz(t0, LOAD_LAST);\n+    __ bind(TAIL); \/\/ 1..15 characters left until last load (last 4 characters)\n+      \/\/ Address of 8 bytes before last 4 characters in UTF-16 string\n+      __ shadd(cnt1, cnt2, cnt1, t0, 1);\n+      \/\/ Address of 16 bytes before last 4 characters in Latin1 string\n+      __ add(tmp2, tmp2, cnt2);\n+      __ ld(tmp4, Address(cnt1, -8));\n+      \/\/ last 16 characters before last load\n+      compare_string_8_x_LU(tmpL, tmpU, DIFF1, DIFF2);\n+      compare_string_8_x_LU(tmpL, tmpU, DIFF1, DIFF2);\n+      __ j(LOAD_LAST);\n+    __ bind(DIFF2);\n+      __ mv(tmpU, tmp4);\n+    __ bind(DIFF1);\n+      __ mv(tmpL, t0);\n+      __ j(CALCULATE_DIFFERENCE);\n+    __ bind(LOAD_LAST);\n+      \/\/ Last 4 UTF-16 characters are already pre-loaded into tmp4 by compare_string_8_x_LU.\n+      \/\/ No need to load it again\n+      __ mv(tmpU, tmp4);\n+      __ ld(tmpL, Address(strL));\n+      __ inflate_lo32(tmp3, tmpL);\n+      __ mv(tmpL, tmp3);\n+      __ xorr(tmp3, tmpU, tmpL);\n+      __ beqz(tmp3, DONE);\n+\n+      \/\/ Find the first different characters in the longwords and\n+      \/\/ compute their difference.\n+    __ bind(CALCULATE_DIFFERENCE);\n+      __ ctzc_bit(tmp4, tmp3);\n+      __ srl(tmp1, tmp1, tmp4);\n+      __ srl(tmp5, tmp5, tmp4);\n+      __ andi(tmp1, tmp1, 0xFFFF);\n+      __ andi(tmp5, tmp5, 0xFFFF);\n+      __ sub(result, tmp1, tmp5);\n+    __ bind(DONE);\n+      __ pop_reg(spilled_regs, sp);\n+      __ ret();\n+    return entry;\n+  }\n+\n+  \/\/ x10  = result\n+  \/\/ x11  = str1\n+  \/\/ x12  = cnt1\n+  \/\/ x13  = str2\n+  \/\/ x14  = cnt2\n+  \/\/ x28  = tmp1\n+  \/\/ x29  = tmp2\n+  \/\/ x30  = tmp3\n+  \/\/ x31  = tmp4\n+  address generate_compare_long_string_same_encoding(bool isLL) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", isLL ?\n+                      \"compare_long_string_same_encoding LL\" : \"compare_long_string_same_encoding UU\");\n+    address entry = __ pc();\n+    Label SMALL_LOOP, CHECK_LAST, DIFF2, TAIL,\n+          LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF;\n+    const Register result = x10, str1 = x11, cnt1 = x12, str2 = x13, cnt2 = x14,\n+                   tmp1 = x28, tmp2 = x29, tmp3 = x30, tmp4 = x7, tmp5 = x31;\n+    RegSet spilled_regs = RegSet::of(tmp4, tmp5);\n+\n+    \/\/ cnt1\/cnt2 contains amount of characters to compare. cnt1 can be re-used\n+    \/\/ update cnt2 counter with already loaded 8 bytes\n+    __ sub(cnt2, cnt2, wordSize \/ (isLL ? 1 : 2));\n+    \/\/ update pointers, because of previous read\n+    __ add(str1, str1, wordSize);\n+    __ add(str2, str2, wordSize);\n+    \/\/ less than 16 bytes left?\n+    __ sub(cnt2, cnt2, isLL ? 16 : 8);\n+    __ push_reg(spilled_regs, sp);\n+    __ bltz(cnt2, TAIL);\n+    __ bind(SMALL_LOOP);\n+      compare_string_16_bytes_same(DIFF, DIFF2);\n+      __ sub(cnt2, cnt2, isLL ? 16 : 8);\n+      __ bgez(cnt2, SMALL_LOOP);\n+    __ bind(TAIL);\n+      __ addi(cnt2, cnt2, isLL ? 16 : 8);\n+      __ beqz(cnt2, LAST_CHECK_AND_LENGTH_DIFF);\n+      __ sub(cnt2, cnt2, isLL ? 8 : 4);\n+      __ blez(cnt2, CHECK_LAST);\n+      __ xorr(tmp4, tmp1, tmp2);\n+      __ bnez(tmp4, DIFF);\n+      __ ld(tmp1, Address(str1));\n+      __ addi(str1, str1, 8);\n+      __ ld(tmp2, Address(str2));\n+      __ addi(str2, str2, 8);\n+      __ sub(cnt2, cnt2, isLL ? 8 : 4);\n+    __ bind(CHECK_LAST);\n+      if (!isLL) {\n+        __ add(cnt2, cnt2, cnt2); \/\/ now in bytes\n+      }\n+      __ xorr(tmp4, tmp1, tmp2);\n+      __ bnez(tmp4, DIFF);\n+      __ add(str1, str1, cnt2);\n+      __ ld(tmp5, Address(str1));\n+      __ add(str2, str2, cnt2);\n+      __ ld(cnt1, Address(str2));\n+      __ xorr(tmp4, tmp5, cnt1);\n+      __ beqz(tmp4, LENGTH_DIFF);\n+      \/\/ Find the first different characters in the longwords and\n+      \/\/ compute their difference.\n+    __ bind(DIFF2);\n+      __ ctzc_bit(tmp3, tmp4, isLL); \/\/ count zero from lsb to msb\n+      __ srl(tmp5, tmp5, tmp3);\n+      __ srl(cnt1, cnt1, tmp3);\n+      if (isLL) {\n+        __ andi(tmp5, tmp5, 0xFF);\n+        __ andi(cnt1, cnt1, 0xFF);\n+      } else {\n+        __ andi(tmp5, tmp5, 0xFFFF);\n+        __ andi(cnt1, cnt1, 0xFFFF);\n+      }\n+      __ sub(result, tmp5, cnt1);\n+      __ j(LENGTH_DIFF);\n+    __ bind(DIFF);\n+      __ ctzc_bit(tmp3, tmp4, isLL); \/\/ count zero from lsb to msb\n+      __ srl(tmp1, tmp1, tmp3);\n+      __ srl(tmp2, tmp2, tmp3);\n+      if (isLL) {\n+        __ andi(tmp1, tmp1, 0xFF);\n+        __ andi(tmp2, tmp2, 0xFF);\n+      } else {\n+        __ andi(tmp1, tmp1, 0xFFFF);\n+        __ andi(tmp2, tmp2, 0xFFFF);\n+      }\n+      __ sub(result, tmp1, tmp2);\n+      __ j(LENGTH_DIFF);\n+    __ bind(LAST_CHECK_AND_LENGTH_DIFF);\n+      __ xorr(tmp4, tmp1, tmp2);\n+      __ bnez(tmp4, DIFF);\n+    __ bind(LENGTH_DIFF);\n+      __ pop_reg(spilled_regs, sp);\n+      __ ret();\n+    return entry;\n+  }\n+\n+  void generate_compare_long_strings() {\n+    StubRoutines::riscv::_compare_long_string_LL = generate_compare_long_string_same_encoding(true);\n+    StubRoutines::riscv::_compare_long_string_UU = generate_compare_long_string_same_encoding(false);\n+    StubRoutines::riscv::_compare_long_string_LU = generate_compare_long_string_different_encoding(true);\n+    StubRoutines::riscv::_compare_long_string_UL = generate_compare_long_string_different_encoding(false);\n+  }\n+\n+  \/\/ x10 result\n+  \/\/ x11 src\n+  \/\/ x12 src count\n+  \/\/ x13 pattern\n+  \/\/ x14 pattern count\n+  address generate_string_indexof_linear(bool needle_isL, bool haystack_isL)\n+  {\n+    const char* stubName = needle_isL\n+           ? (haystack_isL ? \"indexof_linear_ll\" : \"indexof_linear_ul\")\n+           : \"indexof_linear_uu\";\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stubName);\n+    address entry = __ pc();\n+\n+    int needle_chr_size = needle_isL ? 1 : 2;\n+    int haystack_chr_size = haystack_isL ? 1 : 2;\n+    int needle_chr_shift = needle_isL ? 0 : 1;\n+    int haystack_chr_shift = haystack_isL ? 0 : 1;\n+    bool isL = needle_isL && haystack_isL;\n+    \/\/ parameters\n+    Register result = x10, haystack = x11, haystack_len = x12, needle = x13, needle_len = x14;\n+    \/\/ temporary registers\n+    Register mask1 = x20, match_mask = x21, first = x22, trailing_zeros = x23, mask2 = x24, tmp = x25;\n+    \/\/ redefinitions\n+    Register ch1 = x28, ch2 = x29;\n+    RegSet spilled_regs = RegSet::range(x20, x25) + RegSet::range(x28, x29);\n+\n+    __ push_reg(spilled_regs, sp);\n+\n+    Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,\n+          L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,\n+          L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,\n+          L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,\n+          L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,\n+          L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;\n+\n+    __ ld(ch1, Address(needle));\n+    __ ld(ch2, Address(haystack));\n+    \/\/ src.length - pattern.length\n+    __ sub(haystack_len, haystack_len, needle_len);\n+\n+    \/\/ first is needle[0]\n+    __ andi(first, ch1, needle_isL ? 0xFF : 0xFFFF, first);\n+    uint64_t mask0101 = UCONST64(0x0101010101010101);\n+    uint64_t mask0001 = UCONST64(0x0001000100010001);\n+    __ mv(mask1, haystack_isL ? mask0101 : mask0001);\n+    __ mul(first, first, mask1);\n+    uint64_t mask7f7f = UCONST64(0x7f7f7f7f7f7f7f7f);\n+    uint64_t mask7fff = UCONST64(0x7fff7fff7fff7fff);\n+    __ mv(mask2, haystack_isL ? mask7f7f : mask7fff);\n+    if (needle_isL != haystack_isL) {\n+      __ mv(tmp, ch1);\n+    }\n+    __ sub(haystack_len, haystack_len, wordSize \/ haystack_chr_size - 1);\n+    __ blez(haystack_len, L_SMALL);\n+\n+    if (needle_isL != haystack_isL) {\n+      __ inflate_lo32(ch1, tmp, match_mask, trailing_zeros);\n+    }\n+    \/\/ xorr, sub, orr, notr, andr\n+    \/\/ compare and set match_mask[i] with 0x80\/0x8000 (Latin1\/UTF16) if ch2[i] == first[i]\n+    \/\/ eg:\n+    \/\/ first:        aa aa aa aa aa aa aa aa\n+    \/\/ ch2:          aa aa li nx jd ka aa aa\n+    \/\/ match_mask:   80 80 00 00 00 00 80 80\n+    __ compute_match_mask(ch2, first, match_mask, mask1, mask2);\n+\n+    \/\/ search first char of needle, if success, goto L_HAS_ZERO;\n+    __ bnez(match_mask, L_HAS_ZERO);\n+    __ sub(haystack_len, haystack_len, wordSize \/ haystack_chr_size);\n+    __ add(result, result, wordSize \/ haystack_chr_size);\n+    __ add(haystack, haystack, wordSize);\n+    __ bltz(haystack_len, L_POST_LOOP);\n+\n+    __ bind(L_LOOP);\n+    __ ld(ch2, Address(haystack));\n+    __ compute_match_mask(ch2, first, match_mask, mask1, mask2);\n+    __ bnez(match_mask, L_HAS_ZERO);\n+\n+    __ bind(L_LOOP_PROCEED);\n+    __ sub(haystack_len, haystack_len, wordSize \/ haystack_chr_size);\n+    __ add(haystack, haystack, wordSize);\n+    __ add(result, result, wordSize \/ haystack_chr_size);\n+    __ bgez(haystack_len, L_LOOP);\n+\n+    __ bind(L_POST_LOOP);\n+    __ mv(ch2, -wordSize \/ haystack_chr_size);\n+    __ ble(haystack_len, ch2, NOMATCH); \/\/ no extra characters to check\n+    __ ld(ch2, Address(haystack));\n+    __ slli(haystack_len, haystack_len, LogBitsPerByte + haystack_chr_shift);\n+    __ neg(haystack_len, haystack_len);\n+    __ xorr(ch2, first, ch2);\n+    __ sub(match_mask, ch2, mask1);\n+    __ orr(ch2, ch2, mask2);\n+    __ mv(trailing_zeros, -1); \/\/ all bits set\n+    __ j(L_SMALL_PROCEED);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_SMALL);\n+    __ slli(haystack_len, haystack_len, LogBitsPerByte + haystack_chr_shift);\n+    __ neg(haystack_len, haystack_len);\n+    if (needle_isL != haystack_isL) {\n+      __ inflate_lo32(ch1, tmp, match_mask, trailing_zeros);\n+    }\n+    __ xorr(ch2, first, ch2);\n+    __ sub(match_mask, ch2, mask1);\n+    __ orr(ch2, ch2, mask2);\n+    __ mv(trailing_zeros, -1); \/\/ all bits set\n+\n+    __ bind(L_SMALL_PROCEED);\n+    __ srl(trailing_zeros, trailing_zeros, haystack_len); \/\/ mask. zeroes on useless bits.\n+    __ notr(ch2, ch2);\n+    __ andr(match_mask, match_mask, ch2);\n+    __ andr(match_mask, match_mask, trailing_zeros); \/\/ clear useless bits and check\n+    __ beqz(match_mask, NOMATCH);\n+\n+    __ bind(L_SMALL_HAS_ZERO_LOOP);\n+    __ ctzc_bit(trailing_zeros, match_mask, haystack_isL, ch2, tmp); \/\/ count trailing zeros\n+    __ addi(trailing_zeros, trailing_zeros, haystack_isL ? 7 : 15);\n+    __ mv(ch2, wordSize \/ haystack_chr_size);\n+    __ ble(needle_len, ch2, L_SMALL_CMP_LOOP_LAST_CMP2);\n+    __ compute_index(haystack, trailing_zeros, match_mask, result, ch2, tmp, haystack_isL);\n+    __ mv(trailing_zeros, wordSize \/ haystack_chr_size);\n+    __ bne(ch1, ch2, L_SMALL_CMP_LOOP_NOMATCH);\n+\n+    __ bind(L_SMALL_CMP_LOOP);\n+    __ shadd(first, trailing_zeros, needle, first, needle_chr_shift);\n+    __ shadd(ch2, trailing_zeros, haystack, ch2, haystack_chr_shift);\n+    needle_isL ? __ lbu(first, Address(first)) : __ lhu(first, Address(first));\n+    haystack_isL ? __ lbu(ch2, Address(ch2)) : __ lhu(ch2, Address(ch2));\n+    __ add(trailing_zeros, trailing_zeros, 1);\n+    __ bge(trailing_zeros, needle_len, L_SMALL_CMP_LOOP_LAST_CMP);\n+    __ beq(first, ch2, L_SMALL_CMP_LOOP);\n+\n+    __ bind(L_SMALL_CMP_LOOP_NOMATCH);\n+    __ beqz(match_mask, NOMATCH);\n+    __ ctzc_bit(trailing_zeros, match_mask, haystack_isL, tmp, ch2);\n+    __ addi(trailing_zeros, trailing_zeros, haystack_isL ? 7 : 15);\n+    __ add(result, result, 1);\n+    __ add(haystack, haystack, haystack_chr_size);\n+    __ j(L_SMALL_HAS_ZERO_LOOP);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_SMALL_CMP_LOOP_LAST_CMP);\n+    __ bne(first, ch2, L_SMALL_CMP_LOOP_NOMATCH);\n+    __ j(DONE);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_SMALL_CMP_LOOP_LAST_CMP2);\n+    __ compute_index(haystack, trailing_zeros, match_mask, result, ch2, tmp, haystack_isL);\n+    __ bne(ch1, ch2, L_SMALL_CMP_LOOP_NOMATCH);\n+    __ j(DONE);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_HAS_ZERO);\n+    __ ctzc_bit(trailing_zeros, match_mask, haystack_isL, tmp, ch2);\n+    __ addi(trailing_zeros, trailing_zeros, haystack_isL ? 7 : 15);\n+    __ slli(needle_len, needle_len, BitsPerByte * wordSize \/ 2);\n+    __ orr(haystack_len, haystack_len, needle_len); \/\/ restore needle_len(32bits)\n+    __ sub(result, result, 1); \/\/ array index from 0, so result -= 1\n+\n+    __ bind(L_HAS_ZERO_LOOP);\n+    __ mv(needle_len, wordSize \/ haystack_chr_size);\n+    __ srli(ch2, haystack_len, BitsPerByte * wordSize \/ 2);\n+    __ bge(needle_len, ch2, L_CMP_LOOP_LAST_CMP2);\n+    \/\/ load next 8 bytes from haystack, and increase result index\n+    __ compute_index(haystack, trailing_zeros, match_mask, result, ch2, tmp, haystack_isL);\n+    __ add(result, result, 1);\n+    __ mv(trailing_zeros, wordSize \/ haystack_chr_size);\n+    __ bne(ch1, ch2, L_CMP_LOOP_NOMATCH);\n+\n+    \/\/ compare one char\n+    __ bind(L_CMP_LOOP);\n+    __ shadd(needle_len, trailing_zeros, needle, needle_len, needle_chr_shift);\n+    needle_isL ? __ lbu(needle_len, Address(needle_len)) : __ lhu(needle_len, Address(needle_len));\n+    __ shadd(ch2, trailing_zeros, haystack, ch2, haystack_chr_shift);\n+    haystack_isL ? __ lbu(ch2, Address(ch2)) : __ lhu(ch2, Address(ch2));\n+    __ add(trailing_zeros, trailing_zeros, 1); \/\/ next char index\n+    __ srli(tmp, haystack_len, BitsPerByte * wordSize \/ 2);\n+    __ bge(trailing_zeros, tmp, L_CMP_LOOP_LAST_CMP);\n+    __ beq(needle_len, ch2, L_CMP_LOOP);\n+\n+    __ bind(L_CMP_LOOP_NOMATCH);\n+    __ beqz(match_mask, L_HAS_ZERO_LOOP_NOMATCH);\n+    __ ctzc_bit(trailing_zeros, match_mask, haystack_isL, needle_len, ch2); \/\/ find next \"first\" char index\n+    __ addi(trailing_zeros, trailing_zeros, haystack_isL ? 7 : 15);\n+    __ add(haystack, haystack, haystack_chr_size);\n+    __ j(L_HAS_ZERO_LOOP);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_CMP_LOOP_LAST_CMP);\n+    __ bne(needle_len, ch2, L_CMP_LOOP_NOMATCH);\n+    __ j(DONE);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_CMP_LOOP_LAST_CMP2);\n+    __ compute_index(haystack, trailing_zeros, match_mask, result, ch2, tmp, haystack_isL);\n+    __ add(result, result, 1);\n+    __ bne(ch1, ch2, L_CMP_LOOP_NOMATCH);\n+    __ j(DONE);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_HAS_ZERO_LOOP_NOMATCH);\n+    \/\/ 1) Restore \"result\" index. Index was wordSize\/str2_chr_size * N until\n+    \/\/ L_HAS_ZERO block. Byte octet was analyzed in L_HAS_ZERO_LOOP,\n+    \/\/ so, result was increased at max by wordSize\/str2_chr_size - 1, so,\n+    \/\/ respective high bit wasn't changed. L_LOOP_PROCEED will increase\n+    \/\/ result by analyzed characters value, so, we can just reset lower bits\n+    \/\/ in result here. Clear 2 lower bits for UU\/UL and 3 bits for LL\n+    \/\/ 2) restore needle_len and haystack_len values from \"compressed\" haystack_len\n+    \/\/ 3) advance haystack value to represent next haystack octet. result & 7\/3 is\n+    \/\/ index of last analyzed substring inside current octet. So, haystack in at\n+    \/\/ respective start address. We need to advance it to next octet\n+    __ andi(match_mask, result, wordSize \/ haystack_chr_size - 1);\n+    __ srli(needle_len, haystack_len, BitsPerByte * wordSize \/ 2);\n+    __ andi(result, result, haystack_isL ? -8 : -4);\n+    __ slli(tmp, match_mask, haystack_chr_shift);\n+    __ sub(haystack, haystack, tmp);\n+    __ addw(haystack_len, haystack_len, zr);\n+    __ j(L_LOOP_PROCEED);\n+\n+    __ align(OptoLoopAlignment);\n+    __ bind(NOMATCH);\n+    __ mv(result, -1);\n+\n+    __ bind(DONE);\n+    __ pop_reg(spilled_regs, sp);\n+    __ ret();\n+    return entry;\n+  }\n+\n+  void generate_string_indexof_stubs()\n+  {\n+    StubRoutines::riscv::_string_indexof_linear_ll = generate_string_indexof_linear(true, true);\n+    StubRoutines::riscv::_string_indexof_linear_uu = generate_string_indexof_linear(false, false);\n+    StubRoutines::riscv::_string_indexof_linear_ul = generate_string_indexof_linear(true, false);\n+  }\n+\n+#ifdef COMPILER2\n+  address generate_mulAdd()\n+  {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n+\n+    address entry = __ pc();\n+\n+    const Register out     = x10;\n+    const Register in      = x11;\n+    const Register offset  = x12;\n+    const Register len     = x13;\n+    const Register k       = x14;\n+    const Register tmp     = x28;\n+\n+    BLOCK_COMMENT(\"Entry:\");\n+    __ enter();\n+    __ mul_add(out, in, offset, len, k, tmp);\n+    __ leave();\n+    __ ret();\n+\n+    return entry;\n+  }\n+\n+  \/**\n+   *  Arguments:\n+   *\n+   *  Input:\n+   *    c_rarg0   - x address\n+   *    c_rarg1   - x length\n+   *    c_rarg2   - y address\n+   *    c_rarg3   - y length\n+   *    c_rarg4   - z address\n+   *    c_rarg5   - z length\n+   *\/\n+  address generate_multiplyToLen()\n+  {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n+    address entry = __ pc();\n+\n+    const Register x     = x10;\n+    const Register xlen  = x11;\n+    const Register y     = x12;\n+    const Register ylen  = x13;\n+    const Register z     = x14;\n+    const Register zlen  = x15;\n+\n+    const Register tmp1  = x16;\n+    const Register tmp2  = x17;\n+    const Register tmp3  = x7;\n+    const Register tmp4  = x28;\n+    const Register tmp5  = x29;\n+    const Register tmp6  = x30;\n+    const Register tmp7  = x31;\n+\n+    BLOCK_COMMENT(\"Entry:\");\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return entry;\n+  }\n+\n+  address generate_squareToLen()\n+  {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n+    address entry = __ pc();\n+\n+    const Register x     = x10;\n+    const Register xlen  = x11;\n+    const Register z     = x12;\n+    const Register zlen  = x13;\n+    const Register y     = x14; \/\/ == x\n+    const Register ylen  = x15; \/\/ == xlen\n+\n+    const Register tmp1  = x16;\n+    const Register tmp2  = x17;\n+    const Register tmp3  = x7;\n+    const Register tmp4  = x28;\n+    const Register tmp5  = x29;\n+    const Register tmp6  = x30;\n+    const Register tmp7  = x31;\n+\n+    BLOCK_COMMENT(\"Entry:\");\n+    __ enter();\n+    __ mv(y, x);\n+    __ mv(ylen, xlen);\n+    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);\n+    __ leave();\n+    __ ret();\n+\n+    return entry;\n+  }\n+#endif\n+\n+#ifdef COMPILER2\n+  class MontgomeryMultiplyGenerator : public MacroAssembler {\n+\n+    Register Pa_base, Pb_base, Pn_base, Pm_base, inv, Rlen, Ra, Rb, Rm, Rn,\n+      Pa, Pb, Pn, Pm, Rhi_ab, Rlo_ab, Rhi_mn, Rlo_mn, tmp0, tmp1, tmp2, Ri, Rj;\n+\n+    RegSet _toSave;\n+    bool _squaring;\n+\n+  public:\n+    MontgomeryMultiplyGenerator (Assembler *as, bool squaring)\n+      : MacroAssembler(as->code()), _squaring(squaring) {\n+\n+      \/\/ Register allocation\n+\n+      Register reg = c_rarg0;\n+      Pa_base = reg;       \/\/ Argument registers\n+      if (squaring) {\n+        Pb_base = Pa_base;\n+      } else {\n+        Pb_base = ++reg;\n+      }\n+      Pn_base = ++reg;\n+      Rlen= ++reg;\n+      inv = ++reg;\n+      Pm_base = ++reg;\n+\n+                        \/\/ Working registers:\n+      Ra =  ++reg;      \/\/ The current digit of a, b, n, and m.\n+      Rb =  ++reg;\n+      Rm =  ++reg;\n+      Rn =  ++reg;\n+\n+      Pa =  ++reg;      \/\/ Pointers to the current\/next digit of a, b, n, and m.\n+      Pb =  ++reg;\n+      Pm =  ++reg;\n+      Pn =  ++reg;\n+\n+      tmp0 =  ++reg;    \/\/ Three registers which form a\n+      tmp1 =  ++reg;    \/\/ triple-precision accumuator.\n+      tmp2 =  ++reg;\n+\n+      Ri =  x6;         \/\/ Inner and outer loop indexes.\n+      Rj =  x7;\n+\n+      Rhi_ab = x28;     \/\/ Product registers: low and high parts\n+      Rlo_ab = x29;     \/\/ of a*b and m*n.\n+      Rhi_mn = x30;\n+      Rlo_mn = x31;\n+\n+      \/\/ x18 and up are callee-saved.\n+      _toSave = RegSet::range(x18, reg) + Pm_base;\n+    }\n+\n+  private:\n+    void save_regs() {\n+      push_reg(_toSave, sp);\n+    }\n+\n+    void restore_regs() {\n+      pop_reg(_toSave, sp);\n+    }\n+\n+    template <typename T>\n+    void unroll_2(Register count, T block) {\n+      Label loop, end, odd;\n+      beqz(count, end);\n+      andi(t0, count, 0x1);\n+      bnez(t0, odd);\n+      align(16);\n+      bind(loop);\n+      (this->*block)();\n+      bind(odd);\n+      (this->*block)();\n+      addi(count, count, -2);\n+      bgtz(count, loop);\n+      bind(end);\n+    }\n+\n+    template <typename T>\n+    void unroll_2(Register count, T block, Register d, Register s, Register tmp) {\n+      Label loop, end, odd;\n+      beqz(count, end);\n+      andi(tmp, count, 0x1);\n+      bnez(tmp, odd);\n+      align(16);\n+      bind(loop);\n+      (this->*block)(d, s, tmp);\n+      bind(odd);\n+      (this->*block)(d, s, tmp);\n+      addi(count, count, -2);\n+      bgtz(count, loop);\n+      bind(end);\n+    }\n+\n+    void pre1(RegisterOrConstant i) {\n+      block_comment(\"pre1\");\n+      \/\/ Pa = Pa_base;\n+      \/\/ Pb = Pb_base + i;\n+      \/\/ Pm = Pm_base;\n+      \/\/ Pn = Pn_base + i;\n+      \/\/ Ra = *Pa;\n+      \/\/ Rb = *Pb;\n+      \/\/ Rm = *Pm;\n+      \/\/ Rn = *Pn;\n+      if (i.is_register()) {\n+        slli(t0, i.as_register(), LogBytesPerWord);\n+      } else {\n+        mv(t0, i.as_constant());\n+        slli(t0, t0, LogBytesPerWord);\n+      }\n+\n+      mv(Pa, Pa_base);\n+      add(Pb, Pb_base, t0);\n+      mv(Pm, Pm_base);\n+      add(Pn, Pn_base, t0);\n+\n+      ld(Ra, Address(Pa));\n+      ld(Rb, Address(Pb));\n+      ld(Rm, Address(Pm));\n+      ld(Rn, Address(Pn));\n+\n+      \/\/ Zero the m*n result.\n+      mv(Rhi_mn, zr);\n+      mv(Rlo_mn, zr);\n+    }\n+\n+    \/\/ The core multiply-accumulate step of a Montgomery\n+    \/\/ multiplication.  The idea is to schedule operations as a\n+    \/\/ pipeline so that instructions with long latencies (loads and\n+    \/\/ multiplies) have time to complete before their results are\n+    \/\/ used.  This most benefits in-order implementations of the\n+    \/\/ architecture but out-of-order ones also benefit.\n+    void step() {\n+      block_comment(\"step\");\n+      \/\/ MACC(Ra, Rb, tmp0, tmp1, tmp2);\n+      \/\/ Ra = *++Pa;\n+      \/\/ Rb = *--Pb;\n+      mulhu(Rhi_ab, Ra, Rb);\n+      mul(Rlo_ab, Ra, Rb);\n+      addi(Pa, Pa, wordSize);\n+      ld(Ra, Address(Pa));\n+      addi(Pb, Pb, -wordSize);\n+      ld(Rb, Address(Pb));\n+      acc(Rhi_mn, Rlo_mn, tmp0, tmp1, tmp2); \/\/ The pending m*n from the\n+                                            \/\/ previous iteration.\n+      \/\/ MACC(Rm, Rn, tmp0, tmp1, tmp2);\n+      \/\/ Rm = *++Pm;\n+      \/\/ Rn = *--Pn;\n+      mulhu(Rhi_mn, Rm, Rn);\n+      mul(Rlo_mn, Rm, Rn);\n+      addi(Pm, Pm, wordSize);\n+      ld(Rm, Address(Pm));\n+      addi(Pn, Pn, -wordSize);\n+      ld(Rn, Address(Pn));\n+      acc(Rhi_ab, Rlo_ab, tmp0, tmp1, tmp2);\n+    }\n+\n+    void post1() {\n+      block_comment(\"post1\");\n+\n+      \/\/ MACC(Ra, Rb, tmp0, tmp1, tmp2);\n+      \/\/ Ra = *++Pa;\n+      \/\/ Rb = *--Pb;\n+      mulhu(Rhi_ab, Ra, Rb);\n+      mul(Rlo_ab, Ra, Rb);\n+      acc(Rhi_mn, Rlo_mn, tmp0, tmp1, tmp2);  \/\/ The pending m*n\n+      acc(Rhi_ab, Rlo_ab, tmp0, tmp1, tmp2);\n+\n+      \/\/ *Pm = Rm = tmp0 * inv;\n+      mul(Rm, tmp0, inv);\n+      sd(Rm, Address(Pm));\n+\n+      \/\/ MACC(Rm, Rn, tmp0, tmp1, tmp2);\n+      \/\/ tmp0 = tmp1; tmp1 = tmp2; tmp2 = 0;\n+      mulhu(Rhi_mn, Rm, Rn);\n+\n+#ifndef PRODUCT\n+      \/\/ assert(m[i] * n[0] + tmp0 == 0, \"broken Montgomery multiply\");\n+      {\n+        mul(Rlo_mn, Rm, Rn);\n+        add(Rlo_mn, tmp0, Rlo_mn);\n+        Label ok;\n+        beqz(Rlo_mn, ok);\n+        stop(\"broken Montgomery multiply\");\n+        bind(ok);\n+      }\n+#endif\n+      \/\/ We have very carefully set things up so that\n+      \/\/ m[i]*n[0] + tmp0 == 0 (mod b), so we don't have to calculate\n+      \/\/ the lower half of Rm * Rn because we know the result already:\n+      \/\/ it must be -tmp0.  tmp0 + (-tmp0) must generate a carry iff\n+      \/\/ tmp0 != 0.  So, rather than do a mul and an cad we just set\n+      \/\/ the carry flag iff tmp0 is nonzero.\n+      \/\/\n+      \/\/ mul(Rlo_mn, Rm, Rn);\n+      \/\/ cad(zr, tmp0, Rlo_mn);\n+      addi(t0, tmp0, -1);\n+      sltu(t0, t0, tmp0); \/\/ Set carry iff tmp0 is nonzero\n+      cadc(tmp0, tmp1, Rhi_mn, t0);\n+      adc(tmp1, tmp2, zr, t0);\n+      mv(tmp2, zr);\n+    }\n+\n+    void pre2(Register i, Register len) {\n+      block_comment(\"pre2\");\n+      \/\/ Pa = Pa_base + i-len;\n+      \/\/ Pb = Pb_base + len;\n+      \/\/ Pm = Pm_base + i-len;\n+      \/\/ Pn = Pn_base + len;\n+\n+      sub(Rj, i, len);\n+      \/\/ Rj == i-len\n+\n+      \/\/ Ra as temp register\n+      slli(Ra, Rj, LogBytesPerWord);\n+      add(Pa, Pa_base, Ra);\n+      add(Pm, Pm_base, Ra);\n+      slli(Ra, len, LogBytesPerWord);\n+      add(Pb, Pb_base, Ra);\n+      add(Pn, Pn_base, Ra);\n+\n+      \/\/ Ra = *++Pa;\n+      \/\/ Rb = *--Pb;\n+      \/\/ Rm = *++Pm;\n+      \/\/ Rn = *--Pn;\n+      add(Pa, Pa, wordSize);\n+      ld(Ra, Address(Pa));\n+      add(Pb, Pb, -wordSize);\n+      ld(Rb, Address(Pb));\n+      add(Pm, Pm, wordSize);\n+      ld(Rm, Address(Pm));\n+      add(Pn, Pn, -wordSize);\n+      ld(Rn, Address(Pn));\n+\n+      mv(Rhi_mn, zr);\n+      mv(Rlo_mn, zr);\n+    }\n+\n+    void post2(Register i, Register len) {\n+      block_comment(\"post2\");\n+      sub(Rj, i, len);\n+\n+      cad(tmp0, tmp0, Rlo_mn, t0); \/\/ The pending m*n, low part\n+\n+      \/\/ As soon as we know the least significant digit of our result,\n+      \/\/ store it.\n+      \/\/ Pm_base[i-len] = tmp0;\n+      \/\/ Rj as temp register\n+      slli(Rj, Rj, LogBytesPerWord);\n+      add(Rj, Pm_base, Rj);\n+      sd(tmp0, Address(Rj));\n+\n+      \/\/ tmp0 = tmp1; tmp1 = tmp2; tmp2 = 0;\n+      cadc(tmp0, tmp1, Rhi_mn, t0); \/\/ The pending m*n, high part\n+      adc(tmp1, tmp2, zr, t0);\n+      mv(tmp2, zr);\n+    }\n+\n+    \/\/ A carry in tmp0 after Montgomery multiplication means that we\n+    \/\/ should subtract multiples of n from our result in m.  We'll\n+    \/\/ keep doing that until there is no carry.\n+    void normalize(Register len) {\n+      block_comment(\"normalize\");\n+      \/\/ while (tmp0)\n+      \/\/   tmp0 = sub(Pm_base, Pn_base, tmp0, len);\n+      Label loop, post, again;\n+      Register cnt = tmp1, i = tmp2; \/\/ Re-use registers; we're done with them now\n+      beqz(tmp0, post); {\n+        bind(again); {\n+          mv(i, zr);\n+          mv(cnt, len);\n+          slli(Rn, i, LogBytesPerWord);\n+          add(Rm, Pm_base, Rn);\n+          ld(Rm, Address(Rm));\n+          add(Rn, Pn_base, Rn);\n+          ld(Rn, Address(Rn));\n+          li(t0, 1); \/\/ set carry flag, i.e. no borrow\n+          align(16);\n+          bind(loop); {\n+            notr(Rn, Rn);\n+            add(Rm, Rm, t0);\n+            add(Rm, Rm, Rn);\n+            sltu(t0, Rm, Rn);\n+            slli(Rn, i, LogBytesPerWord); \/\/ Rn as temp register\n+            add(Rn, Pm_base, Rn);\n+            sd(Rm, Address(Rn));\n+            add(i, i, 1);\n+            slli(Rn, i, LogBytesPerWord);\n+            add(Rm, Pm_base, Rn);\n+            ld(Rm, Address(Rm));\n+            add(Rn, Pn_base, Rn);\n+            ld(Rn, Address(Rn));\n+            sub(cnt, cnt, 1);\n+          } bnez(cnt, loop);\n+          addi(tmp0, tmp0, -1);\n+          add(tmp0, tmp0, t0);\n+        } bnez(tmp0, again);\n+      } bind(post);\n+    }\n+\n+    \/\/ Move memory at s to d, reversing words.\n+    \/\/    Increments d to end of copied memory\n+    \/\/    Destroys tmp1, tmp2\n+    \/\/    Preserves len\n+    \/\/    Leaves s pointing to the address which was in d at start\n+    void reverse(Register d, Register s, Register len, Register tmp1, Register tmp2) {\n+      assert(tmp1 < x28 && tmp2 < x28, \"register corruption\");\n+\n+      slli(tmp1, len, LogBytesPerWord);\n+      add(s, s, tmp1);\n+      mv(tmp1, len);\n+      unroll_2(tmp1,  &MontgomeryMultiplyGenerator::reverse1, d, s, tmp2);\n+      slli(tmp1, len, LogBytesPerWord);\n+      sub(s, d, tmp1);\n+    }\n+    \/\/ [63...0] -> [31...0][63...32]\n+    void reverse1(Register d, Register s, Register tmp) {\n+      addi(s, s, -wordSize);\n+      ld(tmp, Address(s));\n+      ror_imm(tmp, tmp, 32, t0);\n+      sd(tmp, Address(d));\n+      addi(d, d, wordSize);\n+    }\n+\n+    void step_squaring() {\n+      \/\/ An extra ACC\n+      step();\n+      acc(Rhi_ab, Rlo_ab, tmp0, tmp1, tmp2);\n+    }\n+\n+    void last_squaring(Register i) {\n+      Label dont;\n+      \/\/ if ((i & 1) == 0) {\n+      andi(t0, i, 0x1);\n+      bnez(t0, dont); {\n+        \/\/ MACC(Ra, Rb, tmp0, tmp1, tmp2);\n+        \/\/ Ra = *++Pa;\n+        \/\/ Rb = *--Pb;\n+        mulhu(Rhi_ab, Ra, Rb);\n+        mul(Rlo_ab, Ra, Rb);\n+        acc(Rhi_ab, Rlo_ab, tmp0, tmp1, tmp2);\n+      } bind(dont);\n+    }\n+\n+    void extra_step_squaring() {\n+      acc(Rhi_mn, Rlo_mn, tmp0, tmp1, tmp2);  \/\/ The pending m*n\n+\n+      \/\/ MACC(Rm, Rn, tmp0, tmp1, tmp2);\n+      \/\/ Rm = *++Pm;\n+      \/\/ Rn = *--Pn;\n+      mulhu(Rhi_mn, Rm, Rn);\n+      mul(Rlo_mn, Rm, Rn);\n+      addi(Pm, Pm, wordSize);\n+      ld(Rm, Address(Pm));\n+      addi(Pn, Pn, -wordSize);\n+      ld(Rn, Address(Pn));\n+    }\n+\n+    void post1_squaring() {\n+      acc(Rhi_mn, Rlo_mn, tmp0, tmp1, tmp2);  \/\/ The pending m*n\n+\n+      \/\/ *Pm = Rm = tmp0 * inv;\n+      mul(Rm, tmp0, inv);\n+      sd(Rm, Address(Pm));\n+\n+      \/\/ MACC(Rm, Rn, tmp0, tmp1, tmp2);\n+      \/\/ tmp0 = tmp1; tmp1 = tmp2; tmp2 = 0;\n+      mulhu(Rhi_mn, Rm, Rn);\n+\n+#ifndef PRODUCT\n+      \/\/ assert(m[i] * n[0] + tmp0 == 0, \"broken Montgomery multiply\");\n+      {\n+        mul(Rlo_mn, Rm, Rn);\n+        add(Rlo_mn, tmp0, Rlo_mn);\n+        Label ok;\n+        beqz(Rlo_mn, ok); {\n+          stop(\"broken Montgomery multiply\");\n+        } bind(ok);\n+      }\n+#endif\n+      \/\/ We have very carefully set things up so that\n+      \/\/ m[i]*n[0] + tmp0 == 0 (mod b), so we don't have to calculate\n+      \/\/ the lower half of Rm * Rn because we know the result already:\n+      \/\/ it must be -tmp0.  tmp0 + (-tmp0) must generate a carry iff\n+      \/\/ tmp0 != 0.  So, rather than do a mul and a cad we just set\n+      \/\/ the carry flag iff tmp0 is nonzero.\n+      \/\/\n+      \/\/ mul(Rlo_mn, Rm, Rn);\n+      \/\/ cad(zr, tmp, Rlo_mn);\n+      addi(t0, tmp0, -1);\n+      sltu(t0, t0, tmp0); \/\/ Set carry iff tmp0 is nonzero\n+      cadc(tmp0, tmp1, Rhi_mn, t0);\n+      adc(tmp1, tmp2, zr, t0);\n+      mv(tmp2, zr);\n+    }\n+\n+    \/\/ use t0 as carry\n+    void acc(Register Rhi, Register Rlo,\n+             Register tmp0, Register tmp1, Register tmp2) {\n+      cad(tmp0, tmp0, Rlo, t0);\n+      cadc(tmp1, tmp1, Rhi, t0);\n+      adc(tmp2, tmp2, zr, t0);\n+    }\n+\n+  public:\n+    \/**\n+     * Fast Montgomery multiplication.  The derivation of the\n+     * algorithm is in A Cryptographic Library for the Motorola\n+     * DSP56000, Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.\n+     *\n+     * Arguments:\n+     *\n+     * Inputs for multiplication:\n+     *   c_rarg0   - int array elements a\n+     *   c_rarg1   - int array elements b\n+     *   c_rarg2   - int array elements n (the modulus)\n+     *   c_rarg3   - int length\n+     *   c_rarg4   - int inv\n+     *   c_rarg5   - int array elements m (the result)\n+     *\n+     * Inputs for squaring:\n+     *   c_rarg0   - int array elements a\n+     *   c_rarg1   - int array elements n (the modulus)\n+     *   c_rarg2   - int length\n+     *   c_rarg3   - int inv\n+     *   c_rarg4   - int array elements m (the result)\n+     *\n+     *\/\n+    address generate_multiply() {\n+      Label argh, nothing;\n+      bind(argh);\n+      stop(\"MontgomeryMultiply total_allocation must be <= 8192\");\n+\n+      align(CodeEntryAlignment);\n+      address entry = pc();\n+\n+      beqz(Rlen, nothing);\n+\n+      enter();\n+\n+      \/\/ Make room.\n+      li(Ra, 512);\n+      bgt(Rlen, Ra, argh);\n+      slli(Ra, Rlen, exact_log2(4 * sizeof(jint)));\n+      sub(Ra, sp, Ra);\n+      andi(sp, Ra, -2 * wordSize);\n+\n+      srliw(Rlen, Rlen, 1);  \/\/ length in longwords = len\/2\n+\n+      {\n+        \/\/ Copy input args, reversing as we go.  We use Ra as a\n+        \/\/ temporary variable.\n+        reverse(Ra, Pa_base, Rlen, Ri, Rj);\n+        if (!_squaring)\n+          reverse(Ra, Pb_base, Rlen, Ri, Rj);\n+        reverse(Ra, Pn_base, Rlen, Ri, Rj);\n+      }\n+\n+      \/\/ Push all call-saved registers and also Pm_base which we'll need\n+      \/\/ at the end.\n+      save_regs();\n+\n+#ifndef PRODUCT\n+      \/\/ assert(inv * n[0] == -1UL, \"broken inverse in Montgomery multiply\");\n+      {\n+        ld(Rn, Address(Pn_base));\n+        mul(Rlo_mn, Rn, inv);\n+        li(t0, -1);\n+        Label ok;\n+        beq(Rlo_mn, t0, ok);\n+        stop(\"broken inverse in Montgomery multiply\");\n+        bind(ok);\n+      }\n+#endif\n+\n+      mv(Pm_base, Ra);\n+\n+      mv(tmp0, zr);\n+      mv(tmp1, zr);\n+      mv(tmp2, zr);\n+\n+      block_comment(\"for (int i = 0; i < len; i++) {\");\n+      mv(Ri, zr); {\n+        Label loop, end;\n+        bge(Ri, Rlen, end);\n+\n+        bind(loop);\n+        pre1(Ri);\n+\n+        block_comment(\"  for (j = i; j; j--) {\"); {\n+          mv(Rj, Ri);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::step);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        post1();\n+        addw(Ri, Ri, 1);\n+        blt(Ri, Rlen, loop);\n+        bind(end);\n+        block_comment(\"} \/\/ i\");\n+      }\n+\n+      block_comment(\"for (int i = len; i < 2*len; i++) {\");\n+      mv(Ri, Rlen); {\n+        Label loop, end;\n+        slli(t0, Rlen, 1);\n+        bge(Ri, t0, end);\n+\n+        bind(loop);\n+        pre2(Ri, Rlen);\n+\n+        block_comment(\"  for (j = len*2-i-1; j; j--) {\"); {\n+          slliw(Rj, Rlen, 1);\n+          subw(Rj, Rj, Ri);\n+          subw(Rj, Rj, 1);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::step);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        post2(Ri, Rlen);\n+        addw(Ri, Ri, 1);\n+        slli(t0, Rlen, 1);\n+        blt(Ri, t0, loop);\n+        bind(end);\n+      }\n+      block_comment(\"} \/\/ i\");\n+\n+      normalize(Rlen);\n+\n+      mv(Ra, Pm_base);  \/\/ Save Pm_base in Ra\n+      restore_regs();  \/\/ Restore caller's Pm_base\n+\n+      \/\/ Copy our result into caller's Pm_base\n+      reverse(Pm_base, Ra, Rlen, Ri, Rj);\n+\n+      leave();\n+      bind(nothing);\n+      ret();\n+\n+      return entry;\n+    }\n+\n+    \/**\n+     *\n+     * Arguments:\n+     *\n+     * Inputs:\n+     *   c_rarg0   - int array elements a\n+     *   c_rarg1   - int array elements n (the modulus)\n+     *   c_rarg2   - int length\n+     *   c_rarg3   - int inv\n+     *   c_rarg4   - int array elements m (the result)\n+     *\n+     *\/\n+    address generate_square() {\n+      Label argh;\n+      bind(argh);\n+      stop(\"MontgomeryMultiply total_allocation must be <= 8192\");\n+\n+      align(CodeEntryAlignment);\n+      address entry = pc();\n+\n+      enter();\n+\n+      \/\/ Make room.\n+      li(Ra, 512);\n+      bgt(Rlen, Ra, argh);\n+      slli(Ra, Rlen, exact_log2(4 * sizeof(jint)));\n+      sub(Ra, sp, Ra);\n+      andi(sp, Ra, -2 * wordSize);\n+\n+      srliw(Rlen, Rlen, 1);  \/\/ length in longwords = len\/2\n+\n+      {\n+        \/\/ Copy input args, reversing as we go.  We use Ra as a\n+        \/\/ temporary variable.\n+        reverse(Ra, Pa_base, Rlen, Ri, Rj);\n+        reverse(Ra, Pn_base, Rlen, Ri, Rj);\n+      }\n+\n+      \/\/ Push all call-saved registers and also Pm_base which we'll need\n+      \/\/ at the end.\n+      save_regs();\n+\n+      mv(Pm_base, Ra);\n+\n+      mv(tmp0, zr);\n+      mv(tmp1, zr);\n+      mv(tmp2, zr);\n+\n+      block_comment(\"for (int i = 0; i < len; i++) {\");\n+      mv(Ri, zr); {\n+        Label loop, end;\n+        bind(loop);\n+        bge(Ri, Rlen, end);\n+\n+        pre1(Ri);\n+\n+        block_comment(\"for (j = (i+1)\/2; j; j--) {\"); {\n+          addi(Rj, Ri, 1);\n+          srliw(Rj, Rj, 1);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::step_squaring);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        last_squaring(Ri);\n+\n+        block_comment(\"  for (j = i\/2; j; j--) {\"); {\n+          srliw(Rj, Ri, 1);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::extra_step_squaring);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        post1_squaring();\n+        addi(Ri, Ri, 1);\n+        blt(Ri, Rlen, loop);\n+\n+        bind(end);\n+        block_comment(\"} \/\/ i\");\n+      }\n+\n+      block_comment(\"for (int i = len; i < 2*len; i++) {\");\n+      mv(Ri, Rlen); {\n+        Label loop, end;\n+        bind(loop);\n+        slli(t0, Rlen, 1);\n+        bge(Ri, t0, end);\n+\n+        pre2(Ri, Rlen);\n+\n+        block_comment(\"  for (j = (2*len-i-1)\/2; j; j--) {\"); {\n+          slli(Rj, Rlen, 1);\n+          sub(Rj, Rj, Ri);\n+          sub(Rj, Rj, 1);\n+          srliw(Rj, Rj, 1);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::step_squaring);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        last_squaring(Ri);\n+\n+        block_comment(\"  for (j = (2*len-i)\/2; j; j--) {\"); {\n+          slli(Rj, Rlen, 1);\n+          sub(Rj, Rj, Ri);\n+          srliw(Rj, Rj, 1);\n+          unroll_2(Rj, &MontgomeryMultiplyGenerator::extra_step_squaring);\n+        } block_comment(\"  } \/\/ j\");\n+\n+        post2(Ri, Rlen);\n+        addi(Ri, Ri, 1);\n+        slli(t0, Rlen, 1);\n+        blt(Ri, t0, loop);\n+\n+        bind(end);\n+        block_comment(\"} \/\/ i\");\n+      }\n+\n+      normalize(Rlen);\n+\n+      mv(Ra, Pm_base);  \/\/ Save Pm_base in Ra\n+      restore_regs();  \/\/ Restore caller's Pm_base\n+\n+      \/\/ Copy our result into caller's Pm_base\n+      reverse(Pm_base, Ra, Rlen, Ri, Rj);\n+\n+      leave();\n+      ret();\n+\n+      return entry;\n+    }\n+  };\n+#endif \/\/ COMPILER2\n+\n+  \/\/ Continuation point for throwing of implicit exceptions that are\n+  \/\/ not handled in the current activation. Fabricates an exception\n+  \/\/ oop and initiates normal exception dispatching in this\n+  \/\/ frame. Since we need to preserve callee-saved values (currently\n+  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+  \/\/ map and therefore have to make these stubs into RuntimeStubs\n+  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+  \/\/ be preserved between the fault point and the exception handler\n+  \/\/ then it must assume responsibility for that in\n+  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n+  \/\/ implicit exceptions (e.g., NullPointerException or\n+  \/\/ AbstractMethodError on entry) are either at call sites or\n+  \/\/ otherwise assume that stack unwinding will be initiated, so\n+  \/\/ caller saved registers were assumed volatile in the compiler.\n+\n+#undef __\n+#define __ masm->\n+\n+  address generate_throw_exception(const char* name,\n+                                   address runtime_entry,\n+                                   Register arg1 = noreg,\n+                                   Register arg2 = noreg) {\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. riscv asserts that frame::arg_reg_save_area_bytes == 0\n+    assert_cond(runtime_entry != NULL);\n+    enum layout {\n+      fp_off = 0,\n+      fp_off2,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    const int insts_size = 512;\n+    const int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    assert_cond(oop_maps != NULL && masm != NULL);\n+\n+    address start = __ pc();\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and RA before call\n+\n+    assert(is_even(framesize \/ 2), \"sp not 16-byte aligned\");\n+\n+    \/\/ ra and fp are already in place\n+    __ addi(sp, fp, 0 - ((unsigned)framesize << LogBytesPerInt)); \/\/ prolog\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, fp, the_pc, t0);\n+\n+    \/\/ Call runtime\n+    if (arg1 != noreg) {\n+      assert(arg2 != c_rarg1, \"clobbered\");\n+      __ mv(c_rarg1, arg1);\n+    }\n+    if (arg2 != noreg) {\n+      __ mv(c_rarg2, arg2);\n+    }\n+    __ mv(c_rarg0, xthread);\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    int32_t offset = 0;\n+    __ movptr_with_offset(t0, runtime_entry, offset);\n+    __ jalr(x1, t0, offset);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+    assert_cond(map != NULL);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(true);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+#ifdef ASSERT\n+    Label L;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ bnez(t0, L);\n+    __ should_not_reach_here();\n+    __ bind(L);\n+#endif \/\/ ASSERT\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name,\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    assert(stub != NULL, \"create runtime stub fail!\");\n+    return stub->entry_point();\n+  }\n+\n+  \/\/ Initialization\n+  void generate_initial() {\n+    \/\/ Generate initial stubs and initializes the entry points\n+\n+    \/\/ entry points that exist in all platforms Note: This is code\n+    \/\/ that could be shared among different platforms - however the\n+    \/\/ benefit seems to be smaller than the disadvantage of having a\n+    \/\/ much more complicated generator structure. See also comment in\n+    \/\/ stubRoutines.hpp.\n+\n+    StubRoutines::_forward_exception_entry = generate_forward_exception();\n+\n+    StubRoutines::_call_stub_entry =\n+      generate_call_stub(StubRoutines::_call_stub_return_address);\n+\n+    \/\/ is referenced by megamorphic call\n+    StubRoutines::_catch_exception_entry = generate_catch_exception();\n+\n+    \/\/ Build this early so it's available for the interpreter.\n+    StubRoutines::_throw_StackOverflowError_entry =\n+      generate_throw_exception(\"StackOverflowError throw_exception\",\n+                               CAST_FROM_FN_PTR(address,\n+                                                SharedRuntime::throw_StackOverflowError));\n+    StubRoutines::_throw_delayed_StackOverflowError_entry =\n+      generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n+                               CAST_FROM_FN_PTR(address,\n+                                                SharedRuntime::throw_delayed_StackOverflowError));\n+    \/\/ Safefetch stubs.\n+    generate_safefetch(\"SafeFetch32\", sizeof(int),     &StubRoutines::_safefetch32_entry,\n+                                                       &StubRoutines::_safefetch32_fault_pc,\n+                                                       &StubRoutines::_safefetch32_continuation_pc);\n+    generate_safefetch(\"SafeFetchN\", sizeof(intptr_t), &StubRoutines::_safefetchN_entry,\n+                                                       &StubRoutines::_safefetchN_fault_pc,\n+                                                       &StubRoutines::_safefetchN_continuation_pc);\n+  }\n+\n+  void generate_all() {\n+    \/\/ support for verify_oop (must happen after universe_init)\n+    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n+    StubRoutines::_throw_AbstractMethodError_entry =\n+      generate_throw_exception(\"AbstractMethodError throw_exception\",\n+                               CAST_FROM_FN_PTR(address,\n+                                                SharedRuntime::\n+                                                throw_AbstractMethodError));\n+\n+    StubRoutines::_throw_IncompatibleClassChangeError_entry =\n+      generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n+                               CAST_FROM_FN_PTR(address,\n+                                                SharedRuntime::\n+                                                throw_IncompatibleClassChangeError));\n+\n+    StubRoutines::_throw_NullPointerException_at_call_entry =\n+      generate_throw_exception(\"NullPointerException at call throw_exception\",\n+                               CAST_FROM_FN_PTR(address,\n+                                                SharedRuntime::\n+                                                throw_NullPointerException_at_call));\n+    \/\/ arraycopy stubs used by compilers\n+    generate_arraycopy_stubs();\n+\n+#ifdef COMPILER2\n+    if (UseMulAddIntrinsic) {\n+      StubRoutines::_mulAdd = generate_mulAdd();\n+    }\n+\n+    if (UseMultiplyToLenIntrinsic) {\n+      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n+    }\n+\n+    if (UseSquareToLenIntrinsic) {\n+      StubRoutines::_squareToLen = generate_squareToLen();\n+    }\n+\n+    if (UseMontgomeryMultiplyIntrinsic) {\n+      StubCodeMark mark(this, \"StubRoutines\", \"montgomeryMultiply\");\n+      MontgomeryMultiplyGenerator g(_masm, \/*squaring*\/false);\n+      StubRoutines::_montgomeryMultiply = g.generate_multiply();\n+    }\n+\n+    if (UseMontgomerySquareIntrinsic) {\n+      StubCodeMark mark(this, \"StubRoutines\", \"montgomerySquare\");\n+      MontgomeryMultiplyGenerator g(_masm, \/*squaring*\/true);\n+      StubRoutines::_montgomerySquare = g.generate_square();\n+    }\n+#endif\n+\n+    generate_compare_long_strings();\n+\n+    generate_string_indexof_stubs();\n+\n+    StubRoutines::riscv::set_completed();\n+  }\n+\n+ public:\n+  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n+    if (all) {\n+      generate_all();\n+    } else {\n+      generate_initial();\n+    }\n+  }\n+\n+  ~StubGenerator() {}\n+}; \/\/ end class declaration\n+\n+void StubGenerator_generate(CodeBuffer* code, bool all) {\n+  StubGenerator g(code, all);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":3743,"deletions":0,"binary":false,"changes":3743,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Implementation of the platform-specific part of StubRoutines - for\n+\/\/ a description of how to extend it, see the stubRoutines.hpp file.\n+\n+address StubRoutines::riscv::_get_previous_sp_entry = NULL;\n+\n+address StubRoutines::riscv::_f2i_fixup = NULL;\n+address StubRoutines::riscv::_f2l_fixup = NULL;\n+address StubRoutines::riscv::_d2i_fixup = NULL;\n+address StubRoutines::riscv::_d2l_fixup = NULL;\n+address StubRoutines::riscv::_float_sign_mask = NULL;\n+address StubRoutines::riscv::_float_sign_flip = NULL;\n+address StubRoutines::riscv::_double_sign_mask = NULL;\n+address StubRoutines::riscv::_double_sign_flip = NULL;\n+address StubRoutines::riscv::_zero_blocks = NULL;\n+address StubRoutines::riscv::_compare_long_string_LL = NULL;\n+address StubRoutines::riscv::_compare_long_string_UU = NULL;\n+address StubRoutines::riscv::_compare_long_string_LU = NULL;\n+address StubRoutines::riscv::_compare_long_string_UL = NULL;\n+address StubRoutines::riscv::_string_indexof_linear_ll = NULL;\n+address StubRoutines::riscv::_string_indexof_linear_uu = NULL;\n+address StubRoutines::riscv::_string_indexof_linear_ul = NULL;\n+address StubRoutines::riscv::_large_byte_array_inflate = NULL;\n+\n+bool StubRoutines::riscv::_completed = false;\n","filename":"src\/hotspot\/cpu\/riscv\/stubRoutines_riscv.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_STUBROUTINES_RISCV_HPP\n+#define CPU_RISCV_STUBROUTINES_RISCV_HPP\n+\n+\/\/ This file holds the platform specific parts of the StubRoutines\n+\/\/ definition. See stubRoutines.hpp for a description on how to\n+\/\/ extend it.\n+\n+static bool returns_to_call_stub(address return_pc) {\n+  return return_pc == _call_stub_return_address;\n+}\n+\n+enum platform_dependent_constants {\n+  code_size1 = 19000,          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 28000           \/\/ simply increase if too small (assembler will crash if too small)\n+};\n+\n+class riscv {\n+ friend class StubGenerator;\n+\n+ private:\n+  static address _get_previous_sp_entry;\n+\n+  static address _f2i_fixup;\n+  static address _f2l_fixup;\n+  static address _d2i_fixup;\n+  static address _d2l_fixup;\n+\n+  static address _float_sign_mask;\n+  static address _float_sign_flip;\n+  static address _double_sign_mask;\n+  static address _double_sign_flip;\n+\n+  static address _zero_blocks;\n+\n+  static address _compare_long_string_LL;\n+  static address _compare_long_string_LU;\n+  static address _compare_long_string_UL;\n+  static address _compare_long_string_UU;\n+  static address _string_indexof_linear_ll;\n+  static address _string_indexof_linear_uu;\n+  static address _string_indexof_linear_ul;\n+  static address _large_byte_array_inflate;\n+\n+  static bool _completed;\n+\n+ public:\n+\n+  static address get_previous_sp_entry() {\n+    return _get_previous_sp_entry;\n+  }\n+\n+  static address f2i_fixup() {\n+    return _f2i_fixup;\n+  }\n+\n+  static address f2l_fixup() {\n+    return _f2l_fixup;\n+  }\n+\n+  static address d2i_fixup() {\n+    return _d2i_fixup;\n+  }\n+\n+  static address d2l_fixup() {\n+    return _d2l_fixup;\n+  }\n+\n+  static address float_sign_mask() {\n+    return _float_sign_mask;\n+  }\n+\n+  static address float_sign_flip() {\n+    return _float_sign_flip;\n+  }\n+\n+  static address double_sign_mask() {\n+    return _double_sign_mask;\n+  }\n+\n+  static address double_sign_flip() {\n+    return _double_sign_flip;\n+  }\n+\n+  static address zero_blocks() {\n+    return _zero_blocks;\n+  }\n+\n+  static address compare_long_string_LL() {\n+    return _compare_long_string_LL;\n+  }\n+\n+  static address compare_long_string_LU() {\n+    return _compare_long_string_LU;\n+  }\n+\n+  static address compare_long_string_UL() {\n+    return _compare_long_string_UL;\n+  }\n+\n+  static address compare_long_string_UU() {\n+    return _compare_long_string_UU;\n+  }\n+\n+  static address string_indexof_linear_ul() {\n+    return _string_indexof_linear_ul;\n+  }\n+\n+  static address string_indexof_linear_ll() {\n+    return _string_indexof_linear_ll;\n+  }\n+\n+  static address string_indexof_linear_uu() {\n+    return _string_indexof_linear_uu;\n+  }\n+\n+  static address large_byte_array_inflate() {\n+    return _large_byte_array_inflate;\n+  }\n+\n+  static bool complete() {\n+    return _completed;\n+  }\n+\n+  static void set_completed() {\n+    _completed = true;\n+  }\n+};\n+\n+#endif \/\/ CPU_RISCV_STUBROUTINES_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/stubRoutines_riscv.hpp","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"},{"patch":"@@ -0,0 +1,1833 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"interpreter\/bytecodeHistogram.hpp\"\n+#include \"interpreter\/bytecodeTracer.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interpreterRuntime.hpp\"\n+#include \"interpreter\/templateInterpreterGenerator.hpp\"\n+#include \"interpreter\/templateTable.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/jvmtiThreadState.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include <sys\/types.h>\n+\n+#ifndef PRODUCT\n+#include \"oops\/method.hpp\"\n+#endif \/\/ !PRODUCT\n+\n+\/\/ Size of interpreter code.  Increase if too small.  Interpreter will\n+\/\/ fail with a guarantee (\"not enough space for interpreter generation\");\n+\/\/ if too small.\n+\/\/ Run with +PrintInterpreter to get the VM to print out the size.\n+\/\/ Max size with JVMTI\n+int TemplateInterpreter::InterpreterCodeSize = 256 * 1024;\n+\n+#define __ _masm->\n+\n+\/\/-----------------------------------------------------------------------------\n+\n+address TemplateInterpreterGenerator::generate_slow_signature_handler() {\n+  address entry = __ pc();\n+\n+  __ andi(esp, esp, -16);\n+  __ mv(c_rarg3, esp);\n+  \/\/ xmethod\n+  \/\/ xlocals\n+  \/\/ c_rarg3: first stack arg - wordSize\n+  \/\/ adjust sp\n+\n+  __ addi(sp, c_rarg3, -18 * wordSize);\n+  __ addi(sp, sp, -2 * wordSize);\n+  __ sd(ra, Address(sp, 0));\n+\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address,\n+                              InterpreterRuntime::slow_signature_handler),\n+             xmethod, xlocals, c_rarg3);\n+\n+  \/\/ x10: result handler\n+\n+  \/\/ Stack layout:\n+  \/\/ sp: return address           <- sp\n+  \/\/      1 garbage\n+  \/\/      8 integer args (if static first is unused)\n+  \/\/      1 float\/double identifiers\n+  \/\/      8 double args\n+  \/\/        stack args              <- esp\n+  \/\/        garbage\n+  \/\/        expression stack bottom\n+  \/\/        bcp (NULL)\n+  \/\/        ...\n+\n+  \/\/ Restore ra\n+  __ ld(ra, Address(sp, 0));\n+  __ addi(sp, sp , 2 * wordSize);\n+\n+  \/\/ Do FP first so we can use c_rarg3 as temp\n+  __ lwu(c_rarg3, Address(sp, 9 * wordSize)); \/\/ float\/double identifiers\n+\n+  for (int i = 0; i < Argument::n_float_register_parameters_c; i++) {\n+    const FloatRegister r = g_FPArgReg[i];\n+    Label d, done;\n+\n+    __ andi(t0, c_rarg3, 1UL << i);\n+    __ bnez(t0, d);\n+    __ flw(r, Address(sp, (10 + i) * wordSize));\n+    __ j(done);\n+    __ bind(d);\n+    __ fld(r, Address(sp, (10 + i) * wordSize));\n+    __ bind(done);\n+  }\n+\n+  \/\/ c_rarg0 contains the result from the call of\n+  \/\/ InterpreterRuntime::slow_signature_handler so we don't touch it\n+  \/\/ here.  It will be loaded with the JNIEnv* later.\n+  for (int i = 1; i < Argument::n_int_register_parameters_c; i++) {\n+    const Register rm = g_INTArgReg[i];\n+    __ ld(rm, Address(sp, i * wordSize));\n+  }\n+\n+  __ addi(sp, sp, 18 * wordSize);\n+  __ ret();\n+\n+  return entry;\n+}\n+\n+\/\/ Various method entries\n+address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {\n+  \/\/ xmethod: Method*\n+  \/\/ x30: sender sp\n+  \/\/ esp: args\n+\n+  if (!InlineIntrinsics) {\n+    return NULL; \/\/ Generate a vanilla entry\n+  }\n+\n+  \/\/ These don't need a safepoint check because they aren't virtually\n+  \/\/ callable. We won't enter these intrinsics from compiled code.\n+  \/\/ If in the future we added an intrinsic which was virtually callable\n+  \/\/ we'd have to worry about how to safepoint so that this code is used.\n+\n+  \/\/ mathematical functions inlined by compiler\n+  \/\/ (interpreter must provide identical implementation\n+  \/\/ in order to avoid monotonicity bugs when switching\n+  \/\/ from interpreter to compiler in the middle of some\n+  \/\/ computation)\n+  \/\/\n+  \/\/ stack:\n+  \/\/        [ arg ] <-- esp\n+  \/\/        [ arg ]\n+  \/\/ retaddr in ra\n+\n+  address fn = NULL;\n+  address entry_point = NULL;\n+  Register continuation = ra;\n+  switch (kind) {\n+    case Interpreter::java_lang_math_abs:\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ fabs_d(f10, f10);\n+      __ mv(sp, x30); \/\/ Restore caller's SP\n+      break;\n+    case Interpreter::java_lang_math_sqrt:\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ fsqrt_d(f10, f10);\n+      __ mv(sp, x30);\n+      break;\n+    case Interpreter::java_lang_math_sin :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dsin() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dsin);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dsin());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_cos :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dcos() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dcos);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dcos());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_tan :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dtan() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dtan);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dtan());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_log :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dlog() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dlog);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dlog());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_log10 :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dlog10() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dlog10);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dlog10());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_exp :\n+      entry_point = __ pc();\n+      __ fld(f10, Address(esp));\n+      __ mv(sp, x30);\n+      __ mv(x9, ra);\n+      continuation = x9;  \/\/ The first callee-saved register\n+      if (StubRoutines::dexp() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dexp);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dexp());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_pow :\n+      entry_point = __ pc();\n+      __ mv(x9, ra);\n+      continuation = x9;\n+      __ fld(f10, Address(esp, 2 * Interpreter::stackElementSize));\n+      __ fld(f11, Address(esp));\n+      __ mv(sp, x30);\n+      if (StubRoutines::dpow() == NULL) {\n+        fn = CAST_FROM_FN_PTR(address, SharedRuntime::dpow);\n+      } else {\n+        fn = CAST_FROM_FN_PTR(address, StubRoutines::dpow());\n+      }\n+      __ mv(t0, fn);\n+      __ jalr(t0);\n+      break;\n+    case Interpreter::java_lang_math_fmaD :\n+      if (UseFMA) {\n+        entry_point = __ pc();\n+        __ fld(f10, Address(esp, 4 * Interpreter::stackElementSize));\n+        __ fld(f11, Address(esp, 2 * Interpreter::stackElementSize));\n+        __ fld(f12, Address(esp));\n+        __ fmadd_d(f10, f10, f11, f12);\n+        __ mv(sp, x30); \/\/ Restore caller's SP\n+      }\n+      break;\n+    case Interpreter::java_lang_math_fmaF :\n+      if (UseFMA) {\n+        entry_point = __ pc();\n+        __ flw(f10, Address(esp, 2 * Interpreter::stackElementSize));\n+        __ flw(f11, Address(esp, Interpreter::stackElementSize));\n+        __ flw(f12, Address(esp));\n+        __ fmadd_s(f10, f10, f11, f12);\n+        __ mv(sp, x30); \/\/ Restore caller's SP\n+      }\n+      break;\n+    default:\n+      ;\n+  }\n+  if (entry_point != NULL) {\n+    __ jr(continuation);\n+  }\n+\n+  return entry_point;\n+}\n+\n+\/\/ Abstract method entry\n+\/\/ Attempt to execute abstract method. Throw exception\n+address TemplateInterpreterGenerator::generate_abstract_entry(void) {\n+  \/\/ xmethod: Method*\n+  \/\/ x30: sender SP\n+\n+  address entry_point = __ pc();\n+\n+  \/\/ abstract method entry\n+\n+  \/\/  pop return address, reset last_sp to NULL\n+  __ empty_expression_stack();\n+  __ restore_bcp();      \/\/ bcp must be correct for exception handler   (was destroyed)\n+  __ restore_locals();   \/\/ make sure locals pointer is correct as well (was destroyed)\n+\n+  \/\/ throw exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                     InterpreterRuntime::throw_AbstractMethodErrorWithMethod),\n+                                     xmethod);\n+  \/\/ the call_VM checks for exception, so we should never return here.\n+  __ should_not_reach_here();\n+\n+  return entry_point;\n+}\n+\n+address TemplateInterpreterGenerator::generate_StackOverflowError_handler() {\n+  address entry = __ pc();\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    __ ld(t0, Address(fp, frame::interpreter_frame_monitor_block_top_offset * wordSize));\n+    __ mv(t1, sp);\n+    \/\/ maximal sp for current fp (stack grows negative)\n+    \/\/ check if frame is complete\n+    __ bge(t0, t1, L);\n+    __ stop (\"interpreter frame not set up\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+  \/\/ Restore bcp under the assumption that the current frame is still\n+  \/\/ interpreted\n+  __ restore_bcp();\n+\n+  \/\/ expression stack must be empty before entering the VM if an\n+  \/\/ exception happened\n+  __ empty_expression_stack();\n+  \/\/ throw exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_StackOverflowError));\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_ArrayIndexOutOfBounds_handler() {\n+  address entry = __ pc();\n+  \/\/ expression stack must be empty before entering the VM if an\n+  \/\/ exception happened\n+  __ empty_expression_stack();\n+  \/\/ setup parameters\n+\n+  \/\/ convention: expect aberrant index in register x11\n+  __ zero_extend(c_rarg2, x11, 32);\n+  \/\/ convention: expect array in register x13\n+  __ mv(c_rarg1, x13);\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address,\n+                              InterpreterRuntime::\n+                              throw_ArrayIndexOutOfBoundsException),\n+             c_rarg1, c_rarg2);\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_ClassCastException_handler() {\n+  address entry = __ pc();\n+\n+  \/\/ object is at TOS\n+  __ pop_reg(c_rarg1);\n+\n+  \/\/ expression stack must be empty before entering the VM if an\n+  \/\/ exception happened\n+  __ empty_expression_stack();\n+\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address,\n+                              InterpreterRuntime::\n+                              throw_ClassCastException),\n+             c_rarg1);\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_exception_handler_common(\n+  const char* name, const char* message, bool pass_oop) {\n+  assert(!pass_oop || message == NULL, \"either oop or message but not both\");\n+  address entry = __ pc();\n+  if (pass_oop) {\n+    \/\/ object is at TOS\n+    __ pop_reg(c_rarg2);\n+  }\n+  \/\/ expression stack must be empty before entering the VM if an\n+  \/\/ exception happened\n+  __ empty_expression_stack();\n+  \/\/ setup parameters\n+  __ la(c_rarg1, Address((address)name));\n+  if (pass_oop) {\n+    __ call_VM(x10, CAST_FROM_FN_PTR(address,\n+                                     InterpreterRuntime::\n+                                     create_klass_exception),\n+               c_rarg1, c_rarg2);\n+  } else {\n+    \/\/ kind of lame ExternalAddress can't take NULL because\n+    \/\/ external_word_Relocation will assert.\n+    if (message != NULL) {\n+      __ la(c_rarg2, Address((address)message));\n+    } else {\n+      __ mv(c_rarg2, NULL_WORD);\n+    }\n+    __ call_VM(x10,\n+               CAST_FROM_FN_PTR(address, InterpreterRuntime::create_exception),\n+               c_rarg1, c_rarg2);\n+  }\n+  \/\/ throw exception\n+  __ j(address(Interpreter::throw_exception_entry()));\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_return_entry_for(TosState state, int step, size_t index_size) {\n+  address entry = __ pc();\n+\n+  \/\/ Restore stack bottom in case i2c adjusted stack\n+  __ ld(esp, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  \/\/ and NULL it as marker that esp is now tos until next java call\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ restore_bcp();\n+  __ restore_locals();\n+  __ restore_constant_pool_cache();\n+  __ get_method(xmethod);\n+\n+  if (state == atos) {\n+    Register obj = x10;\n+    Register mdp = x11;\n+    Register tmp = x12;\n+    __ ld(mdp, Address(xmethod, Method::method_data_offset()));\n+    __ profile_return_type(mdp, obj, tmp);\n+  }\n+\n+  \/\/ Pop N words from the stack\n+  __ get_cache_and_index_at_bcp(x11, x12, 1, index_size);\n+  __ ld(x11, Address(x11, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n+  __ andi(x11, x11, ConstantPoolCacheEntry::parameter_size_mask);\n+\n+  __ shadd(esp, x11, esp, t0, 3);\n+\n+  \/\/ Restore machine SP\n+  __ ld(t0, Address(xmethod, Method::const_offset()));\n+  __ lhu(t0, Address(t0, ConstMethod::max_stack_offset()));\n+  __ addi(t0, t0, frame::interpreter_frame_monitor_size() + 2);\n+  __ ld(t1,\n+        Address(fp, frame::interpreter_frame_initial_sp_offset * wordSize));\n+  __ slli(t0, t0, 3);\n+  __ sub(t0, t1, t0);\n+  __ andi(sp, t0, -16);\n+\n+ __ check_and_handle_popframe(xthread);\n+ __ check_and_handle_earlyret(xthread);\n+\n+  __ get_dispatch();\n+  __ dispatch_next(state, step);\n+\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_deopt_entry_for(TosState state,\n+                                                               int step,\n+                                                               address continuation) {\n+  address entry = __ pc();\n+  __ restore_bcp();\n+  __ restore_locals();\n+  __ restore_constant_pool_cache();\n+  __ get_method(xmethod);\n+  __ get_dispatch();\n+\n+  \/\/ Calculate stack limit\n+  __ ld(t0, Address(xmethod, Method::const_offset()));\n+  __ lhu(t0, Address(t0, ConstMethod::max_stack_offset()));\n+  __ addi(t0, t0, frame::interpreter_frame_monitor_size() + 2);\n+  __ ld(t1, Address(fp, frame::interpreter_frame_initial_sp_offset * wordSize));\n+  __ slli(t0, t0, 3);\n+  __ sub(t0, t1, t0);\n+  __ andi(sp, t0, -16);\n+\n+  \/\/ Restore expression stack pointer\n+  __ ld(esp, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  \/\/ NULL last_sp until next java call\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+\n+  \/\/ handle exceptions\n+  {\n+    Label L;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ beqz(t0, L);\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_pending_exception));\n+    __ should_not_reach_here();\n+    __ bind(L);\n+  }\n+\n+  if (continuation == NULL) {\n+    __ dispatch_next(state, step);\n+  } else {\n+    __ jump_to_entry(continuation);\n+  }\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_result_handler_for(BasicType type) {\n+  address entry = __ pc();\n+  if (type == T_OBJECT) {\n+    \/\/ retrieve result from frame\n+    __ ld(x10, Address(fp, frame::interpreter_frame_oop_temp_offset * wordSize));\n+    \/\/ and verify it\n+    __ verify_oop(x10);\n+  } else {\n+   __ cast_primitive_type(type, x10);\n+  }\n+\n+  __ ret();                                  \/\/ return from result handler\n+  return entry;\n+}\n+\n+address TemplateInterpreterGenerator::generate_safept_entry_for(TosState state,\n+                                                                address runtime_entry) {\n+  assert_cond(runtime_entry != NULL);\n+  address entry = __ pc();\n+  __ push(state);\n+  __ call_VM(noreg, runtime_entry);\n+  __ fence(0xf, 0xf);\n+  __ dispatch_via(vtos, Interpreter::_normal_table.table_for(vtos));\n+  return entry;\n+}\n+\n+\/\/ Helpers for commoning out cases in the various type of method entries.\n+\/\/\n+\n+\n+\/\/ increment invocation count & check for overflow\n+\/\/\n+\/\/ Note: checking for negative value instead of overflow\n+\/\/       so we have a 'sticky' overflow test\n+\/\/\n+\/\/ xmethod: method\n+\/\/\n+void TemplateInterpreterGenerator::generate_counter_incr(\n+        Label* overflow,\n+        Label* profile_method,\n+        Label* profile_method_continue) {\n+  Label done;\n+  \/\/ Note: In tiered we increment either counters in Method* or in MDO depending if we're profiling or not.\n+  if (TieredCompilation) {\n+    int increment = InvocationCounter::count_increment;\n+    Label no_mdo;\n+    if (ProfileInterpreter) {\n+      \/\/ Are we profiling?\n+      __ ld(x10, Address(xmethod, Method::method_data_offset()));\n+      __ beqz(x10, no_mdo);\n+      \/\/ Increment counter in the MDO\n+      const Address mdo_invocation_counter(x10, in_bytes(MethodData::invocation_counter_offset()) +\n+                                                in_bytes(InvocationCounter::counter_offset()));\n+      const Address mask(x10, in_bytes(MethodData::invoke_mask_offset()));\n+      __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, t0, t1, false, overflow);\n+      __ j(done);\n+    }\n+    __ bind(no_mdo);\n+    \/\/ Increment counter in MethodCounters\n+    const Address invocation_counter(t1,\n+                  MethodCounters::invocation_counter_offset() +\n+                  InvocationCounter::counter_offset());\n+    __ get_method_counters(xmethod, t1, done);\n+    const Address mask(t1, in_bytes(MethodCounters::invoke_mask_offset()));\n+    __ increment_mask_and_jump(invocation_counter, increment, mask, t0, x11, false, overflow);\n+    __ bind(done);\n+  } else { \/\/ not TieredCompilation\n+    const Address backedge_counter(t1,\n+                  MethodCounters::backedge_counter_offset() +\n+                  InvocationCounter::counter_offset());\n+    const Address invocation_counter(t1,\n+                  MethodCounters::invocation_counter_offset() +\n+                  InvocationCounter::counter_offset());\n+\n+    __ get_method_counters(xmethod, t1, done);\n+\n+    if (ProfileInterpreter) { \/\/ %%% Merge this into MethodData*\n+      __ lwu(x11, Address(t1, MethodCounters::interpreter_invocation_counter_offset()));\n+      __ addw(x11, x11, 1);\n+      __ sw(x11, Address(t1, MethodCounters::interpreter_invocation_counter_offset()));\n+    }\n+    \/\/ Update standard invocation counters\n+    __ lwu(x11, invocation_counter);\n+    __ lwu(x10, backedge_counter);\n+\n+    __ addw(x11, x11, InvocationCounter::count_increment);\n+    __ andi(x10, x10, InvocationCounter::count_mask_value);\n+\n+    __ sw(x11, invocation_counter);\n+    __ addw(x10, x10, x11);                \/\/ add both counters\n+\n+    \/\/ profile_method is non-null only for interpreted method so\n+    \/\/ profile_method != NULL == !native_call\n+\n+    if (ProfileInterpreter && profile_method != NULL) {\n+      \/\/ Test to see if we should create a method data oop\n+      __ ld(t1, Address(xmethod, Method::method_counters_offset()));\n+      __ lwu(t1, Address(t1, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n+      __ blt(x10, t1, *profile_method_continue);\n+\n+      \/\/ if no method data exists, go to profile_method\n+      __ test_method_data_pointer(t1, *profile_method);\n+    }\n+\n+    {\n+      __ ld(t1, Address(xmethod, Method::method_counters_offset()));\n+      __ lwu(t1, Address(t1, in_bytes(MethodCounters::interpreter_invocation_limit_offset())));\n+      __ bltu(x10, t1, done);\n+      __ j(*overflow);\n+    }\n+    __ bind(done);\n+  }\n+}\n+\n+void TemplateInterpreterGenerator::generate_counter_overflow(Label& do_continue) {\n+  __ mv(c_rarg1, zr);\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), c_rarg1);\n+  __ j(do_continue);\n+}\n+\n+\/\/ See if we've got enough room on the stack for locals plus overhead\n+\/\/ below JavaThread::stack_overflow_limit(). If not, throw a StackOverflowError\n+\/\/ without going through the signal handler, i.e., reserved and yellow zones\n+\/\/ will not be made usable. The shadow zone must suffice to handle the\n+\/\/ overflow.\n+\/\/ The expression stack grows down incrementally, so the normal guard\n+\/\/ page mechanism will work for that.\n+\/\/\n+\/\/ NOTE: Since the additional locals are also always pushed (wasn't\n+\/\/ obvious in generate_method_entry) so the guard should work for them\n+\/\/ too.\n+\/\/\n+\/\/ Args:\n+\/\/      x13: number of additional locals this frame needs (what we must check)\n+\/\/      xmethod: Method*\n+\/\/\n+\/\/ Kills:\n+\/\/      x10\n+void TemplateInterpreterGenerator::generate_stack_overflow_check(void) {\n+\n+  \/\/ monitor entry size: see picture of stack set\n+  \/\/ (generate_method_entry) and frame_amd64.hpp\n+  const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+\n+  \/\/ total overhead size: entry_size + (saved fp through expr stack\n+  \/\/ bottom).  be sure to change this if you add\/subtract anything\n+  \/\/ to\/from the overhead area\n+  const int overhead_size =\n+    -(frame::interpreter_frame_initial_sp_offset * wordSize) + entry_size;\n+\n+  const int page_size = os::vm_page_size();\n+\n+  Label after_frame_check;\n+\n+  \/\/ see if the frame is greater than one page in size. If so,\n+  \/\/ then we need to verify there is enough stack space remaining\n+  \/\/ for the additional locals.\n+  __ mv(t0, (page_size - overhead_size) \/ Interpreter::stackElementSize);\n+  __ bleu(x13, t0, after_frame_check);\n+\n+  \/\/ compute sp as if this were going to be the last frame on\n+  \/\/ the stack before the red zone\n+\n+  \/\/ locals + overhead, in bytes\n+  __ mv(x10, overhead_size);\n+  __ shadd(x10, x13, x10, t0, Interpreter::logStackElementSize);  \/\/ 2 slots per parameter.\n+\n+  const Address stack_limit(xthread, JavaThread::stack_overflow_limit_offset());\n+  __ ld(t0, stack_limit);\n+\n+#ifdef ASSERT\n+  Label limit_okay;\n+  \/\/ Verify that thread stack limit is non-zero.\n+  __ bnez(t0, limit_okay);\n+  __ stop(\"stack overflow limit is zero\");\n+  __ bind(limit_okay);\n+#endif\n+\n+  \/\/ Add stack limit to locals.\n+  __ add(x10, x10, t0);\n+\n+  \/\/ Check against the current stack bottom.\n+  __ bgtu(sp, x10, after_frame_check);\n+\n+  \/\/ Remove the incoming args, peeling the machine SP back to where it\n+  \/\/ was in the caller.  This is not strictly necessary, but unless we\n+  \/\/ do so the stack frame may have a garbage FP; this ensures a\n+  \/\/ correct call stack that we can always unwind.  The ANDI should be\n+  \/\/ unnecessary because the sender SP in x30 is always aligned, but\n+  \/\/ it doesn't hurt.\n+  __ andi(sp, x30, -16);\n+\n+  \/\/ Note: the restored frame is not necessarily interpreted.\n+  \/\/ Use the shared runtime version of the StackOverflowError.\n+  assert(StubRoutines::throw_StackOverflowError_entry() != NULL, \"stub not yet generated\");\n+  __ far_jump(RuntimeAddress(StubRoutines::throw_StackOverflowError_entry()));\n+\n+  \/\/ all done with frame size check\n+  __ bind(after_frame_check);\n+}\n+\n+\/\/ Allocate monitor and lock method (asm interpreter)\n+\/\/\n+\/\/ Args:\n+\/\/      xmethod: Method*\n+\/\/      xlocals: locals\n+\/\/\n+\/\/ Kills:\n+\/\/      x10\n+\/\/      c_rarg0, c_rarg1, c_rarg2, c_rarg3, ...(param regs)\n+\/\/      t0, t1 (temporary regs)\n+void TemplateInterpreterGenerator::lock_method() {\n+  \/\/ synchronize method\n+  const Address access_flags(xmethod, Method::access_flags_offset());\n+  const Address monitor_block_top(fp, frame::interpreter_frame_monitor_block_top_offset * wordSize);\n+  const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+\n+#ifdef ASSERT\n+  __ lwu(x10, access_flags);\n+  __ verify_access_flags(x10, JVM_ACC_SYNCHRONIZED, \"method doesn't need synchronization\", false);\n+#endif \/\/ ASSERT\n+\n+  \/\/ get synchronization object\n+  {\n+    Label done;\n+    __ lwu(x10, access_flags);\n+    __ andi(t0, x10, JVM_ACC_STATIC);\n+    \/\/ get receiver (assume this is frequent case)\n+    __ ld(x10, Address(xlocals, Interpreter::local_offset_in_bytes(0)));\n+    __ beqz(t0, done);\n+    __ load_mirror(x10, xmethod);\n+\n+#ifdef ASSERT\n+    {\n+      Label L;\n+      __ bnez(x10, L);\n+      __ stop(\"synchronization object is NULL\");\n+      __ bind(L);\n+    }\n+#endif \/\/ ASSERT\n+\n+    __ bind(done);\n+  }\n+\n+  \/\/ add space for monitor & lock\n+  __ add(sp, sp, - entry_size); \/\/ add space for a monitor entry\n+  __ add(esp, esp, - entry_size);\n+  __ mv(t0, esp);\n+  __ sd(t0, monitor_block_top);  \/\/ set new monitor block top\n+  \/\/ store object\n+  __ sd(x10, Address(esp, BasicObjectLock::obj_offset_in_bytes()));\n+  __ mv(c_rarg1, esp); \/\/ object address\n+  __ lock_object(c_rarg1);\n+}\n+\n+\/\/ Generate a fixed interpreter frame. This is identical setup for\n+\/\/ interpreted methods and for native methods hence the shared code.\n+\/\/\n+\/\/ Args:\n+\/\/      ra: return address\n+\/\/      xmethod: Method*\n+\/\/      xlocals: pointer to locals\n+\/\/      xcpool: cp cache\n+\/\/      stack_pointer: previous sp\n+void TemplateInterpreterGenerator::generate_fixed_frame(bool native_call) {\n+  \/\/ initialize fixed part of activation frame\n+  if (native_call) {\n+    __ add(esp, sp, - 14 * wordSize);\n+    __ mv(xbcp, zr);\n+    __ add(sp, sp, - 14 * wordSize);\n+    \/\/ add 2 zero-initialized slots for native calls\n+    __ sd(zr, Address(sp, 13 * wordSize));\n+    __ sd(zr, Address(sp, 12 * wordSize));\n+  } else {\n+    __ add(esp, sp, - 12 * wordSize);\n+    __ ld(t0, Address(xmethod, Method::const_offset()));     \/\/ get ConstMethod\n+    __ add(xbcp, t0, in_bytes(ConstMethod::codes_offset())); \/\/ get codebase\n+    __ add(sp, sp, - 12 * wordSize);\n+  }\n+  __ sd(xbcp, Address(sp, wordSize));\n+  __ sd(esp, Address(sp, 0));\n+\n+  if (ProfileInterpreter) {\n+    Label method_data_continue;\n+    __ ld(t0, Address(xmethod, Method::method_data_offset()));\n+    __ beqz(t0, method_data_continue);\n+    __ la(t0, Address(t0, in_bytes(MethodData::data_offset())));\n+    __ bind(method_data_continue);\n+  }\n+\n+  __ sd(xmethod, Address(sp, 7 * wordSize));\n+  __ sd(ProfileInterpreter ? t0 : zr, Address(sp, 6 * wordSize));\n+\n+  \/\/ Get mirror and store it in the frame as GC root for this Method*\n+#if INCLUDE_SHENANDOAHGC\n+  if (UseShenandoahGC) {\n+    __ load_mirror(x28, xmethod);\n+    __ sd(zr, Address(sp, 5 * wordSize));\n+    __ sd(x28, Address(sp, 4 * wordSize));\n+  } else\n+#endif\n+  {\n+    __ load_mirror(t2, xmethod);\n+    __ sd(zr, Address(sp, 5 * wordSize));\n+    __ sd(t2, Address(sp, 4 * wordSize));\n+  }\n+\n+  __ ld(xcpool, Address(xmethod, Method::const_offset()));\n+  __ ld(xcpool, Address(xcpool, ConstMethod::constants_offset()));\n+  __ ld(xcpool, Address(xcpool, ConstantPool::cache_offset_in_bytes()));\n+  __ sd(xcpool, Address(sp, 3 * wordSize));\n+  __ sd(xlocals, Address(sp, 2 * wordSize));\n+\n+  __ sd(ra, Address(sp, 11 * wordSize));\n+  __ sd(fp, Address(sp, 10 * wordSize));\n+  __ la(fp, Address(sp, 12 * wordSize)); \/\/ include ra & fp\n+\n+  \/\/ set sender sp\n+  \/\/ leave last_sp as null\n+  __ sd(x30, Address(sp, 9 * wordSize));\n+  __ sd(zr, Address(sp, 8 * wordSize));\n+\n+  \/\/ Move SP out of the way\n+  if (!native_call) {\n+    __ ld(t0, Address(xmethod, Method::const_offset()));\n+    __ lhu(t0, Address(t0, ConstMethod::max_stack_offset()));\n+    __ add(t0, t0, frame::interpreter_frame_monitor_size() + 2);\n+    __ slli(t0, t0, 3);\n+    __ sub(t0, sp, t0);\n+    __ andi(sp, t0, -16);\n+  }\n+}\n+\n+\/\/ End of helpers\n+\n+\/\/ Various method entries\n+\/\/------------------------------------------------------------------------------------------------------------------------\n+\/\/\n+\/\/\n+\n+\/\/ Method entry for java.lang.ref.Reference.get.\n+address TemplateInterpreterGenerator::generate_Reference_get_entry(void) {\n+  \/\/ Code: _aload_0, _getfield, _areturn\n+  \/\/ parameter size = 1\n+  \/\/\n+  \/\/ The code that gets generated by this routine is split into 2 parts:\n+  \/\/    1. The \"intrinsified\" code for G1 (or any SATB based GC),\n+  \/\/    2. The slow path - which is an expansion of the regular method entry.\n+  \/\/\n+  \/\/ Notes:-\n+  \/\/ * In the G1 code we do not check whether we need to block for\n+  \/\/   a safepoint. If G1 is enabled then we must execute the specialized\n+  \/\/   code for Reference.get (except when the Reference object is null)\n+  \/\/   so that we can log the value in the referent field with an SATB\n+  \/\/   update buffer.\n+  \/\/   If the code for the getfield template is modified so that the\n+  \/\/   G1 pre-barrier code is executed when the current method is\n+  \/\/   Reference.get() then going through the normal method entry\n+  \/\/   will be fine.\n+  \/\/ * The G1 code can, however, check the receiver object (the instance\n+  \/\/   of java.lang.Reference) and jump to the slow path if null. If the\n+  \/\/   Reference object is null then we obviously cannot fetch the referent\n+  \/\/   and so we don't need to call the G1 pre-barrier. Thus we can use the\n+  \/\/   regular method entry code to generate the NPE.\n+  \/\/\n+  \/\/ This code is based on generate_accessor_entry.\n+  \/\/\n+  \/\/ xmethod: Method*\n+  \/\/ x30: senderSP must preserve for slow path, set SP to it on fast path\n+\n+  \/\/ ra is live.  It must be saved around calls.\n+\n+  address entry = __ pc();\n+\n+  const int referent_offset = java_lang_ref_Reference::referent_offset;\n+  guarantee(referent_offset > 0, \"referent offset not initialized\");\n+\n+  Label slow_path;\n+  const Register local_0 = c_rarg0;\n+  \/\/ Check if local 0 != NULL\n+  \/\/ If the receiver is null then it is OK to jump to the slow path.\n+  __ ld(local_0, Address(esp, 0));\n+  __ beqz(local_0, slow_path);\n+\n+  __ mv(x9, x30);   \/\/ Move senderSP to a callee-saved register\n+\n+  \/\/ Load the value of the referent field.\n+  const Address field_address(local_0, referent_offset);\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->load_at(_masm, IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT, local_0, field_address, \/*tmp1*\/ t1, \/*tmp2*\/ t0);\n+\n+  \/\/ areturn\n+  __ andi(sp, x9, -16);  \/\/ done with stack\n+  __ ret();\n+\n+  \/\/ generate a vanilla interpreter entry as the slow path\n+  __ bind(slow_path);\n+  __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::zerolocals));\n+  return entry;\n+}\n+\n+\/**\n+ * Method entry for static native methods:\n+ *   int java.util.zip.CRC32.update(int crc, int b)\n+ *\/\n+address TemplateInterpreterGenerator::generate_CRC32_update_entry() {\n+  \/\/ TODO: Unimplemented generate_CRC32_update_entry\n+  return 0;\n+}\n+\n+\/**\n+ * Method entry for static native methods:\n+ *   int java.util.zip.CRC32.updateBytes(int crc, byte[] b, int off, int len)\n+ *   int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)\n+ *\/\n+address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {\n+  \/\/ TODO: Unimplemented generate_CRC32_updateBytes_entry\n+  return 0;\n+}\n+\n+\/**\n+ * Method entry for intrinsic-candidate (non-native) methods:\n+ *   int java.util.zip.CRC32C.updateBytes(int crc, byte[] b, int off, int end)\n+ *   int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)\n+ * Unlike CRC32, CRC32C does not have any methods marked as native\n+ * CRC32C also uses an \"end\" variable instead of the length variable CRC32 uses\n+ *\/\n+address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {\n+  \/\/ TODO: Unimplemented generate_CRC32C_updateBytes_entry\n+  return 0;\n+}\n+\n+void TemplateInterpreterGenerator::bang_stack_shadow_pages(bool native_call) {\n+  \/\/ Bang each page in the shadow zone. We can't assume it's been done for\n+  \/\/ an interpreter frame with greater than a page of locals, so each page\n+  \/\/ needs to be checked.  Only true for non-native.\n+  const int n_shadow_pages = JavaThread::stack_shadow_zone_size() \/ os::vm_page_size();\n+  const int start_page = native_call ? n_shadow_pages : 1;\n+  const int page_size = os::vm_page_size();\n+  for (int pages = start_page; pages <= n_shadow_pages ; pages++) {\n+    __ sub(t0, sp, pages * page_size);\n+    __ sd(zr, Address(t0));\n+  }\n+}\n+\n+\/\/ Interpreter stub for calling a native method. (asm interpreter)\n+\/\/ This sets up a somewhat different looking stack for calling the\n+\/\/ native method than the typical interpreter frame setup.\n+address TemplateInterpreterGenerator::generate_native_entry(bool synchronized) {\n+  \/\/ determine code generation flags\n+  bool inc_counter = UseCompiler || CountCompiledCalls || LogTouchedMethods;\n+\n+  \/\/ x11: Method*\n+  \/\/ x30: sender sp\n+\n+  address entry_point = __ pc();\n+\n+  const Address constMethod       (xmethod, Method::const_offset());\n+  const Address access_flags      (xmethod, Method::access_flags_offset());\n+  const Address size_of_parameters(x12, ConstMethod::\n+                                   size_of_parameters_offset());\n+\n+  \/\/ get parameter size (always needed)\n+  __ ld(x12, constMethod);\n+  __ load_unsigned_short(x12, size_of_parameters);\n+\n+  \/\/ Native calls don't need the stack size check since they have no\n+  \/\/ expression stack and the arguments are already on the stack and\n+  \/\/ we only add a handful of words to the stack.\n+\n+  \/\/ xmethod: Method*\n+  \/\/ x12: size of parameters\n+  \/\/ x30: sender sp\n+\n+  \/\/ for natives the size of locals is zero\n+\n+  \/\/ compute beginning of parameters (xlocals)\n+  __ shadd(xlocals, x12, esp, xlocals, 3);\n+  __ addi(xlocals, xlocals, -wordSize);\n+\n+  \/\/ Pull SP back to minimum size: this avoids holes in the stack\n+  __ andi(sp, esp, -16);\n+\n+  \/\/ initialize fixed part of activation frame\n+  generate_fixed_frame(true);\n+\n+  \/\/ make sure method is native & not abstract\n+#ifdef ASSERT\n+  __ lwu(x10, access_flags);\n+  __ verify_access_flags(x10, JVM_ACC_NATIVE, \"tried to execute non-native method as native\", false);\n+  __ verify_access_flags(x10, JVM_ACC_ABSTRACT, \"tried to execute abstract method in interpreter\");\n+#endif\n+\n+  \/\/ Since at this point in the method invocation the exception\n+  \/\/ handler would try to exit the monitor of synchronized methods\n+  \/\/ which hasn't been entered yet, we set the thread local variable\n+  \/\/ _do_not_unlock_if_synchronized to true. The remove_activation\n+  \/\/ will check this flag.\n+\n+  const Address do_not_unlock_if_synchronized(xthread,\n+                                              in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()));\n+  __ mv(t1, true);\n+  __ sb(t1, do_not_unlock_if_synchronized);\n+\n+  \/\/ increment invocation count & check for overflow\n+  Label invocation_counter_overflow;\n+  if (inc_counter) {\n+    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+  }\n+\n+  Label continue_after_compile;\n+  __ bind(continue_after_compile);\n+\n+  bang_stack_shadow_pages(true);\n+\n+  \/\/ reset the _do_not_unlock_if_synchronized flag\n+  __ sb(zr, do_not_unlock_if_synchronized);\n+\n+  \/\/ check for synchronized methods\n+  \/\/ Must happen AFTER invocation_counter check and stack overflow check,\n+  \/\/ so method is not locked if overflows.\n+  if (synchronized) {\n+    lock_method();\n+  } else {\n+    \/\/ no synchronization necessary\n+#ifdef ASSERT\n+    __ lwu(x10, access_flags);\n+    __ verify_access_flags(x10, JVM_ACC_SYNCHRONIZED, \"method needs synchronization\");\n+#endif\n+  }\n+\n+  \/\/ start execution\n+#ifdef ASSERT\n+  __ verify_frame_setup();\n+#endif\n+\n+  \/\/ jvmti support\n+  __ notify_method_entry();\n+\n+  \/\/ work registers\n+  const Register t = x18;\n+  const Register result_handler = x19;\n+\n+  \/\/ allocate space for parameters\n+  __ ld(t, Address(xmethod, Method::const_offset()));\n+  __ load_unsigned_short(t, Address(t, ConstMethod::size_of_parameters_offset()));\n+\n+  __ slli(t, t, Interpreter::logStackElementSize);\n+  __ sub(x30, esp, t);\n+  __ andi(sp, x30, -16);\n+  __ mv(esp, x30);\n+\n+  \/\/ get signature handler\n+  {\n+    Label L;\n+    __ ld(t, Address(xmethod, Method::signature_handler_offset()));\n+    __ bnez(t, L);\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::prepare_native_call),\n+               xmethod);\n+    __ ld(t, Address(xmethod, Method::signature_handler_offset()));\n+    __ bind(L);\n+  }\n+\n+  \/\/ call signature handler\n+  assert(InterpreterRuntime::SignatureHandlerGenerator::from() == xlocals,\n+         \"adjust this code\");\n+  assert(InterpreterRuntime::SignatureHandlerGenerator::to() == sp,\n+         \"adjust this code\");\n+  assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == t0,\n+          \"adjust this code\");\n+\n+  \/\/ The generated handlers do not touch xmethod (the method).\n+  \/\/ However, large signatures cannot be cached and are generated\n+  \/\/ each time here.  The slow-path generator can do a GC on return,\n+  \/\/ so we must reload it after the call.\n+  __ jalr(t);\n+  __ get_method(xmethod);        \/\/ slow path can do a GC, reload xmethod\n+\n+\n+  \/\/ result handler is in x10\n+  \/\/ set result handler\n+  __ mv(result_handler, x10);\n+  \/\/ pass mirror handle if static call\n+  {\n+    Label L;\n+    __ lwu(t, Address(xmethod, Method::access_flags_offset()));\n+    __ andi(t0, t, JVM_ACC_STATIC);\n+    __ beqz(t0, L);\n+    \/\/ get mirror\n+    __ load_mirror(t, xmethod);\n+    \/\/ copy mirror into activation frame\n+    __ sd(t, Address(fp, frame::interpreter_frame_oop_temp_offset * wordSize));\n+    \/\/ pass handle to mirror\n+    __ addi(c_rarg1, fp, frame::interpreter_frame_oop_temp_offset * wordSize);\n+    __ bind(L);\n+  }\n+\n+  \/\/ get native function entry point in x28\n+  {\n+    Label L;\n+    __ ld(x28, Address(xmethod, Method::native_function_offset()));\n+    address unsatisfied = (SharedRuntime::native_method_throw_unsatisfied_link_error_entry());\n+    __ mv(t1, unsatisfied);\n+    __ ld(t1, t1);\n+    __ bne(x28, t1, L);\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::prepare_native_call),\n+               xmethod);\n+    __ get_method(xmethod);\n+    __ ld(x28, Address(xmethod, Method::native_function_offset()));\n+    __ bind(L);\n+  }\n+\n+  \/\/ pass JNIEnv\n+  __ add(c_rarg0, xthread, in_bytes(JavaThread::jni_environment_offset()));\n+\n+  \/\/ It is enough that the pc() points into the right code\n+  \/\/ segment. It does not have to be the correct return pc.\n+  Label native_return;\n+  __ set_last_Java_frame(esp, fp, native_return, x30);\n+\n+  \/\/ change thread state\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    __ lwu(t, Address(xthread, JavaThread::thread_state_offset()));\n+    __ addi(t0, zr, (u1)_thread_in_Java);\n+    __ beq(t, t0, L);\n+    __ stop(\"Wrong thread state in native stub\");\n+    __ bind(L);\n+  }\n+#endif\n+\n+  \/\/ Change state to native\n+  __ la(t1, Address(xthread, JavaThread::thread_state_offset()));\n+  __ mv(t0, _thread_in_native);\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  __ sw(t0, Address(t1));\n+\n+  \/\/ Call the native method.\n+  __ jalr(x28);\n+  __ bind(native_return);\n+  __ get_method(xmethod);\n+  \/\/ result potentially in x10 or f10\n+\n+  \/\/ make room for the pushes we're about to do\n+  __ sub(t0, esp, 4 * wordSize);\n+  __ andi(sp, t0, -16);\n+\n+  \/\/ NOTE: The order of these pushes is known to frame::interpreter_frame_result\n+  \/\/ in order to extract the result of a method call. If the order of these\n+  \/\/ pushes change or anything else is added to the stack then the code in\n+  \/\/ interpreter_frame_result must also change.\n+  __ push(dtos);\n+  __ push(ltos);\n+\n+  \/\/ change thread state\n+  \/\/ Force all preceding writes to be observed prior to thread state change\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+\n+  __ mv(t0, _thread_in_native_trans);\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(MacroAssembler::AnyAny);\n+\n+  \/\/ check for safepoint operation in progress and\/or pending suspend requests\n+  {\n+    Label L, Continue;\n+    __ safepoint_poll_acquire(L);\n+    __ lwu(t1, Address(xthread, JavaThread::suspend_flags_offset()));\n+    __ beqz(t1, Continue);\n+    __ bind(L);\n+\n+    \/\/ Don't use call_VM as it will see a possible pending exception\n+    \/\/ and forward it and never return here preventing us from\n+    \/\/ clearing _last_native_pc down below. So we do a runtime call by\n+    \/\/ hand.\n+    \/\/\n+    __ mv(c_rarg0, xthread);\n+    __ mv(t1, CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n+    __ jalr(t1);\n+    __ get_method(xmethod);\n+    __ reinit_heapbase();\n+    __ bind(Continue);\n+  }\n+\n+  \/\/ change thread state\n+  \/\/ Force all preceding writes to be observed prior to thread state change\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+\n+  __ mv(t0, _thread_in_Java);\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ reset_last_Java_frame\n+  __ reset_last_Java_frame(true);\n+\n+  if (CheckJNICalls) {\n+    \/\/ clear_pending_jni_exception_check\n+    __ sd(zr, Address(xthread, JavaThread::pending_jni_exception_check_fn_offset()));\n+  }\n+\n+  \/\/ reset handle block\n+  __ ld(t, Address(xthread, JavaThread::active_handles_offset()));\n+  __ sd(zr, Address(t, JNIHandleBlock::top_offset_in_bytes()));\n+\n+  \/\/ If result is an oop unbox and store it in frame where gc will see it\n+  \/\/ and result handler will pick it up\n+\n+  {\n+    Label no_oop;\n+    __ la(t, ExternalAddress(AbstractInterpreter::result_handler(T_OBJECT)));\n+    __ bne(t, result_handler, no_oop);\n+    \/\/ Unbox oop result, e.g. JNIHandles::resolve result.\n+    __ pop(ltos);\n+    __ resolve_jobject(x10, xthread, t);\n+    __ sd(x10, Address(fp, frame::interpreter_frame_oop_temp_offset * wordSize));\n+    \/\/ keep stack depth as expected by pushing oop which will eventually be discarded\n+    __ push(ltos);\n+    __ bind(no_oop);\n+  }\n+\n+  {\n+    Label no_reguard;\n+    __ lwu(t0, Address(xthread, in_bytes(JavaThread::stack_guard_state_offset())));\n+    __ addi(t1, zr, (u1)JavaThread::stack_guard_yellow_reserved_disabled);\n+    __ bne(t0, t1, no_reguard);\n+\n+    __ pusha(); \/\/ only save smashed registers\n+    __ mv(c_rarg0, xthread);\n+    __ mv(t1, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+    __ jalr(t1);\n+    __ popa(); \/\/ only restore smashed registers\n+    __ bind(no_reguard);\n+  }\n+\n+  \/\/ The method register is junk from after the thread_in_native transition\n+  \/\/ until here.  Also can't call_VM until the bcp has been\n+  \/\/ restored.  Need bcp for throwing exception below so get it now.\n+  __ get_method(xmethod);\n+\n+  \/\/ restore bcp to have legal interpreter frame, i.e., bci == 0 <=>\n+  \/\/ xbcp == code_base()\n+  __ ld(xbcp, Address(xmethod, Method::const_offset()));   \/\/ get ConstMethod*\n+  __ add(xbcp, xbcp, in_bytes(ConstMethod::codes_offset()));          \/\/ get codebase\n+  \/\/ handle exceptions (exception handling will handle unlocking!)\n+  {\n+    Label L;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ beqz(t0, L);\n+    \/\/ Note: At some point we may want to unify this with the code\n+    \/\/ used in call_VM_base(); i.e., we should use the\n+    \/\/ StubRoutines::forward_exception code. For now this doesn't work\n+    \/\/ here because the sp is not correctly set at this point.\n+    __ MacroAssembler::call_VM(noreg,\n+                               CAST_FROM_FN_PTR(address,\n+                               InterpreterRuntime::throw_pending_exception));\n+    __ should_not_reach_here();\n+    __ bind(L);\n+  }\n+\n+  \/\/ do unlocking if necessary\n+  {\n+    Label L;\n+    __ lwu(t, Address(xmethod, Method::access_flags_offset()));\n+    __ andi(t0, t, JVM_ACC_SYNCHRONIZED);\n+    __ beqz(t0, L);\n+    \/\/ the code below should be shared with interpreter macro\n+    \/\/ assembler implementation\n+    {\n+      Label unlock;\n+      \/\/ BasicObjectLock will be first in list, since this is a\n+      \/\/ synchronized method. However, need to check that the object\n+      \/\/ has not been unlocked by an explicit monitorexit bytecode.\n+\n+      \/\/ monitor expect in c_rarg1 for slow unlock path\n+      __ la(c_rarg1, Address(fp,   \/\/ address of first monitor\n+                             (intptr_t)(frame::interpreter_frame_initial_sp_offset *\n+                                        wordSize - sizeof(BasicObjectLock))));\n+\n+      __ ld(t, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+      __ bnez(t, unlock);\n+\n+      \/\/ Entry already unlocked, need to throw exception\n+      __ MacroAssembler::call_VM(noreg,\n+                                 CAST_FROM_FN_PTR(address,\n+                                                  InterpreterRuntime::throw_illegal_monitor_state_exception));\n+      __ should_not_reach_here();\n+\n+      __ bind(unlock);\n+      __ unlock_object(c_rarg1);\n+    }\n+    __ bind(L);\n+  }\n+\n+  \/\/ jvmti support\n+  \/\/ Note: This must happen _after_ handling\/throwing any exceptions since\n+  \/\/       the exception handler code notifies the runtime of method exits\n+  \/\/       too. If this happens before, method entry\/exit notifications are\n+  \/\/       not properly paired (was bug - gri 11\/22\/99).\n+  __ notify_method_exit(vtos, InterpreterMacroAssembler::NotifyJVMTI);\n+\n+  __ pop(ltos);\n+  __ pop(dtos);\n+\n+  __ jalr(result_handler);\n+\n+  \/\/ remove activation\n+  __ ld(esp, Address(fp, frame::interpreter_frame_sender_sp_offset * wordSize)); \/\/ get sender sp\n+  \/\/ remove frame anchor\n+  __ leave();\n+\n+  \/\/ restore sender sp\n+  __ mv(sp, esp);\n+\n+  __ ret();\n+\n+  if (inc_counter) {\n+    \/\/ Handle overflow of counter and compile method\n+    __ bind(invocation_counter_overflow);\n+    generate_counter_overflow(continue_after_compile);\n+  }\n+\n+  return entry_point;\n+}\n+\n+\/\/\n+\/\/ Generic interpreted method entry to (asm) interpreter\n+\/\/\n+address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {\n+\n+  \/\/ determine code generation flags\n+  const bool inc_counter  = UseCompiler || CountCompiledCalls || LogTouchedMethods;\n+\n+  \/\/ t0: sender sp\n+  address entry_point = __ pc();\n+\n+  const Address constMethod(xmethod, Method::const_offset());\n+  const Address access_flags(xmethod, Method::access_flags_offset());\n+  const Address size_of_parameters(x13,\n+                                   ConstMethod::size_of_parameters_offset());\n+  const Address size_of_locals(x13, ConstMethod::size_of_locals_offset());\n+\n+  \/\/ get parameter size (always needed)\n+  \/\/ need to load the const method first\n+  __ ld(x13, constMethod);\n+  __ load_unsigned_short(x12, size_of_parameters);\n+\n+  \/\/ x12: size of parameters\n+\n+  __ load_unsigned_short(x13, size_of_locals); \/\/ get size of locals in words\n+  __ sub(x13, x13, x12); \/\/ x13 = no. of additional locals\n+\n+  \/\/ see if we've got enough room on the stack for locals plus overhead.\n+  generate_stack_overflow_check();\n+\n+  \/\/ compute beginning of parameters (xlocals)\n+  __ shadd(xlocals, x12, esp, t1, 3);\n+  __ add(xlocals, xlocals, -wordSize);\n+\n+  \/\/ Make room for additional locals\n+  __ slli(t1, x13, 3);\n+  __ sub(t0, esp, t1);\n+\n+  \/\/ Padding between locals and fixed part of activation frame to ensure\n+  \/\/ SP is always 16-byte aligned.\n+  __ andi(sp, t0, -16);\n+\n+  \/\/ x13 - # of additional locals\n+  \/\/ allocate space for locals\n+  \/\/ explicitly initialize locals\n+  {\n+    Label exit, loop;\n+    __ blez(x13, exit); \/\/ do nothing if x13 <= 0\n+    __ bind(loop);\n+    __ sd(zr, Address(t0));\n+    __ add(t0, t0, wordSize);\n+    __ add(x13, x13, -1); \/\/ until everything initialized\n+    __ bnez(x13, loop);\n+    __ bind(exit);\n+  }\n+\n+  \/\/ And the base dispatch table\n+  __ get_dispatch();\n+\n+  \/\/ initialize fixed part of activation frame\n+  generate_fixed_frame(false);\n+\n+  \/\/ make sure method is not native & not abstract\n+#ifdef ASSERT\n+  __ lwu(x10, access_flags);\n+  __ verify_access_flags(x10, JVM_ACC_NATIVE, \"tried to execute native method as non-native\");\n+  __ verify_access_flags(x10, JVM_ACC_ABSTRACT, \"tried to execute abstract method in interpreter\");\n+#endif\n+\n+  \/\/ Since at this point in the method invocation the exception\n+  \/\/ handler would try to exit the monitor of synchronized methods\n+  \/\/ which hasn't been entered yet, we set the thread local variable\n+  \/\/ _do_not_unlock_if_synchronized to true. The remove_activation\n+  \/\/ will check this flag.\n+\n+  const Address do_not_unlock_if_synchronized(xthread,\n+                                              in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()));\n+  __ mv(t1, true);\n+  __ sb(t1, do_not_unlock_if_synchronized);\n+\n+  Label no_mdp;\n+  const Register mdp = x13;\n+  __ ld(mdp, Address(xmethod, Method::method_data_offset()));\n+  __ beqz(mdp, no_mdp);\n+  __ add(mdp, mdp, in_bytes(MethodData::data_offset()));\n+  __ profile_parameters_type(mdp, x11, x12, x14); \/\/ use x11, x12, x14 as tmp registers\n+  __ bind(no_mdp);\n+\n+  \/\/ increment invocation count & check for overflow\n+  Label invocation_counter_overflow;\n+  Label profile_method;\n+  Label profile_method_continue;\n+  if (inc_counter) {\n+    generate_counter_incr(&invocation_counter_overflow,\n+                          &profile_method,\n+                          &profile_method_continue);\n+    if (ProfileInterpreter) {\n+      __ bind(profile_method_continue);\n+    }\n+  }\n+\n+  Label continue_after_compile;\n+  __ bind(continue_after_compile);\n+\n+  bang_stack_shadow_pages(false);\n+\n+  \/\/ reset the _do_not_unlock_if_synchronized flag\n+  __ sb(zr, do_not_unlock_if_synchronized);\n+\n+  \/\/ check for synchronized methods\n+  \/\/ Must happen AFTER invocation_counter check and stack overflow check,\n+  \/\/ so method is not locked if overflows.\n+  if (synchronized) {\n+    \/\/ Allocate monitor and lock method\n+    lock_method();\n+  } else {\n+    \/\/ no synchronization necessary\n+#ifdef ASSERT\n+    __ lwu(x10, access_flags);\n+    __ verify_access_flags(x10, JVM_ACC_SYNCHRONIZED, \"method needs synchronization\");\n+#endif\n+  }\n+\n+  \/\/ start execution\n+#ifdef ASSERT\n+  __ verify_frame_setup();\n+#endif\n+\n+  \/\/ jvmti support\n+  __ notify_method_entry();\n+\n+  __ dispatch_next(vtos);\n+\n+  \/\/ invocation counter overflow\n+  if (inc_counter) {\n+    if (ProfileInterpreter) {\n+      \/\/ We have decided to profile this method in the interpreter\n+      __ bind(profile_method);\n+      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n+      __ set_method_data_pointer_for_bcp();\n+      \/\/ don't think we need this\n+      __ get_method(x11);\n+      __ j(profile_method_continue);\n+    }\n+    \/\/ Handle overflow of counter and compile method\n+    __ bind(invocation_counter_overflow);\n+    generate_counter_overflow(continue_after_compile);\n+  }\n+\n+  return entry_point;\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Exceptions\n+\n+void TemplateInterpreterGenerator::generate_throw_exception() {\n+  \/\/ Entry point in previous activation (i.e., if the caller was\n+  \/\/ interpreted)\n+  Interpreter::_rethrow_exception_entry = __ pc();\n+  \/\/ Restore sp to interpreter_frame_last_sp even though we are going\n+  \/\/ to empty the expression stack for the exception processing.\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  \/\/ x10: exception\n+  \/\/ x13: return address\/pc that threw exception\n+  __ restore_bcp();    \/\/ xbcp points to call\/send\n+  __ restore_locals();\n+  __ restore_constant_pool_cache();\n+  __ reinit_heapbase();  \/\/ restore xheapbase as heapbase.\n+  __ get_dispatch();\n+\n+  \/\/ Entry point for exceptions thrown within interpreter code\n+  Interpreter::_throw_exception_entry = __ pc();\n+  \/\/ If we came here via a NullPointerException on the receiver of a\n+  \/\/ method, xthread may be corrupt.\n+  __ get_method(xmethod);\n+  \/\/ expression stack is undefined here\n+  \/\/ x10: exception\n+  \/\/ xbcp: exception bcp\n+  __ verify_oop(x10);\n+  __ mv(c_rarg1, x10);\n+\n+  \/\/ expression stack must be empty before entering the VM in case of\n+  \/\/ an exception\n+  __ empty_expression_stack();\n+  \/\/ find exception handler address and preserve exception oop\n+  __ call_VM(x13,\n+             CAST_FROM_FN_PTR(address,\n+                          InterpreterRuntime::exception_handler_for_exception),\n+             c_rarg1);\n+\n+  \/\/ Calculate stack limit\n+  __ ld(t0, Address(xmethod, Method::const_offset()));\n+  __ lhu(t0, Address(t0, ConstMethod::max_stack_offset()));\n+  __ add(t0, t0, frame::interpreter_frame_monitor_size() + 4);\n+  __ ld(t1, Address(fp, frame::interpreter_frame_initial_sp_offset * wordSize));\n+  __ slli(t0, t0, 3);\n+  __ sub(t0, t1, t0);\n+  __ andi(sp, t0, -16);\n+\n+  \/\/ x10: exception handler entry point\n+  \/\/ x13: preserved exception oop\n+  \/\/ xbcp: bcp for exception handler\n+  __ push_ptr(x13); \/\/ push exception which is now the only value on the stack\n+  __ jr(x10); \/\/ jump to exception handler (may be _remove_activation_entry!)\n+\n+  \/\/ If the exception is not handled in the current frame the frame is\n+  \/\/ removed and the exception is rethrown (i.e. exception\n+  \/\/ continuation is _rethrow_exception).\n+  \/\/\n+  \/\/ Note: At this point the bci is still the bxi for the instruction\n+  \/\/ which caused the exception and the expression stack is\n+  \/\/ empty. Thus, for any VM calls at this point, GC will find a legal\n+  \/\/ oop map (with empty expression stack).\n+\n+  \/\/\n+  \/\/ JVMTI PopFrame support\n+  \/\/\n+\n+  Interpreter::_remove_activation_preserving_args_entry = __ pc();\n+  __ empty_expression_stack();\n+  \/\/ Set the popframe_processing bit in pending_popframe_condition\n+  \/\/ indicating that we are currently handling popframe, so that\n+  \/\/ call_VMs that may happen later do not trigger new popframe\n+  \/\/ handling cycles.\n+  __ lwu(x13, Address(xthread, JavaThread::popframe_condition_offset()));\n+  __ ori(x13, x13, JavaThread::popframe_processing_bit);\n+  __ sw(x13, Address(xthread, JavaThread::popframe_condition_offset()));\n+\n+  {\n+    \/\/ Check to see whether we are returning to a deoptimized frame.\n+    \/\/ (The PopFrame call ensures that the caller of the popped frame is\n+    \/\/ either interpreted or compiled and deoptimizes it if compiled.)\n+    \/\/ In this case, we can't call dispatch_next() after the frame is\n+    \/\/ popped, but instead must save the incoming arguments and restore\n+    \/\/ them after deoptimization has occurred.\n+    \/\/\n+    \/\/ Note that we don't compare the return PC against the\n+    \/\/ deoptimization blob's unpack entry because of the presence of\n+    \/\/ adapter frames in C2.\n+    Label caller_not_deoptimized;\n+    __ ld(c_rarg1, Address(fp, frame::return_addr_offset * wordSize));\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::interpreter_contains), c_rarg1);\n+    __ bnez(x10, caller_not_deoptimized);\n+\n+    \/\/ Compute size of arguments for saving when returning to\n+    \/\/ deoptimized caller\n+    __ get_method(x10);\n+    __ ld(x10, Address(x10, Method::const_offset()));\n+    __ load_unsigned_short(x10, Address(x10, in_bytes(ConstMethod::\n+                                                      size_of_parameters_offset())));\n+    __ slli(x10, x10, Interpreter::logStackElementSize);\n+    __ restore_locals();\n+    __ sub(xlocals, xlocals, x10);\n+    __ add(xlocals, xlocals, wordSize);\n+    \/\/ Save these arguments\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                                           Deoptimization::\n+                                           popframe_preserve_args),\n+                          xthread, x10, xlocals);\n+\n+    __ remove_activation(vtos,\n+                         \/* throw_monitor_exception *\/ false,\n+                         \/* install_monitor_exception *\/ false,\n+                         \/* notify_jvmdi *\/ false);\n+\n+    \/\/ Inform deoptimization that it is responsible for restoring\n+    \/\/ these arguments\n+    __ mv(t0, JavaThread::popframe_force_deopt_reexecution_bit);\n+    __ sw(t0, Address(xthread, JavaThread::popframe_condition_offset()));\n+\n+    \/\/ Continue in deoptimization handler\n+    __ ret();\n+\n+    __ bind(caller_not_deoptimized);\n+  }\n+\n+  __ remove_activation(vtos,\n+                       \/* throw_monitor_exception *\/ false,\n+                       \/* install_monitor_exception *\/ false,\n+                       \/* notify_jvmdi *\/ false);\n+\n+  \/\/ Restore the last_sp and null it out\n+  __ ld(esp, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ sd(zr, Address(fp, frame::interpreter_frame_last_sp_offset * wordSize));\n+\n+  __ restore_bcp();\n+  __ restore_locals();\n+  __ restore_constant_pool_cache();\n+  __ get_method(xmethod);\n+  __ get_dispatch();\n+\n+  \/\/ The method data pointer was incremented already during\n+  \/\/ call profiling. We have to restore the mdp for the current bcp.\n+  if (ProfileInterpreter) {\n+    __ set_method_data_pointer_for_bcp();\n+  }\n+\n+  \/\/ Clear the popframe condition flag\n+  __ sw(zr, Address(xthread, JavaThread::popframe_condition_offset()));\n+  assert(JavaThread::popframe_inactive == 0, \"fix popframe_inactive\");\n+\n+#if INCLUDE_JVMTI\n+  {\n+    Label L_done;\n+\n+    __ lbu(t0, Address(xbcp, 0));\n+    __ li(t1, Bytecodes::_invokestatic);\n+    __ bne(t1, t0, L_done);\n+\n+    \/\/ The member name argument must be restored if _invokestatic is re-executed after a PopFrame call.\n+    \/\/ Detect such a case in the InterpreterRuntime function and return the member name argument,or NULL.\n+\n+    __ ld(c_rarg0, Address(xlocals, 0));\n+    __ call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::member_name_arg_or_null),c_rarg0, xmethod, xbcp);\n+\n+    __ beqz(x10, L_done);\n+\n+    __ sd(x10, Address(esp, 0));\n+    __ bind(L_done);\n+  }\n+#endif \/\/ INCLUDE_JVMTI\n+\n+  \/\/ Restore machine SP\n+  __ ld(t0, Address(xmethod, Method::const_offset()));\n+  __ lhu(t0, Address(t0, ConstMethod::max_stack_offset()));\n+  __ add(t0, t0, frame::interpreter_frame_monitor_size() + 4);\n+  __ ld(t1, Address(fp, frame::interpreter_frame_initial_sp_offset * wordSize));\n+  __ slliw(t0, t0, 3);\n+  __ sub(t0, t1, t0);\n+  __ andi(sp, t0, -16);\n+\n+  __ dispatch_next(vtos);\n+  \/\/ end of PopFrame support\n+\n+  Interpreter::_remove_activation_entry = __ pc();\n+\n+  \/\/ preserve exception over this code sequence\n+  __ pop_ptr(x10);\n+  __ sd(x10, Address(xthread, JavaThread::vm_result_offset()));\n+  \/\/ remove the activation (without doing throws on illegalMonitorExceptions)\n+  __ remove_activation(vtos, false, true, false);\n+  \/\/ restore exception\n+  __ get_vm_result(x10, xthread);\n+\n+  \/\/ In between activations - previous activation type unknown yet\n+  \/\/ compute continuation point - the continuation point expects the\n+  \/\/ following registers set up:\n+  \/\/\n+  \/\/ x10: exception\n+  \/\/ ra: return address\/pc that threw exception\n+  \/\/ sp: expression stack of caller\n+  \/\/ fp: fp of caller\n+  \/\/ FIXME: There's no point saving ra here because VM calls don't trash it\n+  __ sub(sp, sp, 2 * wordSize);\n+  __ sd(x10, Address(sp, 0));                   \/\/ save exception\n+  __ sd(ra, Address(sp, wordSize));             \/\/ save return address\n+  __ super_call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                                         SharedRuntime::exception_handler_for_return_address),\n+                        xthread, ra);\n+  __ mv(x11, x10);                              \/\/ save exception handler\n+  __ ld(x10, Address(sp, 0));                   \/\/ restore exception\n+  __ ld(ra, Address(sp, wordSize));             \/\/ restore return address\n+  __ add(sp, sp, 2 * wordSize);\n+  \/\/ We might be returning to a deopt handler that expects x13 to\n+  \/\/ contain the exception pc\n+  __ mv(x13, ra);\n+  \/\/ Note that an \"issuing PC\" is actually the next PC after the call\n+  __ jr(x11);                                   \/\/ jump to exception\n+                                                \/\/ handler of caller\n+}\n+\n+\/\/\n+\/\/ JVMTI ForceEarlyReturn support\n+\/\/\n+address TemplateInterpreterGenerator::generate_earlyret_entry_for(TosState state)  {\n+  address entry = __ pc();\n+\n+  __ restore_bcp();\n+  __ restore_locals();\n+  __ empty_expression_stack();\n+  __ load_earlyret_value(state);\n+\n+  __ ld(t0, Address(xthread, JavaThread::jvmti_thread_state_offset()));\n+  Address cond_addr(t0, JvmtiThreadState::earlyret_state_offset());\n+\n+  \/\/ Clear the earlyret state\n+  assert(JvmtiThreadState::earlyret_inactive == 0, \"should be\");\n+  __ sd(zr, cond_addr);\n+\n+  __ remove_activation(state,\n+                       false, \/* throw_monitor_exception *\/\n+                       false, \/* install_monitor_exception *\/\n+                       true); \/* notify_jvmdi *\/\n+  __ ret();\n+\n+  return entry;\n+}\n+\/\/ end of ForceEarlyReturn support\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Helper for vtos entry point generation\n+\n+void TemplateInterpreterGenerator::set_vtos_entry_points(Template* t,\n+                                                         address& bep,\n+                                                         address& cep,\n+                                                         address& sep,\n+                                                         address& aep,\n+                                                         address& iep,\n+                                                         address& lep,\n+                                                         address& fep,\n+                                                         address& dep,\n+                                                         address& vep) {\n+  assert(t != NULL && t->is_valid() && t->tos_in() == vtos, \"illegal template\");\n+  Label L;\n+  aep = __ pc();  __ push_ptr();  __ j(L);\n+  fep = __ pc();  __ push_f();    __ j(L);\n+  dep = __ pc();  __ push_d();    __ j(L);\n+  lep = __ pc();  __ push_l();    __ j(L);\n+  bep = cep = sep =\n+  iep = __ pc();  __ push_i();\n+  vep = __ pc();\n+  __ bind(L);\n+  generate_and_dispatch(t);\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\n+\/\/ Non-product code\n+#ifndef PRODUCT\n+address TemplateInterpreterGenerator::generate_trace_code(TosState state) {\n+  address entry = __ pc();\n+\n+  __ push_reg(ra);\n+  __ push(state);\n+  __ push_reg(RegSet::range(x10, x17) + RegSet::range(x5, x7) + RegSet::range(x28, x31), sp);\n+  __ mv(c_rarg2, x10);  \/\/ Pass itos\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), c_rarg1, c_rarg2, c_rarg3);\n+  __ pop_reg(RegSet::range(x10, x17) + RegSet::range(x5, x7) + RegSet::range(x28, x31), sp);\n+  __ pop(state);\n+  __ pop_reg(ra);\n+  __ ret();                                   \/\/ return from result handler\n+\n+  return entry;\n+}\n+\n+void TemplateInterpreterGenerator::count_bytecode() {\n+  __ push_reg(t0);\n+  __ push_reg(x10);\n+  __ mv(x10, (address) &BytecodeCounter::_counter_value);\n+  __ li(t0, 1);\n+  __ amoadd_d(zr, x10, t0, Assembler::aqrl);\n+  __ pop_reg(x10);\n+  __ pop_reg(t0);\n+}\n+\n+void TemplateInterpreterGenerator::histogram_bytecode(Template* t) { ; }\n+\n+void TemplateInterpreterGenerator::histogram_bytecode_pair(Template* t) { ; }\n+\n+void TemplateInterpreterGenerator::trace_bytecode(Template* t) {\n+  \/\/ Call a little run-time stub to avoid blow-up for each bytecode.\n+  \/\/ The run-time runtime saves the right registers, depending on\n+  \/\/ the tosca in-state for the given template.\n+\n+  assert(Interpreter::trace_code(t->tos_in()) != NULL, \"entry must have been generated\");\n+  __ jal(Interpreter::trace_code(t->tos_in()));\n+  __ reinit_heapbase();\n+}\n+\n+void TemplateInterpreterGenerator::stop_interpreter_at() {\n+  Label L;\n+  __ push_reg(t0);\n+  __ mv(t0, (address) &BytecodeCounter::_counter_value);\n+  __ ld(t0, Address(t0));\n+  __ mv(t1, StopInterpreterAt);\n+  __ bne(t0, t1, L);\n+  __ ebreak();\n+  __ bind(L);\n+  __ pop_reg(t0);\n+}\n+\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/cpu\/riscv\/templateInterpreterGenerator_riscv.cpp","additions":1833,"deletions":0,"binary":false,"changes":1833,"status":"added"},{"patch":"@@ -0,0 +1,4006 @@\n+\/*\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"interpreter\/interp_masm.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/interpreterRuntime.hpp\"\n+#include \"interpreter\/templateTable.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ Platform-dependent initialization\n+\n+void TemplateTable::pd_initialize() {\n+  \/\/ No RISC-V specific initialization\n+}\n+\n+\/\/ Address computation: local variables\n+\n+static inline Address iaddress(int n) {\n+  return Address(xlocals, Interpreter::local_offset_in_bytes(n));\n+}\n+\n+static inline Address laddress(int n) {\n+  return iaddress(n + 1);\n+}\n+\n+static inline Address faddress(int n) {\n+  return iaddress(n);\n+}\n+\n+static inline Address daddress(int n) {\n+  return laddress(n);\n+}\n+\n+static inline Address aaddress(int n) {\n+  return iaddress(n);\n+}\n+\n+static inline Address iaddress(Register r,  Register temp, InterpreterMacroAssembler* _masm) {\n+  assert_cond(_masm != NULL);\n+  _masm->shadd(temp, r, xlocals, temp, 3);\n+  return Address(temp, 0);\n+}\n+\n+static inline Address laddress(Register r, Register temp,\n+                               InterpreterMacroAssembler* _masm) {\n+  assert_cond(_masm != NULL);\n+  _masm->shadd(temp, r, xlocals, temp, 3);\n+  return Address(temp, Interpreter::local_offset_in_bytes(1));;\n+}\n+\n+static inline Address faddress(Register r, Register temp, InterpreterMacroAssembler* _masm) {\n+  return iaddress(r, temp, _masm);\n+}\n+\n+static inline Address daddress(Register r, Register temp,\n+                               InterpreterMacroAssembler* _masm) {\n+  return laddress(r, temp, _masm);\n+}\n+\n+static inline Address aaddress(Register r, Register temp, InterpreterMacroAssembler* _masm) {\n+  return iaddress(r, temp, _masm);\n+}\n+\n+static inline Address at_rsp() {\n+  return Address(esp, 0);\n+}\n+\n+\/\/ At top of Java expression stack which may be different than esp().  It\n+\/\/ isn't for category 1 objects.\n+static inline Address at_tos   () {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(0));\n+}\n+\n+static inline Address at_tos_p1() {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(1));\n+}\n+\n+static inline Address at_tos_p2() {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(2));\n+}\n+\n+static inline Address at_tos_p3() {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(3));\n+}\n+\n+static inline Address at_tos_p4() {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(4));\n+}\n+\n+static inline Address at_tos_p5() {\n+  return Address(esp,  Interpreter::expr_offset_in_bytes(5));\n+}\n+\n+\/\/ Miscelaneous helper routines\n+\/\/ Store an oop (or NULL) at the Address described by obj.\n+\/\/ If val == noreg this means store a NULL\n+static void do_oop_store(InterpreterMacroAssembler* _masm,\n+                         Address dst,\n+                         Register val,\n+                         DecoratorSet decorators) {\n+  assert(val == noreg || val == x10, \"parameter is just for looks\");\n+  assert_cond(_masm != NULL);\n+  __ store_heap_oop(dst, val, x29, x11, decorators);\n+}\n+\n+static void do_oop_load(InterpreterMacroAssembler* _masm,\n+                        Address src,\n+                        Register dst,\n+                        DecoratorSet decorators) {\n+  assert_cond(_masm != NULL);\n+  __ load_heap_oop(dst, src, x7, x11, decorators);\n+}\n+\n+Address TemplateTable::at_bcp(int offset) {\n+  assert(_desc->uses_bcp(), \"inconsistent uses_bcp information\");\n+  return Address(xbcp, offset);\n+}\n+\n+void TemplateTable::patch_bytecode(Bytecodes::Code bc, Register bc_reg,\n+                                   Register temp_reg, bool load_bc_into_bc_reg\/*=true*\/,\n+                                   int byte_no)\n+{\n+  if (!RewriteBytecodes)  { return; }\n+  Label L_patch_done;\n+\n+  switch (bc) {\n+    case Bytecodes::_fast_aputfield:  \/\/ fall through\n+    case Bytecodes::_fast_bputfield:  \/\/ fall through\n+    case Bytecodes::_fast_zputfield:  \/\/ fall through\n+    case Bytecodes::_fast_cputfield:  \/\/ fall through\n+    case Bytecodes::_fast_dputfield:  \/\/ fall through\n+    case Bytecodes::_fast_fputfield:  \/\/ fall through\n+    case Bytecodes::_fast_iputfield:  \/\/ fall through\n+    case Bytecodes::_fast_lputfield:  \/\/ fall through\n+    case Bytecodes::_fast_sputfield: {\n+      \/\/ We skip bytecode quickening for putfield instructions when\n+      \/\/ the put_code written to the constant pool cache is zero.\n+      \/\/ This is required so that every execution of this instruction\n+      \/\/ calls out to InterpreterRuntime::resolve_get_put to do\n+      \/\/ additional, required work.\n+      assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+      assert(load_bc_into_bc_reg, \"we use bc_reg as temp\");\n+      __ get_cache_and_index_and_bytecode_at_bcp(temp_reg, bc_reg, temp_reg, byte_no, 1);\n+      __ mv(bc_reg, bc);\n+      __ beqz(temp_reg, L_patch_done);\n+      break;\n+    }\n+    default:\n+      assert(byte_no == -1, \"sanity\");\n+      \/\/ the pair bytecodes have already done the load.\n+      if (load_bc_into_bc_reg) {\n+        __ mv(bc_reg, bc);\n+      }\n+  }\n+\n+  if (JvmtiExport::can_post_breakpoint()) {\n+    Label L_fast_patch;\n+    \/\/ if a breakpoint is present we can't rewrite the stream directly\n+    __ load_unsigned_byte(temp_reg, at_bcp(0));\n+    __ addi(temp_reg, temp_reg, -Bytecodes::_breakpoint); \/\/ temp_reg is temporary register.\n+    __ bnez(temp_reg, L_fast_patch);\n+    \/\/ Let breakpoint table handling rewrite to quicker bytecode\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::set_original_bytecode_at), xmethod, xbcp, bc_reg);\n+    __ j(L_patch_done);\n+    __ bind(L_fast_patch);\n+  }\n+\n+#ifdef ASSERT\n+  Label L_okay;\n+  __ load_unsigned_byte(temp_reg, at_bcp(0));\n+  __ beq(temp_reg, bc_reg, L_okay);\n+  __ addi(temp_reg, temp_reg, -(int) Bytecodes::java_code(bc));\n+  __ beqz(temp_reg, L_okay);\n+  __ stop(\"patching the wrong bytecode\");\n+  __ bind(L_okay);\n+#endif\n+\n+  \/\/ patch bytecode\n+  __ sb(bc_reg, at_bcp(0));\n+  __ bind(L_patch_done);\n+}\n+\n+\/\/ Individual instructions\n+\n+void TemplateTable::nop() {\n+  transition(vtos, vtos);\n+  \/\/ nothing to do\n+}\n+\n+void TemplateTable::shouldnotreachhere() {\n+  transition(vtos, vtos);\n+  __ stop(\"should not reach here bytecode\");\n+}\n+\n+void TemplateTable::aconst_null()\n+{\n+  transition(vtos, atos);\n+  __ mv(x10, zr);\n+}\n+\n+void TemplateTable::iconst(int value)\n+{\n+  transition(vtos, itos);\n+  __ li(x10, value);\n+}\n+\n+void TemplateTable::lconst(int value)\n+{\n+  transition(vtos, ltos);\n+  __ li(x10, value);\n+}\n+\n+void TemplateTable::fconst(int value)\n+{\n+  transition(vtos, ftos);\n+  static float fBuf[2] = {1.0, 2.0};\n+  __ mv(t0, (intptr_t)fBuf);\n+  switch (value) {\n+    case 0:\n+      __ fmv_w_x(f10, zr);\n+      break;\n+    case 1:\n+      __ flw(f10, t0, 0);\n+      break;\n+    case 2:\n+      __ flw(f10, t0, sizeof(float));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::dconst(int value)\n+{\n+  transition(vtos, dtos);\n+  static double dBuf[2] = {1.0, 2.0};\n+  __ mv(t0, (intptr_t)dBuf);\n+  switch (value) {\n+    case 0:\n+      __ fmv_d_x(f10, zr);\n+      break;\n+    case 1:\n+      __ fld(f10, t0, 0);\n+      break;\n+    case 2:\n+      __ fld(f10, t0, sizeof(double));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::bipush()\n+{\n+  transition(vtos, itos);\n+  __ load_signed_byte(x10, at_bcp(1));\n+}\n+\n+void TemplateTable::sipush()\n+{\n+  transition(vtos, itos);\n+  __ load_unsigned_short(x10, at_bcp(1));\n+  __ revb_w_w(x10, x10);\n+  __ sraiw(x10, x10, 16);\n+}\n+\n+void TemplateTable::ldc(bool wide)\n+{\n+  transition(vtos, vtos);\n+  Label call_ldc, notFloat, notClass, notInt, Done;\n+\n+  if (wide) {\n+   __ get_unsigned_2_byte_index_at_bcp(x11, 1);\n+  } else {\n+   __ load_unsigned_byte(x11, at_bcp(1));\n+  }\n+  __ get_cpool_and_tags(x12, x10);\n+\n+  const int base_offset = ConstantPool::header_size() * wordSize;\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+\n+  \/\/ get type\n+  __ addi(x13, x11, tags_offset);\n+  __ add(x13, x10, x13);\n+  __ membar(MacroAssembler::AnyAny);\n+  __ lbu(x13, Address(x13, 0));\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+\n+  \/\/ unresolved class - get the resolved class\n+  __ mv(t1, (u1)JVM_CONSTANT_UnresolvedClass);\n+  __ beq(x13, t1, call_ldc);\n+\n+  \/\/ unresolved class in error state - call into runtime to throw the error\n+  \/\/ from the first resolution attempt\n+  __ mv(t1, (u1)JVM_CONSTANT_UnresolvedClassInError);\n+  __ beq(x13, t1, call_ldc);\n+\n+  \/\/ resolved class - need to call vm to get java mirror of the class\n+  __ mv(t1, (u1)JVM_CONSTANT_Class);\n+  __ bne(x13, t1, notClass);\n+\n+  __ bind(call_ldc);\n+  __ mv(c_rarg1, wide);\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::ldc), c_rarg1);\n+  __ push_ptr(x10);\n+  __ verify_oop(x10);\n+  __ j(Done);\n+\n+  __ bind(notClass);\n+  __ mv(t1, (u1)JVM_CONSTANT_Float);\n+  __ bne(x13, t1, notFloat);\n+\n+  \/\/ ftos\n+  __ shadd(x11, x11, x12, x11, 3);\n+  __ flw(f10, Address(x11, base_offset));\n+  __ push_f(f10);\n+  __ j(Done);\n+\n+  __ bind(notFloat);\n+\n+  __ mv(t1, (u1)JVM_CONSTANT_Integer);\n+  __ bne(x13, t1, notInt);\n+\n+  \/\/ itos\n+  __ shadd(x11, x11, x12, x11, 3);\n+  __ lw(x10, Address(x11, base_offset));\n+  __ push_i(x10);\n+  __ j(Done);\n+\n+  __ bind(notInt);\n+  condy_helper(Done);\n+\n+  __ bind(Done);\n+}\n+\n+\/\/ Fast path for caching oop constants.\n+void TemplateTable::fast_aldc(bool wide)\n+{\n+  transition(vtos, atos);\n+\n+  const Register result = x10;\n+  const Register tmp = x11;\n+  const Register rarg = x12;\n+\n+  const int index_size = wide ? sizeof(u2) : sizeof(u1);\n+\n+  Label resolved;\n+\n+  \/\/ We are resolved if the resolved reference cache entry contains a\n+  \/\/ non-null object (String, MethodType, etc.)\n+  assert_different_registers(result, tmp);\n+  __ get_cache_index_at_bcp(tmp, 1, index_size);\n+  __ load_resolved_reference_at_index(result, tmp);\n+  __ bnez(result, resolved);\n+\n+  const address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc);\n+\n+  \/\/ first time invocation - must resolve first\n+  __ mv(rarg, (int)bytecode());\n+  __ call_VM(result, entry, rarg);\n+\n+  __ bind(resolved);\n+\n+  { \/\/ Check for the null sentinel.\n+    \/\/ If we just called the VM, it already did the mapping for us,\n+    \/\/ but it's harmless to retry.\n+    Label notNull;\n+\n+    \/\/ Stash null_sentinel address to get its value later\n+    int32_t offset = 0;\n+    __ movptr_with_offset(rarg, Universe::the_null_sentinel_addr(), offset);\n+    __ ld(tmp, Address(rarg, offset));\n+    __ bne(result, tmp, notNull);\n+    __ mv(result, zr);  \/\/ NULL object reference\n+    __ bind(notNull);\n+  }\n+\n+  if (VerifyOops) {\n+    \/\/ Safe to call with 0 result\n+    __ verify_oop(result);\n+  }\n+}\n+\n+void TemplateTable::ldc2_w()\n+{\n+    transition(vtos, vtos);\n+    Label notDouble, notLong, Done;\n+    __ get_unsigned_2_byte_index_at_bcp(x10, 1);\n+\n+    __ get_cpool_and_tags(x11, x12);\n+    const int base_offset = ConstantPool::header_size() * wordSize;\n+    const int tags_offset = Array<u1>::base_offset_in_bytes();\n+\n+    \/\/ get type\n+    __ add(x12, x12, x10);\n+    __ load_unsigned_byte(x12, Address(x12, tags_offset));\n+    __ mv(t1, JVM_CONSTANT_Double);\n+    __ bne(x12, t1, notDouble);\n+\n+    \/\/ dtos\n+    __ shadd(x12, x10, x11, x12, 3);\n+    __ fld(f10, Address(x12, base_offset));\n+    __ push_d(f10);\n+    __ j(Done);\n+\n+    __ bind(notDouble);\n+    __ mv(t1, (int)JVM_CONSTANT_Long);\n+    __ bne(x12, t1, notLong);\n+\n+    \/\/ ltos\n+    __ shadd(x10, x10, x11, x10, 3);\n+    __ ld(x10, Address(x10, base_offset));\n+    __ push_l(x10);\n+    __ j(Done);\n+\n+    __ bind(notLong);\n+    condy_helper(Done);\n+    __ bind(Done);\n+}\n+\n+void TemplateTable::condy_helper(Label& Done)\n+{\n+  const Register obj = x10;\n+  const Register rarg = x11;\n+  const Register flags = x12;\n+  const Register off = x13;\n+\n+  const address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc);\n+\n+  __ mv(rarg, (int) bytecode());\n+  __ call_VM(obj, entry, rarg);\n+\n+  __ get_vm_result_2(flags, xthread);\n+\n+  \/\/ VMr = obj = base address to find primitive value to push\n+  \/\/ VMr2 = flags = (tos, off) using format of CPCE::_flags\n+  __ mv(off, flags);\n+  __ mv(t0, ConstantPoolCacheEntry::field_index_mask);\n+  __ andrw(off, off, t0);\n+\n+  __ add(off, obj, off);\n+  const Address field(off, 0); \/\/ base + R---->base + offset\n+\n+  __ slli(flags, flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift + ConstantPoolCacheEntry::tos_state_bits));\n+  __ srli(flags, flags, XLEN - ConstantPoolCacheEntry::tos_state_bits); \/\/ (1 << 5) - 4 --> 28~31==> flags:0~3\n+\n+  switch (bytecode()) {\n+    case Bytecodes::_ldc:   \/\/ fall through\n+    case Bytecodes::_ldc_w: {\n+      \/\/ tos in (itos, ftos, stos, btos, ctos, ztos)\n+      Label notInt, notFloat, notShort, notByte, notChar, notBool;\n+      __ mv(t1, itos);\n+      __ bne(flags, t1, notInt);\n+      \/\/ itos\n+      __ lw(x10, field);\n+      __ push(itos);\n+      __ j(Done);\n+\n+      __ bind(notInt);\n+      __ mv(t1, ftos);\n+      __ bne(flags, t1, notFloat);\n+      \/\/ ftos\n+      __ load_float(field);\n+      __ push(ftos);\n+      __ j(Done);\n+\n+      __ bind(notFloat);\n+      __ mv(t1, stos);\n+      __ bne(flags, t1, notShort);\n+      \/\/ stos\n+      __ load_signed_short(x10, field);\n+      __ push(stos);\n+      __ j(Done);\n+\n+      __ bind(notShort);\n+      __ mv(t1, btos);\n+      __ bne(flags, t1, notByte);\n+      \/\/ btos\n+      __ load_signed_byte(x10, field);\n+      __ push(btos);\n+      __ j(Done);\n+\n+      __ bind(notByte);\n+      __ mv(t1, ctos);\n+      __ bne(flags, t1, notChar);\n+      \/\/ ctos\n+      __ load_unsigned_short(x10, field);\n+      __ push(ctos);\n+      __ j(Done);\n+\n+      __ bind(notChar);\n+      __ mv(t1, ztos);\n+      __ bne(flags, t1, notBool);\n+      \/\/ ztos\n+      __ load_signed_byte(x10, field);\n+      __ push(ztos);\n+      __ j(Done);\n+\n+      __ bind(notBool);\n+      break;\n+    }\n+\n+    case Bytecodes::_ldc2_w: {\n+      Label notLong, notDouble;\n+      __ mv(t1, ltos);\n+      __ bne(flags, t1, notLong);\n+      \/\/ ltos\n+      __ ld(x10, field);\n+      __ push(ltos);\n+      __ j(Done);\n+\n+      __ bind(notLong);\n+      __ mv(t1, dtos);\n+      __ bne(flags, t1, notDouble);\n+      \/\/ dtos\n+      __ load_double(field);\n+      __ push(dtos);\n+      __ j(Done);\n+\n+      __ bind(notDouble);\n+      break;\n+    }\n+\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  __ stop(\"bad ldc\/condy\");\n+}\n+\n+void TemplateTable::locals_index(Register reg, int offset)\n+{\n+  __ lbu(reg, at_bcp(offset));\n+  __ neg(reg, reg);\n+}\n+\n+void TemplateTable::iload() {\n+  iload_internal();\n+}\n+\n+void TemplateTable::nofast_iload() {\n+  iload_internal(may_not_rewrite);\n+}\n+\n+void TemplateTable::iload_internal(RewriteControl rc) {\n+  transition(vtos, itos);\n+  if (RewriteFrequentPairs && rc == may_rewrite) {\n+    Label rewrite, done;\n+    const Register bc = x14;\n+\n+    \/\/ get next bytecode\n+    __ load_unsigned_byte(x11, at_bcp(Bytecodes::length_for(Bytecodes::_iload)));\n+\n+    \/\/ if _iload, wait to rewrite to iload2.  We only want to rewrite the\n+    \/\/ last two iloads in a pair.  Comparing against fast_iload means that\n+    \/\/ the next bytecode is neither an iload or a caload, and therefore\n+    \/\/ an iload pair.\n+    __ mv(t1, Bytecodes::_iload);\n+    __ beq(x11, t1, done);\n+\n+    \/\/ if _fast_iload rewrite to _fast_iload2\n+    __ mv(t1, Bytecodes::_fast_iload);\n+    __ mv(bc, Bytecodes::_fast_iload2);\n+    __ beq(x11, t1, rewrite);\n+\n+    \/\/ if _caload rewrite to _fast_icaload\n+    __ mv(t1, Bytecodes::_caload);\n+    __ mv(bc, Bytecodes::_fast_icaload);\n+    __ beq(x11, t1, rewrite);\n+\n+    \/\/ else rewrite to _fast_iload\n+    __ mv(bc, Bytecodes::_fast_iload);\n+\n+    \/\/ rewrite\n+    \/\/ bc: new bytecode\n+    __ bind(rewrite);\n+    patch_bytecode(Bytecodes::_iload, bc, x11, false);\n+    __ bind(done);\n+\n+  }\n+\n+  \/\/ do iload, get the local value into tos\n+  locals_index(x11);\n+  __ lw(x10, iaddress(x11, x10, _masm));\n+}\n+\n+void TemplateTable::fast_iload2()\n+{\n+  transition(vtos, itos);\n+  locals_index(x11);\n+  __ lw(x10, iaddress(x11, x10, _masm));\n+  __ push(itos);\n+  locals_index(x11, 3);\n+  __ lw(x10, iaddress(x11, x10, _masm));\n+}\n+\n+void TemplateTable::fast_iload()\n+{\n+  transition(vtos, itos);\n+  locals_index(x11);\n+  __ lw(x10, iaddress(x11, x10, _masm));\n+}\n+\n+void TemplateTable::lload()\n+{\n+  transition(vtos, ltos);\n+  __ lbu(x11, at_bcp(1));\n+  __ slli(x11, x11, LogBytesPerWord);\n+  __ sub(x11, xlocals, x11);\n+  __ ld(x10, Address(x11, Interpreter::local_offset_in_bytes(1)));\n+}\n+\n+void TemplateTable::fload()\n+{\n+  transition(vtos, ftos);\n+  locals_index(x11);\n+  __ flw(f10, faddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::dload()\n+{\n+  transition(vtos, dtos);\n+  __ lbu(x11, at_bcp(1));\n+  __ slli(x11, x11, LogBytesPerWord);\n+  __ sub(x11, xlocals, x11);\n+  __ fld(f10, Address(x11, Interpreter::local_offset_in_bytes(1)));\n+}\n+\n+void TemplateTable::aload()\n+{\n+  transition(vtos, atos);\n+  locals_index(x11);\n+  __ ld(x10, iaddress(x11, x10, _masm));\n+\n+}\n+\n+void TemplateTable::locals_index_wide(Register reg) {\n+  __ lhu(reg, at_bcp(2));\n+  __ revb_h_h_u(reg, reg); \/\/ reverse bytes in half-word and zero-extend\n+  __ neg(reg, reg);\n+}\n+\n+void TemplateTable::wide_iload() {\n+  transition(vtos, itos);\n+  locals_index_wide(x11);\n+  __ lw(x10, iaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_lload()\n+{\n+  transition(vtos, ltos);\n+  __ lhu(x11, at_bcp(2));\n+  __ revb_h_h_u(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n+  __ slli(x11, x11, LogBytesPerWord);\n+  __ sub(x11, xlocals, x11);\n+  __ ld(x10, Address(x11, Interpreter::local_offset_in_bytes(1)));\n+}\n+\n+void TemplateTable::wide_fload()\n+{\n+  transition(vtos, ftos);\n+  locals_index_wide(x11);\n+  __ flw(f10, faddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_dload()\n+{\n+  transition(vtos, dtos);\n+  __ lhu(x11, at_bcp(2));\n+  __ revb_h_h_u(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n+  __ slli(x11, x11, LogBytesPerWord);\n+  __ sub(x11, xlocals, x11);\n+  __ fld(f10, Address(x11, Interpreter::local_offset_in_bytes(1)));\n+}\n+\n+void TemplateTable::wide_aload()\n+{\n+  transition(vtos, atos);\n+  locals_index_wide(x11);\n+  __ ld(x10, aaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::index_check(Register array, Register index)\n+{\n+  \/\/ destroys x11, t0\n+  \/\/ check array\n+  __ null_check(array, arrayOopDesc::length_offset_in_bytes());\n+  \/\/ sign extend index for use by indexed load\n+  \/\/ check index\n+  const Register length = t0;\n+  __ lwu(length, Address(array, arrayOopDesc::length_offset_in_bytes()));\n+  if (index != x11) {\n+    assert(x11 != array, \"different registers\");\n+    __ mv(x11, index);\n+  }\n+  Label ok;\n+  __ addw(index, index, zr);\n+  __ bltu(index, length, ok);\n+  __ mv(x13, array);\n+  __ mv(t0, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+  __ jr(t0);\n+  __ bind(ok);\n+}\n+\n+void TemplateTable::iaload()\n+{\n+  transition(itos, itos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_INT) >> 2);\n+  __ shadd(x10, x11, x10, t0, 2);\n+  __ access_load_at(T_INT, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+  __ addw(x10, x10, zr); \/\/ signed extended\n+}\n+\n+void TemplateTable::laload()\n+{\n+  transition(itos, ltos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_LONG) >> 3);\n+  __ shadd(x10, x11, x10, t0, 3);\n+  __ access_load_at(T_LONG, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::faload()\n+{\n+  transition(itos, ftos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_FLOAT) >> 2);\n+  __ shadd(x10, x11, x10, t0, 2);\n+  __ access_load_at(T_FLOAT, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::daload()\n+{\n+  transition(itos, dtos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_DOUBLE) >> 3);\n+  __ shadd(x10, x11, x10, t0, 3);\n+  __ access_load_at(T_DOUBLE, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::aaload()\n+{\n+  transition(itos, atos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+  __ shadd(x10, x11, x10, t0, LogBytesPerHeapOop);\n+  do_oop_load(_masm,\n+              Address(x10),\n+              x10,\n+              IS_ARRAY);\n+}\n+\n+void TemplateTable::baload()\n+{\n+  transition(itos, itos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_BYTE) >> 0);\n+  __ shadd(x10, x11, x10, t0, 0);\n+  __ access_load_at(T_BYTE, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::caload()\n+{\n+ transition(itos, itos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_CHAR) >> 1);\n+  __ shadd(x10, x11, x10, t0, 1);\n+  __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+\/\/ iload followed by caload frequent pair\n+void TemplateTable::fast_icaload()\n+{\n+  transition(vtos, itos);\n+  \/\/ load index out of locals\n+  locals_index(x12);\n+  __ lw(x11, iaddress(x12, x11, _masm));\n+  __ pop_ptr(x10);\n+\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11, kills t0\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_CHAR) >> 1); \/\/ addi, max imm is 2^11\n+  __ shadd(x10, x11, x10, t0, 1);\n+  __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::saload()\n+{\n+  transition(itos, itos);\n+  __ mv(x11, x10);\n+  __ pop_ptr(x10);\n+  \/\/ x10: array\n+  \/\/ x11: index\n+  index_check(x10, x11); \/\/ leaves index in x11, kills t0\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_SHORT) >> 1);\n+  __ shadd(x10, x11, x10, t0, 1);\n+  __ access_load_at(T_SHORT, IN_HEAP | IS_ARRAY, x10, Address(x10), noreg, noreg);\n+}\n+\n+void TemplateTable::iload(int n)\n+{\n+  transition(vtos, itos);\n+  __ lw(x10, iaddress(n));\n+}\n+\n+void TemplateTable::lload(int n)\n+{\n+  transition(vtos, ltos);\n+  __ ld(x10, laddress(n));\n+}\n+\n+void TemplateTable::fload(int n)\n+{\n+  transition(vtos, ftos);\n+  __ flw(f10, faddress(n));\n+}\n+\n+void TemplateTable::dload(int n)\n+{\n+  transition(vtos, dtos);\n+  __ fld(f10, daddress(n));\n+}\n+\n+void TemplateTable::aload(int n)\n+{\n+  transition(vtos, atos);\n+  __ ld(x10, iaddress(n));\n+}\n+\n+void TemplateTable::aload_0() {\n+  aload_0_internal();\n+}\n+\n+void TemplateTable::nofast_aload_0() {\n+  aload_0_internal(may_not_rewrite);\n+}\n+\n+void TemplateTable::aload_0_internal(RewriteControl rc) {\n+  \/\/ According to bytecode histograms, the pairs:\n+  \/\/\n+  \/\/ _aload_0, _fast_igetfield\n+  \/\/ _aload_0, _fast_agetfield\n+  \/\/ _aload_0, _fast_fgetfield\n+  \/\/\n+  \/\/ occur frequently. If RewriteFrequentPairs is set, the (slow)\n+  \/\/ _aload_0 bytecode checks if the next bytecode is either\n+  \/\/ _fast_igetfield, _fast_agetfield or _fast_fgetfield and then\n+  \/\/ rewrites the current bytecode into a pair bytecode; otherwise it\n+  \/\/ rewrites the current bytecode into _fast_aload_0 that doesn't do\n+  \/\/ the pair check anymore.\n+  \/\/\n+  \/\/ Note: If the next bytecode is _getfield, the rewrite must be\n+  \/\/       delayed, otherwise we may miss an opportunity for a pair.\n+  \/\/\n+  \/\/ Also rewrite frequent pairs\n+  \/\/   aload_0, aload_1\n+  \/\/   aload_0, iload_1\n+  \/\/ These bytecodes with a small amount of code are most profitable\n+  \/\/ to rewrite\n+  if (RewriteFrequentPairs && rc == may_rewrite) {\n+    Label rewrite, done;\n+    const Register bc = x14;\n+\n+    \/\/ get next bytecode\n+    __ load_unsigned_byte(x11, at_bcp(Bytecodes::length_for(Bytecodes::_aload_0)));\n+\n+    \/\/ if _getfield then wait with rewrite\n+    __ mv(t1, Bytecodes::Bytecodes::_getfield);\n+    __ beq(x11, t1, done);\n+\n+    \/\/ if _igetfield then rewrite to _fast_iaccess_0\n+    assert(Bytecodes::java_code(Bytecodes::_fast_iaccess_0) == Bytecodes::_aload_0, \"fix bytecode definition\");\n+    __ mv(t1, Bytecodes::_fast_igetfield);\n+    __ mv(bc, Bytecodes::_fast_iaccess_0);\n+    __ beq(x11, t1, rewrite);\n+\n+    \/\/ if _agetfield then rewrite to _fast_aaccess_0\n+    assert(Bytecodes::java_code(Bytecodes::_fast_aaccess_0) == Bytecodes::_aload_0, \"fix bytecode definition\");\n+    __ mv(t1, Bytecodes::_fast_agetfield);\n+    __ mv(bc, Bytecodes::_fast_aaccess_0);\n+    __ beq(x11, t1, rewrite);\n+\n+    \/\/ if _fgetfield then rewrite to _fast_faccess_0\n+    assert(Bytecodes::java_code(Bytecodes::_fast_faccess_0) == Bytecodes::_aload_0, \"fix bytecode definition\");\n+    __ mv(t1, Bytecodes::_fast_fgetfield);\n+    __ mv(bc, Bytecodes::_fast_faccess_0);\n+    __ beq(x11, t1, rewrite);\n+\n+    \/\/ else rewrite to _fast_aload0\n+    assert(Bytecodes::java_code(Bytecodes::_fast_aload_0) == Bytecodes::_aload_0, \"fix bytecode definition\");\n+    __ mv(bc, Bytecodes::Bytecodes::_fast_aload_0);\n+\n+    \/\/ rewrite\n+    \/\/ bc: new bytecode\n+    __ bind(rewrite);\n+    patch_bytecode(Bytecodes::_aload_0, bc, x11, false);\n+\n+    __ bind(done);\n+  }\n+\n+  \/\/ Do actual aload_0 (must do this after patch_bytecode which might call VM and GC might change oop).\n+  aload(0);\n+}\n+\n+void TemplateTable::istore()\n+{\n+  transition(itos, vtos);\n+  locals_index(x11);\n+  __ sw(x10, iaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::lstore()\n+{\n+  transition(ltos, vtos);\n+  locals_index(x11);\n+  __ sd(x10, laddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::fstore() {\n+  transition(ftos, vtos);\n+  locals_index(x11);\n+  __ fsw(f10, iaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::dstore() {\n+  transition(dtos, vtos);\n+  locals_index(x11);\n+  __ fsd(f10, daddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::astore()\n+{\n+  transition(vtos, vtos);\n+  __ pop_ptr(x10);\n+  locals_index(x11);\n+  __ sd(x10, aaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_istore() {\n+  transition(vtos, vtos);\n+  __ pop_i();\n+  locals_index_wide(x11);\n+  __ sw(x10, iaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_lstore() {\n+  transition(vtos, vtos);\n+  __ pop_l();\n+  locals_index_wide(x11);\n+  __ sd(x10, laddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_fstore() {\n+  transition(vtos, vtos);\n+  __ pop_f();\n+  locals_index_wide(x11);\n+  __ fsw(f10, faddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_dstore() {\n+  transition(vtos, vtos);\n+  __ pop_d();\n+  locals_index_wide(x11);\n+  __ fsd(f10, daddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::wide_astore() {\n+  transition(vtos, vtos);\n+  __ pop_ptr(x10);\n+  locals_index_wide(x11);\n+  __ sd(x10, aaddress(x11, t0, _masm));\n+}\n+\n+void TemplateTable::iastore() {\n+  transition(itos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ x10: value\n+  \/\/ x11: index\n+  \/\/ x13: array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_INT) >> 2);\n+  __ shadd(t0, x11, x13, t0, 2);\n+  __ access_store_at(T_INT, IN_HEAP | IS_ARRAY, Address(t0, 0), x10, noreg, noreg);\n+}\n+\n+void TemplateTable::lastore() {\n+  transition(ltos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ x10: value\n+  \/\/ x11: index\n+  \/\/ x13: array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_LONG) >> 3);\n+  __ shadd(t0, x11, x13, t0, 3);\n+  __ access_store_at(T_LONG, IN_HEAP | IS_ARRAY, Address(t0, 0), x10, noreg, noreg);\n+}\n+\n+void TemplateTable::fastore() {\n+  transition(ftos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ f10: value\n+  \/\/ x11:  index\n+  \/\/ x13:  array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_FLOAT) >> 2);\n+  __ shadd(t0, x11, x13, t0, 2);\n+  __ access_store_at(T_FLOAT, IN_HEAP | IS_ARRAY, Address(t0, 0), noreg \/* ftos *\/, noreg, noreg);\n+}\n+\n+void TemplateTable::dastore() {\n+  transition(dtos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ f10: value\n+  \/\/ x11:  index\n+  \/\/ x13:  array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_DOUBLE) >> 3);\n+  __ shadd(t0, x11, x13, t0, 3);\n+  __ access_store_at(T_DOUBLE, IN_HEAP | IS_ARRAY, Address(t0, 0), noreg \/* dtos *\/, noreg, noreg);\n+}\n+\n+void TemplateTable::aastore() {\n+  Label is_null, ok_is_subtype, done;\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., array, index, value\n+  __ ld(x10, at_tos());    \/\/ value\n+  __ ld(x12, at_tos_p1()); \/\/ index\n+  __ ld(x13, at_tos_p2()); \/\/ array\n+\n+  index_check(x13, x12);     \/\/ kills x11\n+  __ add(x14, x12, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+  __ shadd(x14, x14, x13, x14, LogBytesPerHeapOop);\n+\n+  Address element_address(x14, 0);\n+\n+  \/\/ do array store check - check for NULL value first\n+  __ beqz(x10, is_null);\n+\n+  \/\/ Move subklass into x11\n+  __ load_klass(x11, x10);\n+  \/\/ Move superklass into x10\n+  __ load_klass(x10, x13);\n+  __ ld(x10, Address(x10,\n+                     ObjArrayKlass::element_klass_offset()));\n+  \/\/ Compress array + index * oopSize + 12 into a single register.  Frees x12.\n+\n+  \/\/ Generate subtype check.  Blows x12, x15\n+  \/\/ Superklass in x10.  Subklass in x11.\n+  __ gen_subtype_check(x11, ok_is_subtype); \/\/todo\n+\n+  \/\/ Come here on failure\n+  \/\/ object is at TOS\n+  __ j(Interpreter::_throw_ArrayStoreException_entry);\n+\n+  \/\/ Come here on success\n+  __ bind(ok_is_subtype);\n+\n+  \/\/ Get the value we will store\n+  __ ld(x10, at_tos());\n+  \/\/ Now store using the appropriate barrier\n+  do_oop_store(_masm, element_address, x10, IS_ARRAY);\n+  __ j(done);\n+\n+  \/\/ Have a NULL in x10, x13=array, x12=index.  Store NULL at ary[idx]\n+  __ bind(is_null);\n+  __ profile_null_seen(x12);\n+\n+  \/\/ Store a NULL\n+  do_oop_store(_masm, element_address, noreg, IS_ARRAY);\n+\n+  \/\/ Pop stack arguments\n+  __ bind(done);\n+  __ add(esp, esp, 3 * Interpreter::stackElementSize);\n+\n+}\n+\n+void TemplateTable::bastore()\n+{\n+  transition(itos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ x10: value\n+  \/\/ x11: index\n+  \/\/ x13: array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+\n+  \/\/ Need to check whether array is boolean or byte\n+  \/\/ since both types share the bastore bytecode.\n+  __ load_klass(x12, x13);\n+  __ lwu(x12, Address(x12, Klass::layout_helper_offset()));\n+  Label L_skip;\n+  __ andi(t0, x12, Klass::layout_helper_boolean_diffbit());\n+  __ beqz(t0, L_skip);\n+  __ andi(x10, x10, 1);  \/\/ if it is a T_BOOLEAN array, mask the stored value to 0\/1\n+  __ bind(L_skip);\n+\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_BYTE) >> 0);\n+\n+  __ add(x11, x13, x11);\n+  __ access_store_at(T_BYTE, IN_HEAP | IS_ARRAY, Address(x11, 0), x10, noreg, noreg);\n+}\n+\n+void TemplateTable::castore()\n+{\n+  transition(itos, vtos);\n+  __ pop_i(x11);\n+  __ pop_ptr(x13);\n+  \/\/ x10: value\n+  \/\/ x11: index\n+  \/\/ x13: array\n+  index_check(x13, x11); \/\/ prefer index in x11\n+  __ add(x11, x11, arrayOopDesc::base_offset_in_bytes(T_CHAR) >> 1);\n+  __ shadd(t0, x11, x13, t0, 1);\n+  __ access_store_at(T_CHAR, IN_HEAP | IS_ARRAY, Address(t0, 0), x10, noreg, noreg);\n+}\n+\n+void TemplateTable::sastore()\n+{\n+  castore();\n+}\n+\n+void TemplateTable::istore(int n)\n+{\n+  transition(itos, vtos);\n+  __ sd(x10, iaddress(n));\n+}\n+\n+void TemplateTable::lstore(int n)\n+{\n+  transition(ltos, vtos);\n+  __ sd(x10, laddress(n));\n+}\n+\n+void TemplateTable::fstore(int n)\n+{\n+  transition(ftos, vtos);\n+  __ fsw(f10, faddress(n));\n+}\n+\n+void TemplateTable::dstore(int n)\n+{\n+  transition(dtos, vtos);\n+  __ fsd(f10, daddress(n));\n+}\n+\n+void TemplateTable::astore(int n)\n+{\n+  transition(vtos, vtos);\n+  __ pop_ptr(x10);\n+  __ sd(x10, iaddress(n));\n+}\n+\n+void TemplateTable::pop()\n+{\n+  transition(vtos, vtos);\n+  __ addi(esp, esp, Interpreter::stackElementSize);\n+}\n+\n+void TemplateTable::pop2()\n+{\n+  transition(vtos, vtos);\n+  __ addi(esp, esp, 2 * Interpreter::stackElementSize);\n+}\n+\n+void TemplateTable::dup()\n+{\n+  transition(vtos, vtos);\n+  __ ld(x10, Address(esp, 0));\n+  __ push_reg(x10);\n+  \/\/ stack: ..., a, a\n+}\n+\n+void TemplateTable::dup_x1()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b\n+  __ ld(x10, at_tos());  \/\/ load b\n+  __ ld(x12, at_tos_p1());  \/\/ load a\n+  __ sd(x10, at_tos_p1());  \/\/ store b\n+  __ sd(x12, at_tos());  \/\/ store a\n+  __ push_reg(x10);                  \/\/ push b\n+  \/\/ stack: ..., b, a, b\n+}\n+\n+void TemplateTable::dup_x2()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b, c\n+  __ ld(x10, at_tos());  \/\/ load c\n+  __ ld(x12, at_tos_p2());  \/\/ load a\n+  __ sd(x10, at_tos_p2());  \/\/ store c in a\n+  __ push_reg(x10);      \/\/ push c\n+  \/\/ stack: ..., c, b, c, c\n+  __ ld(x10, at_tos_p2());  \/\/ load b\n+  __ sd(x12, at_tos_p2());  \/\/ store a in b\n+  \/\/ stack: ..., c, a, c, c\n+  __ sd(x10, at_tos_p1());  \/\/ store b in c\n+  \/\/ stack: ..., c, a, b, c\n+}\n+\n+void TemplateTable::dup2()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b\n+  __ ld(x10, at_tos_p1());  \/\/ load a\n+  __ push_reg(x10);                  \/\/ push a\n+  __ ld(x10, at_tos_p1());  \/\/ load b\n+  __ push_reg(x10);                  \/\/ push b\n+  \/\/ stack: ..., a, b, a, b\n+}\n+\n+void TemplateTable::dup2_x1()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b, c\n+  __ ld(x12, at_tos());     \/\/ load c\n+  __ ld(x10, at_tos_p1());  \/\/ load b\n+  __ push_reg(x10);             \/\/ push b\n+  __ push_reg(x12);             \/\/ push c\n+  \/\/ stack: ..., a, b, c, b, c\n+  __ sd(x12, at_tos_p3());  \/\/ store c in b\n+  \/\/ stack: ..., a, c, c, b, c\n+  __ ld(x12, at_tos_p4());  \/\/ load a\n+  __ sd(x12, at_tos_p2());  \/\/ store a in 2nd c\n+  \/\/ stack: ..., a, c, a, b, c\n+  __ sd(x10, at_tos_p4());  \/\/ store b in a\n+  \/\/ stack: ..., b, c, a, b, c\n+}\n+\n+void TemplateTable::dup2_x2()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b, c, d\n+  __ ld(x12, at_tos());     \/\/ load d\n+  __ ld(x10, at_tos_p1());  \/\/ load c\n+  __ push_reg(x10);             \/\/ push c\n+  __ push_reg(x12);             \/\/ push d\n+  \/\/ stack: ..., a, b, c, d, c, d\n+  __ ld(x10, at_tos_p4());  \/\/ load b\n+  __ sd(x10, at_tos_p2());  \/\/ store b in d\n+  __ sd(x12, at_tos_p4());  \/\/ store d in b\n+  \/\/ stack: ..., a, d, c, b, c, d\n+  __ ld(x12, at_tos_p5());  \/\/ load a\n+  __ ld(x10, at_tos_p3());  \/\/ load c\n+  __ sd(x12, at_tos_p3());  \/\/ store a in c\n+  __ sd(x10, at_tos_p5());  \/\/ store c in a\n+  \/\/ stack: ..., c, d, a, b, c, d\n+}\n+\n+void TemplateTable::swap()\n+{\n+  transition(vtos, vtos);\n+  \/\/ stack: ..., a, b\n+  __ ld(x12, at_tos_p1());  \/\/ load a\n+  __ ld(x10, at_tos());     \/\/ load b\n+  __ sd(x12, at_tos());     \/\/ store a in b\n+  __ sd(x10, at_tos_p1());  \/\/ store b in a\n+  \/\/ stack: ..., b, a\n+}\n+\n+void TemplateTable::iop2(Operation op)\n+{\n+  transition(itos, itos);\n+  \/\/ x10 <== x11 op x10\n+  __ pop_i(x11);\n+  switch (op) {\n+    case add  : __ addw(x10, x11, x10);  break;\n+    case sub  : __ subw(x10, x11, x10);  break;\n+    case mul  : __ mulw(x10, x11, x10);  break;\n+    case _and : __ andrw(x10, x11, x10); break;\n+    case _or  : __ orrw(x10, x11, x10);  break;\n+    case _xor : __ xorrw(x10, x11, x10); break;\n+    case shl  : __ sllw(x10, x11, x10);  break;\n+    case shr  : __ sraw(x10, x11, x10);  break;\n+    case ushr : __ srlw(x10, x11, x10);  break;\n+    default   : ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::lop2(Operation op)\n+{\n+  transition(ltos, ltos);\n+  \/\/ x10 <== x11 op x10\n+  __ pop_l(x11);\n+  switch (op) {\n+    case add  : __ add(x10, x11, x10);  break;\n+    case sub  : __ sub(x10, x11, x10);  break;\n+    case mul  : __ mul(x10, x11, x10);  break;\n+    case _and : __ andr(x10, x11, x10); break;\n+    case _or  : __ orr(x10, x11, x10);  break;\n+    case _xor : __ xorr(x10, x11, x10); break;\n+    default   : ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::idiv()\n+{\n+  transition(itos, itos);\n+  \/\/ explicitly check for div0\n+  Label no_div0;\n+  __ bnez(x10, no_div0);\n+  __ mv(t0, Interpreter::_throw_ArithmeticException_entry);\n+  __ jr(t0);\n+  __ bind(no_div0);\n+  __ pop_i(x11);\n+  \/\/ x10 <== x11 idiv x10\n+  __ corrected_idivl(x10, x11, x10, \/* want_remainder *\/ false);\n+}\n+\n+void TemplateTable::irem()\n+{\n+  transition(itos, itos);\n+  \/\/ explicitly check for div0\n+  Label no_div0;\n+  __ bnez(x10, no_div0);\n+  __ mv(t0, Interpreter::_throw_ArithmeticException_entry);\n+  __ jr(t0);\n+  __ bind(no_div0);\n+  __ pop_i(x11);\n+  \/\/ x10 <== x11 irem x10\n+  __ corrected_idivl(x10, x11, x10, \/* want_remainder *\/ true);\n+}\n+\n+void TemplateTable::lmul()\n+{\n+  transition(ltos, ltos);\n+  __ pop_l(x11);\n+  __ mul(x10, x10, x11);\n+}\n+\n+void TemplateTable::ldiv()\n+{\n+  transition(ltos, ltos);\n+  \/\/ explicitly check for div0\n+  Label no_div0;\n+  __ bnez(x10, no_div0);\n+  __ mv(t0, Interpreter::_throw_ArithmeticException_entry);\n+  __ jr(t0);\n+  __ bind(no_div0);\n+  __ pop_l(x11);\n+  \/\/ x10 <== x11 ldiv x10\n+  __ corrected_idivq(x10, x11, x10, \/* want_remainder *\/ false);\n+}\n+\n+void TemplateTable::lrem()\n+{\n+  transition(ltos, ltos);\n+  \/\/ explicitly check for div0\n+  Label no_div0;\n+  __ bnez(x10, no_div0);\n+  __ mv(t0, Interpreter::_throw_ArithmeticException_entry);\n+  __ jr(t0);\n+  __ bind(no_div0);\n+  __ pop_l(x11);\n+  \/\/ x10 <== x11 lrem x10\n+  __ corrected_idivq(x10, x11, x10, \/* want_remainder *\/ true);\n+}\n+\n+void TemplateTable::lshl()\n+{\n+  transition(itos, ltos);\n+  \/\/ shift count is in x10\n+  __ pop_l(x11);\n+  __ sll(x10, x11, x10);\n+}\n+\n+void TemplateTable::lshr()\n+{\n+  transition(itos, ltos);\n+  \/\/ shift count is in x10\n+  __ pop_l(x11);\n+  __ sra(x10, x11, x10);\n+}\n+\n+void TemplateTable::lushr()\n+{\n+  transition(itos, ltos);\n+  \/\/ shift count is in x10\n+  __ pop_l(x11);\n+  __ srl(x10, x11, x10);\n+}\n+\n+void TemplateTable::fop2(Operation op)\n+{\n+  transition(ftos, ftos);\n+  switch (op) {\n+    case add:\n+      __ pop_f(f11);\n+      __ fadd_s(f10, f11, f10);\n+      break;\n+    case sub:\n+      __ pop_f(f11);\n+      __ fsub_s(f10, f11, f10);\n+      break;\n+    case mul:\n+      __ pop_f(f11);\n+      __ fmul_s(f10, f11, f10);\n+      break;\n+    case div:\n+      __ pop_f(f11);\n+      __ fdiv_s(f10, f11, f10);\n+      break;\n+    case rem:\n+      __ fmv_s(f11, f10);\n+      __ pop_f(f10);\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::dop2(Operation op)\n+{\n+  transition(dtos, dtos);\n+  switch (op) {\n+    case add:\n+      __ pop_d(f11);\n+      __ fadd_d(f10, f11, f10);\n+      break;\n+    case sub:\n+      __ pop_d(f11);\n+      __ fsub_d(f10, f11, f10);\n+      break;\n+    case mul:\n+      __ pop_d(f11);\n+      __ fmul_d(f10, f11, f10);\n+      break;\n+    case div:\n+      __ pop_d(f11);\n+      __ fdiv_d(f10, f11, f10);\n+      break;\n+    case rem:\n+      __ fmv_d(f11, f10);\n+      __ pop_d(f10);\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::ineg()\n+{\n+  transition(itos, itos);\n+  __ negw(x10, x10);\n+}\n+\n+void TemplateTable::lneg()\n+{\n+  transition(ltos, ltos);\n+  __ neg(x10, x10);\n+}\n+\n+void TemplateTable::fneg()\n+{\n+  transition(ftos, ftos);\n+  __ fneg_s(f10, f10);\n+}\n+\n+void TemplateTable::dneg()\n+{\n+  transition(dtos, dtos);\n+  __ fneg_d(f10, f10);\n+}\n+\n+void TemplateTable::iinc()\n+{\n+  transition(vtos, vtos);\n+  __ load_signed_byte(x11, at_bcp(2)); \/\/ get constant\n+  locals_index(x12);\n+  __ ld(x10, iaddress(x12, x10, _masm));\n+  __ addw(x10, x10, x11);\n+  __ sd(x10, iaddress(x12, t0, _masm));\n+}\n+\n+void TemplateTable::wide_iinc()\n+{\n+  transition(vtos, vtos);\n+  __ lwu(x11, at_bcp(2)); \/\/ get constant and index\n+  __ revb_h_w_u(x11, x11); \/\/ reverse bytes in half-word (32bit) and zero-extend\n+  __ zero_extend(x12, x11, 16);\n+  __ neg(x12, x12);\n+  __ slli(x11, x11, 32);\n+  __ srai(x11, x11, 48);\n+  __ ld(x10, iaddress(x12, t0, _masm));\n+  __ addw(x10, x10, x11);\n+  __ sd(x10, iaddress(x12, t0, _masm));\n+}\n+\n+void TemplateTable::convert()\n+{\n+  \/\/ Checking\n+#ifdef ASSERT\n+  {\n+    TosState tos_in  = ilgl;\n+    TosState tos_out = ilgl;\n+    switch (bytecode()) {\n+      case Bytecodes::_i2l: \/\/ fall through\n+      case Bytecodes::_i2f: \/\/ fall through\n+      case Bytecodes::_i2d: \/\/ fall through\n+      case Bytecodes::_i2b: \/\/ fall through\n+      case Bytecodes::_i2c: \/\/ fall through\n+      case Bytecodes::_i2s: tos_in = itos; break;\n+      case Bytecodes::_l2i: \/\/ fall through\n+      case Bytecodes::_l2f: \/\/ fall through\n+      case Bytecodes::_l2d: tos_in = ltos; break;\n+      case Bytecodes::_f2i: \/\/ fall through\n+      case Bytecodes::_f2l: \/\/ fall through\n+      case Bytecodes::_f2d: tos_in = ftos; break;\n+      case Bytecodes::_d2i: \/\/ fall through\n+      case Bytecodes::_d2l: \/\/ fall through\n+      case Bytecodes::_d2f: tos_in = dtos; break;\n+      default             : ShouldNotReachHere();\n+    }\n+    switch (bytecode()) {\n+      case Bytecodes::_l2i: \/\/ fall through\n+      case Bytecodes::_f2i: \/\/ fall through\n+      case Bytecodes::_d2i: \/\/ fall through\n+      case Bytecodes::_i2b: \/\/ fall through\n+      case Bytecodes::_i2c: \/\/ fall through\n+      case Bytecodes::_i2s: tos_out = itos; break;\n+      case Bytecodes::_i2l: \/\/ fall through\n+      case Bytecodes::_f2l: \/\/ fall through\n+      case Bytecodes::_d2l: tos_out = ltos; break;\n+      case Bytecodes::_i2f: \/\/ fall through\n+      case Bytecodes::_l2f: \/\/ fall through\n+      case Bytecodes::_d2f: tos_out = ftos; break;\n+      case Bytecodes::_i2d: \/\/ fall through\n+      case Bytecodes::_l2d: \/\/ fall through\n+      case Bytecodes::_f2d: tos_out = dtos; break;\n+      default             : ShouldNotReachHere();\n+    }\n+    transition(tos_in, tos_out);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Conversion\n+  switch (bytecode()) {\n+    case Bytecodes::_i2l:\n+      __ sign_extend(x10, x10, 32);\n+      break;\n+    case Bytecodes::_i2f:\n+      __ fcvt_s_w(f10, x10);\n+      break;\n+    case Bytecodes::_i2d:\n+      __ fcvt_d_w(f10, x10);\n+      break;\n+    case Bytecodes::_i2b:\n+      __ sign_extend(x10, x10, 8);\n+      break;\n+    case Bytecodes::_i2c:\n+      __ zero_extend(x10, x10, 16);\n+      break;\n+    case Bytecodes::_i2s:\n+      __ sign_extend(x10, x10, 16);\n+      break;\n+    case Bytecodes::_l2i:\n+      __ addw(x10, x10, zr);\n+      break;\n+    case Bytecodes::_l2f:\n+      __ fcvt_s_l(f10, x10);\n+      break;\n+    case Bytecodes::_l2d:\n+      __ fcvt_d_l(f10, x10);\n+      break;\n+    case Bytecodes::_f2i:\n+      __ fcvt_w_s_safe(x10, f10);\n+      break;\n+    case Bytecodes::_f2l:\n+      __ fcvt_l_s_safe(x10, f10);\n+      break;\n+    case Bytecodes::_f2d:\n+      __ fcvt_d_s(f10, f10);\n+      break;\n+    case Bytecodes::_d2i:\n+      __ fcvt_w_d_safe(x10, f10);\n+      break;\n+    case Bytecodes::_d2l:\n+      __ fcvt_l_d_safe(x10, f10);\n+      break;\n+    case Bytecodes::_d2f:\n+      __ fcvt_s_d(f10, f10);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void TemplateTable::lcmp()\n+{\n+  transition(ltos, itos);\n+  __ pop_l(x11);\n+  __ cmp_l2i(t0, x11, x10);\n+  __ mv(x10, t0);\n+}\n+\n+void TemplateTable::float_cmp(bool is_float, int unordered_result)\n+{\n+  \/\/ For instruction feq, flt and fle, the result is 0 if either operand is NaN\n+  if (is_float) {\n+    __ pop_f(f11);\n+    \/\/ if unordered_result < 0:\n+    \/\/   we want -1 for unordered or less than, 0 for equal and 1 for\n+    \/\/   greater than.\n+    \/\/ else:\n+    \/\/   we want -1 for less than, 0 for equal and 1 for unordered or\n+    \/\/   greater than.\n+    \/\/ f11 primary, f10 secondary\n+    __ float_compare(x10, f11, f10, unordered_result);\n+  } else {\n+    __ pop_d(f11);\n+    \/\/ if unordered_result < 0:\n+    \/\/   we want -1 for unordered or less than, 0 for equal and 1 for\n+    \/\/   greater than.\n+    \/\/ else:\n+    \/\/   we want -1 for less than, 0 for equal and 1 for unordered or\n+    \/\/   greater than.\n+    \/\/ f11 primary, f10 secondary\n+    __ double_compare(x10, f11, f10, unordered_result);\n+  }\n+}\n+\n+void TemplateTable::branch(bool is_jsr, bool is_wide)\n+{\n+  \/\/ We might be moving to a safepoint.  The thread which calls\n+  \/\/ Interpreter::notice_safepoints() will effectively flush its cache\n+  \/\/ when it makes a system call, but we need to do something to\n+  \/\/ ensure that we see the changed dispatch table.\n+  __ membar(MacroAssembler::LoadLoad);\n+\n+  __ profile_taken_branch(x10, x11);\n+  const ByteSize be_offset = MethodCounters::backedge_counter_offset() +\n+                             InvocationCounter::counter_offset();\n+  const ByteSize inv_offset = MethodCounters::invocation_counter_offset() +\n+                              InvocationCounter::counter_offset();\n+\n+  \/\/ load branch displacement\n+  if (!is_wide) {\n+    __ lhu(x12, at_bcp(1));\n+    __ revb_h_h(x12, x12); \/\/ reverse bytes in half-word and sign-extend\n+  } else {\n+    __ lwu(x12, at_bcp(1));\n+    __ revb_w_w(x12, x12); \/\/ reverse bytes in word and sign-extend\n+  }\n+\n+  \/\/ Handle all the JSR stuff here, then exit.\n+  \/\/ It's much shorter and cleaner than intermingling with the non-JSR\n+  \/\/ normal-branch stuff occurring below.\n+\n+  if (is_jsr) {\n+    \/\/ compute return address as bci\n+    __ ld(t1, Address(xmethod, Method::const_offset()));\n+    __ add(t1, t1,\n+           in_bytes(ConstMethod::codes_offset()) - (is_wide ? 5 : 3));\n+    __ sub(x11, xbcp, t1);\n+    __ push_i(x11);\n+    \/\/ Adjust the bcp by the 16-bit displacement in x12\n+    __ add(xbcp, xbcp, x12);\n+    __ load_unsigned_byte(t0, Address(xbcp, 0));\n+    \/\/ load the next target bytecode into t0, it is the argument of dispatch_only\n+    __ dispatch_only(vtos, \/*generate_poll*\/true);\n+    return;\n+  }\n+\n+  \/\/ Normal (non-jsr) branch handling\n+\n+  \/\/ Adjust the bcp by the displacement in x12\n+  __ add(xbcp, xbcp, x12);\n+\n+  assert(UseLoopCounter || !UseOnStackReplacement,\n+         \"on-stack-replacement requires loop counters\");\n+  Label backedge_counter_overflow;\n+  Label profile_method;\n+  Label dispatch;\n+  if (UseLoopCounter) {\n+    \/\/ increment backedge counter for backward branches\n+    \/\/ x10: MDO\n+    \/\/ x11: MDO bumped taken-count\n+    \/\/ x12: target offset\n+    __ bgtz(x12, dispatch); \/\/ count only if backward branch\n+\n+    \/\/ check if MethodCounters exists\n+    Label has_counters;\n+    __ ld(t0, Address(xmethod, Method::method_counters_offset()));\n+    __ bnez(t0, has_counters);\n+    __ push_reg(x10);\n+    __ push_reg(x11);\n+    __ push_reg(x12);\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+            InterpreterRuntime::build_method_counters), xmethod);\n+    __ pop_reg(x12);\n+    __ pop_reg(x11);\n+    __ pop_reg(x10);\n+    __ ld(t0, Address(xmethod, Method::method_counters_offset()));\n+    __ beqz(t0, dispatch); \/\/ No MethodCounters allocated, OutOfMemory\n+    __ bind(has_counters);\n+\n+    if (TieredCompilation) {\n+      Label no_mdo;\n+      int increment = InvocationCounter::count_increment;\n+      if (ProfileInterpreter) {\n+        \/\/ Are we profiling?\n+        __ ld(x11, Address(xmethod, in_bytes(Method::method_data_offset())));\n+        __ beqz(x11, no_mdo);\n+        \/\/ Increment the MDO backedge counter\n+        const Address mdo_backedge_counter(x11, in_bytes(MethodData::backedge_counter_offset()) +\n+                                           in_bytes(InvocationCounter::counter_offset()));\n+        const Address mask(x11, in_bytes(MethodData::backedge_mask_offset()));\n+        __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n+                                   x10, t0, false,\n+                                   UseOnStackReplacement ? &backedge_counter_overflow : &dispatch);\n+        __ j(dispatch);\n+      }\n+      __ bind(no_mdo);\n+      \/\/ Increment backedge counter in MethodCounters*\n+      __ ld(t0, Address(xmethod, Method::method_counters_offset()));\n+      const Address mask(t0, in_bytes(MethodCounters::backedge_mask_offset()));\n+      __ increment_mask_and_jump(Address(t0, be_offset), increment, mask,\n+                                 x10, t1, false,\n+                                 UseOnStackReplacement ? &backedge_counter_overflow : &dispatch);\n+    } else { \/\/ not TieredCompilation\n+      \/\/ increment counter\n+      __ ld(t1, Address(xmethod, Method::method_counters_offset()));\n+      __ lwu(x10, Address(t1, be_offset));     \/\/ load backedge counter\n+      __ addw(t0, x10, InvocationCounter::count_increment); \/\/ increment counter\n+      __ sw(t0, Address(t1, be_offset));       \/\/ store counter\n+\n+      __ lwu(x10, Address(t1, inv_offset));    \/\/ load invocation counter\n+      __ andi(x10, x10, (unsigned)InvocationCounter::count_mask_value, x13); \/\/ and the status bits\n+      __ addw(x10, x10, t0);        \/\/ add both counters\n+\n+      if (ProfileInterpreter) {\n+        \/\/ Test to see if we should create a method data oop\n+        __ lwu(t0, Address(t1, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n+        __ blt(x10, t0, dispatch);\n+\n+        \/\/ if no method data exists, go to profile method\n+        __ test_method_data_pointer(x10, profile_method);\n+\n+        if (UseOnStackReplacement) {\n+          \/\/ check for overflow against x11 which is the MDO taken count\n+          __ lwu(t0, Address(t1, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n+          __ bltu(x11, t0, dispatch); \/\/ Intel == Assembler::below, lo:unsigned lower\n+\n+          \/\/ When ProfileInterpreter is on, the backedge_count comes\n+          \/\/ from the MethodData*, which value does not get reset on\n+          \/\/ the call to frequency_counter_overflow().  To avoid\n+          \/\/ excessive calls to the overflow routine while the method is\n+          \/\/ being compiled, add a second test to make sure the overflow\n+          \/\/ function is called only once every overflow_frequency.\n+          const int overflow_frequency = 1024;\n+          __ andi(x11, x11, overflow_frequency - 1);\n+          __ beqz(x11, backedge_counter_overflow);\n+\n+        }\n+      } else {\n+        if (UseOnStackReplacement) {\n+          \/\/ check for overflow against x10, which is the sum of the\n+          \/\/ counters\n+          __ lwu(t0, Address(t1, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n+          __ bgeu(x10, t0, backedge_counter_overflow); \/\/ Intel == Assembler::aboveEqual\n+        }\n+      }\n+    }\n+    __ bind(dispatch);\n+  }\n+  \/\/ Pre-load the next target bytecode into t0\n+  __ load_unsigned_byte(t0, Address(xbcp, 0));\n+\n+  \/\/ continue with the bytecode @ target\n+  \/\/ t0: target bytecode\n+  \/\/ xbcp: target bcp\n+  __ dispatch_only(vtos, \/*generate_poll*\/true);\n+\n+  if (UseLoopCounter) {\n+    if (ProfileInterpreter && !TieredCompilation) {\n+      \/\/ Out-of-line code to allocate method data oop.\n+      __ bind(profile_method);\n+      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n+      __ load_unsigned_byte(x11, Address(xbcp, 0));  \/\/ restore target bytecode\n+      __ set_method_data_pointer_for_bcp();\n+      __ j(dispatch);\n+    }\n+\n+    if (UseOnStackReplacement) {\n+      \/\/ invocation counter overflow\n+      __ bind(backedge_counter_overflow);\n+      __ neg(x12, x12);\n+      __ add(x12, x12, xbcp);     \/\/ branch xbcp\n+      \/\/ IcoResult frequency_counter_overflow([JavaThread*], address branch_bcp)\n+      __ call_VM(noreg,\n+                 CAST_FROM_FN_PTR(address,\n+                                  InterpreterRuntime::frequency_counter_overflow),\n+                 x12);\n+      __ load_unsigned_byte(x11, Address(xbcp, 0));  \/\/ restore target bytecode\n+\n+      \/\/ x10: osr nmethod (osr ok) or NULL (osr not possible)\n+      \/\/ w11: target bytecode\n+      \/\/ x12: temporary\n+      __ beqz(x10, dispatch);     \/\/ test result -- no osr if null\n+      \/\/ nmethod may have been invalidated (VM may block upon call_VM return)\n+      __ lbu(x12, Address(x10, nmethod::state_offset()));\n+      if (nmethod::in_use != 0) {\n+        __ sub(x12, x12, nmethod::in_use);\n+      }\n+      __ bnez(x12, dispatch);\n+\n+      \/\/ We have the address of an on stack replacement routine in x10\n+      \/\/ We need to prepare to execute the OSR method. First we must\n+      \/\/ migrate the locals and monitors off of the stack.\n+\n+      __ mv(x9, x10);                             \/\/ save the nmethod\n+\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n+\n+      \/\/ x10 is OSR buffer, move it to expected parameter location\n+      __ mv(j_rarg0, x10);\n+\n+      \/\/ remove activation\n+      \/\/ get sender esp\n+      __ ld(esp,\n+          Address(fp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+      \/\/ remove frame anchor\n+      __ leave();\n+      \/\/ Ensure compiled code always sees stack at proper alignment\n+      __ andi(sp, esp, -16);\n+\n+      \/\/ and begin the OSR nmethod\n+      __ ld(t0, Address(x9, nmethod::osr_entry_point_offset()));\n+      __ jr(t0);\n+    }\n+  }\n+}\n+\n+void TemplateTable::if_0cmp(Condition cc)\n+{\n+  transition(itos, vtos);\n+  \/\/ assume branch is more often taken than not (loops use backward branches)\n+  Label not_taken;\n+\n+  __ addw(x10, x10, zr);\n+  switch (cc) {\n+    case equal:\n+      __ bnez(x10, not_taken);\n+      break;\n+    case not_equal:\n+      __ beqz(x10, not_taken);\n+      break;\n+    case less:\n+      __ bgez(x10, not_taken);\n+      break;\n+    case less_equal:\n+      __ bgtz(x10, not_taken);\n+      break;\n+    case greater:\n+      __ blez(x10, not_taken);\n+      break;\n+    case greater_equal:\n+      __ bltz(x10, not_taken);\n+      break;\n+    default:\n+      break;\n+  }\n+\n+  branch(false, false);\n+  __ bind(not_taken);\n+  __ profile_not_taken_branch(x10);\n+}\n+\n+void TemplateTable::if_icmp(Condition cc)\n+{\n+  transition(itos, vtos);\n+  \/\/ assume branch is more often taken than not (loops use backward branches)\n+  Label not_taken;\n+  __ pop_i(x11);\n+  __ addw(x10, x10, zr);\n+  switch (cc) {\n+    case equal:\n+      __ bne(x11, x10, not_taken);\n+      break;\n+    case not_equal:\n+      __ beq(x11, x10, not_taken);\n+      break;\n+    case less:\n+      __ bge(x11, x10, not_taken);\n+      break;\n+    case less_equal:\n+      __ bgt(x11, x10, not_taken);\n+      break;\n+    case greater:\n+      __ ble(x11, x10, not_taken);\n+      break;\n+    case greater_equal:\n+      __ blt(x11, x10, not_taken);\n+      break;\n+    default:\n+      break;\n+  }\n+\n+  branch(false, false);\n+  __ bind(not_taken);\n+  __ profile_not_taken_branch(x10);\n+}\n+\n+void TemplateTable::if_nullcmp(Condition cc)\n+{\n+  transition(atos, vtos);\n+  \/\/ assume branch is more often taken than not (loops use backward branches)\n+  Label not_taken;\n+  if (cc == equal) {\n+    __ bnez(x10, not_taken);\n+  } else {\n+    __ beqz(x10, not_taken);\n+  }\n+  branch(false, false);\n+  __ bind(not_taken);\n+  __ profile_not_taken_branch(x10);\n+}\n+\n+void TemplateTable::if_acmp(Condition cc)\n+{\n+  transition(atos, vtos);\n+  \/\/ assume branch is more often taken than not (loops use backward branches)\n+  Label not_taken;\n+  __ pop_ptr(x11);\n+\n+  if (cc == equal) {\n+    __ bne(x11, x10, not_taken);\n+  } else if (cc == not_equal) {\n+    __ beq(x11, x10, not_taken);\n+  }\n+  branch(false, false);\n+  __ bind(not_taken);\n+  __ profile_not_taken_branch(x10);\n+}\n+\n+void TemplateTable::ret() {\n+  transition(vtos, vtos);\n+  \/\/ We might be moving to a safepoint.  The thread which calls\n+  \/\/ Interpreter::notice_safepoints() will effectively flush its cache\n+  \/\/ when it makes a system call, but we need to do something to\n+  \/\/ ensure that we see the changed dispatch table.\n+  __ membar(MacroAssembler::LoadLoad);\n+\n+  locals_index(x11);\n+  __ ld(x11, aaddress(x11, t1, _masm)); \/\/ get return bci, compute return bcp\n+  __ profile_ret(x11, x12);\n+  __ ld(xbcp, Address(xmethod, Method::const_offset()));\n+  __ add(xbcp, xbcp, x11);\n+  __ addi(xbcp, xbcp, in_bytes(ConstMethod::codes_offset()));\n+  __ dispatch_next(vtos, 0, \/*generate_poll*\/true);\n+}\n+\n+void TemplateTable::wide_ret() {\n+  transition(vtos, vtos);\n+  locals_index_wide(x11);\n+  __ ld(x11, aaddress(x11, t0, _masm)); \/\/ get return bci, compute return bcp\n+  __ profile_ret(x11, x12);\n+  __ ld(xbcp, Address(xmethod, Method::const_offset()));\n+  __ add(xbcp, xbcp, x11);\n+  __ add(xbcp, xbcp, in_bytes(ConstMethod::codes_offset()));\n+  __ dispatch_next(vtos, 0, \/*generate_poll*\/true);\n+}\n+\n+void TemplateTable::tableswitch() {\n+  Label default_case, continue_execution;\n+  transition(itos, vtos);\n+  \/\/ align xbcp\n+  __ la(x11, at_bcp(BytesPerInt));\n+  __ andi(x11, x11, -BytesPerInt);\n+  \/\/ load lo & hi\n+  __ lwu(x12, Address(x11, BytesPerInt));\n+  __ lwu(x13, Address(x11, 2 * BytesPerInt));\n+  __ revb_w_w(x12, x12); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  \/\/ check against lo & hi\n+  __ blt(x10, x12, default_case);\n+  __ bgt(x10, x13, default_case);\n+  \/\/ lookup dispatch offset\n+  __ subw(x10, x10, x12);\n+  __ shadd(x13, x10, x11, t0, 2);\n+  __ lwu(x13, Address(x13, 3 * BytesPerInt));\n+  __ profile_switch_case(x10, x11, x12);\n+  \/\/ continue execution\n+  __ bind(continue_execution);\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ add(xbcp, xbcp, x13);\n+  __ load_unsigned_byte(t0, Address(xbcp));\n+  __ dispatch_only(vtos, \/*generate_poll*\/true);\n+  \/\/ handle default\n+  __ bind(default_case);\n+  __ profile_switch_default(x10);\n+  __ lwu(x13, Address(x11, 0));\n+  __ j(continue_execution);\n+}\n+\n+void TemplateTable::lookupswitch() {\n+  transition(itos, itos);\n+  __ stop(\"lookupswitch bytecode should have been rewritten\");\n+}\n+\n+void TemplateTable::fast_linearswitch() {\n+  transition(itos, vtos);\n+  Label loop_entry, loop, found, continue_execution;\n+  \/\/ bswap x10 so we can avoid bswapping the table entries\n+  __ revb_w_w(x10, x10); \/\/ reverse bytes in word (32bit) and sign-extend\n+  \/\/ align xbcp\n+  __ la(x9, at_bcp(BytesPerInt)); \/\/ btw: should be able to get rid of\n+                                    \/\/ this instruction (change offsets\n+                                    \/\/ below)\n+  __ andi(x9, x9, -BytesPerInt);\n+  \/\/ set counter\n+  __ lwu(x11, Address(x9, BytesPerInt));\n+  __ revb_w(x11, x11);\n+  __ j(loop_entry);\n+  \/\/ table search\n+  __ bind(loop);\n+  __ shadd(t0, x11, x9, t0, 3);\n+  __ lw(t0, Address(t0, 2 * BytesPerInt));\n+  __ beq(x10, t0, found);\n+  __ bind(loop_entry);\n+  __ addi(x11, x11, -1);\n+  __ bgez(x11, loop);\n+  \/\/ default case\n+  __ profile_switch_default(x10);\n+  __ lwu(x13, Address(x9, 0));\n+  __ j(continue_execution);\n+  \/\/ entry found -> get offset\n+  __ bind(found);\n+  __ shadd(t0, x11, x9, t0, 3);\n+  __ lwu(x13, Address(t0, 3 * BytesPerInt));\n+  __ profile_switch_case(x11, x10, x9);\n+  \/\/ continue execution\n+  __ bind(continue_execution);\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ add(xbcp, xbcp, x13);\n+  __ lbu(t0, Address(xbcp, 0));\n+  __ dispatch_only(vtos, \/*generate_poll*\/true);\n+}\n+\n+void TemplateTable::fast_binaryswitch() {\n+  transition(itos, vtos);\n+  \/\/ Implementation using the following core algorithm:\n+  \/\/\n+  \/\/ int binary_search(int key, LookupswitchPair* array, int n)\n+  \/\/   binary_search start:\n+  \/\/   #Binary search according to \"Methodik des Programmierens\" by\n+  \/\/   # Edsger W. Dijkstra and W.H.J. Feijen, Addison Wesley Germany 1985.\n+  \/\/   int i = 0;\n+  \/\/   int j = n;\n+  \/\/   while (i + 1 < j) do\n+  \/\/     # invariant P: 0 <= i < j <= n and (a[i] <= key < a[j] or Q)\n+  \/\/     # with      Q: for all i: 0 <= i < n: key < a[i]\n+  \/\/     # where a stands for the array and assuming that the (inexisting)\n+  \/\/     # element a[n] is infinitely big.\n+  \/\/     int h = (i + j) >> 1\n+  \/\/     # i < h < j\n+  \/\/     if (key < array[h].fast_match())\n+  \/\/     then [j = h]\n+  \/\/     else [i = h]\n+  \/\/   end\n+  \/\/   # R: a[i] <= key < a[i+1] or Q\n+  \/\/   # (i.e., if key is within array, i is the correct index)\n+  \/\/   return i\n+  \/\/ binary_search end\n+\n+\n+  \/\/ Register allocation\n+  const Register key   = x10; \/\/ already set (tosca)\n+  const Register array = x11;\n+  const Register i     = x12;\n+  const Register j     = x13;\n+  const Register h     = x14;\n+  const Register temp  = x15;\n+\n+  \/\/ Find array start\n+  __ la(array, at_bcp(3 * BytesPerInt));  \/\/ btw: should be able to\n+                                          \/\/ get rid of this\n+                                          \/\/ instruction (change\n+                                          \/\/ offsets below)\n+  __ andi(array, array, -BytesPerInt);\n+\n+  \/\/ Initialize i & j\n+  __ mv(i, zr);                            \/\/ i = 0\n+  __ lwu(j, Address(array, -BytesPerInt)); \/\/ j = length(array)\n+\n+  \/\/ Convert j into native byteordering\n+  __ revb_w(j, j);\n+\n+  \/\/ And start\n+  Label entry;\n+  __ j(entry);\n+\n+  \/\/ binary search loop\n+  {\n+    Label loop;\n+    __ bind(loop);\n+    __ addw(h, i, j);                           \/\/ h = i + j\n+    __ srliw(h, h, 1);                          \/\/ h = (i + j) >> 1\n+    \/\/ if [key < array[h].fast_match()]\n+    \/\/ then [j = h]\n+    \/\/ else [i = h]\n+    \/\/ Convert array[h].match to native byte-ordering before compare\n+    __ shadd(temp, h, array, temp, 3);\n+    __ ld(temp, Address(temp, 0));\n+    __ revb_w_w(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n+\n+    Label L_done, L_greater;\n+    __ bge(key, temp, L_greater);\n+    \/\/ if [key < array[h].fast_match()] then j = h\n+    __ mv(j, h);\n+    __ j(L_done);\n+    __ bind(L_greater);\n+    \/\/ if [key >= array[h].fast_match()] then i = h\n+    __ mv(i, h);\n+    __ bind(L_done);\n+\n+    \/\/ while [i + 1 < j]\n+    __ bind(entry);\n+    __ addiw(h, i, 1);         \/\/ i + 1\n+    __ blt(h, j, loop);        \/\/ i + 1 < j\n+  }\n+\n+  \/\/ end of binary search, result index is i (must check again!)\n+  Label default_case;\n+  \/\/ Convert array[i].match to native byte-ordering before compare\n+  __ shadd(temp, i, array, temp, 3);\n+  __ ld(temp, Address(temp, 0));\n+  __ revb_w_w(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ bne(key, temp, default_case);\n+\n+  \/\/ entry found -> j = offset\n+  __ shadd(temp, i, array, temp, 3);\n+  __ lwu(j, Address(temp, BytesPerInt));\n+  __ profile_switch_case(i, key, array);\n+  __ revb_w_w(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n+\n+  __ add(temp, xbcp, j);\n+  __ load_unsigned_byte(t0, Address(temp, 0));\n+\n+  __ add(xbcp, xbcp, j);\n+  __ la(xbcp, Address(xbcp, 0));\n+  __ dispatch_only(vtos, \/*generate_poll*\/true);\n+\n+  \/\/ default case -> j = default offset\n+  __ bind(default_case);\n+  __ profile_switch_default(i);\n+  __ lwu(j, Address(array, -2 * BytesPerInt));\n+  __ revb_w_w(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n+\n+  __ add(temp, xbcp, j);\n+  __ load_unsigned_byte(t0, Address(temp, 0));\n+\n+  __ add(xbcp, xbcp, j);\n+  __ la(xbcp, Address(xbcp, 0));\n+  __ dispatch_only(vtos, \/*generate_poll*\/true);\n+}\n+\n+void TemplateTable::_return(TosState state)\n+{\n+  transition(state, state);\n+  assert(_desc->calls_vm(),\n+         \"inconsistent calls_vm information\"); \/\/ call in remove_activation\n+\n+  if (_desc->bytecode() == Bytecodes::_return_register_finalizer) {\n+    assert(state == vtos, \"only valid state\");\n+\n+    __ ld(c_rarg1, aaddress(0));\n+    __ load_klass(x13, c_rarg1);\n+    __ lwu(x13, Address(x13, Klass::access_flags_offset()));\n+    Label skip_register_finalizer;\n+    __ andi(t0, x13, JVM_ACC_HAS_FINALIZER);\n+    __ beqz(t0, skip_register_finalizer);\n+\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), c_rarg1);\n+\n+    __ bind(skip_register_finalizer);\n+  }\n+\n+  \/\/ Issue a StoreStore barrier after all stores but before return\n+  \/\/ from any constructor for any class with a final field. We don't\n+  \/\/ know if this is a finalizer, so we always do so.\n+  if (_desc->bytecode() == Bytecodes::_return) {\n+    __ membar(MacroAssembler::StoreStore);\n+  }\n+\n+  \/\/ Narrow result if state is itos but result type is smaller.\n+  \/\/ Need to narrow in the return bytecode rather than in generate_return_entry\n+  \/\/ since compiled code callers expect the result to already be narrowed.\n+  if (state == itos) {\n+    __ narrow(x10);\n+  }\n+\n+  __ remove_activation(state);\n+  __ ret();\n+}\n+\n+\n+\/\/ ----------------------------------------------------------------------------\n+\/\/ Volatile variables demand their effects be made known to all CPU's\n+\/\/ in order.  Store buffers on most chips allow reads & writes to\n+\/\/ reorder; the JMM's ReadAfterWrite.java test fails in -Xint mode\n+\/\/ without some kind of memory barrier (i.e., it's not sufficient that\n+\/\/ the interpreter does not reorder volatile references, the hardware\n+\/\/ also must not reorder them).\n+\/\/\n+\/\/ According to the new Java Memory Model (JMM):\n+\/\/ (1) All volatiles are serialized wrt to each other.  ALSO reads &\n+\/\/     writes act as aquire & release, so:\n+\/\/ (2) A read cannot let unrelated NON-volatile memory refs that\n+\/\/     happen after the read float up to before the read.  It's OK for\n+\/\/     non-volatile memory refs that happen before the volatile read to\n+\/\/     float down below it.\n+\/\/ (3) Similar a volatile write cannot let unrelated NON-volatile\n+\/\/     memory refs that happen BEFORE the write float down to after the\n+\/\/     write.  It's OK for non-volatile memory refs that happen after the\n+\/\/     volatile write to float up before it.\n+\/\/\n+\/\/ We only put in barriers around volatile refs (they are expensive),\n+\/\/ not _between_ memory refs (that would require us to track the\n+\/\/ flavor of the previous memory refs).  Requirements (2) and (3)\n+\/\/ require some barriers before volatile stores and after volatile\n+\/\/ loads.  These nearly cover requirement (1) but miss the\n+\/\/ volatile-store-volatile-load case.  This final case is placed after\n+\/\/ volatile-stores although it could just as well go before\n+\/\/ volatile-loads.\n+\n+void TemplateTable::resolve_cache_and_index(int byte_no,\n+                                            Register Rcache,\n+                                            Register index,\n+                                            size_t index_size) {\n+  const Register temp = x9;\n+  assert_different_registers(Rcache, index, temp);\n+\n+  Label resolved;\n+\n+  Bytecodes::Code code = bytecode();\n+  switch (code) {\n+    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+    default: break;\n+  }\n+\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);\n+  __ mv(t0, (int) code);\n+  __ beq(temp, t0, resolved);\n+\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ mv(temp, (int) code);\n+  __ call_VM(noreg, entry, temp);\n+\n+  \/\/ Update registers with resolved info\n+  __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);\n+  \/\/ n.b. unlike x86 Rcache is now rcpool plus the indexed offset\n+  \/\/ so all clients ofthis method must be modified accordingly\n+  __ bind(resolved);\n+}\n+\n+\/\/ The Rcache and index registers must be set before call\n+\/\/ n.b unlike x86 cache already includes the index offset\n+void TemplateTable::load_field_cp_cache_entry(Register obj,\n+                                              Register cache,\n+                                              Register index,\n+                                              Register off,\n+                                              Register flags,\n+                                              bool is_static = false) {\n+  assert_different_registers(cache, index, flags, off);\n+\n+  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n+  \/\/ Field offset\n+  __ ld(off, Address(cache, in_bytes(cp_base_offset +\n+                                     ConstantPoolCacheEntry::f2_offset())));\n+  \/\/ Flags\n+  __ lwu(flags, Address(cache, in_bytes(cp_base_offset +\n+                                        ConstantPoolCacheEntry::flags_offset())));\n+\n+  \/\/ klass overwrite register\n+  if (is_static) {\n+    __ ld(obj, Address(cache, in_bytes(cp_base_offset +\n+                                       ConstantPoolCacheEntry::f1_offset())));\n+    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n+    __ ld(obj, Address(obj, mirror_offset));\n+    __ resolve_oop_handle(obj);\n+  }\n+}\n+\n+void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n+                                               Register method,\n+                                               Register itable_index,\n+                                               Register flags,\n+                                               bool is_invokevirtual,\n+                                               bool is_invokevfinal, \/*unused*\/\n+                                               bool is_invokedynamic) {\n+  \/\/ setup registers\n+  const Register cache = t1;\n+  const Register index = x14;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(method, cache, index);\n+  assert_different_registers(itable_index, flags);\n+  assert_different_registers(itable_index, cache, index);\n+  \/\/ determine constant pool cache field offsets\n+  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n+  const int method_offset = in_bytes(ConstantPoolCache::base_offset() +\n+                                     (is_invokevirtual ?\n+                                      ConstantPoolCacheEntry::f2_offset() :\n+                                      ConstantPoolCacheEntry::f1_offset()));\n+  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset());\n+  \/\/ access constant pool cache fields\n+  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::f2_offset());\n+\n+  const size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));\n+  resolve_cache_and_index(byte_no, cache, index, index_size);\n+  __ ld(method, Address(cache, method_offset));\n+\n+  if (itable_index != noreg) {\n+    __ ld(itable_index, Address(cache, index_offset));\n+  }\n+  __ lwu(flags, Address(cache, flags_offset));\n+}\n+\n+\/\/ The registers cache and index expected to be set before call.\n+\/\/ Correct values of the cache and index registers are preserved.\n+void TemplateTable::jvmti_post_field_access(Register cache, Register index,\n+                                            bool is_static, bool has_tos) {\n+  \/\/ do the JVMTI work here to avoid disturbing the register state below\n+  \/\/ We use c_rarg registers here beacause we want to use the register used in\n+  \/\/ the call to the VM\n+  if (JvmtiExport::can_post_field_access()) {\n+    \/\/ Check to see if a field access watch has been set before we\n+    \/\/ take the time to call into the VM.\n+    Label L1;\n+    assert_different_registers(cache, index, x10);\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()), offset);\n+    __ lwu(x10, Address(t0, offset));\n+\n+    __ beqz(x10, L1);\n+\n+    __ get_cache_and_index_at_bcp(c_rarg2, c_rarg3, 1);\n+    __ la(c_rarg2, Address(c_rarg2, in_bytes(ConstantPoolCache::base_offset())));\n+\n+    if (is_static) {\n+      __ mv(c_rarg1, zr); \/\/ NULL object reference\n+    } else {\n+      __ ld(c_rarg1, at_tos()); \/\/ get object pointer without popping it\n+      __ verify_oop(c_rarg1);\n+    }\n+    \/\/ c_rarg1: object pointer or NULL\n+    \/\/ c_rarg2: cache entry pointer\n+    \/\/ c_rarg3: jvalue object on the stack\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                       InterpreterRuntime::post_field_access),\n+                                       c_rarg1, c_rarg2, c_rarg3);\n+    __ get_cache_and_index_at_bcp(cache, index, 1);\n+    __ bind(L1);\n+  }\n+}\n+\n+void TemplateTable::pop_and_check_object(Register r)\n+{\n+  __ pop_ptr(r);\n+  __ null_check(r);  \/\/ for field access must check obj.\n+  __ verify_oop(r);\n+}\n+\n+void TemplateTable::getfield_or_static(int byte_no, bool is_static, RewriteControl rc)\n+{\n+  const Register cache     = x12;\n+  const Register index     = x13;\n+  const Register obj       = x14;\n+  const Register off       = x9;\n+  const Register flags     = x10;\n+  const Register raw_flags = x16;\n+  const Register bc        = x14; \/\/ uses same reg as obj, so don't mix them\n+\n+  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  jvmti_post_field_access(cache, index, is_static, false);\n+  load_field_cp_cache_entry(obj, cache, index, off, raw_flags, is_static);\n+\n+  if (!is_static) {\n+    \/\/ obj is on the stack\n+    pop_and_check_object(obj);\n+  }\n+\n+  __ add(off, obj, off);\n+  const Address field(off);\n+\n+  Label Done, notByte, notBool, notInt, notShort, notChar,\n+              notLong, notFloat, notObj, notDouble;\n+\n+  __ slli(flags, raw_flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift +\n+                                    ConstantPoolCacheEntry::tos_state_bits));\n+  __ srli(flags, flags, XLEN - ConstantPoolCacheEntry::tos_state_bits);\n+\n+  assert(btos == 0, \"change code, btos != 0\");\n+  __ bnez(flags, notByte);\n+\n+  \/\/ Dont't rewrite getstatic, only getfield\n+  if (is_static) {\n+    rc = may_not_rewrite;\n+  }\n+\n+  \/\/ btos\n+  __ access_load_at(T_BYTE, IN_HEAP, x10, field, noreg, noreg);\n+  __ push(btos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_bgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notByte);\n+  __ sub(t0, flags, (u1)ztos);\n+  __ bnez(t0, notBool);\n+\n+  \/\/ ztos (same code as btos)\n+  __ access_load_at(T_BOOLEAN, IN_HEAP, x10, field, noreg, noreg);\n+  __ push(ztos);\n+  \/\/ Rewirte bytecode to be faster\n+  if (rc == may_rewrite) {\n+    \/\/ uses btos rewriting, no truncating to t\/f bit is needed for getfield\n+    patch_bytecode(Bytecodes::_fast_bgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notBool);\n+  __ sub(t0, flags, (u1)atos);\n+  __ bnez(t0, notObj);\n+  \/\/ atos\n+  do_oop_load(_masm, field, x10, IN_HEAP);\n+  __ push(atos);\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_agetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notObj);\n+  __ sub(t0, flags, (u1)itos);\n+  __ bnez(t0, notInt);\n+  \/\/ itos\n+  __ access_load_at(T_INT, IN_HEAP, x10, field, noreg, noreg);\n+  __ addw(x10, x10, zr); \/\/ signed extended\n+  __ push(itos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_igetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notInt);\n+  __ sub(t0, flags, (u1)ctos);\n+  __ bnez(t0, notChar);\n+  \/\/ ctos\n+  __ access_load_at(T_CHAR, IN_HEAP, x10, field, noreg, noreg);\n+  __ push(ctos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_cgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notChar);\n+  __ sub(t0, flags, (u1)stos);\n+  __ bnez(t0, notShort);\n+  \/\/ stos\n+  __ access_load_at(T_SHORT, IN_HEAP, x10, field, noreg, noreg);\n+  __ push(stos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_sgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notShort);\n+  __ sub(t0, flags, (u1)ltos);\n+  __ bnez(t0, notLong);\n+  \/\/ ltos\n+  __ access_load_at(T_LONG, IN_HEAP, x10, field, noreg, noreg);\n+  __ push(ltos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_lgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notLong);\n+  __ sub(t0, flags, (u1)ftos);\n+  __ bnez(t0, notFloat);\n+  \/\/ ftos\n+  __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+  __ push(ftos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_fgetfield, bc, x11);\n+  }\n+  __ j(Done);\n+\n+  __ bind(notFloat);\n+#ifdef ASSERT\n+  __ sub(t0, flags, (u1)dtos);\n+  __ bnez(t0, notDouble);\n+#endif\n+  \/\/ dtos\n+  __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+  __ push(dtos);\n+  \/\/ Rewrite bytecode to be faster\n+  if (rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_dgetfield, bc, x11);\n+  }\n+#ifdef ASSERT\n+  __ j(Done);\n+\n+  __ bind(notDouble);\n+  __ stop(\"Bad state\");\n+#endif\n+\n+  __ bind(Done);\n+\n+  Label notVolatile;\n+  __ andi(t0, raw_flags, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+  __ beqz(t0, notVolatile);\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  __ bind(notVolatile);\n+}\n+\n+void TemplateTable::getfield(int byte_no)\n+{\n+  getfield_or_static(byte_no, false);\n+}\n+\n+void TemplateTable::nofast_getfield(int byte_no) {\n+  getfield_or_static(byte_no, false, may_not_rewrite);\n+}\n+\n+void TemplateTable::getstatic(int byte_no)\n+{\n+  getfield_or_static(byte_no, true);\n+}\n+\n+\/\/ The registers cache and index expected to be set before call.\n+\/\/ The function may destroy various registers, just not the cache and index registers.\n+void TemplateTable::jvmti_post_field_mod(Register cache, Register index, bool is_static) {\n+  transition(vtos, vtos);\n+\n+  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n+\n+  if (JvmtiExport::can_post_field_modification()) {\n+    \/\/ Check to see if a field modification watch has been set before\n+    \/\/ we take the time to call into the VM.\n+    Label L1;\n+    assert_different_registers(cache, index, x10);\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address)JvmtiExport::get_field_modification_count_addr()), offset);\n+    __ lwu(x10, Address(t0, offset));\n+    __ beqz(x10, L1);\n+\n+    __ get_cache_and_index_at_bcp(c_rarg2, t0, 1);\n+\n+    if (is_static) {\n+      \/\/ Life is simple. Null out the object pointer.\n+      __ mv(c_rarg1, zr);\n+    } else {\n+      \/\/ Life is harder. The stack holds the value on top, followed by\n+      \/\/ the object. We don't know the size of the value, though; it\n+      \/\/ could be one or two words depending on its type. As a result,\n+      \/\/ we must find the type to determine where the object is.\n+      __ lwu(c_rarg3, Address(c_rarg2,\n+                              in_bytes(cp_base_offset +\n+                                       ConstantPoolCacheEntry::flags_offset())));\n+      __ srli(c_rarg3, c_rarg3, ConstantPoolCacheEntry::tos_state_shift);\n+      ConstantPoolCacheEntry::verify_tos_state_shift();\n+      Label nope2, done, ok;\n+      __ ld(c_rarg1, at_tos_p1());   \/\/ initially assume a one word jvalue\n+      __ sub(t0, c_rarg3, ltos);\n+      __ beqz(t0, ok);\n+      __ sub(t0, c_rarg3, dtos);\n+      __ bnez(t0, nope2);\n+      __ bind(ok);\n+      __ ld(c_rarg1, at_tos_p2());  \/\/ ltos (two word jvalue);\n+      __ bind(nope2);\n+    }\n+    \/\/ cache entry pointer\n+    __ add(c_rarg2, c_rarg2, in_bytes(cp_base_offset));\n+    \/\/ object (tos)\n+    __ mv(c_rarg3, esp);\n+    \/\/ c_rarg1: object pointer set up above (NULL if static)\n+    \/\/ c_rarg2: cache entry pointer\n+    \/\/ c_rarg3: jvalue object on  the stack\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::post_field_modification),\n+                                c_rarg1, c_rarg2, c_rarg3);\n+    __ get_cache_and_index_at_bcp(cache, index, 1);\n+    __ bind(L1);\n+  }\n+}\n+\n+void TemplateTable::putfield_or_static(int byte_no, bool is_static, RewriteControl rc) {\n+  transition(vtos, vtos);\n+\n+  const Register cache = x12;\n+  const Register index = x13;\n+  const Register obj   = x12;\n+  const Register off   = x9;\n+  const Register flags = x10;\n+  const Register bc    = x14;\n+\n+  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  jvmti_post_field_mod(cache, index, is_static);\n+  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n+\n+  Label Done;\n+  __ mv(x15, flags);\n+\n+  {\n+    Label notVolatile;\n+    __ andi(t0, x15, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);\n+    __ bind(notVolatile);\n+  }\n+\n+  Label notByte, notBool, notInt, notShort, notChar,\n+        notLong, notFloat, notObj, notDouble;\n+\n+  __ slli(flags, flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift +\n+                                ConstantPoolCacheEntry::tos_state_bits));\n+  __ srli(flags, flags, XLEN - ConstantPoolCacheEntry::tos_state_bits);\n+\n+  assert(btos == 0, \"change code, btos != 0\");\n+  __ bnez(flags, notByte);\n+\n+  \/\/ Don't rewrite putstatic, only putfield\n+  if (is_static) {\n+    rc = may_not_rewrite;\n+  }\n+\n+  \/\/ btos\n+  {\n+    __ pop(btos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0); \/\/ off register as temparator register.\n+    __ access_store_at(T_BYTE, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_bputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notByte);\n+  __ sub(t0, flags, (u1)ztos);\n+  __ bnez(t0, notBool);\n+\n+  \/\/ ztos\n+  {\n+    __ pop(ztos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_BOOLEAN, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_zputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notBool);\n+  __ sub(t0, flags, (u1)atos);\n+  __ bnez(t0, notObj);\n+\n+  \/\/ atos\n+  {\n+    __ pop(atos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    \/\/ Store into the field\n+    do_oop_store(_masm, field, x10, IN_HEAP);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_aputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notObj);\n+  __ sub(t0, flags, (u1)itos);\n+  __ bnez(t0, notInt);\n+\n+  \/\/ itos\n+  {\n+    __ pop(itos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_INT, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_iputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notInt);\n+  __ sub(t0, flags, (u1)ctos);\n+  __ bnez(t0, notChar);\n+\n+  \/\/ ctos\n+  {\n+    __ pop(ctos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_CHAR, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_cputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notChar);\n+  __ sub(t0, flags, (u1)stos);\n+  __ bnez(t0, notShort);\n+\n+  \/\/ stos\n+  {\n+    __ pop(stos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_SHORT, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_sputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notShort);\n+  __ sub(t0, flags, (u1)ltos);\n+  __ bnez(t0, notLong);\n+\n+  \/\/ ltos\n+  {\n+    __ pop(ltos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_LONG, IN_HEAP, field, x10, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_lputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notLong);\n+  __ sub(t0, flags, (u1)ftos);\n+  __ bnez(t0, notFloat);\n+\n+  \/\/ ftos\n+  {\n+    __ pop(ftos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_fputfield, bc, x11, true, byte_no);\n+    }\n+    __ j(Done);\n+  }\n+\n+  __ bind(notFloat);\n+#ifdef ASSERT\n+  __ sub(t0, flags, (u1)dtos);\n+  __ bnez(t0, notDouble);\n+#endif\n+\n+  \/\/ dtos\n+  {\n+    __ pop(dtos);\n+    \/\/ field address\n+    if (!is_static) {\n+      pop_and_check_object(obj);\n+    }\n+    __ add(off, obj, off); \/\/ if static, obj from cache, else obj from stack.\n+    const Address field(off, 0);\n+    __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_dputfield, bc, x11, true, byte_no);\n+    }\n+  }\n+\n+#ifdef ASSERT\n+  __ j(Done);\n+\n+  __ bind(notDouble);\n+  __ stop(\"Bad state\");\n+#endif\n+\n+  __ bind(Done);\n+\n+  {\n+    Label notVolatile;\n+    __ andi(t0, x15, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::StoreLoad | MacroAssembler::StoreStore);\n+    __ bind(notVolatile);\n+  }\n+}\n+\n+void TemplateTable::putfield(int byte_no)\n+{\n+  putfield_or_static(byte_no, false);\n+}\n+\n+void TemplateTable::nofast_putfield(int byte_no) {\n+  putfield_or_static(byte_no, false, may_not_rewrite);\n+}\n+\n+void TemplateTable::putstatic(int byte_no) {\n+  putfield_or_static(byte_no, true);\n+}\n+\n+void TemplateTable::jvmti_post_fast_field_mod()\n+{\n+  if (JvmtiExport::can_post_field_modification()) {\n+    \/\/ Check to see if a field modification watch has been set before\n+    \/\/ we take the time to call into the VM.\n+    Label L2;\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address)JvmtiExport::get_field_modification_count_addr()), offset);\n+    __ lwu(c_rarg3, Address(t0, offset));\n+    __ beqz(c_rarg3, L2);\n+    __ pop_ptr(x9);                  \/\/ copy the object pointer from tos\n+    __ verify_oop(x9);\n+    __ push_ptr(x9);                 \/\/ put the object pointer back on tos\n+    \/\/ Save tos values before call_VM() clobbers them. Since we have\n+    \/\/ to do it for every data type, we use the saved values as the\n+    \/\/ jvalue object.\n+    switch (bytecode()) {          \/\/ load values into the jvalue object\n+      case Bytecodes::_fast_aputfield: __ push_ptr(x10); break;\n+      case Bytecodes::_fast_bputfield: \/\/ fall through\n+      case Bytecodes::_fast_zputfield: \/\/ fall through\n+      case Bytecodes::_fast_sputfield: \/\/ fall through\n+      case Bytecodes::_fast_cputfield: \/\/ fall through\n+      case Bytecodes::_fast_iputfield: __ push_i(x10); break;\n+      case Bytecodes::_fast_dputfield: __ push_d(); break;\n+      case Bytecodes::_fast_fputfield: __ push_f(); break;\n+      case Bytecodes::_fast_lputfield: __ push_l(x10); break;\n+\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    __ mv(c_rarg3, esp);             \/\/ points to jvalue on the stack\n+    \/\/ access constant pool cache entry\n+    __ get_cache_entry_pointer_at_bcp(c_rarg2, x10, 1);\n+    __ verify_oop(x9);\n+    \/\/ x9: object pointer copied above\n+    \/\/ c_rarg2: cache entry pointer\n+    \/\/ c_rarg3: jvalue object on the stack\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::post_field_modification),\n+               x9, c_rarg2, c_rarg3);\n+\n+    switch (bytecode()) {             \/\/ restore tos values\n+      case Bytecodes::_fast_aputfield: __ pop_ptr(x10); break;\n+      case Bytecodes::_fast_bputfield: \/\/ fall through\n+      case Bytecodes::_fast_zputfield: \/\/ fall through\n+      case Bytecodes::_fast_sputfield: \/\/ fall through\n+      case Bytecodes::_fast_cputfield: \/\/ fall through\n+      case Bytecodes::_fast_iputfield: __ pop_i(x10); break;\n+      case Bytecodes::_fast_dputfield: __ pop_d(); break;\n+      case Bytecodes::_fast_fputfield: __ pop_f(); break;\n+      case Bytecodes::_fast_lputfield: __ pop_l(x10); break;\n+      default: break;\n+    }\n+    __ bind(L2);\n+  }\n+}\n+\n+void TemplateTable::fast_storefield(TosState state)\n+{\n+  transition(state, vtos);\n+\n+  ByteSize base = ConstantPoolCache::base_offset();\n+\n+  jvmti_post_fast_field_mod();\n+\n+  \/\/ access constant pool cache\n+  __ get_cache_and_index_at_bcp(x12, x11, 1);\n+\n+  \/\/ Must prevent reordering of the following cp cache loads with bytecode load\n+  __ membar(MacroAssembler::LoadLoad);\n+\n+  \/\/ test for volatile with x13\n+  __ lwu(x13, Address(x12, in_bytes(base +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+\n+  \/\/ replace index with field offset from cache entry\n+  __ ld(x11, Address(x12, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));\n+\n+  {\n+    Label notVolatile;\n+    __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);\n+    __ bind(notVolatile);\n+  }\n+\n+  \/\/ Get object from stack\n+  pop_and_check_object(x12);\n+\n+  \/\/ field address\n+  __ add(x11, x12, x11);\n+  const Address field(x11, 0);\n+\n+  \/\/ access field\n+  switch (bytecode()) {\n+    case Bytecodes::_fast_aputfield:\n+      do_oop_store(_masm, field, x10, IN_HEAP);\n+      break;\n+    case Bytecodes::_fast_lputfield:\n+      __ access_store_at(T_LONG, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_iputfield:\n+      __ access_store_at(T_INT, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_zputfield:\n+      __ access_store_at(T_BOOLEAN, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_bputfield:\n+      __ access_store_at(T_BYTE, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_sputfield:\n+      __ access_store_at(T_SHORT, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_cputfield:\n+      __ access_store_at(T_CHAR, IN_HEAP, field, x10, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_fputfield:\n+      __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_dputfield:\n+      __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  {\n+    Label notVolatile;\n+    __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::StoreLoad | MacroAssembler::StoreStore);\n+    __ bind(notVolatile);\n+  }\n+}\n+\n+void TemplateTable::fast_accessfield(TosState state)\n+{\n+  transition(atos, state);\n+  \/\/ Do the JVMTI work here to avoid disturbing the register state below\n+  if (JvmtiExport::can_post_field_access()) {\n+    \/\/ Check to see if a field access watch has been set before we\n+    \/\/ take the time to call into the VM.\n+    Label L1;\n+    int32_t offset = 0;\n+    __ la_patchable(t0, ExternalAddress((address)JvmtiExport::get_field_access_count_addr()), offset);\n+    __ lwu(x12, Address(t0, offset));\n+    __ beqz(x12, L1);\n+    \/\/ access constant pool cache entry\n+    __ get_cache_entry_pointer_at_bcp(c_rarg2, t1, 1);\n+    __ verify_oop(x10);\n+    __ push_ptr(x10);  \/\/ save object pointer before call_VM() clobbers it\n+    __ mv(c_rarg1, x10);\n+    \/\/ c_rarg1: object pointer copied above\n+    \/\/ c_rarg2: cache entry pointer\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::post_field_access),\n+               c_rarg1, c_rarg2);\n+    __ pop_ptr(x10); \/\/ restore object pointer\n+    __ bind(L1);\n+  }\n+\n+  \/\/ access constant pool cache\n+  __ get_cache_and_index_at_bcp(x12, x11, 1);\n+\n+  \/\/ Must prevent reordering of the following cp cache loads with bytecode load\n+  __ membar(MacroAssembler::LoadLoad);\n+\n+  __ ld(x11, Address(x12, in_bytes(ConstantPoolCache::base_offset() +\n+                                   ConstantPoolCacheEntry::f2_offset())));\n+  __ lwu(x13, Address(x12, in_bytes(ConstantPoolCache::base_offset() +\n+                                    ConstantPoolCacheEntry::flags_offset())));\n+\n+  \/\/ x10: object\n+  __ verify_oop(x10);\n+  __ null_check(x10);\n+  __ add(x11, x10, x11);\n+  const Address field(x11, 0);\n+\n+  \/\/ access field\n+  switch (bytecode()) {\n+    case Bytecodes::_fast_agetfield:\n+      do_oop_load(_masm, field, x10, IN_HEAP);\n+      __ verify_oop(x10);\n+      break;\n+    case Bytecodes::_fast_lgetfield:\n+      __ access_load_at(T_LONG, IN_HEAP, x10, field, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_igetfield:\n+      __ access_load_at(T_INT, IN_HEAP, x10, field, noreg, noreg);\n+      __ addw(x10, x10, zr); \/\/ signed extended\n+      break;\n+    case Bytecodes::_fast_bgetfield:\n+      __ access_load_at(T_BYTE, IN_HEAP, x10, field, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_sgetfield:\n+      __ access_load_at(T_SHORT, IN_HEAP, x10, field, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_cgetfield:\n+      __ access_load_at(T_CHAR, IN_HEAP, x10, field, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_fgetfield:\n+      __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+      break;\n+    case Bytecodes::_fast_dgetfield:\n+      __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* dtos *\/, field, noreg, noreg);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  {\n+    Label notVolatile;\n+    __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+    __ bind(notVolatile);\n+  }\n+}\n+\n+void TemplateTable::fast_xaccess(TosState state)\n+{\n+  transition(vtos, state);\n+\n+  \/\/ get receiver\n+  __ ld(x10, aaddress(0));\n+  \/\/ access constant pool cache\n+  __ get_cache_and_index_at_bcp(x12, x13, 2);\n+  __ ld(x11, Address(x12, in_bytes(ConstantPoolCache::base_offset() +\n+                                   ConstantPoolCacheEntry::f2_offset())));\n+\n+  \/\/ make sure exception is reported in correct bcp range (getfield is\n+  \/\/ next instruction)\n+  __ addi(xbcp, xbcp, 1);\n+  __ null_check(x10);\n+  switch (state) {\n+    case itos:\n+      __ add(x10, x10, x11);\n+      __ access_load_at(T_INT, IN_HEAP, x10, Address(x10, 0), noreg, noreg);\n+      __ addw(x10, x10, zr); \/\/ signed extended\n+      break;\n+    case atos:\n+      __ add(x10, x10, x11);\n+      do_oop_load(_masm, Address(x10, 0), x10, IN_HEAP);\n+      __ verify_oop(x10);\n+      break;\n+    case ftos:\n+      __ add(x10, x10, x11);\n+      __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, Address(x10), noreg, noreg);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  {\n+    Label notVolatile;\n+    __ lwu(x13, Address(x12, in_bytes(ConstantPoolCache::base_offset() +\n+                                      ConstantPoolCacheEntry::flags_offset())));\n+    __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_volatile_shift);\n+    __ beqz(t0, notVolatile);\n+    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+    __ bind(notVolatile);\n+  }\n+\n+  __ sub(xbcp, xbcp, 1);\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Calls\n+\n+void TemplateTable::prepare_invoke(int byte_no,\n+                                   Register method, \/\/ linked method (or i-klass)\n+                                   Register index,  \/\/ itable index, MethodType, etc.\n+                                   Register recv,   \/\/ if caller wants to see it\n+                                   Register flags   \/\/ if caller wants to test it\n+                                   ) {\n+  \/\/ determine flags\n+  const Bytecodes::Code code = bytecode();\n+  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n+  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n+  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n+  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n+  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n+  const bool load_receiver       = (recv  != noreg);\n+  const bool save_flags          = (flags != noreg);\n+  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n+  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n+  assert(flags == noreg || flags == x13, \"\");\n+  assert(recv  == noreg || recv  == x12, \"\");\n+\n+  \/\/ setup registers & access constant pool cache\n+  if (recv == noreg) {\n+    recv = x12;\n+  }\n+  if (flags == noreg) {\n+    flags = x13;\n+  }\n+  assert_different_registers(method, index, recv, flags);\n+\n+  \/\/ save 'interpreter return address'\n+  __ save_bcp();\n+\n+  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  if (is_invokedynamic || is_invokehandle) {\n+    Label L_no_push;\n+    __ andi(t0, flags, 1UL << ConstantPoolCacheEntry::has_appendix_shift);\n+    __ beqz(t0, L_no_push);\n+    \/\/ Push the appendix as a trailing parameter.\n+    \/\/ This must be done before we get the receiver,\n+    \/\/ since the parameter_size includes it.\n+    __ push_reg(x9);\n+    __ mv(x9, index);\n+    assert(ConstantPoolCacheEntry::_indy_resolved_references_appendix_offset == 0, \"appendix expected at index+0\");\n+    __ load_resolved_reference_at_index(index, x9);\n+    __ pop_reg(x9);\n+    __ push_reg(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n+    __ bind(L_no_push);\n+  }\n+\n+  \/\/ load receiver if needed (note: no return address pushed yet)\n+  if (load_receiver) {\n+    __ andi(recv, flags, ConstantPoolCacheEntry::parameter_size_mask); \/\/ parameter_size_mask = 1 << 8\n+    __ shadd(t0, recv, esp, t0, 3);\n+    __ ld(recv, Address(t0, -Interpreter::expr_offset_in_bytes(1)));\n+    __ verify_oop(recv);\n+  }\n+\n+  \/\/ compute return type\n+  __ slli(t1, flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift + ConstantPoolCacheEntry::tos_state_bits));\n+  __ srli(t1, t1, XLEN - ConstantPoolCacheEntry::tos_state_bits); \/\/ (1 << 5) - 4 --> 28~31==> t1:0~3\n+\n+  \/\/ load return address\n+  {\n+    const address table_addr = (address) Interpreter::invoke_return_entry_table_for(code);\n+    __ mv(t0, table_addr);\n+    __ shadd(t0, t1, t0, t1, 3);\n+    __ ld(ra, Address(t0, 0));\n+  }\n+}\n+\n+void TemplateTable::invokevirtual_helper(Register index,\n+                                         Register recv,\n+                                         Register flags)\n+{\n+  \/\/ Uses temporary registers x10, x13\n+  assert_different_registers(index, recv, x10, x13);\n+  \/\/ Test for an invoke of a final method\n+  Label notFinal;\n+  __ andi(t0, flags, 1UL << ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ beqz(t0, notFinal);\n+\n+  const Register method = index;  \/\/ method must be xmethod\n+  assert(method == xmethod, \"Method must be xmethod for interpreter calling convention\");\n+\n+  \/\/ do the call - the index is actually the method to call\n+  \/\/ that is, f2 is a vtable index if !is_vfinal, else f2 is a Method*\n+\n+  \/\/ It's final, need a null check here!\n+  __ null_check(recv);\n+\n+  \/\/ profile this call\n+  __ profile_final_call(x10);\n+  __ profile_arguments_type(x10, method, x14, true);\n+\n+  __ jump_from_interpreted(method);\n+\n+  __ bind(notFinal);\n+\n+  \/\/ get receiver klass\n+  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n+  __ load_klass(x10, recv);\n+\n+  \/\/ profile this call\n+  __ profile_virtual_call(x10, xlocals, x13);\n+\n+  \/\/ get target Method & entry point\n+  __ lookup_virtual_method(x10, index, method);\n+  __ profile_arguments_type(x13, method, x14, true);\n+  __ jump_from_interpreted(method);\n+}\n+\n+void TemplateTable::invokevirtual(int byte_no)\n+{\n+  transition(vtos, vtos);\n+  assert(byte_no == f2_byte, \"use this argument\");\n+\n+  prepare_invoke(byte_no, xmethod, noreg, x12, x13);\n+\n+  \/\/ xmethod: index (actually a Method*)\n+  \/\/ x12: receiver\n+  \/\/ x13: flags\n+\n+  invokevirtual_helper(xmethod, x12, x13);\n+}\n+\n+void TemplateTable::invokespecial(int byte_no)\n+{\n+  transition(vtos, vtos);\n+  assert(byte_no == f1_byte, \"use this argument\");\n+\n+  prepare_invoke(byte_no, xmethod, noreg,  \/\/ get f1 Method*\n+                 x12);  \/\/ get receiver also for null check\n+  __ verify_oop(x12);\n+  __ null_check(x12);\n+  \/\/ do the call\n+  __ profile_call(x10);\n+  __ profile_arguments_type(x10, xmethod, xbcp, false);\n+  __ jump_from_interpreted(xmethod);\n+}\n+\n+void TemplateTable::invokestatic(int byte_no)\n+{\n+  transition(vtos, vtos);\n+  assert(byte_no == f1_byte, \"use this arugment\");\n+\n+  prepare_invoke(byte_no, xmethod);  \/\/ get f1 Method*\n+  \/\/ do the call\n+  __ profile_call(x10);\n+  __ profile_arguments_type(x10, xmethod, x14, false);\n+  __ jump_from_interpreted(xmethod);\n+}\n+\n+void TemplateTable::fast_invokevfinal(int byte_no)\n+{\n+  __ call_Unimplemented();\n+}\n+\n+void TemplateTable::invokeinterface(int byte_no) {\n+  transition(vtos, vtos);\n+  assert(byte_no == f1_byte, \"use this argument\");\n+\n+  prepare_invoke(byte_no, x10, xmethod,  \/\/ get f1 Klass*, f2 Method*\n+                 x12, x13);  \/\/ recv, flags\n+\n+  \/\/ x10: interface klass (from f1)\n+  \/\/ xmethod: method (from f2)\n+  \/\/ x12: receiver\n+  \/\/ x13: flags\n+\n+  \/\/ First check for Object case, then private interface method,\n+  \/\/ then regular interface method.\n+\n+  \/\/ Special case of invokeinterface called for virtual method of\n+  \/\/ java.lang.Object. See cpCache.cpp for details\n+  Label notObjectMethod;\n+  __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_forced_virtual_shift);\n+  __ beqz(t0, notObjectMethod);\n+\n+  invokevirtual_helper(xmethod, x12, x13);\n+  __ bind(notObjectMethod);\n+\n+  Label no_such_interface;\n+\n+  \/\/ Check for private method invocation - indicated by vfinal\n+  Label notVFinal;\n+  __ andi(t0, x13, 1UL << ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ beqz(t0, notVFinal);\n+\n+  \/\/ Check receiver klass into x13 - also a null check\n+  __ null_check(x12, oopDesc::klass_offset_in_bytes());\n+  __ load_klass(x13, x12);\n+\n+  Label subtype;\n+  __ check_klass_subtype(x13, x10, x14, subtype);\n+  \/\/ If we get here the typecheck failed\n+  __ j(no_such_interface);\n+  __ bind(subtype);\n+\n+  __ profile_final_call(x10);\n+  __ profile_arguments_type(x10, xmethod, x14, true);\n+  __ jump_from_interpreted(xmethod);\n+\n+  __ bind(notVFinal);\n+\n+  \/\/ Get receiver klass into x13 - also a null check\n+  __ restore_locals();\n+  __ null_check(x12, oopDesc::klass_offset_in_bytes());\n+  __ load_klass(x13, x12);\n+\n+  Label no_such_method;\n+\n+  \/\/ Preserve method for the throw_AbstractMethodErrorVerbose.\n+  __ mv(x28, xmethod);\n+  \/\/ Receiver subtype check against REFC.\n+  \/\/ Superklass in x10. Subklass in x13. Blows t1, x30\n+  __ lookup_interface_method(\/\/ inputs: rec. class, interface, itable index\n+                             x13, x10, noreg,\n+                             \/\/ outputs: scan temp. reg, scan temp. reg\n+                             t1, x30,\n+                             no_such_interface,\n+                             \/*return_method=*\/false);\n+\n+  \/\/ profile this call\n+  __ profile_virtual_call(x13, x30, x9);\n+\n+  \/\/ Get declaring interface class from method, and itable index\n+  __ ld(x10, Address(xmethod, Method::const_offset()));\n+  __ ld(x10, Address(x10, ConstMethod::constants_offset()));\n+  __ ld(x10, Address(x10, ConstantPool::pool_holder_offset_in_bytes()));\n+  __ lwu(xmethod, Address(xmethod, Method::itable_index_offset()));\n+  __ subw(xmethod, xmethod, Method::itable_index_max);\n+  __ negw(xmethod, xmethod);\n+\n+  \/\/ Preserve recvKlass for throw_AbstractMethodErrorVerbose\n+  __ mv(xlocals, x13);\n+  __ lookup_interface_method(\/\/ inputs: rec. class, interface, itable index\n+                             xlocals, x10, xmethod,\n+                             \/\/ outputs: method, scan temp. reg\n+                             xmethod, x30,\n+                             no_such_interface);\n+\n+  \/\/ xmethod: Method to call\n+  \/\/ x12: receiver\n+  \/\/ Check for abstract method error\n+  \/\/ Note: This should be done more efficiently via a throw_abstract_method_error\n+  \/\/       interpreter entry point and a conditional jump to it in case of a null\n+  \/\/       method.\n+  __ beqz(xmethod, no_such_method);\n+\n+  __ profile_arguments_type(x13, xmethod, x30, true);\n+\n+  \/\/ do the call\n+  \/\/ x12: receiver\n+  \/\/ xmethod: Method\n+  __ jump_from_interpreted(xmethod);\n+  __ should_not_reach_here();\n+\n+  \/\/ exception handling code follows ...\n+  \/\/ note: must restore interpreter registers to canonical\n+  \/\/       state for exception handling to work correctly!\n+\n+  __ bind(no_such_method);\n+  \/\/ throw exception\n+  __ restore_bcp();    \/\/ bcp must be correct for exception handler   (was destroyed)\n+  __ restore_locals(); \/\/ make sure locals pointer is correct as well (was destroyed)\n+  \/\/ Pass arguments for generating a verbose error message.\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodErrorVerbose), x13, x28);\n+  \/\/ the call_VM checks for exception, so we should never return here.\n+  __ should_not_reach_here();\n+\n+  __ bind(no_such_interface);\n+  \/\/ throw exceptiong\n+  __ restore_bcp();    \/\/ bcp must be correct for exception handler   (was destroyed)\n+  __ restore_locals(); \/\/ make sure locals pointer is correct as well (was destroyed)\n+  \/\/ Pass arguments for generating a verbose error message.\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                                     InterpreterRuntime::throw_IncompatibleClassChangeErrorVerbose), x13, x10);\n+  \/\/ the call_VM checks for exception, so we should never return here.\n+  __ should_not_reach_here();\n+  return;\n+}\n+\n+void TemplateTable::invokehandle(int byte_no) {\n+  transition(vtos, vtos);\n+  assert(byte_no == f1_byte, \"use this argument\");\n+\n+  prepare_invoke(byte_no, xmethod, x10, x12);\n+  __ verify_method_ptr(x12);\n+  __ verify_oop(x12);\n+  __ null_check(x12);\n+\n+  \/\/ FIXME: profile the LambdaForm also\n+\n+  \/\/ x30 is safe to use here as a temp reg because it is about to\n+  \/\/ be clobbered by jump_from_interpreted().\n+  __ profile_final_call(x30);\n+  __ profile_arguments_type(x30, xmethod, x14, true);\n+\n+  __ jump_from_interpreted(xmethod);\n+}\n+\n+void TemplateTable::invokedynamic(int byte_no) {\n+  transition(vtos, vtos);\n+  assert(byte_no == f1_byte, \"use this argument\");\n+\n+  prepare_invoke(byte_no, xmethod, x10);\n+\n+  \/\/ x10: CallSite object (from cpool->resolved_references[])\n+  \/\/ xmethod: MH.linkToCallSite method (from f2)\n+\n+  \/\/ Note: x10_callsite is already pushed by prepare_invoke\n+\n+  \/\/ %%% should make a type profile for any invokedynamic that takes a ref argument\n+  \/\/ profile this call\n+  __ profile_call(xbcp);\n+  __ profile_arguments_type(x13, xmethod, x30, false);\n+\n+  __ verify_oop(x10);\n+\n+  __ jump_from_interpreted(xmethod);\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Allocation\n+\n+void TemplateTable::_new() {\n+  transition(vtos, atos);\n+\n+  __ get_unsigned_2_byte_index_at_bcp(x13, 1);\n+  Label slow_case;\n+  Label done;\n+  Label initialize_header;\n+  Label initialize_object; \/\/ including clearing the fields\n+\n+  __ get_cpool_and_tags(x14, x10);\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is update (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ add(t0, x10, x13);\n+  __ la(t0, Address(t0, tags_offset));\n+  __ membar(MacroAssembler::AnyAny);\n+  __ lbu(t0, t0);\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  __ sub(t1, t0, (u1)JVM_CONSTANT_Class);\n+  __ bnez(t1, slow_case);\n+\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_offset(x14, x13, x14, t0);\n+\n+  \/\/ make sure klass is initialized & doesn't have finalizer\n+  \/\/ make sure klass is fully initialized\n+  __ lbu(t0, Address(x14, InstanceKlass::init_state_offset()));\n+  __ sub(t1, t0, (u1)InstanceKlass::fully_initialized);\n+  __ bnez(t1, slow_case);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  __ lwu(x13, Address(x14, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  __ andi(t0, x13, Klass::_lh_instance_slow_path_bit);\n+  __ bnez(t0, slow_case);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc = Universe::heap()->supports_inline_contig_alloc();\n+\n+  if (UseTLAB) {\n+    __ tlab_allocate(x10, x13, 0, noreg, x11, slow_case);\n+\n+    if (ZeroTLAB) {\n+      \/\/ the fields have been already cleared\n+      __ j(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      __ j(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    \/\/ x13: instance size in bytes\n+    if (allow_shared_alloc) {\n+      __ eden_allocate(x10, x13, 0, x28, slow_case);\n+    }\n+  }\n+\n+  \/\/ If USETLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialized need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    \/\/ The object is initialized before the header. If the object size is\n+    \/\/ zero, go directly to the header initialization.\n+    __ bind(initialize_object);\n+    __ sub(x13, x13, sizeof(oopDesc));\n+    __ beqz(x13, initialize_header);\n+\n+    \/\/ Initialize obejct fields\n+    {\n+      __ add(x12, x10, sizeof(oopDesc));\n+      Label loop;\n+      __ bind(loop);\n+      __ sd(zr, Address(x12));\n+      __ add(x12, x12, BytesPerLong);\n+      __ sub(x13, x13, BytesPerLong);\n+      __ bnez(x13, loop);\n+    }\n+\n+    \/\/ initialize object header only.\n+    __ bind(initialize_header);\n+    if (UseBiasedLocking) {\n+      __ ld(t0, Address(x14, Klass::prototype_header_offset()));\n+    } else {\n+      __ mv(t0, (intptr_t)markOopDesc::prototype());\n+    }\n+    __ sd(t0, Address(x10, oopDesc::mark_offset_in_bytes()));\n+    __ store_klass_gap(x10, zr);   \/\/ zero klass gap for compressed oops\n+    __ store_klass(x10, x14);      \/\/ store klass last\n+\n+    {\n+      SkipIfEqual skip(_masm, &DTraceAllocProbes, false);\n+      \/\/ Trigger dtrace event for fastpath\n+      __ push(atos); \/\/ save the return value\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), x10);\n+      __ pop(atos); \/\/ restore the return value\n+    }\n+    __ j(done);\n+  }\n+\n+  \/\/ slow case\n+  __ bind(slow_case);\n+  __ get_constant_pool(c_rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ verify_oop(x10);\n+\n+  \/\/ continue\n+  __ bind(done);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(MacroAssembler::StoreStore);\n+}\n+\n+void TemplateTable::newarray() {\n+  transition(itos, atos);\n+  __ load_unsigned_byte(c_rarg1, at_bcp(1));\n+  __ mv(c_rarg2, x10);\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray),\n+          c_rarg1, c_rarg2);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(MacroAssembler::StoreStore);\n+}\n+\n+void TemplateTable::anewarray() {\n+  transition(itos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  __ mv(c_rarg3, x10);\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::anewarray),\n+          c_rarg1, c_rarg2, c_rarg3);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(MacroAssembler::StoreStore);\n+}\n+\n+void TemplateTable::arraylength() {\n+  transition(atos, itos);\n+  __ null_check(x10, arrayOopDesc::length_offset_in_bytes());\n+  __ lwu(x10, Address(x10, arrayOopDesc::length_offset_in_bytes()));\n+}\n+\n+void TemplateTable::checkcast()\n+{\n+  transition(atos, atos);\n+  Label done, is_null, ok_is_subtype, quicked, resolved;\n+  __ beqz(x10, is_null);\n+\n+  \/\/ Get cpool & tags index\n+  __ get_cpool_and_tags(x12, x13); \/\/ x12=cpool, x13=tags array\n+  __ get_unsigned_2_byte_index_at_bcp(x9, 1); \/\/ x9=index\n+  \/\/ See if bytecode has already been quicked\n+  __ add(t0, x13, Array<u1>::base_offset_in_bytes());\n+  __ add(x11, t0, x9);\n+  __ membar(MacroAssembler::AnyAny);\n+  __ lbu(x11, x11);\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  __ sub(t0, x11, (u1)JVM_CONSTANT_Class);\n+  __ beqz(t0, quicked);\n+\n+  __ push(atos); \/\/ save receiver for result, and for GC\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));\n+  \/\/ vm_result_2 has metadata result\n+  __ get_vm_result_2(x10, xthread);\n+  __ pop_reg(x13); \/\/ restore receiver\n+  __ j(resolved);\n+\n+  \/\/ Get superklass in x10 and subklass in x13\n+  __ bind(quicked);\n+  __ mv(x13, x10); \/\/ Save object in x13; x10 needed for subtype check\n+  __ load_resolved_klass_at_offset(x12, x9, x10, t0); \/\/ x10 = klass\n+\n+  __ bind(resolved);\n+  __ load_klass(x9, x13);\n+\n+  \/\/ Generate subtype check.  Blows x12, x15.  Object in x13.\n+  \/\/ Superklass in x10.  Subklass in x9.\n+  __ gen_subtype_check(x9, ok_is_subtype);\n+\n+  \/\/ Come here on failure\n+  __ push_reg(x13);\n+  \/\/ object is at TOS\n+  __ j(Interpreter::_throw_ClassCastException_entry);\n+\n+  \/\/ Come here on success\n+  __ bind(ok_is_subtype);\n+  __ mv(x10, x13); \/\/ Restore object in x13\n+\n+  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  if (ProfileInterpreter) {\n+    __ j(done);\n+    __ bind(is_null);\n+    __ profile_null_seen(x12);\n+  } else {\n+    __ bind(is_null);   \/\/ same as 'done'\n+  }\n+  __ bind(done);\n+}\n+\n+void TemplateTable::instanceof() {\n+  transition(atos, itos);\n+  Label done, is_null, ok_is_subtype, quicked, resolved;\n+  __ beqz(x10, is_null);\n+\n+  \/\/ Get cpool & tags index\n+  __ get_cpool_and_tags(x12, x13); \/\/ x12=cpool, x13=tags array\n+  __ get_unsigned_2_byte_index_at_bcp(x9, 1); \/\/ x9=index\n+  \/\/ See if bytecode has already been quicked\n+  __ add(t0, x13, Array<u1>::base_offset_in_bytes());\n+  __ add(x11, t0, x9);\n+  __ membar(MacroAssembler::AnyAny);\n+  __ lbu(x11, x11);\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+  __ sub(t0, x11, (u1)JVM_CONSTANT_Class);\n+  __ beqz(t0, quicked);\n+\n+  __ push(atos); \/\/ save receiver for result, and for GC\n+  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));\n+  \/\/ vm_result_2 has metadata result\n+  __ get_vm_result_2(x10, xthread);\n+  __ pop_reg(x13); \/\/ restore receiver\n+  __ verify_oop(x13);\n+  __ load_klass(x13, x13);\n+  __ j(resolved);\n+\n+  \/\/ Get superklass in x10 and subklass in x13\n+  __ bind(quicked);\n+  __ load_klass(x13, x10);\n+  __ load_resolved_klass_at_offset(x12, x9, x10, t0);\n+\n+  __ bind(resolved);\n+\n+  \/\/ Generate subtype check.  Blows x12, x15\n+  \/\/ Superklass in x10.  Subklass in x13.\n+  __ gen_subtype_check(x13, ok_is_subtype);\n+\n+  \/\/ Come here on failure\n+  __ mv(x10, zr);\n+  __ j(done);\n+  \/\/ Come here on success\n+  __ bind(ok_is_subtype);\n+  __ li(x10, 1);\n+\n+  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  if (ProfileInterpreter) {\n+    __ j(done);\n+    __ bind(is_null);\n+    __ profile_null_seen(x12);\n+  } else {\n+    __ bind(is_null);   \/\/ same as 'done'\n+  }\n+  __ bind(done);\n+  \/\/ x10 = 0: obj == NULL or  obj is not an instanceof the specified klass\n+  \/\/ x10 = 1: obj != NULL and obj is     an instanceof the specified klass\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Breakpoints\n+void TemplateTable::_breakpoint() {\n+  \/\/ Note: We get here even if we are single stepping..\n+  \/\/ jbug inists on setting breakpoints at every bytecode\n+  \/\/ even if we are in single step mode.\n+\n+  transition(vtos, vtos);\n+\n+  \/\/ get the unpatched byte code\n+  __ get_method(c_rarg1);\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address,\n+                              InterpreterRuntime::get_original_bytecode_at),\n+             c_rarg1, xbcp);\n+  __ mv(x9, x10);\n+\n+  \/\/ post the breakpoint event\n+  __ call_VM(noreg,\n+             CAST_FROM_FN_PTR(address, InterpreterRuntime::_breakpoint),\n+             xmethod, xbcp);\n+\n+  \/\/ complete the execution of original bytecode\n+  __ mv(t0, x9);\n+  __ dispatch_only_normal(vtos);\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Exceptions\n+\n+void TemplateTable::athrow() {\n+  transition(atos, vtos);\n+  __ null_check(x10);\n+  __ j(Interpreter::throw_exception_entry());\n+}\n+\n+\/\/-----------------------------------------------------------------------------\n+\/\/ Synchronization\n+\/\/\n+\/\/ Note: monitorenter & exit are symmetric routines; which is reflected\n+\/\/       in the assembly code structure as well\n+\/\/\n+\/\/ Stack layout:\n+\/\/\n+\/\/ [expressions  ] <--- esp               = expression stack top\n+\/\/ ..\n+\/\/ [expressions  ]\n+\/\/ [monitor entry] <--- monitor block top = expression stack bot\n+\/\/ ..\n+\/\/ [monitor entry]\n+\/\/ [frame data   ] <--- monitor block bot\n+\/\/ ...\n+\/\/ [saved fp     ] <--- fp\n+void TemplateTable::monitorenter()\n+{\n+  transition(atos, vtos);\n+\n+   \/\/ check for NULL object\n+   __ null_check(x10);\n+\n+   const Address monitor_block_top(\n+         fp, frame::interpreter_frame_monitor_block_top_offset * wordSize);\n+   const Address monitor_block_bot(\n+         fp, frame::interpreter_frame_initial_sp_offset * wordSize);\n+   const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+\n+   Label allocated;\n+\n+   \/\/ initialize entry pointer\n+   __ mv(c_rarg1, zr); \/\/ points to free slot or NULL\n+\n+   \/\/ find a free slot in the monitor block (result in c_rarg1)\n+   {\n+     Label entry, loop, exit, notUsed;\n+     __ ld(c_rarg3, monitor_block_top); \/\/ points to current entry,\n+                                        \/\/ starting with top-most entry\n+     __ la(c_rarg2, monitor_block_bot); \/\/ points to word before bottom\n+\n+     __ j(entry);\n+\n+     __ bind(loop);\n+     \/\/ check if current entry is used\n+     \/\/ if not used then remember entry in c_rarg1\n+     __ ld(t0, Address(c_rarg3, BasicObjectLock::obj_offset_in_bytes()));\n+     __ bnez(t0, notUsed);\n+     __ mv(c_rarg1, c_rarg3);\n+     __ bind(notUsed);\n+     \/\/ check if current entry is for same object\n+     \/\/ if same object then stop searching\n+     __ beq(x10, t0, exit);\n+     \/\/ otherwise advance to next entry\n+     __ add(c_rarg3, c_rarg3, entry_size);\n+     __ bind(entry);\n+     \/\/ check if bottom reached\n+     \/\/ if not at bottom then check this entry\n+     __ bne(c_rarg3, c_rarg2, loop);\n+     __ bind(exit);\n+   }\n+\n+   __ bnez(c_rarg1, allocated); \/\/ check if a slot has been found and\n+                             \/\/ if found, continue with that on\n+\n+   \/\/ allocate one if there's no free slot\n+   {\n+     Label entry, loop;\n+     \/\/ 1. compute new pointers            \/\/ esp: old expression stack top\n+     __ ld(c_rarg1, monitor_block_bot);    \/\/ c_rarg1: old expression stack bottom\n+     __ sub(esp, esp, entry_size);         \/\/ move expression stack top\n+     __ sub(c_rarg1, c_rarg1, entry_size); \/\/ move expression stack bottom\n+     __ mv(c_rarg3, esp);                  \/\/ set start value for copy loop\n+     __ sd(c_rarg1, monitor_block_bot);    \/\/ set new monitor block bottom\n+     __ sub(sp, sp, entry_size);           \/\/ make room for the monitor\n+\n+     __ j(entry);\n+     \/\/ 2. move expression stack contents\n+     __ bind(loop);\n+     __ ld(c_rarg2, Address(c_rarg3, entry_size)); \/\/ load expression stack\n+                                                   \/\/ word from old location\n+     __ sd(c_rarg2, Address(c_rarg3, 0));          \/\/ and store it at new location\n+     __ add(c_rarg3, c_rarg3, wordSize);           \/\/ advance to next word\n+     __ bind(entry);\n+     __ bne(c_rarg3, c_rarg1, loop);    \/\/ check if bottom reached.if not at bottom\n+                                        \/\/ then copy next word\n+   }\n+\n+   \/\/ call run-time routine\n+   \/\/ c_rarg1: points to monitor entry\n+   __ bind(allocated);\n+\n+   \/\/ Increment bcp to point to the next bytecode, so exception\n+   \/\/ handling for async. exceptions work correctly.\n+   \/\/ The object has already been poped from the stack, so the\n+   \/\/ expression stack looks correct.\n+   __ addi(xbcp, xbcp, 1);\n+\n+   \/\/ store object\n+   __ sd(x10, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+   __ lock_object(c_rarg1);\n+\n+   \/\/ check to make sure this monitor doesn't cause stack overflow after locking\n+   __ save_bcp();  \/\/ in case of exception\n+   __ generate_stack_overflow_check(0);\n+\n+   \/\/ The bcp has already been incremented. Just need to dispatch to\n+   \/\/ next instruction.\n+   __ dispatch_next(vtos);\n+}\n+\n+void TemplateTable::monitorexit()\n+{\n+  transition(atos, vtos);\n+\n+  \/\/ check for NULL object\n+  __ null_check(x10);\n+\n+  const Address monitor_block_top(\n+        fp, frame::interpreter_frame_monitor_block_top_offset * wordSize);\n+  const Address monitor_block_bot(\n+        fp, frame::interpreter_frame_initial_sp_offset * wordSize);\n+  const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+\n+  Label found;\n+\n+  \/\/ find matching slot\n+  {\n+    Label entry, loop;\n+    __ ld(c_rarg1, monitor_block_top); \/\/ points to current entry,\n+                                        \/\/ starting with top-most entry\n+    __ la(c_rarg2, monitor_block_bot); \/\/ points to word before bottom\n+                                        \/\/ of monitor block\n+    __ j(entry);\n+\n+    __ bind(loop);\n+    \/\/ check if current entry is for same object\n+    __ ld(t0, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+    \/\/ if same object then stop searching\n+    __ beq(x10, t0, found);\n+    \/\/ otherwise advance to next entry\n+    __ add(c_rarg1, c_rarg1, entry_size);\n+    __ bind(entry);\n+    \/\/ check if bottom reached\n+    \/\/ if not at bottom then check this entry\n+    __ bne(c_rarg1, c_rarg2, loop);\n+  }\n+\n+  \/\/ error handling. Unlocking was not block-structured\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                   InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+\n+  \/\/ call run-time routine\n+  __ bind(found);\n+  __ push_ptr(x10); \/\/ make sure object is on stack (contract with oopMaps)\n+  __ unlock_object(c_rarg1);\n+  __ pop_ptr(x10); \/\/ discard object\n+}\n+\n+\/\/ Wide instructions\n+void TemplateTable::wide()\n+{\n+  __ load_unsigned_byte(x9, at_bcp(1));\n+  __ mv(t0, (address)Interpreter::_wentry_point);\n+  __ shadd(t0, x9, t0, t1, 3);\n+  __ ld(t0, Address(t0));\n+  __ jr(t0);\n+}\n+\n+\/\/ Multi arrays\n+void TemplateTable::multianewarray() {\n+  transition(vtos, atos);\n+  __ load_unsigned_byte(x10, at_bcp(3)); \/\/ get number of dimensions\n+  \/\/ last dim is on top of stack; we want address of first one:\n+  \/\/ first_addr = last_addr + (ndims - 1) * wordSize\n+  __ shadd(c_rarg1, x10, esp, c_rarg1, 3);\n+  __ sub(c_rarg1, c_rarg1, wordSize);\n+  call_VM(x10,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray),\n+          c_rarg1);\n+  __ load_unsigned_byte(x11, at_bcp(3));\n+  __ shadd(esp, x11, esp, t0, 3);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":4006,"deletions":0,"binary":false,"changes":4006,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_TEMPLATETABLE_RISCV_HPP\n+#define CPU_RISCV_TEMPLATETABLE_RISCV_HPP\n+\n+static void prepare_invoke(int byte_no,\n+                           Register method,         \/\/ linked method (or i-klass)\n+                           Register index = noreg,  \/\/ itable index, MethodType, etc.\n+                           Register recv  = noreg,  \/\/ if caller wants to see it\n+                           Register flags = noreg   \/\/ if caller wants to test it\n+                           );\n+static void invokevirtual_helper(Register index, Register recv,\n+                                 Register flags);\n+\n+\/\/ Helpers\n+static void index_check(Register array, Register index);\n+\n+#endif \/\/ CPU_RISCV_TEMPLATETABLE_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VMSTRUCTS_RISCV_HPP\n+#define CPU_RISCV_VMSTRUCTS_RISCV_HPP\n+\n+\/\/ These are the CPU-specific fields, types and integer\n+\/\/ constants required by the Serviceability Agent. This file is\n+\/\/ referenced by vmStructs.cpp.\n+\n+#define VM_STRUCTS_CPU(nonstatic_field, static_field, unchecked_nonstatic_field, volatile_nonstatic_field, nonproduct_nonstatic_field, c2_nonstatic_field, unchecked_c1_static_field, unchecked_c2_static_field) \\\n+  volatile_nonstatic_field(JavaFrameAnchor, _last_Java_fp, intptr_t*)\n+\n+#define VM_TYPES_CPU(declare_type, declare_toplevel_type, declare_oop_type, declare_integer_type, declare_unsigned_integer_type, declare_c1_toplevel_type, declare_c2_type, declare_c2_toplevel_type)\n+\n+#define VM_INT_CONSTANTS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)\n+\n+#define VM_LONG_CONSTANTS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)\n+\n+#endif \/\/ CPU_RISCV_VMSTRUCTS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/vmStructs_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"vm_version_ext_riscv.hpp\"\n+\n+\/\/ VM_Version_Ext statics\n+int VM_Version_Ext::_no_of_threads = 0;\n+int VM_Version_Ext::_no_of_cores = 0;\n+int VM_Version_Ext::_no_of_sockets = 0;\n+bool VM_Version_Ext::_initialized = false;\n+char VM_Version_Ext::_cpu_name[CPU_TYPE_DESC_BUF_SIZE] = {0};\n+char VM_Version_Ext::_cpu_desc[CPU_DETAILED_DESC_BUF_SIZE] = {0};\n+\n+void VM_Version_Ext::initialize_cpu_information(void) {\n+  \/\/ do nothing if cpu info has been initialized\n+  if (_initialized) {\n+    return;\n+  }\n+\n+  _no_of_cores  = os::processor_count();\n+  _no_of_threads = _no_of_cores;\n+  _no_of_sockets = _no_of_cores;\n+  snprintf(_cpu_name, CPU_TYPE_DESC_BUF_SIZE - 1, \"RISCV64\");\n+  snprintf(_cpu_desc, CPU_DETAILED_DESC_BUF_SIZE, \"RISCV64 %s\", _features_string);\n+  _initialized = true;\n+}\n+\n+int VM_Version_Ext::number_of_threads(void) {\n+  initialize_cpu_information();\n+  return _no_of_threads;\n+}\n+\n+int VM_Version_Ext::number_of_cores(void) {\n+  initialize_cpu_information();\n+  return _no_of_cores;\n+}\n+\n+int VM_Version_Ext::number_of_sockets(void) {\n+  initialize_cpu_information();\n+  return _no_of_sockets;\n+}\n+\n+const char* VM_Version_Ext::cpu_name(void) {\n+  initialize_cpu_information();\n+  char* tmp = NEW_C_HEAP_ARRAY_RETURN_NULL(char, CPU_TYPE_DESC_BUF_SIZE, mtTracing);\n+  if (NULL == tmp) {\n+    return NULL;\n+  }\n+  strncpy(tmp, _cpu_name, CPU_TYPE_DESC_BUF_SIZE);\n+  return tmp;\n+}\n+\n+const char* VM_Version_Ext::cpu_description(void) {\n+  initialize_cpu_information();\n+  char* tmp = NEW_C_HEAP_ARRAY_RETURN_NULL(char, CPU_DETAILED_DESC_BUF_SIZE, mtTracing);\n+  if (NULL == tmp) {\n+    return NULL;\n+  }\n+  strncpy(tmp, _cpu_desc, CPU_DETAILED_DESC_BUF_SIZE);\n+  return tmp;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_ext_riscv.cpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VM_VERSION_EXT_RISCV_HPP\n+#define CPU_RISCV_VM_VERSION_EXT_RISCV_HPP\n+\n+#include \"runtime\/vm_version.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class VM_Version_Ext : public VM_Version {\n+ private:\n+  static const size_t      CPU_TYPE_DESC_BUF_SIZE = 256;\n+  static const size_t      CPU_DETAILED_DESC_BUF_SIZE = 4096;\n+\n+  static int               _no_of_threads;\n+  static int               _no_of_cores;\n+  static int               _no_of_sockets;\n+  static bool              _initialized;\n+  static char              _cpu_name[CPU_TYPE_DESC_BUF_SIZE];\n+  static char              _cpu_desc[CPU_DETAILED_DESC_BUF_SIZE];\n+\n+ public:\n+  static int number_of_threads(void);\n+  static int number_of_cores(void);\n+  static int number_of_sockets(void);\n+\n+  static const char* cpu_name(void);\n+  static const char* cpu_description(void);\n+  static void initialize_cpu_information(void);\n+\n+};\n+\n+#endif \/\/ CPU_RISCV_VM_VERSION_EXT_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_ext_riscv.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,206 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include OS_HEADER_INLINE(os)\n+\n+const char* VM_Version::_uarch = \"\";\n+uint32_t VM_Version::_initial_vector_length = 0;\n+\n+void VM_Version::initialize() {\n+  get_os_cpu_info();\n+\n+  if (FLAG_IS_DEFAULT(UseFMA)) {\n+    FLAG_SET_DEFAULT(UseFMA, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(AllocatePrefetchDistance)) {\n+    FLAG_SET_DEFAULT(AllocatePrefetchDistance, 0);\n+  }\n+\n+  if (UseAES || UseAESIntrinsics) {\n+    if (UseAES && !FLAG_IS_DEFAULT(UseAES)) {\n+      warning(\"AES instructions are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAES, false);\n+    }\n+    if (UseAESIntrinsics && !FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n+      warning(\"AES intrinsics are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAESIntrinsics, false);\n+    }\n+  }\n+\n+  if (UseAESCTRIntrinsics) {\n+    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+  }\n+\n+  if (UseSHA) {\n+    warning(\"SHA instructions are not available on this CPU\");\n+    FLAG_SET_DEFAULT(UseSHA, false);\n+  }\n+\n+  if (UseSHA1Intrinsics) {\n+    warning(\"Intrinsics for SHA-1 crypto hash functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseSHA1Intrinsics, false);\n+  }\n+\n+  if (UseSHA256Intrinsics) {\n+    warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+  }\n+\n+  if (UseSHA512Intrinsics) {\n+    warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n+  }\n+\n+  if (UseCRC32Intrinsics) {\n+    warning(\"CRC32 intrinsics are not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseCRC32Intrinsics, false);\n+  }\n+\n+  if (UseCRC32CIntrinsics) {\n+    warning(\"CRC32C intrinsics are not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseCRC32CIntrinsics, false);\n+  }\n+\n+  if (UseRVV) {\n+    if (!(_features & CPU_V)) {\n+      warning(\"RVV is not supported on this CPU\");\n+      FLAG_SET_DEFAULT(UseRVV, false);\n+    } else {\n+      \/\/ read vector length from vector CSR vlenb\n+      _initial_vector_length = get_current_vector_length();\n+    }\n+  }\n+\n+  if (UseRVB && !(_features & CPU_B)) {\n+    warning(\"RVB is not supported on this CPU\");\n+    FLAG_SET_DEFAULT(UseRVB, false);\n+  }\n+\n+  if (UseRVC && !(_features & CPU_C)) {\n+    warning(\"RVC is not supported on this CPU\");\n+    FLAG_SET_DEFAULT(UseRVC, false);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(AvoidUnalignedAccesses)) {\n+    FLAG_SET_DEFAULT(AvoidUnalignedAccesses, true);\n+  }\n+\n+  if (UseRVB) {\n+    if (FLAG_IS_DEFAULT(UsePopCountInstruction)) {\n+      FLAG_SET_DEFAULT(UsePopCountInstruction, true);\n+    }\n+  } else {\n+    FLAG_SET_DEFAULT(UsePopCountInstruction, false);\n+  }\n+\n+  char buf[512];\n+  buf[0] = '\\0';\n+  if (_uarch != NULL && strcmp(_uarch, \"\") != 0) snprintf(buf, sizeof(buf), \"%s,\", _uarch);\n+  strcat(buf, \"rv64\");\n+#define ADD_FEATURE_IF_SUPPORTED(id, name, bit) if (_features & CPU_##id) strcat(buf, name);\n+  CPU_FEATURE_FLAGS(ADD_FEATURE_IF_SUPPORTED)\n+#undef ADD_FEATURE_IF_SUPPORTED\n+\n+  _features_string = os::strdup(buf);\n+\n+#ifdef COMPILER2\n+  c2_initialize();\n+#endif \/\/ COMPILER2\n+\n+  UNSUPPORTED_OPTION(CriticalJNINatives);\n+\n+  FLAG_SET_DEFAULT(UseMembar, true);\n+}\n+\n+#ifdef COMPILER2\n+void VM_Version::c2_initialize() {\n+  if (UseCMoveUnconditionally) {\n+    FLAG_SET_DEFAULT(UseCMoveUnconditionally, false);\n+  }\n+\n+  if (ConditionalMoveLimit > 0) {\n+    FLAG_SET_DEFAULT(ConditionalMoveLimit, 0);\n+  }\n+\n+  if (!UseRVV) {\n+    FLAG_SET_DEFAULT(SpecialEncodeISOArray, false);\n+  }\n+\n+  if (!UseRVV && MaxVectorSize) {\n+    FLAG_SET_DEFAULT(MaxVectorSize, 0);\n+  }\n+\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(MaxVectorSize)) {\n+      MaxVectorSize = _initial_vector_length;\n+    } else if (MaxVectorSize < 16) {\n+      warning(\"RVV does not support vector length less than 16 bytes. Disabling RVV.\");\n+      UseRVV = false;\n+    } else if (is_power_of_2(MaxVectorSize)) {\n+      if (MaxVectorSize > _initial_vector_length) {\n+        warning(\"Current system only supports max RVV vector length %d. Set MaxVectorSize to %d\",\n+                _initial_vector_length, _initial_vector_length);\n+      }\n+      MaxVectorSize = _initial_vector_length;\n+    } else {\n+      vm_exit_during_initialization(err_msg(\"Unsupported MaxVectorSize: %d\", (int)MaxVectorSize));\n+    }\n+  }\n+\n+  \/\/ disable prefetch\n+  if (FLAG_IS_DEFAULT(AllocatePrefetchStyle)) {\n+    FLAG_SET_DEFAULT(AllocatePrefetchStyle, 0);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(UseMulAddIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseMulAddIntrinsic, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseMultiplyToLenIntrinsic, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(UseSquareToLenIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseSquareToLenIntrinsic, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(UseMontgomeryMultiplyIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseMontgomeryMultiplyIntrinsic, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(UseMontgomerySquareIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseMontgomerySquareIntrinsic, true);\n+  }\n+}\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":206,"deletions":0,"binary":false,"changes":206,"status":"added"},{"patch":"@@ -0,0 +1,70 @@\n+\/*\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VM_VERSION_RISCV_HPP\n+#define CPU_RISCV_VM_VERSION_RISCV_HPP\n+\n+#include \"runtime\/abstract_vm_version.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class VM_Version : public Abstract_VM_Version {\n+#ifdef COMPILER2\n+private:\n+  static void c2_initialize();\n+#endif \/\/ COMPILER2\n+\n+protected:\n+  static const char* _uarch;\n+  static uint32_t _initial_vector_length;\n+  static void get_os_cpu_info();\n+  static uint32_t get_current_vector_length();\n+\n+public:\n+  \/\/ Initialization\n+  static void initialize();\n+\n+  enum Feature_Flag {\n+#define CPU_FEATURE_FLAGS(decl)               \\\n+    decl(I,            \"i\",            8)     \\\n+    decl(M,            \"m\",           12)     \\\n+    decl(A,            \"a\",            0)     \\\n+    decl(F,            \"f\",            5)     \\\n+    decl(D,            \"d\",            3)     \\\n+    decl(C,            \"c\",            2)     \\\n+    decl(V,            \"v\",           21)     \\\n+    decl(B,            \"b\",            1)\n+\n+#define DECLARE_CPU_FEATURE_FLAG(id, name, bit) CPU_##id = (1 << bit),\n+    CPU_FEATURE_FLAGS(DECLARE_CPU_FEATURE_FLAG)\n+#undef DECLARE_CPU_FEATURE_FLAG\n+  };\n+\n+  static void initialize_cpu_information(void);\n+};\n+\n+#endif \/\/ CPU_RISCV_VM_VERSION_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2006, 2010, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"code\/vmreg.hpp\"\n+\n+void VMRegImpl::set_regName() {\n+  int i = 0;\n+  Register reg = ::as_Register(0);\n+  for ( ; i < ConcreteRegisterImpl::max_gpr ; ) {\n+    for (int j = 0 ; j < RegisterImpl::max_slots_per_register ; j++) {\n+      regName[i++] = reg->name();\n+    }\n+    reg = reg->successor();\n+  }\n+\n+  FloatRegister freg = ::as_FloatRegister(0);\n+  for ( ; i < ConcreteRegisterImpl::max_fpr ; ) {\n+    for (int j = 0 ; j < FloatRegisterImpl::max_slots_per_register ; j++) {\n+      regName[i++] = freg->name();\n+    }\n+    freg = freg->successor();\n+  }\n+\n+  for ( ; i < ConcreteRegisterImpl::number_of_registers ; i++) {\n+    regName[i] = \"NON-GPR-FPR\";\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/vmreg_riscv.cpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2006, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VMREG_RISCV_HPP\n+#define CPU_RISCV_VMREG_RISCV_HPP\n+\n+inline bool is_Register() {\n+  return (unsigned int) value() < (unsigned int) ConcreteRegisterImpl::max_gpr;\n+}\n+\n+inline bool is_FloatRegister() {\n+  return value() >= ConcreteRegisterImpl::max_gpr && value() < ConcreteRegisterImpl::max_fpr;\n+}\n+\n+inline Register as_Register() {\n+  assert(is_Register(), \"must be\");\n+  return ::as_Register(value() \/ RegisterImpl::max_slots_per_register);\n+}\n+\n+inline FloatRegister as_FloatRegister() {\n+  assert(is_FloatRegister() && is_even(value()), \"must be\");\n+  return ::as_FloatRegister((value() - ConcreteRegisterImpl::max_gpr) \/\n+                            FloatRegisterImpl::max_slots_per_register);\n+}\n+\n+inline bool is_concrete() {\n+  assert(is_reg(), \"must be\");\n+  return is_even(value());\n+}\n+\n+#endif \/\/ CPU_RISCV_VMREG_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/vmreg_riscv.hpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2006, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_VM_VMREG_RISCV_INLINE_HPP\n+#define CPU_RISCV_VM_VMREG_RISCV_INLINE_HPP\n+\n+inline VMReg RegisterImpl::as_VMReg() const {\n+  if (this == noreg) {\n+    return VMRegImpl::Bad();\n+  }\n+  return VMRegImpl::as_VMReg(encoding() * RegisterImpl::max_slots_per_register);\n+}\n+\n+inline VMReg FloatRegisterImpl::as_VMReg() const {\n+  return VMRegImpl::as_VMReg((encoding() * FloatRegisterImpl::max_slots_per_register) +\n+                             ConcreteRegisterImpl::max_gpr);\n+}\n+\n+inline VMReg VectorRegisterImpl::as_VMReg() const {\n+  return VMRegImpl::as_VMReg((encoding() * VectorRegisterImpl::max_slots_per_register) +\n+                             ConcreteRegisterImpl::max_fpr);\n+}\n+\n+#endif \/\/ CPU_RISCV_VM_VMREG_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/vmreg_riscv.inline.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,260 @@\n+\/*\n+ * Copyright (c) 2003, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"assembler_riscv.inline.hpp\"\n+#include \"code\/vtableStubs.hpp\"\n+#include \"interp_masm_riscv.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/compiledICHolder.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klassVtable.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/runtime.hpp\"\n+#endif\n+\n+\/\/ machine-dependent part of VtableStubs: create VtableStub of correct size and\n+\/\/ initialize its code\n+\n+#define __ masm->\n+\n+#ifndef PRODUCT\n+extern \"C\" void bad_compiled_vtable_index(JavaThread* thread, oop receiver, int index);\n+#endif\n+\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+  \/\/ Read \"A word on VtableStub sizing\" in share\/code\/vtableStubs.hpp for details on stub sizing.\n+  const int stub_code_length = code_size_limit(true);\n+  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index);\n+  \/\/ Can be NULL if there is no free space in the code cache.\n+  if (s == NULL) {\n+    return NULL;\n+  }\n+\n+  \/\/ Count unused bytes in instruction sequences of variable size.\n+  \/\/ We add them to the computed buffer size in order to avoid\n+  \/\/ overflow in subsequently generated stubs.\n+  address   start_pc = NULL;\n+  int       slop_bytes = 0;\n+  int       slop_delta = 0;\n+\n+  ResourceMark    rm;\n+  CodeBuffer      cb(s->entry_point(), stub_code_length);\n+  MacroAssembler* masm = new MacroAssembler(&cb);\n+  assert_cond(masm != NULL);\n+\n+#if (!defined(PRODUCT) && defined(COMPILER2))\n+  if (CountCompiledCalls) {\n+    __ la(t2, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n+    __ add_memory_int64(Address(t2), 1);\n+  }\n+#endif\n+\n+  \/\/ get receiver (need to skip return address on top of stack)\n+  assert(VtableStub::receiver_location() == j_rarg0->as_VMReg(), \"receiver expected in j_rarg0\");\n+\n+  \/\/ get receiver klass\n+  address npe_addr = __ pc();\n+  __ load_klass(t2, j_rarg0);\n+\n+#ifndef PRODUCT\n+  if (DebugVtables) {\n+    Label L;\n+    start_pc = __ pc();\n+\n+    \/\/ check offset vs vtable length\n+    __ lwu(t0, Address(t2, Klass::vtable_length_offset()));\n+    __ mvw(t1, vtable_index * vtableEntry::size());\n+    __ bgt(t0, t1, L);\n+    __ enter();\n+    __ mv(x12, vtable_index);\n+\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, bad_compiled_vtable_index), j_rarg0, x12);\n+    const ptrdiff_t estimate = 256;\n+    const ptrdiff_t codesize = __ pc() - start_pc;\n+    slop_delta = estimate - codesize;  \/\/ call_VM varies in length, depending on data\n+    slop_bytes += slop_delta;\n+    assert(slop_delta >= 0, \"vtable #%d: Code size estimate (%d) for DebugVtables too small, required: %d\", vtable_index, (int)estimate, (int)codesize);\n+\n+    __ leave();\n+    __ bind(L);\n+  }\n+#endif \/\/ PRODUCT\n+\n+  start_pc = __ pc();\n+  __ lookup_virtual_method(t2, vtable_index, xmethod);\n+  \/\/ lookup_virtual_method generates\n+  \/\/ 4 instructions (maximum value encountered in normal case):li(lui + addiw) + add + ld\n+  \/\/ 1 instruction (best case):ld * 1\n+  slop_delta = 16 - (int)(__ pc() - start_pc);\n+  slop_bytes += slop_delta;\n+  assert(slop_delta >= 0, \"negative slop(%d) encountered, adjust code size estimate!\", slop_delta);\n+\n+#ifndef PRODUCT\n+  if (DebugVtables) {\n+    Label L;\n+    __ beqz(xmethod, L);\n+    __ ld(t0, Address(xmethod, Method::from_compiled_offset()));\n+    __ bnez(t0, L);\n+    __ stop(\"Vtable entry is NULL\");\n+    __ bind(L);\n+  }\n+#endif \/\/ PRODUCT\n+\n+  \/\/ x10: receiver klass\n+  \/\/ xmethod: Method*\n+  \/\/ x12: receiver\n+  address ame_addr = __ pc();\n+  __ ld(t0, Address(xmethod, Method::from_compiled_offset()));\n+  __ jr(t0);\n+\n+  masm->flush();\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, 0);\n+\n+  return s;\n+}\n+\n+VtableStub* VtableStubs::create_itable_stub(int itable_index) {\n+  \/\/ Read \"A word on VtableStub sizing\" in share\/code\/vtableStubs.hpp for details on stub sizing.\n+  const int stub_code_length = code_size_limit(false);\n+  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index);\n+  \/\/ Can be NULL if there is no free space in the code cache.\n+  if (s == NULL) {\n+    return NULL;\n+  }\n+  \/\/ Count unused bytes in instruction sequences of variable size.\n+  \/\/ We add them to the computed buffer size in order to avoid\n+  \/\/ overflow in subsequently generated stubs.\n+  address   start_pc = NULL;\n+  int       slop_bytes = 0;\n+  int       slop_delta = 0;\n+\n+  ResourceMark    rm;\n+  CodeBuffer      cb(s->entry_point(), stub_code_length);\n+  MacroAssembler* masm = new MacroAssembler(&cb);\n+  assert_cond(masm != NULL);\n+\n+#if (!defined(PRODUCT) && defined(COMPILER2))\n+  if (CountCompiledCalls) {\n+    __ la(x18, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n+    __ add_memory_int64(Address(x18), 1);\n+  }\n+#endif\n+\n+  \/\/ get receiver (need to skip return address on top of stack)\n+  assert(VtableStub::receiver_location() == j_rarg0->as_VMReg(), \"receiver expected in j_rarg0\");\n+\n+  \/\/ Entry arguments:\n+  \/\/  t2: CompiledICHolder\n+  \/\/  j_rarg0: Receiver\n+\n+  \/\/ This stub is called from compiled code which has no callee-saved registers,\n+  \/\/ so all registers except arguments are free at this point.\n+  const Register recv_klass_reg     = x18;\n+  const Register holder_klass_reg   = x19; \/\/ declaring interface klass (DECC)\n+  const Register resolved_klass_reg = xmethod; \/\/ resolved interface klass (REFC)\n+  const Register temp_reg           = x28;\n+  const Register temp_reg2          = x29;\n+  const Register icholder_reg       = t1;\n+\n+  Label L_no_such_interface;\n+\n+  __ ld(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n+  __ ld(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+\n+  start_pc = __ pc();\n+\n+  \/\/ get receiver klass (also an implicit null-check)\n+  address npe_addr = __ pc();\n+  __ load_klass(recv_klass_reg, j_rarg0);\n+\n+  \/\/ Receiver subtype check against REFC.\n+  __ lookup_interface_method(\/\/ inputs: rec. class, interface\n+                             recv_klass_reg, resolved_klass_reg, noreg,\n+                             \/\/ outputs:  scan temp. reg1, scan temp. reg2\n+                             temp_reg2, temp_reg,\n+                             L_no_such_interface,\n+                             \/*return_method=*\/false);\n+\n+  const ptrdiff_t typecheckSize = __ pc() - start_pc;\n+  start_pc = __ pc();\n+\n+  \/\/ Get selected method from declaring class and itable index\n+  __ lookup_interface_method(\/\/ inputs: rec. class, interface, itable index\n+                             recv_klass_reg, holder_klass_reg, itable_index,\n+                             \/\/ outputs: method, scan temp. reg\n+                             xmethod, temp_reg,\n+                             L_no_such_interface);\n+\n+  const ptrdiff_t lookupSize = __ pc() - start_pc;\n+\n+  \/\/ Reduce \"estimate\" such that \"padding\" does not drop below 8.\n+  const ptrdiff_t estimate = 256;\n+  const ptrdiff_t codesize = typecheckSize + lookupSize;\n+  slop_delta = (int)(estimate - codesize);\n+  slop_bytes += slop_delta;\n+  assert(slop_delta >= 0, \"itable #%d: Code size estimate (%d) for lookup_interface_method too small, required: %d\", itable_index, (int)estimate, (int)codesize);\n+\n+#ifdef ASSERT\n+  if (DebugVtables) {\n+    Label L2;\n+    __ beqz(xmethod, L2);\n+    __ ld(t0, Address(xmethod, Method::from_compiled_offset()));\n+    __ bnez(t0, L2);\n+    __ stop(\"compiler entrypoint is null\");\n+    __ bind(L2);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ xmethod: Method*\n+  \/\/ j_rarg0: receiver\n+  address ame_addr = __ pc();\n+  __ ld(t0, Address(xmethod, Method::from_compiled_offset()));\n+  __ jr(t0);\n+\n+  __ bind(L_no_such_interface);\n+  \/\/ Handle IncompatibleClassChangeError in itable stubs.\n+  \/\/ More detailed error message.\n+  \/\/ We force resolving of the call site by jumping to the \"handle\n+  \/\/ wrong method\" stub, and so let the interpreter runtime do all the\n+  \/\/ dirty work.\n+  assert(SharedRuntime::get_handle_wrong_method_stub() != NULL, \"check initialization order\");\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+  masm->flush();\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, 0);\n+\n+  return s;\n+}\n+\n+int VtableStub::pd_code_alignment() {\n+  \/\/ RISCV cache line size is not an architected constant. We just align on word size.\n+  const unsigned int icache_line_size = wordSize;\n+  return icache_line_size;\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/vtableStubs_riscv.cpp","additions":260,"deletions":0,"binary":false,"changes":260,"status":"added"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2019, SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2019 SAP SE. All rights reserved.\n@@ -1450,1 +1450,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on s390\");\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1973,1 +1973,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on x86\");\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2840,0 +2840,2 @@\n+#elif defined(RISCV)\n+  strncpy(cpuinfo, \"RISCV64\", length);\n@@ -4071,1 +4073,2 @@\n-    SPARC_ONLY(4 * M);\n+    SPARC_ONLY(4 * M)\n+    RISCV64_ONLY(2 * M);\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,26 @@\n+\/*\n+ * Copyright (c) 1999, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/\/ nothing required here\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/assembler_linux_riscv.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"added"},{"patch":"@@ -0,0 +1,119 @@\n+\/*\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_ATOMIC_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_ATOMIC_LINUX_RISCV_HPP\n+\n+#include \"runtime\/vm_version.hpp\"\n+\n+\/\/ Implementation of class atomic\n+\n+\/\/ Note that memory_order_conservative requires a full barrier after atomic stores.\n+\/\/ See https:\/\/patchwork.kernel.org\/patch\/3575821\/\n+\n+#define FULL_MEM_BARRIER  __sync_synchronize()\n+#define READ_MEM_BARRIER  __atomic_thread_fence(__ATOMIC_ACQUIRE);\n+#define WRITE_MEM_BARRIER __atomic_thread_fence(__ATOMIC_RELEASE);\n+\n+template<size_t byte_size>\n+struct Atomic::PlatformAdd\n+  : Atomic::FetchAndAdd<Atomic::PlatformAdd<byte_size> >\n+{\n+  template<typename I, typename D>\n+  D add_and_fetch(I add_value, D volatile* dest, atomic_memory_order order) const {\n+    D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);\n+    FULL_MEM_BARRIER;\n+    return res;\n+  }\n+\n+  template<typename I, typename D>\n+  D fetch_and_add(I add_value, D volatile* dest, atomic_memory_order order) const {\n+    return add_and_fetch(add_value, dest, order) - add_value;\n+  }\n+};\n+\n+template<size_t byte_size>\n+template<typename T>\n+inline T Atomic::PlatformXchg<byte_size>::operator()(T exchange_value,\n+                                                     T volatile* dest,\n+                                                     atomic_memory_order order) const {\n+  STATIC_ASSERT(byte_size == sizeof(T));\n+  T res = __atomic_exchange_n(dest, exchange_value, __ATOMIC_RELEASE);\n+  FULL_MEM_BARRIER;\n+  return res;\n+}\n+\n+\/\/ __attribute__((unused)) on dest is to get rid of spurious GCC warnings.\n+template<size_t byte_size>\n+template<typename T>\n+inline T Atomic::PlatformCmpxchg<byte_size>::operator()(T exchange_value,\n+                                                        T volatile* dest __attribute__((unused)),\n+                                                        T compare_value,\n+                                                        atomic_memory_order order) const {\n+  STATIC_ASSERT(byte_size == sizeof(T));\n+  T value = compare_value;\n+  if (order != memory_order_relaxed) {\n+    FULL_MEM_BARRIER;\n+  }\n+\n+  __atomic_compare_exchange(dest, &value, &exchange_value, \/* weak *\/ false,\n+                            __ATOMIC_RELAXED, __ATOMIC_RELAXED);\n+\n+  if (order != memory_order_relaxed) {\n+    FULL_MEM_BARRIER;\n+  }\n+  return value;\n+}\n+\n+template<>\n+template<typename T>\n+inline T Atomic::PlatformCmpxchg<4>::operator()(T exchange_value,\n+                                                T volatile* dest __attribute__((unused)),\n+                                                T compare_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(T));\n+  if (order != memory_order_relaxed) {\n+    FULL_MEM_BARRIER;\n+  }\n+  T rv;\n+  int tmp;\n+  __asm volatile(\n+    \"1:\\n\\t\"\n+    \" addiw     %[tmp], %[cv], 0\\n\\t\" \/\/ make sure compare_value signed_extend\n+    \" lr.w.aq   %[rv], (%[dest])\\n\\t\"\n+    \" bne       %[rv], %[tmp], 2f\\n\\t\"\n+    \" sc.w.rl   %[tmp], %[ev], (%[dest])\\n\\t\"\n+    \" bnez      %[tmp], 1b\\n\\t\"\n+    \"2:\\n\\t\"\n+    : [rv] \"=&r\" (rv), [tmp] \"=&r\" (tmp)\n+    : [ev] \"r\" (exchange_value), [dest] \"r\" (dest), [cv] \"r\" (compare_value)\n+    : \"memory\");\n+  if (order != memory_order_relaxed) {\n+    FULL_MEM_BARRIER;\n+  }\n+  return rv;\n+}\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_ATOMIC_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/atomic_linux_riscv.hpp","additions":119,"deletions":0,"binary":false,"changes":119,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n+\n+#include <byteswap.h>\n+\n+\/\/ Efficient swapping of data bytes from Java byte\n+\/\/ ordering to native byte ordering and vice versa.\n+inline u2   Bytes::swap_u2(u2 x) {\n+  return bswap_16(x);\n+}\n+\n+inline u4   Bytes::swap_u4(u4 x) {\n+  return bswap_32(x);\n+}\n+\n+inline u8 Bytes::swap_u8(u8 x) {\n+  return bswap_64(x);\n+}\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/bytes_linux_riscv.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_VM_COPY_LINUX_RISCV_INLINE_HPP\n+#define OS_CPU_LINUX_RISCV_VM_COPY_LINUX_RISCV_INLINE_HPP\n+\n+static void pd_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {\n+  (void)memmove(to, from, count * HeapWordSize);\n+}\n+\n+static void pd_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {\n+  switch (count) {\n+    case 8:  to[7] = from[7];   \/\/ fall through\n+    case 7:  to[6] = from[6];   \/\/ fall through\n+    case 6:  to[5] = from[5];   \/\/ fall through\n+    case 5:  to[4] = from[4];   \/\/ fall through\n+    case 4:  to[3] = from[3];   \/\/ fall through\n+    case 3:  to[2] = from[2];   \/\/ fall through\n+    case 2:  to[1] = from[1];   \/\/ fall through\n+    case 1:  to[0] = from[0];   \/\/ fall through\n+    case 0:  break;\n+    default:\n+      memcpy(to, from, count * HeapWordSize);\n+      break;\n+  }\n+}\n+\n+static void pd_disjoint_words_atomic(const HeapWord* from, HeapWord* to, size_t count) {\n+  switch (count) {\n+    case 8:  to[7] = from[7];\n+    case 7:  to[6] = from[6];\n+    case 6:  to[5] = from[5];\n+    case 5:  to[4] = from[4];\n+    case 4:  to[3] = from[3];\n+    case 3:  to[2] = from[2];\n+    case 2:  to[1] = from[1];\n+    case 1:  to[0] = from[0];\n+    case 0:  break;\n+    default:\n+      while (count-- > 0) {\n+        *to++ = *from++;\n+      }\n+      break;\n+  }\n+}\n+\n+static void pd_aligned_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {\n+  pd_conjoint_words(from, to, count);\n+}\n+\n+static void pd_aligned_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {\n+  pd_disjoint_words(from, to, count);\n+}\n+\n+static void pd_conjoint_bytes(const void* from, void* to, size_t count) {\n+  (void)memmove(to, from, count);\n+}\n+\n+static void pd_conjoint_bytes_atomic(const void* from, void* to, size_t count) {\n+  pd_conjoint_bytes(from, to, count);\n+}\n+\n+static void pd_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {\n+  _Copy_conjoint_jshorts_atomic(from, to, count);\n+}\n+\n+static void pd_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {\n+  _Copy_conjoint_jints_atomic(from, to, count);\n+}\n+\n+static void pd_conjoint_jlongs_atomic(const jlong* from, jlong* to, size_t count) {\n+  _Copy_conjoint_jlongs_atomic(from, to, count);\n+}\n+\n+static void pd_conjoint_oops_atomic(const oop* from, oop* to, size_t count) {\n+  assert(BytesPerLong == BytesPerOop, \"jlongs and oops must be the same size.\");\n+  _Copy_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);\n+}\n+\n+static void pd_arrayof_conjoint_bytes(const HeapWord* from, HeapWord* to, size_t count) {\n+  _Copy_arrayof_conjoint_bytes(from, to, count);\n+}\n+\n+static void pd_arrayof_conjoint_jshorts(const HeapWord* from, HeapWord* to, size_t count) {\n+  _Copy_arrayof_conjoint_jshorts(from, to, count);\n+}\n+\n+static void pd_arrayof_conjoint_jints(const HeapWord* from, HeapWord* to, size_t count) {\n+  _Copy_arrayof_conjoint_jints(from, to, count);\n+}\n+\n+static void pd_arrayof_conjoint_jlongs(const HeapWord* from, HeapWord* to, size_t count) {\n+  _Copy_arrayof_conjoint_jlongs(from, to, count);\n+}\n+\n+static void pd_arrayof_conjoint_oops(const HeapWord* from, HeapWord* to, size_t count) {\n+  assert(!UseCompressedOops, \"foo!\");\n+  assert(BytesPerLong == BytesPerOop, \"jlongs and oops must be the same size\");\n+  _Copy_arrayof_conjoint_jlongs(from, to, count);\n+}\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_VM_COPY_LINUX_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/copy_linux_riscv.inline.hpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_VM_GLOBALS_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_VM_GLOBALS_LINUX_RISCV_HPP\n+\n+\/\/ Sets the default values for platform dependent flags used by the runtime system.\n+\/\/ (see globals.hpp)\n+\n+define_pd_global(bool,  DontYieldALot,            false);\n+define_pd_global(intx,  ThreadStackSize,          2048); \/\/ 0 => use system default\n+define_pd_global(intx,  VMThreadStackSize,        2048);\n+\n+define_pd_global(intx,  CompilerThreadStackSize,  2048);\n+\n+define_pd_global(uintx, JVMInvokeMethodSlack,     8192);\n+\n+\/\/ Used on 64 bit platforms for UseCompressedOops base address\n+define_pd_global(uintx, HeapBaseMinAddress,       2 * G);\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_VM_GLOBALS_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/globals_linux_riscv.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_ORDERACCESS_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_ORDERACCESS_LINUX_RISCV_HPP\n+\n+\/\/ Included in orderAccess.hpp header file.\n+\n+#include \"runtime\/vm_version.hpp\"\n+\n+\/\/ Implementation of class OrderAccess.\n+\n+inline void OrderAccess::loadload()   { acquire(); }\n+inline void OrderAccess::storestore() { release(); }\n+inline void OrderAccess::loadstore()  { acquire(); }\n+inline void OrderAccess::storeload()  { fence(); }\n+\n+inline void OrderAccess::acquire() {\n+  READ_MEM_BARRIER;\n+}\n+\n+inline void OrderAccess::release() {\n+  WRITE_MEM_BARRIER;\n+}\n+\n+inline void OrderAccess::fence() {\n+  FULL_MEM_BARRIER;\n+}\n+\n+\n+template<size_t byte_size>\n+struct OrderAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+{\n+  template <typename T>\n+  T operator()(const volatile T* p) const { T data; __atomic_load(const_cast<T*>(p), &data, __ATOMIC_ACQUIRE); return data; }\n+};\n+\n+template<size_t byte_size>\n+struct OrderAccess::PlatformOrderedStore<byte_size, RELEASE_X>\n+{\n+  template <typename T>\n+  void operator()(T v, volatile T* p) const { __atomic_store(const_cast<T*>(p), &v, __ATOMIC_RELEASE); }\n+};\n+\n+template<size_t byte_size>\n+struct OrderAccess::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n+{\n+  template <typename T>\n+  void operator()(T v, volatile T* p) const { release_store(p, v); OrderAccess::fence(); }\n+};\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_ORDERACCESS_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/orderAccess_linux_riscv.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,624 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/\/ no precompiled headers\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"code\/nativeInst.hpp\"\n+#include \"code\/vtableStubs.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"jvm.h\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"os_share_linux.hpp\"\n+#include \"prims\/jniFastGetField.hpp\"\n+#include \"prims\/jvm_misc.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/extendedPC.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/osThread.hpp\"\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+\n+\/\/ put OS-includes here\n+# include <dlfcn.h>\n+# include <fpu_control.h>\n+# include <errno.h>\n+# include <pthread.h>\n+# include <signal.h>\n+# include <stdio.h>\n+# include <stdlib.h>\n+# include <sys\/mman.h>\n+# include <sys\/resource.h>\n+# include <sys\/socket.h>\n+# include <sys\/stat.h>\n+# include <sys\/time.h>\n+# include <sys\/types.h>\n+# include <sys\/utsname.h>\n+# include <sys\/wait.h>\n+# include <poll.h>\n+# include <pwd.h>\n+# include <ucontext.h>\n+# include <unistd.h>\n+\n+#define REG_LR       1\n+#define REG_FP       8\n+\n+NOINLINE address os::current_stack_pointer() {\n+  return (address)__builtin_frame_address(0);\n+}\n+\n+char* os::non_memory_address_word() {\n+  \/\/ Must never look like an address returned by reserve_memory,\n+  return (char*) -1;\n+}\n+\n+address os::Linux::ucontext_get_pc(const ucontext_t * uc) {\n+  return (address)uc->uc_mcontext.__gregs[REG_PC];\n+}\n+\n+void os::Linux::ucontext_set_pc(ucontext_t * uc, address pc) {\n+  uc->uc_mcontext.__gregs[REG_PC] = (intptr_t)pc;\n+}\n+\n+intptr_t* os::Linux::ucontext_get_sp(const ucontext_t * uc) {\n+  return (intptr_t*)uc->uc_mcontext.__gregs[REG_SP];\n+}\n+\n+intptr_t* os::Linux::ucontext_get_fp(const ucontext_t * uc) {\n+  return (intptr_t*)uc->uc_mcontext.__gregs[REG_FP];\n+}\n+\n+\/\/ For Forte Analyzer AsyncGetCallTrace profiling support - thread\n+\/\/ is currently interrupted by SIGPROF.\n+\/\/ os::Solaris::fetch_frame_from_ucontext() tries to skip nested signal\n+\/\/ frames. Currently we don't do that on Linux, so it's the same as\n+\/\/ os::fetch_frame_from_context().\n+ExtendedPC os::Linux::fetch_frame_from_ucontext(Thread* thread,\n+  const ucontext_t* uc, intptr_t** ret_sp, intptr_t** ret_fp) {\n+\n+  assert(thread != NULL, \"just checking\");\n+  assert(ret_sp != NULL, \"just checking\");\n+  assert(ret_fp != NULL, \"just checking\");\n+\n+  return os::fetch_frame_from_context(uc, ret_sp, ret_fp);\n+}\n+\n+ExtendedPC os::fetch_frame_from_context(const void* ucVoid,\n+                    intptr_t** ret_sp, intptr_t** ret_fp) {\n+\n+  ExtendedPC epc;\n+  const ucontext_t* uc = (const ucontext_t*)ucVoid;\n+\n+  if (uc != NULL) {\n+    epc = ExtendedPC(os::Linux::ucontext_get_pc(uc));\n+    if (ret_sp != NULL) {\n+      *ret_sp = os::Linux::ucontext_get_sp(uc);\n+    }\n+    if (ret_fp != NULL) {\n+      *ret_fp = os::Linux::ucontext_get_fp(uc);\n+    }\n+  } else {\n+    \/\/ construct empty ExtendedPC for return value checking\n+    epc = ExtendedPC(NULL);\n+    if (ret_sp != NULL) {\n+      *ret_sp = (intptr_t *)NULL;\n+    }\n+    if (ret_fp != NULL) {\n+      *ret_fp = (intptr_t *)NULL;\n+    }\n+  }\n+\n+  return epc;\n+}\n+\n+frame os::fetch_frame_from_context(const void* ucVoid) {\n+  intptr_t* frame_sp = NULL;\n+  intptr_t* frame_fp = NULL;\n+  ExtendedPC epc = fetch_frame_from_context(ucVoid, &frame_sp, &frame_fp);\n+  return frame(frame_sp, frame_fp, epc.pc());\n+}\n+\n+bool os::Linux::get_frame_at_stack_banging_point(JavaThread* thread, ucontext_t* uc, frame* fr) {\n+  address pc = (address) os::Linux::ucontext_get_pc(uc);\n+  if (Interpreter::contains(pc)) {\n+    \/\/ interpreter performs stack banging after the fixed frame header has\n+    \/\/ been generated while the compilers perform it before. To maintain\n+    \/\/ semantic consistency between interpreted and compiled frames, the\n+    \/\/ method returns the Java sender of the current frame.\n+    *fr = os::fetch_frame_from_context(uc);\n+    if (!fr->is_first_java_frame()) {\n+      assert(fr->safe_for_sender(thread), \"Safety check\");\n+      *fr = fr->java_sender();\n+    }\n+  } else {\n+    \/\/ more complex code with compiled code\n+    assert(!Interpreter::contains(pc), \"Interpreted methods should have been handled above\");\n+    CodeBlob* cb = CodeCache::find_blob(pc);\n+    if (cb == NULL || !cb->is_nmethod() || cb->is_frame_complete_at(pc)) {\n+      \/\/ Not sure where the pc points to, fallback to default\n+      \/\/ stack overflow handling\n+      return false;\n+    } else {\n+      \/\/ In compiled code, the stack banging is performed before RA\n+      \/\/ has been saved in the frame.  RA is live, and SP and FP\n+      \/\/ belong to the caller.\n+      intptr_t* fp = os::Linux::ucontext_get_fp(uc);\n+      intptr_t* sp = os::Linux::ucontext_get_sp(uc);\n+      address pc = (address)(uc->uc_mcontext.__gregs[REG_LR]\n+                         - NativeInstruction::instruction_size);\n+      *fr = frame(sp, fp, pc);\n+      if (!fr->is_java_frame()) {\n+        assert(fr->safe_for_sender(thread), \"Safety check\");\n+        assert(!fr->is_first_frame(), \"Safety check\");\n+        *fr = fr->java_sender();\n+      }\n+    }\n+  }\n+  assert(fr->is_java_frame(), \"Safety check\");\n+  return true;\n+}\n+\n+\/\/ By default, gcc always saves frame pointer rfp on this stack. This\n+\/\/ may get turned off by -fomit-frame-pointer.\n+frame os::get_sender_for_C_frame(frame* fr) {\n+  return frame(fr->sender_sp(), fr->link(), fr->sender_pc());\n+}\n+\n+NOINLINE frame os::current_frame() {\n+  intptr_t **sender_sp = (intptr_t **)__builtin_frame_address(0);\n+  if (sender_sp != NULL) {\n+    frame myframe((intptr_t*)os::current_stack_pointer(),\n+                  sender_sp[frame::link_offset],\n+                  CAST_FROM_FN_PTR(address, os::current_frame));\n+    if (os::is_first_C_frame(&myframe)) {\n+      \/\/ stack is not walkable\n+      return frame();\n+    } else {\n+      return os::get_sender_for_C_frame(&myframe);\n+    }\n+  } else {\n+    ShouldNotReachHere();\n+    return frame();\n+  }\n+}\n+\n+\/\/ Utility functions\n+extern \"C\" JNIEXPORT int\n+JVM_handle_linux_signal(int sig,\n+                        siginfo_t* info,\n+                        void* ucVoid,\n+                        int abort_if_unrecognized) {\n+  ucontext_t* uc = (ucontext_t*) ucVoid;\n+\n+  Thread* t = Thread::current_or_null_safe();\n+\n+  \/\/ Must do this before SignalHandlerMark, if crash protection installed we will longjmp away\n+  \/\/ (no destructors can be run)\n+  os::ThreadCrashProtection::check_crash_protection(sig, t);\n+\n+  SignalHandlerMark shm(t);\n+\n+  \/\/ Note: it's not uncommon that JNI code uses signal\/sigset to install\n+  \/\/ then restore certain signal handler (e.g. to temporarily block SIGPIPE,\n+  \/\/ or have a SIGILL handler when detecting CPU type). When that happens,\n+  \/\/ JVM_handle_linux_signal() might be invoked with junk info\/ucVoid. To\n+  \/\/ avoid unnecessary crash when libjsig is not preloaded, try handle signals\n+  \/\/ that do not require siginfo\/ucontext first.\n+\n+  if (sig == SIGPIPE || sig == SIGXFSZ) {\n+    \/\/ allow chained handler to go first\n+    if (os::Linux::chained_handler(sig, info, ucVoid)) {\n+      return true;\n+    } else {\n+      \/\/ Ignoring SIGPIPE\/SIGXFSZ - see bugs 4229104 or 6499219\n+      return true;\n+    }\n+  }\n+\n+#ifdef CAN_SHOW_REGISTERS_ON_ASSERT\n+  if ((sig == SIGSEGV || sig == SIGBUS) && info != NULL && info->si_addr == g_assert_poison) {\n+    if (handle_assert_poison_fault(ucVoid, info->si_addr)) {\n+      return 1;\n+    }\n+  }\n+#endif\n+\n+  JavaThread* thread = NULL;\n+  VMThread* vmthread = NULL;\n+  if (os::Linux::signal_handlers_are_installed) {\n+    if (t != NULL ){\n+      if(t->is_Java_thread()) {\n+        thread = (JavaThread *) t;\n+      }\n+      else if(t->is_VM_thread()){\n+        vmthread = (VMThread *)t;\n+      }\n+    }\n+  }\n+\n+  \/\/ Handle SafeFetch faults\n+  if ((sig == SIGSEGV || sig == SIGBUS) && uc != NULL) {\n+    address const pc = (address) os::Linux::ucontext_get_pc(uc);\n+    if (pc && StubRoutines::is_safefetch_fault(pc)) {\n+      os::Linux::ucontext_set_pc(uc, StubRoutines::continuation_for_safefetch_fault(pc));\n+      return 1;\n+    }\n+  }\n+\n+  \/\/ decide if this trap can be handled by a stub\n+  address stub = NULL;\n+\n+  address pc          = NULL;\n+\n+  \/\/%note os_trap_1\n+  if (info != NULL && uc != NULL && thread != NULL) {\n+    pc = (address) os::Linux::ucontext_get_pc(uc);\n+\n+    \/\/ Handle ALL stack overflow variations here\n+    if (sig == SIGSEGV) {\n+      address addr = (address) info->si_addr;\n+\n+      \/\/ check if fault address is within thread stack\n+      if (thread->on_local_stack(addr)) {\n+        \/\/ stack overflow\n+        if (thread->in_stack_yellow_reserved_zone(addr)) {\n+          if (thread->thread_state() == _thread_in_Java) {\n+            if (thread->in_stack_reserved_zone(addr)) {\n+              frame fr;\n+              if (os::Linux::get_frame_at_stack_banging_point(thread, uc, &fr)) {\n+                assert(fr.is_java_frame(), \"Must be a Java frame\");\n+                frame activation =\n+                  SharedRuntime::look_for_reserved_stack_annotated_method(thread, fr);\n+                if (activation.sp() != NULL) {\n+                  thread->disable_stack_reserved_zone();\n+                  if (activation.is_interpreted_frame()) {\n+                    thread->set_reserved_stack_activation((address)(\n+                      activation.fp() + frame::interpreter_frame_initial_sp_offset));\n+                  } else {\n+                    thread->set_reserved_stack_activation((address)activation.unextended_sp());\n+                  }\n+                  return 1;\n+                }\n+              }\n+            }\n+            \/\/ Throw a stack overflow exception.  Guard pages will be reenabled\n+            \/\/ while unwinding the stack.\n+            thread->disable_stack_yellow_reserved_zone();\n+            stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW);\n+          } else {\n+            \/\/ Thread was in the vm or native code.  Return and try to finish.\n+            thread->disable_stack_yellow_reserved_zone();\n+            return 1;\n+          }\n+        } else if (thread->in_stack_red_zone(addr)) {\n+          \/\/ Fatal red zone violation.  Disable the guard pages and fall through\n+          \/\/ to handle_unexpected_exception way down below.\n+          thread->disable_stack_red_zone();\n+          tty->print_raw_cr(\"An irrecoverable stack overflow has occurred.\");\n+\n+          \/\/ This is a likely cause, but hard to verify. Let's just print\n+          \/\/ it as a hint.\n+          tty->print_raw_cr(\"Please check if any of your loaded .so files has \"\n+                            \"enabled executable stack (see man page execstack(8))\");\n+        } else {\n+          \/\/ Accessing stack address below sp may cause SEGV if current\n+          \/\/ thread has MAP_GROWSDOWN stack. This should only happen when\n+          \/\/ current thread was created by user code with MAP_GROWSDOWN flag\n+          \/\/ and then attached to VM. See notes in os_linux.cpp.\n+          if (thread->osthread()->expanding_stack() == 0) {\n+             thread->osthread()->set_expanding_stack();\n+             if (os::Linux::manually_expand_stack(thread, addr)) {\n+               thread->osthread()->clear_expanding_stack();\n+               return 1;\n+             }\n+             thread->osthread()->clear_expanding_stack();\n+          } else {\n+             fatal(\"recursive segv. expanding stack.\");\n+          }\n+        }\n+      }\n+    }\n+\n+    if (thread->thread_state() == _thread_in_Java) {\n+      \/\/ Java thread running in Java code => find exception handler if any\n+      \/\/ a fault inside compiled code, the interpreter, or a stub\n+\n+      \/\/ Handle signal from NativeJump::patch_verified_entry().\n+      if ((sig == SIGILL || sig == SIGTRAP)\n+          && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+        if (TraceTraps) {\n+          tty->print_cr(\"trap: zombie_not_entrant (%s)\", (sig == SIGTRAP) ? \"SIGTRAP\" : \"SIGILL\");\n+        }\n+        stub = SharedRuntime::get_handle_wrong_method_stub();\n+      } else if (sig == SIGSEGV && os::is_poll_address((address)info->si_addr)) {\n+        stub = SharedRuntime::get_poll_stub(pc);\n+      } else if (sig == SIGBUS \/* && info->si_code == BUS_OBJERR *\/) {\n+        \/\/ BugId 4454115: A read from a MappedByteBuffer can fault\n+        \/\/ here if the underlying file has been truncated.\n+        \/\/ Do not crash the VM in such a case.\n+        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CompiledMethod* nm = (cb != NULL) ? cb->as_compiled_method_or_null() : NULL;\n+        if (nm != NULL && nm->has_unsafe_access()) {\n+          address next_pc = pc + NativeCall::instruction_size;\n+          stub = SharedRuntime::handle_unsafe_access(thread, next_pc);\n+        }\n+      } else if (sig == SIGFPE  &&\n+                 (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV)) {\n+        stub =\n+          SharedRuntime::\n+          continuation_for_implicit_exception(thread,\n+                                              pc,\n+                                              SharedRuntime::\n+                                              IMPLICIT_DIVIDE_BY_ZERO);\n+      } else if (sig == SIGSEGV &&\n+               !MacroAssembler::needs_explicit_null_check((intptr_t)info->si_addr)) {\n+          \/\/ Determination of interpreter\/vtable stub\/compiled code null exception\n+          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);\n+      }\n+    } else if (thread->thread_state() == _thread_in_vm &&\n+               sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n+               thread->doing_unsafe_access()) {\n+      address next_pc = pc + NativeCall::instruction_size;\n+      stub = SharedRuntime::handle_unsafe_access(thread, next_pc);\n+    }\n+\n+    \/\/ jni_fast_Get<Primitive>Field can trap at certain pc's if a GC kicks in\n+    \/\/ and the heap gets shrunk before the field access.\n+    if ((sig == SIGSEGV) || (sig == SIGBUS)) {\n+      address addr = JNI_FastGetField::find_slowcase_pc(pc);\n+      if (addr != (address)-1) {\n+        stub = addr;\n+      }\n+    }\n+\n+    \/\/ Check to see if we caught the safepoint code in the\n+    \/\/ process of write protecting the memory serialization page.\n+    \/\/ It write enables the page immediately after protecting it\n+    \/\/ so we can just return to retry the write.\n+    if ((sig == SIGSEGV) &&\n+        os::is_memory_serialize_page(thread, (address) info->si_addr)) {\n+      \/\/ Block current thread until the memory serialize page permission restored.\n+      os::block_on_serialize_page_trap();\n+      return true;\n+    }\n+  }\n+\n+  if (stub != NULL) {\n+    \/\/ save all thread context in case we need to restore it\n+    if (thread != NULL) thread->set_saved_exception_pc(pc);\n+\n+    os::Linux::ucontext_set_pc(uc, stub);\n+    return true;\n+  }\n+\n+  \/\/ signal-chaining\n+  if (os::Linux::chained_handler(sig, info, ucVoid)) {\n+     return true;\n+  }\n+\n+  if (!abort_if_unrecognized) {\n+    \/\/ caller wants another chance, so give it to him\n+    return false;\n+  }\n+\n+  if (pc == NULL && uc != NULL) {\n+    pc = os::Linux::ucontext_get_pc(uc);\n+  }\n+\n+  \/\/ unmask current signal\n+  sigset_t newset;\n+  sigemptyset(&newset);\n+  sigaddset(&newset, sig);\n+  sigprocmask(SIG_UNBLOCK, &newset, NULL);\n+\n+  VMError::report_and_die(t, sig, pc, info, ucVoid);\n+\n+  ShouldNotReachHere();\n+  return true; \/\/ Mute compiler\n+}\n+\n+void os::Linux::init_thread_fpu_state(void) {\n+}\n+\n+int os::Linux::get_fpu_control_word(void) {\n+  return 0;\n+}\n+\n+void os::Linux::set_fpu_control_word(int fpu_control) {\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ thread stack\n+\n+\/\/ Minimum usable stack sizes required to get to user code. Space for\n+\/\/ HotSpot guard pages is added later.\n+size_t os::Posix::_compiler_thread_min_stack_allowed = 72 * K;\n+size_t os::Posix::_java_thread_min_stack_allowed = 72 * K;\n+size_t os::Posix::_vm_internal_thread_min_stack_allowed = 72 * K;\n+\n+\/\/ return default stack size for thr_type\n+size_t os::Posix::default_stack_size(os::ThreadType thr_type) {\n+  \/\/ default stack size (compiler thread needs larger stack)\n+  size_t s = (thr_type == os::compiler_thread ? 4 * M : 1 * M);\n+  return s;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ helper functions for fatal error handler\n+\n+static const char* reg_abi_names[] = {\n+  \"pc\",\n+  \"x1(ra)\", \"x2(sp)\", \"x3(gp)\", \"x4(tp)\",\n+  \"x5(t0)\", \"x6(t1)\", \"x7(t2)\",\n+  \"x8(s0)\", \"x9(s1)\",\n+  \"x10(a0)\", \"x11(a1)\", \"x12(a2)\", \"x13(a3)\", \"x14(a4)\", \"x15(a5)\", \"x16(a6)\", \"x17(a7)\",\n+  \"x18(s2)\", \"x19(s3)\", \"x20(s4)\", \"x21(s5)\", \"x22(s6)\", \"x23(s7)\", \"x24(s8)\", \"x25(s9)\", \"x26(s10)\", \"x27(s11)\",\n+  \"x28(t3)\", \"x29(t4)\",\"x30(t5)\", \"x31(t6)\"\n+};\n+\n+void os::print_context(outputStream *st, const void *context) {\n+  if (context == NULL) {\n+    return;\n+  }\n+\n+  const ucontext_t *uc = (const ucontext_t*)context;\n+  st->print_cr(\"Registers:\");\n+  for (int r = 0; r < 32; r++) {\n+    st->print(\"%-*.*s=\", 8, 8, reg_abi_names[r]);\n+    print_location(st, uc->uc_mcontext.__gregs[r]);\n+  }\n+  st->cr();\n+\n+  intptr_t *frame_sp = (intptr_t *)os::Linux::ucontext_get_sp(uc);\n+  st->print_cr(\"Top of Stack: (sp=\" PTR_FORMAT \")\", p2i(frame_sp));\n+  print_hex_dump(st, (address)frame_sp, (address)(frame_sp + 64), sizeof(intptr_t));\n+  st->cr();\n+\n+  \/\/ Note: it may be unsafe to inspect memory near pc. For example, pc may\n+  \/\/ point to garbage if entry point in an nmethod is corrupted. Leave\n+  \/\/ this at the end, and hope for the best.\n+  address pc = os::Linux::ucontext_get_pc(uc);\n+  print_instructions(st, pc, sizeof(char));\n+  st->cr();\n+}\n+\n+void os::print_register_info(outputStream *st, const void *context) {\n+  if (context == NULL) {\n+    return;\n+  }\n+\n+  const ucontext_t *uc = (const ucontext_t*)context;\n+\n+  st->print_cr(\"Register to memory mapping:\");\n+  st->cr();\n+\n+  \/\/ this is horrendously verbose but the layout of the registers in the\n+  \/\/ context does not match how we defined our abstract Register set, so\n+  \/\/ we can't just iterate through the gregs area\n+\n+  \/\/ this is only for the \"general purpose\" registers\n+\n+  for (int r = 0; r < 32; r++)\n+    st->print_cr(\"%-*.*s=\" INTPTR_FORMAT, 8, 8, reg_abi_names[r], (uintptr_t)uc->uc_mcontext.__gregs[r]);\n+  st->cr();\n+}\n+\n+void os::setup_fpu() {\n+}\n+\n+#ifndef PRODUCT\n+void os::verify_stack_alignment() {\n+  assert(((intptr_t)os::current_stack_pointer() & (StackAlignmentInBytes-1)) == 0, \"incorrect stack alignment\");\n+}\n+#endif\n+\n+int os::extra_bang_size_in_bytes() {\n+  return 0;\n+}\n+\n+extern \"C\" {\n+  int SpinPause() {\n+    return 0;\n+  }\n+\n+  void _Copy_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {\n+    if (from > to) {\n+      const jshort *end = from + count;\n+      while (from < end) {\n+        *(to++) = *(from++);\n+      }\n+    } else if (from < to) {\n+      const jshort *end = from;\n+      from += count - 1;\n+      to   += count - 1;\n+      while (from >= end) {\n+        *(to--) = *(from--);\n+      }\n+    }\n+  }\n+  void _Copy_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {\n+    if (from > to) {\n+      const jint *end = from + count;\n+      while (from < end) {\n+        *(to++) = *(from++);\n+      }\n+    } else if (from < to) {\n+      const jint *end = from;\n+      from += count - 1;\n+      to   += count - 1;\n+      while (from >= end) {\n+        *(to--) = *(from--);\n+      }\n+    }\n+  }\n+  void _Copy_conjoint_jlongs_atomic(const jlong* from, jlong* to, size_t count) {\n+    if (from > to) {\n+      const jlong *end = from + count;\n+      while (from < end) {\n+        os::atomic_copy64(from++, to++);\n+      }\n+    } else if (from < to) {\n+      const jlong *end = from;\n+      from += count - 1;\n+      to   += count - 1;\n+      while (from >= end) {\n+        os::atomic_copy64(from--, to--);\n+      }\n+    }\n+  }\n+\n+  void _Copy_arrayof_conjoint_bytes(const HeapWord* from,\n+                                    HeapWord* to,\n+                                    size_t    count) {\n+    memmove(to, from, count);\n+  }\n+  void _Copy_arrayof_conjoint_jshorts(const HeapWord* from,\n+                                      HeapWord* to,\n+                                      size_t    count) {\n+    memmove(to, from, count * 2);\n+  }\n+  void _Copy_arrayof_conjoint_jints(const HeapWord* from,\n+                                    HeapWord* to,\n+                                    size_t    count) {\n+    memmove(to, from, count * 4);\n+  }\n+  void _Copy_arrayof_conjoint_jlongs(const HeapWord* from,\n+                                     HeapWord* to,\n+                                     size_t    count) {\n+    memmove(to, from, count * 8);\n+  }\n+};\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":624,"deletions":0,"binary":false,"changes":624,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_VM_OS_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_VM_OS_LINUX_RISCV_HPP\n+\n+  static void setup_fpu();\n+\n+  \/\/ Used to register dynamic code cache area with the OS\n+  \/\/ Note: Currently only used in 64 bit Windows implementations\n+  static bool register_code_area(char *low, char *high) { return true; }\n+\n+  \/\/ Atomically copy 64 bits of data\n+  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n+    *(jlong *) dst = *(const jlong *) src;\n+  }\n+\n+  \/\/ SYSCALL_RISCV_FLUSH_ICACHE is used to flush instruction cache. The \"fence.i\" instruction\n+  \/\/ only work on the current hart, so kernel provides the icache flush syscall to flush icache\n+  \/\/ on each hart. You can pass a flag to determine a global or local icache flush.\n+  static void icache_flush(long int start, long int end)\n+  {\n+    const int SYSCALL_RISCV_FLUSH_ICACHE = 259;\n+    register long int __a7 asm (\"a7\") = SYSCALL_RISCV_FLUSH_ICACHE;\n+    register long int __a0 asm (\"a0\") = start;\n+    register long int __a1 asm (\"a1\") = end;\n+    \/\/ the flush can be applied to either all threads or only the current.\n+    \/\/ 0 means a global icache flush, and the icache flush will be applied\n+    \/\/ to other harts concurrently executing.\n+    register long int __a2 asm (\"a2\") = 0;\n+    __asm__ volatile (\"ecall\\n\\t\"\n+                      : \"+r\" (__a0)\n+                      : \"r\" (__a0), \"r\" (__a1), \"r\" (__a2), \"r\" (__a7)\n+                      : \"memory\");\n+  }\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_VM_OS_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,38 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_VM_PREFETCH_LINUX_RISCV_INLINE_HPP\n+#define OS_CPU_LINUX_RISCV_VM_PREFETCH_LINUX_RISCV_INLINE_HPP\n+\n+#include \"runtime\/prefetch.hpp\"\n+\n+\n+inline void Prefetch::read (void *loc, intx interval) {\n+}\n+\n+inline void Prefetch::write(void *loc, intx interval) {\n+}\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_VM_PREFETCH_LINUX_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/prefetch_linux_riscv.inline.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/metaspaceShared.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+\n+frame JavaThread::pd_last_frame() {\n+  assert(has_last_Java_frame(), \"must have last_Java_sp() when suspended\");\n+  return frame(_anchor.last_Java_sp(), _anchor.last_Java_fp(), _anchor.last_Java_pc());\n+}\n+\n+\/\/ For Forte Analyzer AsyncGetCallTrace profiling support - thread is\n+\/\/ currently interrupted by SIGPROF\n+bool JavaThread::pd_get_top_frame_for_signal_handler(frame* fr_addr,\n+  void* ucontext, bool isInJava) {\n+\n+  assert(Thread::current() == this, \"caller must be current thread\");\n+  return pd_get_top_frame(fr_addr, ucontext, isInJava);\n+}\n+\n+bool JavaThread::pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava) {\n+  return pd_get_top_frame(fr_addr, ucontext, isInJava);\n+}\n+\n+bool JavaThread::pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava) {\n+  \/\/ If we have a last_Java_frame, then we should use it even if\n+  \/\/ isInJava == true.  It should be more reliable than ucontext info.\n+  if (has_last_Java_frame() && frame_anchor()->walkable()) {\n+    *fr_addr = pd_last_frame();\n+    return true;\n+  }\n+\n+  \/\/ At this point, we don't have a last_Java_frame, so\n+  \/\/ we try to glean some information out of the ucontext\n+  \/\/ if we were running Java code when SIGPROF came in.\n+  if (isInJava) {\n+    ucontext_t* uc = (ucontext_t*) ucontext;\n+\n+    intptr_t* ret_fp = NULL;\n+    intptr_t* ret_sp = NULL;\n+    ExtendedPC addr = os::Linux::fetch_frame_from_ucontext(this, uc,\n+      &ret_sp, &ret_fp);\n+    if (addr.pc() == NULL || ret_sp == NULL ) {\n+      \/\/ ucontext wasn't useful\n+      return false;\n+    }\n+\n+    if (MetaspaceShared::is_in_trampoline_frame(addr.pc())) {\n+      \/\/ In the middle of a trampoline call. Bail out for safety.\n+      \/\/ This happens rarely so shouldn't affect profiling.\n+      return false;\n+    }\n+\n+    frame ret_frame(ret_sp, ret_fp, addr.pc());\n+    if (!ret_frame.safe_for_sender(this)) {\n+#ifdef COMPILER2\n+      frame ret_frame2(ret_sp, NULL, addr.pc());\n+      if (!ret_frame2.safe_for_sender(this)) {\n+        \/\/ nothing else to try if the frame isn't good\n+        return false;\n+      }\n+      ret_frame = ret_frame2;\n+#else\n+      \/\/ nothing else to try if the frame isn't good\n+      return false;\n+#endif \/* COMPILER2 *\/\n+    }\n+    *fr_addr = ret_frame;\n+    return true;\n+  }\n+\n+  \/\/ nothing else to try\n+  return false;\n+}\n+\n+void JavaThread::cache_global_variables() { }\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/thread_linux_riscv.cpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_THREAD_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_THREAD_LINUX_RISCV_HPP\n+\n+ private:\n+  void pd_initialize() {\n+    _anchor.clear();\n+  }\n+\n+  frame pd_last_frame();\n+\n+ public:\n+  \/\/ Mutators are highly dangerous....\n+  intptr_t* last_Java_fp()                       { return _anchor.last_Java_fp(); }\n+  void  set_last_Java_fp(intptr_t* fp)           { _anchor.set_last_Java_fp(fp);   }\n+\n+  void set_base_of_stack_pointer(intptr_t* base_sp) {\n+  }\n+\n+  static ByteSize last_Java_fp_offset()          {\n+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_fp_offset();\n+  }\n+\n+  intptr_t* base_of_stack_pointer() {\n+    return NULL;\n+  }\n+  void record_base_of_stack_pointer() {\n+  }\n+\n+  bool pd_get_top_frame_for_signal_handler(frame* fr_addr, void* ucontext,\n+    bool isInJava);\n+\n+  bool pd_get_top_frame_for_profiling(frame* fr_addr, void* ucontext, bool isInJava);\n+private:\n+  bool pd_get_top_frame(frame* fr_addr, void* ucontext, bool isInJava);\n+public:\n+  \/\/ These routines are only used on cpu architectures that\n+  \/\/ have separate register stacks (Itanium).\n+  static bool register_stack_overflow() { return false; }\n+  static void enable_register_stack_guard() {}\n+  static void disable_register_stack_guard() {}\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_THREAD_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/thread_linux_riscv.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_VM_VMSTRUCTS_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_VM_VMSTRUCTS_LINUX_RISCV_HPP\n+\n+\/\/ These are the OS and CPU-specific fields, types and integer\n+\/\/ constants required by the Serviceability Agent. This file is\n+\/\/ referenced by vmStructs.cpp.\n+\n+#define VM_STRUCTS_OS_CPU(nonstatic_field, static_field, unchecked_nonstatic_field, volatile_nonstatic_field, nonproduct_nonstatic_field, c2_nonstatic_field, unchecked_c1_static_field, unchecked_c2_static_field) \\\n+                                                                                                                                     \\\n+  \/******************************\/                                                                                                   \\\n+  \/* Threads (NOTE: incomplete) *\/                                                                                                   \\\n+  \/******************************\/                                                                                                   \\\n+  nonstatic_field(OSThread,                      _thread_id,                                      OSThread::thread_id_t)             \\\n+  nonstatic_field(OSThread,                      _pthread_id,                                     pthread_t)\n+\n+\n+#define VM_TYPES_OS_CPU(declare_type, declare_toplevel_type, declare_oop_type, declare_integer_type, declare_unsigned_integer_type, declare_c1_toplevel_type, declare_c2_type, declare_c2_toplevel_type) \\\n+                                                                          \\\n+  \/**********************\/                                                \\\n+  \/* Posix Thread IDs   *\/                                                \\\n+  \/**********************\/                                                \\\n+                                                                          \\\n+  declare_integer_type(OSThread::thread_id_t)                             \\\n+  declare_unsigned_integer_type(pthread_t)\n+\n+#define VM_INT_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)\n+\n+#define VM_LONG_CONSTANTS_OS_CPU(declare_constant, declare_preprocessor_constant, declare_c1_constant, declare_c2_constant, declare_c2_preprocessor_constant)\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_VM_VMSTRUCTS_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/vmStructs_linux_riscv.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/register.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n+\n+#include <asm\/hwcap.h>\n+#include <sys\/auxv.h>\n+\n+#ifndef HWCAP_ISA_I\n+#define HWCAP_ISA_I  (1 << ('I' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_M\n+#define HWCAP_ISA_M  (1 << ('M' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_A\n+#define HWCAP_ISA_A  (1 << ('A' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_F\n+#define HWCAP_ISA_F  (1 << ('F' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_D\n+#define HWCAP_ISA_D  (1 << ('D' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_C\n+#define HWCAP_ISA_C  (1 << ('C' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_V\n+#define HWCAP_ISA_V  (1 << ('V' - 'A'))\n+#endif\n+\n+#ifndef HWCAP_ISA_B\n+#define HWCAP_ISA_B  (1 << ('B' - 'A'))\n+#endif\n+\n+#define read_csr(csr)                                           \\\n+({                                                              \\\n+        register unsigned long __v;                             \\\n+        __asm__ __volatile__ (\"csrr %0, %1\"                     \\\n+                              : \"=r\" (__v)                      \\\n+                              : \"i\" (csr)                       \\\n+                              : \"memory\");                      \\\n+        __v;                                                    \\\n+})\n+\n+uint32_t VM_Version::get_current_vector_length() {\n+  assert(_features & CPU_V, \"should not call this\");\n+  return (uint32_t)read_csr(CSR_VLENB);\n+}\n+\n+void VM_Version::get_os_cpu_info() {\n+\n+  uint64_t auxv = getauxval(AT_HWCAP);\n+\n+  STATIC_ASSERT(CPU_I == HWCAP_ISA_I);\n+  STATIC_ASSERT(CPU_M == HWCAP_ISA_M);\n+  STATIC_ASSERT(CPU_A == HWCAP_ISA_A);\n+  STATIC_ASSERT(CPU_F == HWCAP_ISA_F);\n+  STATIC_ASSERT(CPU_D == HWCAP_ISA_D);\n+  STATIC_ASSERT(CPU_C == HWCAP_ISA_C);\n+  STATIC_ASSERT(CPU_V == HWCAP_ISA_V);\n+  STATIC_ASSERT(CPU_B == HWCAP_ISA_B);\n+  _features = auxv & (\n+      HWCAP_ISA_I |\n+      HWCAP_ISA_M |\n+      HWCAP_ISA_A |\n+      HWCAP_ISA_F |\n+      HWCAP_ISA_D |\n+      HWCAP_ISA_C |\n+      HWCAP_ISA_V |\n+      HWCAP_ISA_B);\n+\n+  if (FILE *f = fopen(\"\/proc\/cpuinfo\", \"r\")) {\n+    char buf[512], *p;\n+    while (fgets(buf, sizeof (buf), f) != NULL) {\n+      if ((p = strchr(buf, ':')) != NULL) {\n+        if (strncmp(buf, \"uarch\", sizeof \"uarch\" - 1) == 0) {\n+          char* uarch = os::strdup(p + 2);\n+          uarch[strcspn(uarch, \"\\n\")] = '\\0';\n+          _uarch = uarch;\n+          break;\n+        }\n+      }\n+    }\n+    fclose(f);\n+  }\n+}\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/vm_version_linux_riscv.cpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,1 +202,0 @@\n-    case lir_cmove:\n@@ -255,3 +254,1 @@\n-  : LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n-  , _type(type)\n+  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL, type)\n@@ -265,3 +262,1 @@\n-  LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n-  , _type(type)\n+  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL, type)\n@@ -275,3 +270,1 @@\n-  : LIR_Op(lir_cond_float_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n-  , _type(type)\n+  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL, type)\n@@ -299,7 +292,7 @@\n-  switch (_cond) {\n-    case lir_cond_equal:        _cond = lir_cond_notEqual;     break;\n-    case lir_cond_notEqual:     _cond = lir_cond_equal;        break;\n-    case lir_cond_less:         _cond = lir_cond_greaterEqual; break;\n-    case lir_cond_lessEqual:    _cond = lir_cond_greater;      break;\n-    case lir_cond_greaterEqual: _cond = lir_cond_less;         break;\n-    case lir_cond_greater:      _cond = lir_cond_lessEqual;    break;\n+  switch (cond()) {\n+    case lir_cond_equal:        set_cond(lir_cond_notEqual);     break;\n+    case lir_cond_notEqual:     set_cond(lir_cond_equal);        break;\n+    case lir_cond_less:         set_cond(lir_cond_greaterEqual); break;\n+    case lir_cond_lessEqual:    set_cond(lir_cond_greater);      break;\n+    case lir_cond_greaterEqual: set_cond(lir_cond_less);         break;\n+    case lir_cond_greater:      set_cond(lir_cond_lessEqual);    break;\n@@ -528,0 +521,7 @@\n+      assert(opBranch->_tmp1->is_illegal() && opBranch->_tmp2->is_illegal() &&\n+             opBranch->_tmp3->is_illegal() && opBranch->_tmp4->is_illegal() &&\n+             opBranch->_tmp5->is_illegal(), \"not used\");\n+\n+      if (opBranch->_opr1->is_valid()) do_input(opBranch->_opr1);\n+      if (opBranch->_opr2->is_valid()) do_input(opBranch->_opr2);\n+\n@@ -618,2 +618,2 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n-      LIR_Op2* op2 = (LIR_Op2*)op;\n+      assert(op->as_Op4() != NULL, \"must be\");\n+      LIR_Op4* op4 = (LIR_Op4*)op;\n@@ -621,3 +621,3 @@\n-      assert(op2->_info == NULL && op2->_tmp1->is_illegal() && op2->_tmp2->is_illegal() &&\n-             op2->_tmp3->is_illegal() && op2->_tmp4->is_illegal() && op2->_tmp5->is_illegal(), \"not used\");\n-      assert(op2->_opr1->is_valid() && op2->_opr2->is_valid() && op2->_result->is_valid(), \"used\");\n+      assert(op4->_info == NULL && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n+             op4->_tmp3->is_illegal() && op4->_tmp4->is_illegal() && op4->_tmp5->is_illegal(), \"not used\");\n+      assert(op4->_opr1->is_valid() && op4->_opr2->is_valid() && op4->_result->is_valid(), \"used\");\n@@ -625,4 +625,6 @@\n-      do_input(op2->_opr1);\n-      do_input(op2->_opr2);\n-      do_temp(op2->_opr2);\n-      do_output(op2->_result);\n+      do_input(op4->_opr1);\n+      do_input(op4->_opr2);\n+      if (op4->_opr3->is_valid()) do_input(op4->_opr3);\n+      if (op4->_opr4->is_valid()) do_input(op4->_opr4);\n+      do_temp(op4->_opr2);\n+      do_output(op4->_result);\n@@ -1051,0 +1053,4 @@\n+void LIR_Op4::emit_code(LIR_Assembler* masm) {\n+  masm->emit_op4(this);\n+}\n+\n@@ -1087,0 +1093,4 @@\n+#ifdef RISCV\n+  , _cmp_opr1(LIR_OprFact::illegalOpr)\n+  , _cmp_opr2(LIR_OprFact::illegalOpr)\n+#endif\n@@ -1104,0 +1114,32 @@\n+#ifdef RISCV\n+void LIR_List::set_cmp_oprs(LIR_Op* op) {\n+  switch (op->code()) {\n+    case lir_cmp:\n+      _cmp_opr1 = op->as_Op2()->in_opr1();\n+      _cmp_opr2 = op->as_Op2()->in_opr2();\n+      break;\n+    case lir_branch: \/\/ fall through\n+    case lir_cond_float_branch:\n+      assert(op->as_OpBranch()->cond() == lir_cond_always ||\n+            (_cmp_opr1 != LIR_OprFact::illegalOpr && _cmp_opr2 != LIR_OprFact::illegalOpr),\n+            \"conditional branches must have legal operands\");\n+      if (op->as_OpBranch()->cond() != lir_cond_always) {\n+        op->as_Op2()->set_in_opr1(_cmp_opr1);\n+        op->as_Op2()->set_in_opr2(_cmp_opr2);\n+      }\n+      break;\n+    case lir_cmove:\n+      op->as_Op4()->set_in_opr3(_cmp_opr1);\n+      op->as_Op4()->set_in_opr4(_cmp_opr2);\n+      break;\n+#if INCLUDE_ZGC\n+    case lir_zloadbarrier_test:\n+      _cmp_opr1 = FrameMap::as_opr(t1);\n+      _cmp_opr2 = LIR_OprFact::intConst(0);\n+      break;\n+#endif\n+    default:\n+      break;\n+  }\n+}\n+#endif\n@@ -1683,1 +1725,0 @@\n-     case lir_cmove:                 s = \"cmove\";         break;\n@@ -1708,0 +1749,2 @@\n+     \/\/ LIR_Op4\n+     case lir_cmove:                 s = \"cmove\";         break;\n@@ -1844,0 +1887,2 @@\n+  in_opr1()->print(out); out->print(\" \");\n+  in_opr2()->print(out); out->print(\" \");\n@@ -1930,1 +1975,1 @@\n-  if (code() == lir_cmove || code() == lir_cmp) {\n+  if (code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch) {\n@@ -1981,0 +2026,9 @@\n+\/\/ LIR_Op4\n+void LIR_Op4::print_instr(outputStream* out) const {\n+  print_condition(out, condition()); out->print(\" \");\n+  in_opr1()->print(out);             out->print(\" \");\n+  in_opr2()->print(out);             out->print(\" \");\n+  in_opr3()->print(out);             out->print(\" \");\n+  in_opr4()->print(out);             out->print(\" \");\n+  result_opr()->print(out);\n+}\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":83,"deletions":29,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -870,0 +870,1 @@\n+class    LIR_Op4;\n@@ -919,2 +920,0 @@\n-      , lir_branch\n-      , lir_cond_float_branch\n@@ -932,0 +931,2 @@\n+      , lir_branch\n+      , lir_cond_float_branch\n@@ -936,1 +937,0 @@\n-      , lir_cmove\n@@ -967,0 +967,3 @@\n+  , begin_op4\n+      , lir_cmove\n+  , end_op4\n@@ -1004,0 +1007,5 @@\n+#ifdef INCLUDE_ZGC\n+  , begin_opZLoadBarrierTest\n+    , lir_zloadbarrier_test\n+  , end_opZLoadBarrierTest\n+#endif\n@@ -1137,0 +1145,1 @@\n+  virtual LIR_Op4* as_Op4() { return NULL; }\n@@ -1413,45 +1422,0 @@\n-\n-class LIR_OpBranch: public LIR_Op {\n- friend class LIR_OpVisitState;\n-\n- private:\n-  LIR_Condition _cond;\n-  BasicType     _type;\n-  Label*        _label;\n-  BlockBegin*   _block;  \/\/ if this is a branch to a block, this is the block\n-  BlockBegin*   _ublock; \/\/ if this is a float-branch, this is the unorderd block\n-  CodeStub*     _stub;   \/\/ if this is a branch to a stub, this is the stub\n-\n- public:\n-  LIR_OpBranch(LIR_Condition cond, BasicType type, Label* lbl)\n-    : LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL)\n-    , _cond(cond)\n-    , _type(type)\n-    , _label(lbl)\n-    , _block(NULL)\n-    , _ublock(NULL)\n-    , _stub(NULL) { }\n-\n-  LIR_OpBranch(LIR_Condition cond, BasicType type, BlockBegin* block);\n-  LIR_OpBranch(LIR_Condition cond, BasicType type, CodeStub* stub);\n-\n-  \/\/ for unordered comparisons\n-  LIR_OpBranch(LIR_Condition cond, BasicType type, BlockBegin* block, BlockBegin* ublock);\n-\n-  LIR_Condition cond()        const              { return _cond;        }\n-  BasicType     type()        const              { return _type;        }\n-  Label*        label()       const              { return _label;       }\n-  BlockBegin*   block()       const              { return _block;       }\n-  BlockBegin*   ublock()      const              { return _ublock;      }\n-  CodeStub*     stub()        const              { return _stub;       }\n-\n-  void          change_block(BlockBegin* b);\n-  void          change_ublock(BlockBegin* b);\n-  void          negate_cond();\n-\n-  virtual void emit_code(LIR_Assembler* masm);\n-  virtual LIR_OpBranch* as_OpBranch() { return this; }\n-  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n-};\n-\n-\n@@ -1617,1 +1581,1 @@\n-  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL)\n+  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n@@ -1621,2 +1585,0 @@\n-    , _type(T_ILLEGAL)\n-    , _condition(condition)\n@@ -1624,0 +1586,1 @@\n+    , _type(type)\n@@ -1628,2 +1591,3 @@\n-    , _tmp5(LIR_OprFact::illegalOpr) {\n-    assert(code == lir_cmp || code == lir_assert, \"code check\");\n+    , _tmp5(LIR_OprFact::illegalOpr)\n+    , _condition(condition) {\n+    assert(code == lir_cmp || code == lir_branch || code == lir_cond_float_branch || code == lir_assert, \"code check\");\n@@ -1654,1 +1618,0 @@\n-    , _condition(lir_cond_unknown)\n@@ -1660,2 +1623,3 @@\n-    , _tmp5(LIR_OprFact::illegalOpr) {\n-    assert(code != lir_cmp && is_in_range(code, begin_op2, end_op2), \"code check\");\n+    , _tmp5(LIR_OprFact::illegalOpr)\n+    , _condition(lir_cond_unknown) {\n+    assert(code != lir_cmp && code != lir_branch && code != lir_cond_float_branch && is_in_range(code, begin_op2, end_op2), \"code check\");\n@@ -1670,1 +1634,0 @@\n-    , _condition(lir_cond_unknown)\n@@ -1676,2 +1639,3 @@\n-    , _tmp5(tmp5) {\n-    assert(code != lir_cmp && is_in_range(code, begin_op2, end_op2), \"code check\");\n+    , _tmp5(tmp5)\n+    , _condition(lir_cond_unknown) {\n+    assert(code != lir_cmp && code != lir_branch && code != lir_cond_float_branch && is_in_range(code, begin_op2, end_op2), \"code check\");\n@@ -1689,1 +1653,1 @@\n-    assert(code() == lir_cmp || code() == lir_cmove || code() == lir_assert, \"only valid for cmp and cmove and assert\"); return _condition;\n+    assert(code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch || code() == lir_assert, \"only valid for branch and assert\"); return _condition;\n@@ -1692,1 +1656,1 @@\n-    assert(code() == lir_cmp || code() == lir_cmove, \"only valid for cmp and cmove\");  _condition = condition;\n+    assert(code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch, \"only valid for branch\"); _condition = condition;\n@@ -1706,0 +1670,45 @@\n+class LIR_OpBranch: public LIR_Op2 {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  Label*        _label;\n+  BlockBegin*   _block;  \/\/ if this is a branch to a block, this is the block\n+  BlockBegin*   _ublock; \/\/ if this is a float-branch, this is the unorderd block\n+  CodeStub*     _stub;   \/\/ if this is a branch to a stub, this is the stub\n+\n+ public:\n+  LIR_OpBranch(LIR_Condition cond, BasicType type, Label* lbl)\n+    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL, type)\n+    , _label(lbl)\n+    , _block(NULL)\n+    , _ublock(NULL)\n+    , _stub(NULL) { }\n+\n+  LIR_OpBranch(LIR_Condition cond, BasicType type, BlockBegin* block);\n+  LIR_OpBranch(LIR_Condition cond, BasicType type, CodeStub* stub);\n+\n+  \/\/ for unordered comparisons\n+  LIR_OpBranch(LIR_Condition cond, BasicType type, BlockBegin* block, BlockBegin* ublock);\n+\n+  LIR_Condition cond() const {\n+    return condition();\n+  }\n+\n+  void set_cond(LIR_Condition cond) {\n+    set_condition(cond);\n+  }\n+\n+  Label*        label()       const              { return _label;       }\n+  BlockBegin*   block()       const              { return _block;       }\n+  BlockBegin*   ublock()      const              { return _ublock;      }\n+  CodeStub*     stub()        const              { return _stub;        }\n+\n+  void          change_block(BlockBegin* b);\n+  void          change_ublock(BlockBegin* b);\n+  void          negate_cond();\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpBranch* as_OpBranch() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1769,0 +1778,57 @@\n+class LIR_Op4: public LIR_Op {\n+  friend class LIR_OpVisitState;\n+ protected:\n+  LIR_Opr   _opr1;\n+  LIR_Opr   _opr2;\n+  LIR_Opr   _opr3;\n+  LIR_Opr   _opr4;\n+  BasicType _type;\n+  LIR_Opr   _tmp1;\n+  LIR_Opr   _tmp2;\n+  LIR_Opr   _tmp3;\n+  LIR_Opr   _tmp4;\n+  LIR_Opr   _tmp5;\n+  LIR_Condition _condition;\n+\n+ public:\n+  LIR_Op4(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr opr4,\n+          LIR_Opr result, BasicType type)\n+    : LIR_Op(code, result, NULL)\n+    , _opr1(opr1)\n+    , _opr2(opr2)\n+    , _opr3(opr3)\n+    , _opr4(opr4)\n+    , _type(type)\n+    , _tmp1(LIR_OprFact::illegalOpr)\n+    , _tmp2(LIR_OprFact::illegalOpr)\n+    , _tmp3(LIR_OprFact::illegalOpr)\n+    , _tmp4(LIR_OprFact::illegalOpr)\n+    , _tmp5(LIR_OprFact::illegalOpr)\n+    , _condition(condition) {\n+    assert(code == lir_cmove, \"code check\");\n+    assert(type != T_ILLEGAL, \"cmove should have type\");\n+  }\n+\n+  LIR_Opr in_opr1() const                        { return _opr1; }\n+  LIR_Opr in_opr2() const                        { return _opr2; }\n+  LIR_Opr in_opr3() const                        { return _opr3; }\n+  LIR_Opr in_opr4() const                        { return _opr4; }\n+  BasicType type()  const                        { return _type; }\n+  LIR_Opr tmp1_opr() const                       { return _tmp1; }\n+  LIR_Opr tmp2_opr() const                       { return _tmp2; }\n+  LIR_Opr tmp3_opr() const                       { return _tmp3; }\n+  LIR_Opr tmp4_opr() const                       { return _tmp4; }\n+  LIR_Opr tmp5_opr() const                       { return _tmp5; }\n+\n+  LIR_Condition condition() const                { return _condition; }\n+  void set_condition(LIR_Condition condition)    { _condition = condition; }\n+\n+  void set_in_opr1(LIR_Opr opr)                  { _opr1 = opr; }\n+  void set_in_opr2(LIR_Opr opr)                  { _opr2 = opr; }\n+  void set_in_opr3(LIR_Opr opr)                  { _opr3 = opr; }\n+  void set_in_opr4(LIR_Opr opr)                  { _opr4 = opr; }\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_Op4* as_Op4() { return this; }\n+\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n@@ -1991,0 +2057,4 @@\n+#ifdef RISCV\n+  LIR_Opr       _cmp_opr1;\n+  LIR_Opr       _cmp_opr2;\n+#endif\n@@ -2003,0 +2073,6 @@\n+#ifdef RISCV\n+    set_cmp_oprs(op);\n+    \/\/ lir_cmp set cmp oprs only on riscv\n+    if (op->code() == lir_cmp) return;\n+#endif\n+\n@@ -2019,0 +2095,4 @@\n+#ifdef RISCV\n+  void set_cmp_oprs(LIR_Op* op);\n+#endif\n+\n@@ -2152,2 +2232,3 @@\n-  void cmove(LIR_Condition condition, LIR_Opr src1, LIR_Opr src2, LIR_Opr dst, BasicType type) {\n-    append(new LIR_Op2(lir_cmove, condition, src1, src2, dst, type));\n+  void cmove(LIR_Condition condition, LIR_Opr src1, LIR_Opr src2, LIR_Opr dst, BasicType type,\n+             LIR_Opr cmp_opr1 = LIR_OprFact::illegalOpr, LIR_Opr cmp_opr2 = LIR_OprFact::illegalOpr) {\n+    append(new LIR_Op4(lir_cmove, condition, src1, src2, cmp_opr1, cmp_opr2, dst, type));\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":145,"deletions":64,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -712,4 +712,0 @@\n-    case lir_cmove:\n-      cmove(op->condition(), op->in_opr1(), op->in_opr2(), op->result_opr(), op->type());\n-      break;\n-\n@@ -779,0 +775,11 @@\n+void LIR_Assembler::emit_op4(LIR_Op4* op) {\n+  switch(op->code()) {\n+    case lir_cmove:\n+      cmove(op->condition(), op->in_opr1(), op->in_opr2(), op->result_opr(), op->type(), op->in_opr3(), op->in_opr4());\n+      break;\n+\n+    default:\n+      Unimplemented();\n+      break;\n+  }\n+}\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -193,0 +193,1 @@\n+  void emit_op4(LIR_Op4* op);\n@@ -225,2 +226,2 @@\n-  void cmove(LIR_Condition code, LIR_Opr left, LIR_Opr right, LIR_Opr result, BasicType type);\n-\n+  void cmove(LIR_Condition code, LIR_Opr left, LIR_Opr right, LIR_Opr result, BasicType type,\n+             LIR_Opr cmp_opr1 = LIR_OprFact::illegalOpr, LIR_Opr cmp_opr2 = LIR_OprFact::illegalOpr);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1245,2 +1245,2 @@\n-      assert(op->as_Op2() != NULL, \"lir_cmove must be LIR_Op2\");\n-      LIR_Op2* cmove = (LIR_Op2*)op;\n+      assert(op->as_Op4() != NULL, \"lir_cmove must be LIR_Op4\");\n+      LIR_Op4* cmove = (LIR_Op4*)op;\n@@ -1249,1 +1249,1 @@\n-      LIR_Opr move_to = cmove->result_opr();\n+      LIR_Opr move_to   = cmove->result_opr();\n@@ -3151,0 +3151,3 @@\n+#ifndef RISCV\n+  \/\/ Disable these optimizations on riscv temporarily, because it does not\n+  \/\/ work when the comparison operands are bound to branches or cmoves.\n@@ -3158,0 +3161,1 @@\n+#endif\n@@ -6295,1 +6299,1 @@\n-              LIR_Op2* prev_cmove = NULL;\n+              LIR_Op4* prev_cmove = NULL;\n@@ -6301,2 +6305,2 @@\n-                  assert(prev_op->as_Op2() != NULL, \"cmove must be of type LIR_Op2\");\n-                  prev_cmove = (LIR_Op2*)prev_op;\n+                  assert(prev_op->as_Op4() != NULL, \"cmove must be of type LIR_Op4\");\n+                  prev_cmove = (LIR_Op4*)prev_op;\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2018, 2022, Red Hat, Inc. All rights reserved.\n@@ -34,1 +34,1 @@\n-#if !(defined AARCH64 || defined AMD64 || defined IA32)\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined RISCV64)\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -103,1 +103,1 @@\n-      LIR_Op(),\n+      LIR_Op(lir_zloadbarrier_test, LIR_OprFact::illegalOpr, NULL),\n","filename":"src\/hotspot\/share\/gc\/z\/c1\/zBarrierSetC1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-#elif defined(SPARC) || defined(ARM) || defined(AARCH64)\n+#elif defined(SPARC) || defined(ARM) || defined(AARCH64) || defined(RISCV)\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrBigEndian.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/opto\/regmask.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -199,1 +199,2 @@\n-                 SPARC_ONLY(\"sparc\")\n+                 SPARC_ONLY(\"sparc\")             \\\n+                 RISCV64_ONLY(\"riscv64\")\n","filename":"src\/hotspot\/share\/runtime\/abstract_vm_version.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1237,1 +1237,1 @@\n-#if !(defined(PPC64) || defined(AARCH64))\n+#if !(defined(PPC64) || defined(AARCH64) || defined(RISCV64))\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -145,1 +145,1 @@\n-#if defined(PPC64) || defined (AARCH64)\n+#if defined(PPC64) || defined (AARCH64) || defined(RISCV64)\n","filename":"src\/hotspot\/share\/runtime\/thread.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -604,0 +604,26 @@\n+#if defined(RISCV32) || defined(RISCV64)\n+#define RISCV\n+#define RISCV_ONLY(code) code\n+#define NOT_RISCV(code)\n+#else\n+#undef RISCV\n+#define RISCV_ONLY(code)\n+#define NOT_RISCV(code) code\n+#endif\n+\n+#ifdef RISCV32\n+#define RISCV32_ONLY(code) code\n+#define NOT_RISCV32(code)\n+#else\n+#define RISCV32_ONLY(code)\n+#define NOT_RISCV32(code) code\n+#endif\n+\n+#ifdef RISCV64\n+#define RISCV64_ONLY(code) code\n+#define NOT_RISCV64(code)\n+#else\n+#define RISCV64_ONLY(code)\n+#define NOT_RISCV64(code) code\n+#endif\n+\n","filename":"src\/hotspot\/share\/utilities\/macros.hpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,0 +61,4 @@\n+#ifdef riscv64\n+#include \"sun_jvm_hotspot_debugger_riscv64_RISCV64ThreadContext.h\"\n+#endif\n+\n@@ -400,1 +404,1 @@\n-#if defined(i386) || defined(amd64) || defined(sparc) || defined(sparcv9) | defined(ppc64) || defined(ppc64le) || defined(aarch64)\n+#if defined(i386) || defined(amd64) || defined(sparc) || defined(sparcv9) | defined(ppc64) || defined(ppc64le) || defined(aarch64) || defined(riscv64)\n@@ -428,0 +432,3 @@\n+#ifdef riscv64\n+#define NPRGREG sun_jvm_hotspot_debugger_riscv64_RISCV64ThreadContext_NPRGREG\n+#endif\n@@ -537,0 +544,38 @@\n+#if defined(riscv64)\n+#define REG_INDEX(reg)  sun_jvm_hotspot_debugger_riscv64_RISCV64ThreadContext_##reg\n+\n+  regs[REG_INDEX(PC)]  = gregs.pc;\n+  regs[REG_INDEX(LR)]  = gregs.ra;\n+  regs[REG_INDEX(SP)]  = gregs.sp;\n+  regs[REG_INDEX(R3)]  = gregs.gp;\n+  regs[REG_INDEX(R4)]  = gregs.tp;\n+  regs[REG_INDEX(R5)]  = gregs.t0;\n+  regs[REG_INDEX(R6)]  = gregs.t1;\n+  regs[REG_INDEX(R7)]  = gregs.t2;\n+  regs[REG_INDEX(R8)]  = gregs.s0;\n+  regs[REG_INDEX(R9)]  = gregs.s1;\n+  regs[REG_INDEX(R10)]  = gregs.a0;\n+  regs[REG_INDEX(R11)]  = gregs.a1;\n+  regs[REG_INDEX(R12)]  = gregs.a2;\n+  regs[REG_INDEX(R13)]  = gregs.a3;\n+  regs[REG_INDEX(R14)]  = gregs.a4;\n+  regs[REG_INDEX(R15)]  = gregs.a5;\n+  regs[REG_INDEX(R16)]  = gregs.a6;\n+  regs[REG_INDEX(R17)]  = gregs.a7;\n+  regs[REG_INDEX(R18)]  = gregs.s2;\n+  regs[REG_INDEX(R19)]  = gregs.s3;\n+  regs[REG_INDEX(R20)]  = gregs.s4;\n+  regs[REG_INDEX(R21)]  = gregs.s5;\n+  regs[REG_INDEX(R22)]  = gregs.s6;\n+  regs[REG_INDEX(R23)]  = gregs.s7;\n+  regs[REG_INDEX(R24)]  = gregs.s8;\n+  regs[REG_INDEX(R25)]  = gregs.s9;\n+  regs[REG_INDEX(R26)]  = gregs.s10;\n+  regs[REG_INDEX(R27)]  = gregs.s11;\n+  regs[REG_INDEX(R28)]  = gregs.t3;\n+  regs[REG_INDEX(R29)]  = gregs.t4;\n+  regs[REG_INDEX(R30)]  = gregs.t5;\n+  regs[REG_INDEX(R31)]  = gregs.t6;\n+\n+#endif \/* riscv64 *\/\n+\n","filename":"src\/jdk.hotspot.agent\/linux\/native\/libsaproc\/LinuxDebuggerLocal.c","additions":47,"deletions":2,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,0 +46,2 @@\n+#elif defined(riscv64)\n+#include <asm\/ptrace.h>\n","filename":"src\/jdk.hotspot.agent\/linux\/native\/libsaproc\/libproc.h","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+import sun.jvm.hotspot.debugger.MachineDescriptionRISCV64;\n@@ -601,0 +602,2 @@\n+        } else if (cpu.equals(\"riscv64\")) {\n+            machDesc = new MachineDescriptionRISCV64();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/HotSpotAgent.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2003, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger;\n+\n+public class MachineDescriptionRISCV64 extends MachineDescriptionTwosComplement implements MachineDescription {\n+  public long getAddressSize() {\n+    return 8;\n+  }\n+\n+  public boolean isLP64() {\n+    return true;\n+  }\n+\n+  public boolean isBigEndian() {\n+    return false;\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/MachineDescriptionRISCV64.java","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+import sun.jvm.hotspot.debugger.riscv64.*;\n@@ -43,0 +44,1 @@\n+import sun.jvm.hotspot.debugger.linux.riscv64.*;\n@@ -119,1 +121,8 @@\n-     } else {\n+    } else if (cpu.equals(\"riscv64\")) {\n+       RISCV64ThreadContext context = (RISCV64ThreadContext) thread.getContext();\n+       Address fp = context.getRegisterAsAddress(RISCV64ThreadContext.FP);\n+       if (fp == null) return null;\n+       Address pc  = context.getRegisterAsAddress(RISCV64ThreadContext.PC);\n+       if (pc == null) return null;\n+       return new LinuxRISCV64CFrame(dbg, fp, pc);\n+    } else {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/linux\/LinuxCDebugger.java","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2003, 2013, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.linux.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.linux.*;\n+import sun.jvm.hotspot.debugger.cdbg.*;\n+import sun.jvm.hotspot.debugger.cdbg.basic.*;\n+\n+public final class LinuxRISCV64CFrame extends BasicCFrame {\n+   private static final int C_FRAME_LINK_OFFSET        = -2;\n+   private static final int C_FRAME_RETURN_ADDR_OFFSET = -1;\n+\n+   public LinuxRISCV64CFrame(LinuxDebugger dbg, Address fp, Address pc) {\n+      super(dbg.getCDebugger());\n+      this.fp = fp;\n+      this.pc = pc;\n+      this.dbg = dbg;\n+   }\n+\n+   \/\/ override base class impl to avoid ELF parsing\n+   public ClosestSymbol closestSymbolToPC() {\n+      \/\/ try native lookup in debugger.\n+      return dbg.lookup(dbg.getAddressValue(pc()));\n+   }\n+\n+   public Address pc() {\n+      return pc;\n+   }\n+\n+   public Address localVariableBase() {\n+      return fp;\n+   }\n+\n+   public CFrame sender(ThreadProxy thread) {\n+      RISCV64ThreadContext context = (RISCV64ThreadContext) thread.getContext();\n+      Address rsp = context.getRegisterAsAddress(RISCV64ThreadContext.SP);\n+\n+      if ((fp == null) || fp.lessThan(rsp)) {\n+        return null;\n+      }\n+\n+      \/\/ Check alignment of fp\n+      if (dbg.getAddressValue(fp) % (2 * ADDRESS_SIZE) != 0) {\n+        return null;\n+      }\n+\n+      Address nextFP = fp.getAddressAt(C_FRAME_LINK_OFFSET * ADDRESS_SIZE);\n+      if (nextFP == null || nextFP.lessThanOrEqual(fp)) {\n+        return null;\n+      }\n+      Address nextPC  = fp.getAddressAt(C_FRAME_RETURN_ADDR_OFFSET * ADDRESS_SIZE);\n+      if (nextPC == null) {\n+        return null;\n+      }\n+      return new LinuxRISCV64CFrame(dbg, nextFP, nextPC);\n+   }\n+\n+   \/\/ package\/class internals only\n+   private static final int ADDRESS_SIZE = 8;\n+   private Address pc;\n+   private Address sp;\n+   private Address fp;\n+   private LinuxDebugger dbg;\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/linux\/riscv64\/LinuxRISCV64CFrame.java","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2003, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.linux.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.linux.*;\n+\n+public class LinuxRISCV64ThreadContext extends RISCV64ThreadContext {\n+  private LinuxDebugger debugger;\n+\n+  public LinuxRISCV64ThreadContext(LinuxDebugger debugger) {\n+    super();\n+    this.debugger = debugger;\n+  }\n+\n+  public void setRegisterAsAddress(int index, Address value) {\n+    setRegister(index, debugger.getAddressValue(value));\n+  }\n+\n+  public Address getRegisterAsAddress(int index) {\n+    return debugger.newAddress(getRegister(index));\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/linux\/riscv64\/LinuxRISCV64ThreadContext.java","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.proc.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.proc.*;\n+import sun.jvm.hotspot.utilities.*;\n+\n+public class ProcRISCV64Thread implements ThreadProxy {\n+    private ProcDebugger debugger;\n+    private int         id;\n+\n+    public ProcRISCV64Thread(ProcDebugger debugger, Address addr) {\n+        this.debugger = debugger;\n+\n+        \/\/ FIXME: the size here should be configurable. However, making it\n+        \/\/ so would produce a dependency on the \"types\" package from the\n+        \/\/ debugger package, which is not desired.\n+        this.id       = (int) addr.getCIntegerAt(0, 4, true);\n+    }\n+\n+    public ProcRISCV64Thread(ProcDebugger debugger, long id) {\n+        this.debugger = debugger;\n+        this.id = (int) id;\n+    }\n+\n+    public ThreadContext getContext() throws IllegalThreadStateException {\n+        ProcRISCV64ThreadContext context = new ProcRISCV64ThreadContext(debugger);\n+        long[] regs = debugger.getThreadIntegerRegisterSet(id);\n+        if (Assert.ASSERTS_ENABLED) {\n+            Assert.that(regs.length == RISCV64ThreadContext.NPRGREG, \"size mismatch\");\n+        }\n+        for (int i = 0; i < regs.length; i++) {\n+            context.setRegister(i, regs[i]);\n+        }\n+        return context;\n+    }\n+\n+    public boolean canSetContext() throws DebuggerException {\n+        return false;\n+    }\n+\n+    public void setContext(ThreadContext context)\n+    throws IllegalThreadStateException, DebuggerException {\n+        throw new DebuggerException(\"Unimplemented\");\n+    }\n+\n+    public String toString() {\n+        return \"t@\" + id;\n+    }\n+\n+    public boolean equals(Object obj) {\n+        if ((obj == null) || !(obj instanceof ProcRISCV64Thread)) {\n+            return false;\n+        }\n+\n+        return (((ProcRISCV64Thread) obj).id == id);\n+    }\n+\n+    public int hashCode() {\n+        return id;\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/proc\/riscv64\/ProcRISCV64Thread.java","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.proc.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.proc.*;\n+\n+public class ProcRISCV64ThreadContext extends RISCV64ThreadContext {\n+    private ProcDebugger debugger;\n+\n+    public ProcRISCV64ThreadContext(ProcDebugger debugger) {\n+        super();\n+        this.debugger = debugger;\n+    }\n+\n+    public void setRegisterAsAddress(int index, Address value) {\n+        setRegister(index, debugger.getAddressValue(value));\n+    }\n+\n+    public Address getRegisterAsAddress(int index) {\n+        return debugger.newAddress(getRegister(index));\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/proc\/riscv64\/ProcRISCV64ThreadContext.java","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.proc.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.proc.*;\n+\n+public class ProcRISCV64ThreadFactory implements ProcThreadFactory {\n+    private ProcDebugger debugger;\n+\n+    public ProcRISCV64ThreadFactory(ProcDebugger debugger) {\n+        this.debugger = debugger;\n+    }\n+\n+    public ThreadProxy createThreadWrapper(Address threadIdentifierAddr) {\n+        return new ProcRISCV64Thread(debugger, threadIdentifierAddr);\n+    }\n+\n+    public ThreadProxy createThreadWrapper(long id) {\n+        return new ProcRISCV64Thread(debugger, id);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/proc\/riscv64\/ProcRISCV64ThreadFactory.java","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.remote.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.remote.*;\n+import sun.jvm.hotspot.utilities.*;\n+\n+public class RemoteRISCV64Thread extends RemoteThread  {\n+  public RemoteRISCV64Thread(RemoteDebuggerClient debugger, Address addr) {\n+     super(debugger, addr);\n+  }\n+\n+  public RemoteRISCV64Thread(RemoteDebuggerClient debugger, long id) {\n+     super(debugger, id);\n+  }\n+\n+  public ThreadContext getContext() throws IllegalThreadStateException {\n+    RemoteRISCV64ThreadContext context = new RemoteRISCV64ThreadContext(debugger);\n+    long[] regs = (addr != null)? debugger.getThreadIntegerRegisterSet(addr) :\n+                                  debugger.getThreadIntegerRegisterSet(id);\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(regs.length == RISCV64ThreadContext.NPRGREG, \"size of register set must match\");\n+    }\n+    for (int i = 0; i < regs.length; i++) {\n+      context.setRegister(i, regs[i]);\n+    }\n+    return context;\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/remote\/riscv64\/RemoteRISCV64Thread.java","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.remote.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.debugger.remote.*;\n+\n+public class RemoteRISCV64ThreadContext extends RISCV64ThreadContext {\n+  private RemoteDebuggerClient debugger;\n+\n+  public RemoteRISCV64ThreadContext(RemoteDebuggerClient debugger) {\n+    super();\n+    this.debugger = debugger;\n+  }\n+\n+  public void setRegisterAsAddress(int index, Address value) {\n+    setRegister(index, debugger.getAddressValue(value));\n+  }\n+\n+  public Address getRegisterAsAddress(int index) {\n+    return debugger.newAddress(getRegister(index));\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/remote\/riscv64\/RemoteRISCV64ThreadContext.java","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.remote.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.remote.*;\n+\n+public class RemoteRISCV64ThreadFactory implements RemoteThreadFactory {\n+  private RemoteDebuggerClient debugger;\n+\n+  public RemoteRISCV64ThreadFactory(RemoteDebuggerClient debugger) {\n+    this.debugger = debugger;\n+  }\n+\n+  public ThreadProxy createThreadWrapper(Address threadIdentifierAddr) {\n+    return new RemoteRISCV64Thread(debugger, threadIdentifierAddr);\n+  }\n+\n+  public ThreadProxy createThreadWrapper(long id) {\n+    return new RemoteRISCV64Thread(debugger, id);\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/remote\/riscv64\/RemoteRISCV64ThreadFactory.java","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,172 @@\n+\/*\n+ * Copyright (c) 2003, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.debugger.riscv64;\n+\n+import java.lang.annotation.Native;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.cdbg.*;\n+\n+\/** Specifies the thread context on riscv64 platforms; only a sub-portion\n+ * of the context is guaranteed to be present on all operating\n+ * systems. *\/\n+\n+public abstract class RISCV64ThreadContext implements ThreadContext {\n+    \/\/ Taken from \/usr\/include\/asm\/sigcontext.h on Linux\/RISCV64.\n+\n+    \/\/  \/*\n+    \/\/   * Signal context structure - contains all info to do with the state\n+    \/\/   * before the signal handler was invoked.\n+    \/\/   *\/\n+    \/\/ struct sigcontext {\n+    \/\/   struct user_regs_struct sc_regs;\n+    \/\/   union __riscv_fp_state sc_fpregs;\n+    \/\/ };\n+    \/\/\n+    \/\/ struct user_regs_struct {\n+    \/\/    unsigned long pc;\n+    \/\/    unsigned long ra;\n+    \/\/    unsigned long sp;\n+    \/\/    unsigned long gp;\n+    \/\/    unsigned long tp;\n+    \/\/    unsigned long t0;\n+    \/\/    unsigned long t1;\n+    \/\/    unsigned long t2;\n+    \/\/    unsigned long s0;\n+    \/\/    unsigned long s1;\n+    \/\/    unsigned long a0;\n+    \/\/    unsigned long a1;\n+    \/\/    unsigned long a2;\n+    \/\/    unsigned long a3;\n+    \/\/    unsigned long a4;\n+    \/\/    unsigned long a5;\n+    \/\/    unsigned long a6;\n+    \/\/    unsigned long a7;\n+    \/\/    unsigned long s2;\n+    \/\/    unsigned long s3;\n+    \/\/    unsigned long s4;\n+    \/\/    unsigned long s5;\n+    \/\/    unsigned long s6;\n+    \/\/    unsigned long s7;\n+    \/\/    unsigned long s8;\n+    \/\/    unsigned long s9;\n+    \/\/    unsigned long s10;\n+    \/\/    unsigned long s11;\n+    \/\/    unsigned long t3;\n+    \/\/    unsigned long t4;\n+    \/\/    unsigned long t5;\n+    \/\/    unsigned long t6;\n+    \/\/ };\n+\n+    \/\/ NOTE: the indices for the various registers must be maintained as\n+    \/\/ listed across various operating systems. However, only a small\n+    \/\/ subset of the registers' values are guaranteed to be present (and\n+    \/\/ must be present for the SA's stack walking to work)\n+\n+    \/\/ One instance of the Native annotation is enough to trigger header generation\n+    \/\/ for this file.\n+    @Native\n+    public static final int R0 = 0;\n+    public static final int R1 = 1;\n+    public static final int R2 = 2;\n+    public static final int R3 = 3;\n+    public static final int R4 = 4;\n+    public static final int R5 = 5;\n+    public static final int R6 = 6;\n+    public static final int R7 = 7;\n+    public static final int R8 = 8;\n+    public static final int R9 = 9;\n+    public static final int R10 = 10;\n+    public static final int R11 = 11;\n+    public static final int R12 = 12;\n+    public static final int R13 = 13;\n+    public static final int R14 = 14;\n+    public static final int R15 = 15;\n+    public static final int R16 = 16;\n+    public static final int R17 = 17;\n+    public static final int R18 = 18;\n+    public static final int R19 = 19;\n+    public static final int R20 = 20;\n+    public static final int R21 = 21;\n+    public static final int R22 = 22;\n+    public static final int R23 = 23;\n+    public static final int R24 = 24;\n+    public static final int R25 = 25;\n+    public static final int R26 = 26;\n+    public static final int R27 = 27;\n+    public static final int R28 = 28;\n+    public static final int R29 = 29;\n+    public static final int R30 = 30;\n+    public static final int R31 = 31;\n+\n+    public static final int NPRGREG = 32;\n+\n+    public static final int PC = R0;\n+    public static final int LR = R1;\n+    public static final int SP = R2;\n+    public static final int FP = R8;\n+\n+    private long[] data;\n+\n+    public RISCV64ThreadContext() {\n+        data = new long[NPRGREG];\n+    }\n+\n+    public int getNumRegisters() {\n+        return NPRGREG;\n+    }\n+\n+    public String getRegisterName(int index) {\n+        switch (index) {\n+        case LR: return \"lr\";\n+        case SP: return \"sp\";\n+        case PC: return \"pc\";\n+        default:\n+            return \"r\" + index;\n+        }\n+    }\n+\n+    public void setRegister(int index, long value) {\n+        data[index] = value;\n+    }\n+\n+    public long getRegister(int index) {\n+        return data[index];\n+    }\n+\n+    public CFrame getTopFrame(Debugger dbg) {\n+        return null;\n+    }\n+\n+    \/** This can't be implemented in this class since we would have to\n+     * tie the implementation to, for example, the debugging system *\/\n+    public abstract void setRegisterAsAddress(int index, Address value);\n+\n+    \/** This can't be implemented in this class since we would have to\n+     * tie the implementation to, for example, the debugging system *\/\n+    public abstract Address getRegisterAsAddress(int index);\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/risv64\/RISCV64ThreadContext.java","additions":172,"deletions":0,"binary":false,"changes":172,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,1 @@\n+import sun.jvm.hotspot.runtime.linux_riscv64.LinuxRISCV64JavaThreadPDAccess;\n@@ -102,0 +103,2 @@\n+            } else if (cpu.equals(\"riscv64\")) {\n+                access = new LinuxRISCV64JavaThreadPDAccess();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,132 @@\n+\/*\n+ * Copyright (c) 2003, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime.linux_riscv64;\n+\n+import java.io.*;\n+import java.util.*;\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.runtime.*;\n+import sun.jvm.hotspot.runtime.riscv64.*;\n+import sun.jvm.hotspot.types.*;\n+import sun.jvm.hotspot.utilities.*;\n+\n+public class LinuxRISCV64JavaThreadPDAccess implements JavaThreadPDAccess {\n+  private static AddressField  lastJavaFPField;\n+  private static AddressField  osThreadField;\n+\n+  \/\/ Field from OSThread\n+  private static CIntegerField osThreadThreadIDField;\n+\n+  \/\/ This is currently unneeded but is being kept in case we change\n+  \/\/ the currentFrameGuess algorithm\n+  private static final long GUESS_SCAN_RANGE = 128 * 1024;\n+\n+  static {\n+    VM.registerVMInitializedObserver(new Observer() {\n+        public void update(Observable o, Object data) {\n+          initialize(VM.getVM().getTypeDataBase());\n+        }\n+      });\n+  }\n+\n+  private static synchronized void initialize(TypeDataBase db) {\n+    Type type = db.lookupType(\"JavaThread\");\n+    osThreadField           = type.getAddressField(\"_osthread\");\n+\n+    Type anchorType = db.lookupType(\"JavaFrameAnchor\");\n+    lastJavaFPField         = anchorType.getAddressField(\"_last_Java_fp\");\n+\n+    Type osThreadType = db.lookupType(\"OSThread\");\n+    osThreadThreadIDField   = osThreadType.getCIntegerField(\"_thread_id\");\n+  }\n+\n+  public Address getLastJavaFP(Address addr) {\n+    return lastJavaFPField.getValue(addr.addOffsetTo(sun.jvm.hotspot.runtime.JavaThread.getAnchorField().getOffset()));\n+  }\n+\n+  public Address getLastJavaPC(Address addr) {\n+    return null;\n+  }\n+\n+  public Address getBaseOfStackPointer(Address addr) {\n+    return null;\n+  }\n+\n+  public Frame getLastFramePD(JavaThread thread, Address addr) {\n+    Address fp = thread.getLastJavaFP();\n+    if (fp == null) {\n+      return null; \/\/ no information\n+    }\n+    return new RISCV64Frame(thread.getLastJavaSP(), fp);\n+  }\n+\n+  public RegisterMap newRegisterMap(JavaThread thread, boolean updateMap) {\n+    return new RISCV64RegisterMap(thread, updateMap);\n+  }\n+\n+  public Frame getCurrentFrameGuess(JavaThread thread, Address addr) {\n+    ThreadProxy t = getThreadProxy(addr);\n+    RISCV64ThreadContext context = (RISCV64ThreadContext) t.getContext();\n+    RISCV64CurrentFrameGuess guesser = new RISCV64CurrentFrameGuess(context, thread);\n+    if (!guesser.run(GUESS_SCAN_RANGE)) {\n+      return null;\n+    }\n+    if (guesser.getPC() == null) {\n+      return new RISCV64Frame(guesser.getSP(), guesser.getFP());\n+    } else {\n+      return new RISCV64Frame(guesser.getSP(), guesser.getFP(), guesser.getPC());\n+    }\n+  }\n+\n+  public void printThreadIDOn(Address addr, PrintStream tty) {\n+    tty.print(getThreadProxy(addr));\n+  }\n+\n+  public void printInfoOn(Address threadAddr, PrintStream tty) {\n+    tty.print(\"Thread id: \");\n+    printThreadIDOn(threadAddr, tty);\n+  }\n+\n+  public Address getLastSP(Address addr) {\n+    ThreadProxy t = getThreadProxy(addr);\n+    RISCV64ThreadContext context = (RISCV64ThreadContext) t.getContext();\n+    return context.getRegisterAsAddress(RISCV64ThreadContext.SP);\n+  }\n+\n+  public ThreadProxy getThreadProxy(Address addr) {\n+    \/\/ Addr is the address of the JavaThread.\n+    \/\/ Fetch the OSThread (for now and for simplicity, not making a\n+    \/\/ separate \"OSThread\" class in this package)\n+    Address osThreadAddr = osThreadField.getValue(addr);\n+    \/\/ Get the address of the _thread_id from the OSThread\n+    Address threadIdAddr = osThreadAddr.addOffsetTo(osThreadThreadIDField.getOffset());\n+\n+    JVMDebugger debugger = VM.getVM().getDebugger();\n+    return debugger.getThreadForIdentifierAddress(threadIdAddr);\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/linux_riscv64\/LinuxRISCV64JavaThreadPDAccess.java","additions":132,"deletions":0,"binary":false,"changes":132,"status":"added"},{"patch":"@@ -0,0 +1,223 @@\n+\/*\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2019, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.debugger.riscv64.*;\n+import sun.jvm.hotspot.code.*;\n+import sun.jvm.hotspot.interpreter.*;\n+import sun.jvm.hotspot.runtime.*;\n+import sun.jvm.hotspot.runtime.riscv64.*;\n+\n+\/** <P> Should be able to be used on all riscv64 platforms we support\n+    (Linux\/riscv64) to implement JavaThread's \"currentFrameGuess()\"\n+    functionality. Input is an RISCV64ThreadContext; output is SP, FP,\n+    and PC for an RISCV64Frame. Instantiation of the RISCV64Frame is\n+    left to the caller, since we may need to subclass RISCV64Frame to\n+    support signal handler frames on Unix platforms. <\/P>\n+\n+    <P> Algorithm is to walk up the stack within a given range (say,\n+    512K at most) looking for a plausible PC and SP for a Java frame,\n+    also considering those coming in from the context. If we find a PC\n+    that belongs to the VM (i.e., in generated code like the\n+    interpreter or CodeCache) then we try to find an associated FP.\n+    We repeat this until we either find a complete frame or run out of\n+    stack to look at. <\/P> *\/\n+\n+public class RISCV64CurrentFrameGuess {\n+  private RISCV64ThreadContext context;\n+  private JavaThread       thread;\n+  private Address          spFound;\n+  private Address          fpFound;\n+  private Address          pcFound;\n+\n+  private static final boolean DEBUG = System.getProperty(\"sun.jvm.hotspot.runtime.riscv64.RISCV64Frame.DEBUG\")\n+                                       != null;\n+\n+  public RISCV64CurrentFrameGuess(RISCV64ThreadContext context,\n+                              JavaThread thread) {\n+    this.context = context;\n+    this.thread  = thread;\n+  }\n+\n+  \/** Returns false if not able to find a frame within a reasonable range. *\/\n+  public boolean run(long regionInBytesToSearch) {\n+    Address sp  = context.getRegisterAsAddress(RISCV64ThreadContext.SP);\n+    Address pc  = context.getRegisterAsAddress(RISCV64ThreadContext.PC);\n+    Address fp  = context.getRegisterAsAddress(RISCV64ThreadContext.FP);\n+    if (sp == null) {\n+      \/\/ Bail out if no last java frame either\n+      if (thread.getLastJavaSP() != null) {\n+        setValues(thread.getLastJavaSP(), thread.getLastJavaFP(), null);\n+        return true;\n+      }\n+      return false;\n+    }\n+    Address end = sp.addOffsetTo(regionInBytesToSearch);\n+    VM vm       = VM.getVM();\n+\n+    setValues(null, null, null); \/\/ Assume we're not going to find anything\n+\n+    if (vm.isJavaPCDbg(pc)) {\n+      if (vm.isClientCompiler()) {\n+        \/\/ If the topmost frame is a Java frame, we are (pretty much)\n+        \/\/ guaranteed to have a viable FP. We should be more robust\n+        \/\/ than this (we have the potential for losing entire threads'\n+        \/\/ stack traces) but need to see how much work we really have\n+        \/\/ to do here. Searching the stack for an (SP, FP) pair is\n+        \/\/ hard since it's easy to misinterpret inter-frame stack\n+        \/\/ pointers as base-of-frame pointers; we also don't know the\n+        \/\/ sizes of C1 frames (not registered in the nmethod) so can't\n+        \/\/ derive them from SP.\n+\n+        setValues(sp, fp, pc);\n+        return true;\n+      } else {\n+        if (vm.getInterpreter().contains(pc)) {\n+          if (DEBUG) {\n+            System.out.println(\"CurrentFrameGuess: choosing interpreter frame: sp = \" +\n+                               sp + \", fp = \" + fp + \", pc = \" + pc);\n+          }\n+          setValues(sp, fp, pc);\n+          return true;\n+        }\n+\n+        \/\/ For the server compiler, FP is not guaranteed to be valid\n+        \/\/ for compiled code. In addition, an earlier attempt at a\n+        \/\/ non-searching algorithm (see below) failed because the\n+        \/\/ stack pointer from the thread context was pointing\n+        \/\/ (considerably) beyond the ostensible end of the stack, into\n+        \/\/ garbage; walking from the topmost frame back caused a crash.\n+        \/\/\n+        \/\/ This algorithm takes the current PC as a given and tries to\n+        \/\/ find the correct corresponding SP by walking up the stack\n+        \/\/ and repeatedly performing stackwalks (very inefficient).\n+        \/\/\n+        \/\/ FIXME: there is something wrong with stackwalking across\n+        \/\/ adapter frames...this is likely to be the root cause of the\n+        \/\/ failure with the simpler algorithm below.\n+\n+        for (long offset = 0;\n+             offset < regionInBytesToSearch;\n+             offset += vm.getAddressSize()) {\n+          try {\n+            Address curSP = sp.addOffsetTo(offset);\n+            Frame frame = new RISCV64Frame(curSP, null, pc);\n+            RegisterMap map = thread.newRegisterMap(false);\n+            while (frame != null) {\n+              if (frame.isEntryFrame() && frame.entryFrameIsFirst()) {\n+                \/\/ We were able to traverse all the way to the\n+                \/\/ bottommost Java frame.\n+                \/\/ This sp looks good. Keep it.\n+                if (DEBUG) {\n+                  System.out.println(\"CurrentFrameGuess: Choosing sp = \" + curSP + \", pc = \" + pc);\n+                }\n+                setValues(curSP, null, pc);\n+                return true;\n+              }\n+              frame = frame.sender(map);\n+            }\n+          } catch (Exception e) {\n+            if (DEBUG) {\n+              System.out.println(\"CurrentFrameGuess: Exception \" + e + \" at offset \" + offset);\n+            }\n+            \/\/ Bad SP. Try another.\n+          }\n+        }\n+\n+        \/\/ Were not able to find a plausible SP to go with this PC.\n+        \/\/ Bail out.\n+        return false;\n+      }\n+    } else {\n+      \/\/ If the current program counter was not known to us as a Java\n+      \/\/ PC, we currently assume that we are in the run-time system\n+      \/\/ and attempt to look to thread-local storage for saved SP and\n+      \/\/ FP. Note that if these are null (because we were, in fact,\n+      \/\/ in Java code, i.e., vtable stubs or similar, and the SA\n+      \/\/ didn't have enough insight into the target VM to understand\n+      \/\/ that) then we are going to lose the entire stack trace for\n+      \/\/ the thread, which is sub-optimal. FIXME.\n+\n+      if (DEBUG) {\n+        System.out.println(\"CurrentFrameGuess: choosing last Java frame: sp = \" +\n+                           thread.getLastJavaSP() + \", fp = \" + thread.getLastJavaFP());\n+      }\n+      if (thread.getLastJavaSP() == null) {\n+        return false; \/\/ No known Java frames on stack\n+      }\n+\n+      \/\/ The runtime has a nasty habit of not saving fp in the frame\n+      \/\/ anchor, leaving us to grovel about in the stack to find a\n+      \/\/ plausible address.  Fortunately, this only happens in\n+      \/\/ compiled code; there we always have a valid PC, and we always\n+      \/\/ push LR and FP onto the stack as a pair, with FP at the lower\n+      \/\/ address.\n+      pc = thread.getLastJavaPC();\n+      fp = thread.getLastJavaFP();\n+      sp = thread.getLastJavaSP();\n+\n+      if (fp == null) {\n+        CodeCache cc = vm.getCodeCache();\n+        if (cc.contains(pc)) {\n+          CodeBlob cb = cc.findBlob(pc);\n+          if (DEBUG) {\n+            System.out.println(\"FP is null.  Found blob frame size \" + cb.getFrameSize());\n+          }\n+          \/\/ See if we can derive a frame pointer from SP and PC\n+          long link_offset = cb.getFrameSize() - 2 * VM.getVM().getAddressSize();\n+          if (link_offset >= 0) {\n+            fp = sp.addOffsetTo(link_offset);\n+          }\n+        }\n+      }\n+\n+      \/\/ We found a PC in the frame anchor. Check that it's plausible, and\n+      \/\/ if it is, use it.\n+      if (vm.isJavaPCDbg(pc)) {\n+        setValues(sp, fp, pc);\n+      } else {\n+        setValues(sp, fp, null);\n+      }\n+\n+      return true;\n+    }\n+  }\n+\n+  public Address getSP() { return spFound; }\n+  public Address getFP() { return fpFound; }\n+  \/** May be null if getting values from thread-local storage; take\n+      care to call the correct RISCV64Frame constructor to recover this if\n+      necessary *\/\n+  public Address getPC() { return pcFound; }\n+\n+  private void setValues(Address sp, Address fp, Address pc) {\n+    spFound = sp;\n+    fpFound = fp;\n+    pcFound = pc;\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/riscv64\/RISCV64CurrentFrameGuess.java","additions":223,"deletions":0,"binary":false,"changes":223,"status":"added"},{"patch":"@@ -0,0 +1,554 @@\n+\/*\n+ * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2019, Red Hat Inc.\n+ * Copyright (c) 2021, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime.riscv64;\n+\n+import java.util.*;\n+import sun.jvm.hotspot.code.*;\n+import sun.jvm.hotspot.compiler.*;\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.oops.*;\n+import sun.jvm.hotspot.runtime.*;\n+import sun.jvm.hotspot.types.*;\n+import sun.jvm.hotspot.utilities.*;\n+\n+\/** Specialization of and implementation of abstract methods of the\n+    Frame class for the riscv64 family of CPUs. *\/\n+\n+public class RISCV64Frame extends Frame {\n+  private static final boolean DEBUG;\n+  static {\n+    DEBUG = System.getProperty(\"sun.jvm.hotspot.runtime.RISCV64.RISCV64Frame.DEBUG\") != null;\n+  }\n+\n+  \/\/ Java frames\n+  private static final int LINK_OFFSET                =  -2;\n+  private static final int RETURN_ADDR_OFFSET         =  -1;\n+  private static final int SENDER_SP_OFFSET           =   0;\n+\n+  \/\/ Interpreter frames\n+  private static final int INTERPRETER_FRAME_SENDER_SP_OFFSET = -3;\n+  private static final int INTERPRETER_FRAME_LAST_SP_OFFSET   = INTERPRETER_FRAME_SENDER_SP_OFFSET - 1;\n+  private static final int INTERPRETER_FRAME_METHOD_OFFSET    = INTERPRETER_FRAME_LAST_SP_OFFSET - 1;\n+  private static       int INTERPRETER_FRAME_MDX_OFFSET;         \/\/ Non-core builds only\n+  private static       int INTERPRETER_FRAME_PADDING_OFFSET;\n+  private static       int INTERPRETER_FRAME_MIRROR_OFFSET;\n+  private static       int INTERPRETER_FRAME_CACHE_OFFSET;\n+  private static       int INTERPRETER_FRAME_LOCALS_OFFSET;\n+  private static       int INTERPRETER_FRAME_BCX_OFFSET;\n+  private static       int INTERPRETER_FRAME_INITIAL_SP_OFFSET;\n+  private static       int INTERPRETER_FRAME_MONITOR_BLOCK_TOP_OFFSET;\n+  private static       int INTERPRETER_FRAME_MONITOR_BLOCK_BOTTOM_OFFSET;\n+\n+  \/\/ Entry frames\n+  private static       int ENTRY_FRAME_CALL_WRAPPER_OFFSET = -10;\n+\n+  \/\/ Native frames\n+  private static final int NATIVE_FRAME_INITIAL_PARAM_OFFSET =  2;\n+\n+  private static VMReg fp = new VMReg(8);\n+\n+  static {\n+    VM.registerVMInitializedObserver(new Observer() {\n+        public void update(Observable o, Object data) {\n+          initialize(VM.getVM().getTypeDataBase());\n+        }\n+      });\n+  }\n+\n+  private static synchronized void initialize(TypeDataBase db) {\n+    INTERPRETER_FRAME_MDX_OFFSET                  = INTERPRETER_FRAME_METHOD_OFFSET - 1;\n+    INTERPRETER_FRAME_PADDING_OFFSET              = INTERPRETER_FRAME_MDX_OFFSET - 1;\n+    INTERPRETER_FRAME_MIRROR_OFFSET               = INTERPRETER_FRAME_PADDING_OFFSET - 1;\n+    INTERPRETER_FRAME_CACHE_OFFSET                = INTERPRETER_FRAME_MIRROR_OFFSET - 1;\n+    INTERPRETER_FRAME_LOCALS_OFFSET               = INTERPRETER_FRAME_CACHE_OFFSET - 1;\n+    INTERPRETER_FRAME_BCX_OFFSET                  = INTERPRETER_FRAME_LOCALS_OFFSET - 1;\n+    INTERPRETER_FRAME_INITIAL_SP_OFFSET           = INTERPRETER_FRAME_BCX_OFFSET - 1;\n+    INTERPRETER_FRAME_MONITOR_BLOCK_TOP_OFFSET    = INTERPRETER_FRAME_INITIAL_SP_OFFSET;\n+    INTERPRETER_FRAME_MONITOR_BLOCK_BOTTOM_OFFSET = INTERPRETER_FRAME_INITIAL_SP_OFFSET;\n+  }\n+\n+\n+  \/\/ an additional field beyond sp and pc:\n+  Address raw_fp; \/\/ frame pointer\n+  private Address raw_unextendedSP;\n+\n+  private RISCV64Frame() {\n+  }\n+\n+  private void adjustForDeopt() {\n+    if ( pc != null) {\n+      \/\/ Look for a deopt pc and if it is deopted convert to original pc\n+      CodeBlob cb = VM.getVM().getCodeCache().findBlob(pc);\n+      if (cb != null && cb.isJavaMethod()) {\n+        NMethod nm = (NMethod) cb;\n+        if (pc.equals(nm.deoptHandlerBegin())) {\n+          if (Assert.ASSERTS_ENABLED) {\n+            Assert.that(this.getUnextendedSP() != null, \"null SP in Java frame\");\n+          }\n+          \/\/ adjust pc if frame is deoptimized.\n+          pc = this.getUnextendedSP().getAddressAt(nm.origPCOffset());\n+          deoptimized = true;\n+        }\n+      }\n+    }\n+  }\n+\n+  public RISCV64Frame(Address raw_sp, Address raw_fp, Address pc) {\n+    this.raw_sp = raw_sp;\n+    this.raw_unextendedSP = raw_sp;\n+    this.raw_fp = raw_fp;\n+    this.pc = pc;\n+    adjustUnextendedSP();\n+\n+    \/\/ Frame must be fully constructed before this call\n+    adjustForDeopt();\n+\n+    if (DEBUG) {\n+      System.out.println(\"RISCV64Frame(sp, fp, pc): \" + this);\n+      dumpStack();\n+    }\n+  }\n+\n+  public RISCV64Frame(Address raw_sp, Address raw_fp) {\n+    this.raw_sp = raw_sp;\n+    this.raw_unextendedSP = raw_sp;\n+    this.raw_fp = raw_fp;\n+\n+    \/\/ We cannot assume SP[-1] always contains a valid return PC (e.g. if\n+    \/\/ the callee is a C\/C++ compiled frame). If the PC is not known to\n+    \/\/ Java then this.pc is null.\n+    Address savedPC = raw_sp.getAddressAt(-1 * VM.getVM().getAddressSize());\n+    if (VM.getVM().isJavaPCDbg(savedPC)) {\n+      this.pc = savedPC;\n+    }\n+\n+    adjustUnextendedSP();\n+\n+    \/\/ Frame must be fully constructed before this call\n+    adjustForDeopt();\n+\n+    if (DEBUG) {\n+      System.out.println(\"RISCV64Frame(sp, fp): \" + this);\n+      dumpStack();\n+    }\n+  }\n+\n+  public RISCV64Frame(Address raw_sp, Address raw_unextendedSp, Address raw_fp, Address pc) {\n+    this.raw_sp = raw_sp;\n+    this.raw_unextendedSP = raw_unextendedSp;\n+    this.raw_fp = raw_fp;\n+    this.pc = pc;\n+    adjustUnextendedSP();\n+\n+    \/\/ Frame must be fully constructed before this call\n+    adjustForDeopt();\n+\n+    if (DEBUG) {\n+      System.out.println(\"RISCV64Frame(sp, unextendedSP, fp, pc): \" + this);\n+      dumpStack();\n+    }\n+\n+  }\n+\n+  public Object clone() {\n+    RISCV64Frame frame = new RISCV64Frame();\n+    frame.raw_sp = raw_sp;\n+    frame.raw_unextendedSP = raw_unextendedSP;\n+    frame.raw_fp = raw_fp;\n+    frame.pc = pc;\n+    frame.deoptimized = deoptimized;\n+    return frame;\n+  }\n+\n+  public boolean equals(Object arg) {\n+    if (arg == null) {\n+      return false;\n+    }\n+\n+    if (!(arg instanceof RISCV64Frame)) {\n+      return false;\n+    }\n+\n+    RISCV64Frame other = (RISCV64Frame) arg;\n+\n+    return (AddressOps.equal(getSP(), other.getSP()) &&\n+            AddressOps.equal(getUnextendedSP(), other.getUnextendedSP()) &&\n+            AddressOps.equal(getFP(), other.getFP()) &&\n+            AddressOps.equal(getPC(), other.getPC()));\n+  }\n+\n+  public int hashCode() {\n+    if (raw_sp == null) {\n+      return 0;\n+    }\n+\n+    return raw_sp.hashCode();\n+  }\n+\n+  public String toString() {\n+    return \"sp: \" + (getSP() == null? \"null\" : getSP().toString()) +\n+         \", unextendedSP: \" + (getUnextendedSP() == null? \"null\" : getUnextendedSP().toString()) +\n+         \", fp: \" + (getFP() == null? \"null\" : getFP().toString()) +\n+         \", pc: \" + (pc == null? \"null\" : pc.toString());\n+  }\n+\n+  \/\/ accessors for the instance variables\n+  public Address getFP() { return raw_fp; }\n+  public Address getSP() { return raw_sp; }\n+  public Address getID() { return raw_sp; }\n+\n+  \/\/ FIXME: not implemented yet\n+  public boolean isSignalHandlerFrameDbg() { return false; }\n+  public int     getSignalNumberDbg()      { return 0;     }\n+  public String  getSignalNameDbg()        { return null;  }\n+\n+  public boolean isInterpretedFrameValid() {\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(isInterpretedFrame(), \"Not an interpreted frame\");\n+    }\n+\n+    \/\/ These are reasonable sanity checks\n+    if (getFP() == null || getFP().andWithMask(0x3) != null) {\n+      return false;\n+    }\n+\n+    if (getSP() == null || getSP().andWithMask(0x3) != null) {\n+      return false;\n+    }\n+\n+    if (getFP().addOffsetTo(INTERPRETER_FRAME_INITIAL_SP_OFFSET * VM.getVM().getAddressSize()).lessThan(getSP())) {\n+      return false;\n+    }\n+\n+    \/\/ These are hacks to keep us out of trouble.\n+    \/\/ The problem with these is that they mask other problems\n+    if (getFP().lessThanOrEqual(getSP())) {\n+      \/\/ this attempts to deal with unsigned comparison above\n+      return false;\n+    }\n+\n+    if (getFP().minus(getSP()) > 4096 * VM.getVM().getAddressSize()) {\n+      \/\/ stack frames shouldn't be large.\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  public Frame sender(RegisterMap regMap, CodeBlob cb) {\n+    RISCV64RegisterMap map = (RISCV64RegisterMap) regMap;\n+\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(map != null, \"map must be set\");\n+    }\n+\n+    \/\/ Default is we done have to follow them. The sender_for_xxx will\n+    \/\/ update it accordingly\n+    map.setIncludeArgumentOops(false);\n+\n+    if (isEntryFrame())       return senderForEntryFrame(map);\n+    if (isInterpretedFrame()) return senderForInterpreterFrame(map);\n+\n+    if(cb == null) {\n+      cb = VM.getVM().getCodeCache().findBlob(getPC());\n+    } else {\n+      if (Assert.ASSERTS_ENABLED) {\n+        Assert.that(cb.equals(VM.getVM().getCodeCache().findBlob(getPC())), \"Must be the same\");\n+      }\n+    }\n+\n+    if (cb != null) {\n+      return senderForCompiledFrame(map, cb);\n+    }\n+\n+    \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n+    \/\/ methods that exists in the core system.\n+    return new RISCV64Frame(getSenderSP(), getLink(), getSenderPC());\n+  }\n+\n+  private Frame senderForEntryFrame(RISCV64RegisterMap map) {\n+    if (DEBUG) {\n+      System.out.println(\"senderForEntryFrame\");\n+    }\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(map != null, \"map must be set\");\n+    }\n+    \/\/ Java frame called from C; skip all C frames and return top C\n+    \/\/ frame of that chunk as the sender\n+    RISCV64JavaCallWrapper jcw = (RISCV64JavaCallWrapper) getEntryFrameCallWrapper();\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(!entryFrameIsFirst(), \"next Java fp must be non zero\");\n+      Assert.that(jcw.getLastJavaSP().greaterThan(getSP()), \"must be above this frame on stack\");\n+    }\n+    RISCV64Frame fr;\n+    if (jcw.getLastJavaPC() != null) {\n+      fr = new RISCV64Frame(jcw.getLastJavaSP(), jcw.getLastJavaFP(), jcw.getLastJavaPC());\n+    } else {\n+      fr = new RISCV64Frame(jcw.getLastJavaSP(), jcw.getLastJavaFP());\n+    }\n+    map.clear();\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(map.getIncludeArgumentOops(), \"should be set by clear\");\n+    }\n+    return fr;\n+  }\n+\n+  \/\/------------------------------------------------------------------------------\n+  \/\/ frame::adjust_unextended_sp\n+  private void adjustUnextendedSP() {\n+    \/\/ If we are returning to a compiled MethodHandle call site, the\n+    \/\/ saved_fp will in fact be a saved value of the unextended SP.  The\n+    \/\/ simplest way to tell whether we are returning to such a call site\n+    \/\/ is as follows:\n+\n+    CodeBlob cb = cb();\n+    NMethod senderNm = (cb == null) ? null : cb.asNMethodOrNull();\n+    if (senderNm != null) {\n+      \/\/ If the sender PC is a deoptimization point, get the original\n+      \/\/ PC.  For MethodHandle call site the unextended_sp is stored in\n+      \/\/ saved_fp.\n+      if (senderNm.isDeoptMhEntry(getPC())) {\n+        raw_unextendedSP = getFP();\n+      }\n+      else if (senderNm.isDeoptEntry(getPC())) {\n+      }\n+      else if (senderNm.isMethodHandleReturn(getPC())) {\n+        raw_unextendedSP = getFP();\n+      }\n+    }\n+  }\n+\n+  private Frame senderForInterpreterFrame(RISCV64RegisterMap map) {\n+    if (DEBUG) {\n+      System.out.println(\"senderForInterpreterFrame\");\n+    }\n+    Address unextendedSP = addressOfStackSlot(INTERPRETER_FRAME_SENDER_SP_OFFSET).getAddressAt(0);\n+    Address sp = addressOfStackSlot(SENDER_SP_OFFSET);\n+    \/\/ We do not need to update the callee-save register mapping because above\n+    \/\/ us is either another interpreter frame or a converter-frame, but never\n+    \/\/ directly a compiled frame.\n+    \/\/ 11\/24\/04 SFG. With the removal of adapter frames this is no longer true.\n+    \/\/ However c2 no longer uses callee save register for java calls so there\n+    \/\/ are no callee register to find.\n+\n+    if (map.getUpdateMap())\n+      updateMapWithSavedLink(map, addressOfStackSlot(LINK_OFFSET));\n+\n+    return new RISCV64Frame(sp, unextendedSP, getLink(), getSenderPC());\n+  }\n+\n+  private void updateMapWithSavedLink(RegisterMap map, Address savedFPAddr) {\n+    map.setLocation(fp, savedFPAddr);\n+  }\n+\n+  private Frame senderForCompiledFrame(RISCV64RegisterMap map, CodeBlob cb) {\n+    if (DEBUG) {\n+      System.out.println(\"senderForCompiledFrame\");\n+    }\n+\n+    \/\/\n+    \/\/ NOTE: some of this code is (unfortunately) duplicated  RISCV64CurrentFrameGuess\n+    \/\/\n+\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(map != null, \"map must be set\");\n+    }\n+\n+    \/\/ frame owned by optimizing compiler\n+    if (Assert.ASSERTS_ENABLED) {\n+        Assert.that(cb.getFrameSize() >= 0, \"must have non-zero frame size\");\n+    }\n+    Address senderSP = getUnextendedSP().addOffsetTo(cb.getFrameSize());\n+\n+    \/\/ The return_address is always the word on the stack\n+    Address senderPC = senderSP.getAddressAt(-1 * VM.getVM().getAddressSize());\n+\n+    \/\/ This is the saved value of FP which may or may not really be an FP.\n+    \/\/ It is only an FP if the sender is an interpreter frame.\n+    Address savedFPAddr = senderSP.addOffsetTo(-2 * VM.getVM().getAddressSize());\n+\n+    if (map.getUpdateMap()) {\n+      \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n+      \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n+      \/\/ outside of update_register_map.\n+      map.setIncludeArgumentOops(cb.callerMustGCArguments());\n+\n+      if (cb.getOopMaps() != null) {\n+        ImmutableOopMapSet.updateRegisterMap(this, cb, map, true);\n+      }\n+\n+      \/\/ Since the prolog does the save and restore of FP there is no oopmap\n+      \/\/ for it so we must fill in its location as if there was an oopmap entry\n+      \/\/ since if our caller was compiled code there could be live jvm state in it.\n+      updateMapWithSavedLink(map, savedFPAddr);\n+    }\n+\n+    return new RISCV64Frame(senderSP, savedFPAddr.getAddressAt(0), senderPC);\n+  }\n+\n+  protected boolean hasSenderPD() {\n+    return true;\n+  }\n+\n+  public long frameSize() {\n+    return (getSenderSP().minus(getSP()) \/ VM.getVM().getAddressSize());\n+  }\n+\n+    public Address getLink() {\n+        try {\n+            if (DEBUG) {\n+                System.out.println(\"Reading link at \" + addressOfStackSlot(LINK_OFFSET)\n+                        + \" = \" + addressOfStackSlot(LINK_OFFSET).getAddressAt(0));\n+            }\n+            return addressOfStackSlot(LINK_OFFSET).getAddressAt(0);\n+        } catch (Exception e) {\n+            if (DEBUG)\n+                System.out.println(\"Returning null\");\n+            return null;\n+        }\n+    }\n+\n+  public Address getUnextendedSP() { return raw_unextendedSP; }\n+\n+  \/\/ Return address:\n+  public Address getSenderPCAddr() { return addressOfStackSlot(RETURN_ADDR_OFFSET); }\n+  public Address getSenderPC()     { return getSenderPCAddr().getAddressAt(0);      }\n+\n+  \/\/ return address of param, zero origin index.\n+  public Address getNativeParamAddr(int idx) {\n+    return addressOfStackSlot(NATIVE_FRAME_INITIAL_PARAM_OFFSET + idx);\n+  }\n+\n+  public Address getSenderSP()     { return addressOfStackSlot(SENDER_SP_OFFSET); }\n+\n+  public Address addressOfInterpreterFrameLocals() {\n+    return addressOfStackSlot(INTERPRETER_FRAME_LOCALS_OFFSET);\n+  }\n+\n+  private Address addressOfInterpreterFrameBCX() {\n+    return addressOfStackSlot(INTERPRETER_FRAME_BCX_OFFSET);\n+  }\n+\n+  public int getInterpreterFrameBCI() {\n+    \/\/ FIXME: this is not atomic with respect to GC and is unsuitable\n+    \/\/ for use in a non-debugging, or reflective, system. Need to\n+    \/\/ figure out how to express this.\n+    Address bcp = addressOfInterpreterFrameBCX().getAddressAt(0);\n+    Address methodHandle = addressOfInterpreterFrameMethod().getAddressAt(0);\n+    Method method = (Method)Metadata.instantiateWrapperFor(methodHandle);\n+    return bcpToBci(bcp, method);\n+  }\n+\n+  public Address addressOfInterpreterFrameMDX() {\n+    return addressOfStackSlot(INTERPRETER_FRAME_MDX_OFFSET);\n+  }\n+\n+  \/\/ expression stack\n+  \/\/ (the max_stack arguments are used by the GC; see class FrameClosure)\n+\n+  public Address addressOfInterpreterFrameExpressionStack() {\n+    Address monitorEnd = interpreterFrameMonitorEnd().address();\n+    return monitorEnd.addOffsetTo(-1 * VM.getVM().getAddressSize());\n+  }\n+\n+  public int getInterpreterFrameExpressionStackDirection() { return -1; }\n+\n+  \/\/ top of expression stack\n+  public Address addressOfInterpreterFrameTOS() {\n+    return getSP();\n+  }\n+\n+  \/** Expression stack from top down *\/\n+  public Address addressOfInterpreterFrameTOSAt(int slot) {\n+    return addressOfInterpreterFrameTOS().addOffsetTo(slot * VM.getVM().getAddressSize());\n+  }\n+\n+  public Address getInterpreterFrameSenderSP() {\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(isInterpretedFrame(), \"interpreted frame expected\");\n+    }\n+    return addressOfStackSlot(INTERPRETER_FRAME_SENDER_SP_OFFSET).getAddressAt(0);\n+  }\n+\n+  \/\/ Monitors\n+  public BasicObjectLock interpreterFrameMonitorBegin() {\n+    return new BasicObjectLock(addressOfStackSlot(INTERPRETER_FRAME_MONITOR_BLOCK_BOTTOM_OFFSET));\n+  }\n+\n+  public BasicObjectLock interpreterFrameMonitorEnd() {\n+    Address result = addressOfStackSlot(INTERPRETER_FRAME_MONITOR_BLOCK_TOP_OFFSET).getAddressAt(0);\n+    if (Assert.ASSERTS_ENABLED) {\n+      \/\/ make sure the pointer points inside the frame\n+      Assert.that(AddressOps.gt(getFP(), result), \"result must <  than frame pointer\");\n+      Assert.that(AddressOps.lte(getSP(), result), \"result must >= than stack pointer\");\n+    }\n+    return new BasicObjectLock(result);\n+  }\n+\n+  public int interpreterFrameMonitorSize() {\n+    return BasicObjectLock.size();\n+  }\n+\n+  \/\/ Method\n+  public Address addressOfInterpreterFrameMethod() {\n+    return addressOfStackSlot(INTERPRETER_FRAME_METHOD_OFFSET);\n+  }\n+\n+  \/\/ Constant pool cache\n+  public Address addressOfInterpreterFrameCPCache() {\n+    return addressOfStackSlot(INTERPRETER_FRAME_CACHE_OFFSET);\n+  }\n+\n+  \/\/ Entry frames\n+  public JavaCallWrapper getEntryFrameCallWrapper() {\n+    return new RISCV64JavaCallWrapper(addressOfStackSlot(ENTRY_FRAME_CALL_WRAPPER_OFFSET).getAddressAt(0));\n+  }\n+\n+  protected Address addressOfSavedOopResult() {\n+    \/\/ offset is 2 for compiler2 and 3 for compiler1\n+    return getSP().addOffsetTo((VM.getVM().isClientCompiler() ? 2 : 3) *\n+                               VM.getVM().getAddressSize());\n+  }\n+\n+  protected Address addressOfSavedReceiver() {\n+    return getSP().addOffsetTo(-4 * VM.getVM().getAddressSize());\n+  }\n+\n+  private void dumpStack() {\n+    for (Address addr = getSP().addOffsetTo(-4 * VM.getVM().getAddressSize());\n+         AddressOps.lt(addr, getSP());\n+         addr = addr.addOffsetTo(VM.getVM().getAddressSize())) {\n+      System.out.println(addr + \": \" + addr.getAddressAt(0));\n+    }\n+    System.out.println(\"-----------------------\");\n+    for (Address addr = getSP();\n+         AddressOps.lte(addr, getSP().addOffsetTo(20 * VM.getVM().getAddressSize()));\n+         addr = addr.addOffsetTo(VM.getVM().getAddressSize())) {\n+      System.out.println(addr + \": \" + addr.getAddressAt(0));\n+    }\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/riscv64\/RISCV64Frame.java","additions":554,"deletions":0,"binary":false,"changes":554,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2003, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime.riscv64;\n+\n+import java.util.*;\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.types.*;\n+import sun.jvm.hotspot.runtime.*;\n+import sun.jvm.hotspot.utilities.*;\n+\n+public class RISCV64JavaCallWrapper extends JavaCallWrapper {\n+  private static AddressField lastJavaFPField;\n+\n+  static {\n+    VM.registerVMInitializedObserver(new Observer() {\n+        public void update(Observable o, Object data) {\n+          initialize(VM.getVM().getTypeDataBase());\n+        }\n+      });\n+  }\n+\n+  private static synchronized void initialize(TypeDataBase db) {\n+    Type type = db.lookupType(\"JavaFrameAnchor\");\n+\n+    lastJavaFPField  = type.getAddressField(\"_last_Java_fp\");\n+  }\n+\n+  public RISCV64JavaCallWrapper(Address addr) {\n+    super(addr);\n+  }\n+\n+  public Address getLastJavaFP() {\n+    return lastJavaFPField.getValue(addr.addOffsetTo(anchorField.getOffset()));\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/riscv64\/RISCV64JavaCallWrapper.java","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2001, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, Red Hat Inc.\n+ * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime.riscv64;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.runtime.*;\n+\n+public class RISCV64RegisterMap extends RegisterMap {\n+\n+  \/** This is the only public constructor *\/\n+  public RISCV64RegisterMap(JavaThread thread, boolean updateMap) {\n+    super(thread, updateMap);\n+  }\n+\n+  protected RISCV64RegisterMap(RegisterMap map) {\n+    super(map);\n+  }\n+\n+  public Object clone() {\n+    RISCV64RegisterMap retval = new RISCV64RegisterMap(this);\n+    return retval;\n+  }\n+\n+  \/\/ no PD state to clear or copy:\n+  protected void clearPD() {}\n+  protected void initializePD() {}\n+  protected void initializeFromPD(RegisterMap map) {}\n+  protected Address getLocationPD(VMReg reg) { return null; }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/riscv64\/RISCV64RegisterMap.java","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,1 +57,1 @@\n-        new String[] {\"i386\", \"x86\", \"x86_64\", \"amd64\", \"sparc\", \"sparcv9\", \"ppc64\", \"ppc64le\", \"aarch64\"};\n+        new String[] {\"i386\", \"x86\", \"x86_64\", \"amd64\", \"sparc\", \"sparcv9\", \"ppc64\", \"ppc64le\", \"aarch64\", \"riscv64\"};\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/PlatformInfo.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n- * @requires os.arch==\"aarch64\" | os.arch==\"amd64\" | os.arch == \"ppc64le\"\n+ * @requires os.arch==\"aarch64\" | os.arch==\"amd64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -57,1 +57,2 @@\n-            \"amd64\".equals(System.getProperty(\"os.arch\"))   ? \"test\" : null;\n+            \"amd64\".equals(System.getProperty(\"os.arch\"))   ? \"test\" :\n+            \"riscv64\".equals(System.getProperty(\"os.arch\")) ? \"andi\" : null;\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestBit.java","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Alibaba Group Holding Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test libm intrinsics\n+ * @library \/test\/lib \/\n+ *\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   -XX:-BackgroundCompilation -XX:-UseOnStackReplacement\n+ *                   compiler.floatingpoint.TestLibmIntrinsics\n+ *\/\n+\n+package compiler.floatingpoint;\n+\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+import sun.hotspot.WhiteBox;\n+\n+import java.lang.reflect.Method;\n+\n+public class TestLibmIntrinsics {\n+\n+    private static final WhiteBox WHITE_BOX = WhiteBox.getWhiteBox();\n+\n+    private static final double pi = 3.1415926;\n+\n+    private static final double expected = 2.5355263553695413;\n+\n+    static double m() {\n+        return Math.pow(pi, Math.sin(Math.cos(Math.tan(Math.log(Math.log10(Math.exp(pi)))))));\n+    }\n+\n+    static public void main(String[] args) throws NoSuchMethodException {\n+        Method test_method = compiler.floatingpoint.TestLibmIntrinsics.class.getDeclaredMethod(\"m\");\n+\n+        double interpreter_result = m();\n+\n+        \/\/ Compile with C1 if possible\n+        WHITE_BOX.enqueueMethodForCompilation(test_method, CompilerWhiteBoxTest.COMP_LEVEL_SIMPLE);\n+\n+        double c1_result = m();\n+\n+        WHITE_BOX.deoptimizeMethod(test_method);\n+\n+        \/\/ Compile it with C2 if possible\n+        WHITE_BOX.enqueueMethodForCompilation(test_method, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+\n+        double c2_result = m();\n+\n+        if (interpreter_result != c1_result ||\n+            interpreter_result != c2_result ||\n+            c1_result != c2_result) {\n+            System.out.println(\"interpreter = \" + interpreter_result + \" c1 = \" + c1_result + \" c2 = \" + c2_result);\n+            throw new RuntimeException(\"Test Failed\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/floatingpoint\/TestLibmIntrinsics.java","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,1 @@\n+import compiler.intrinsics.sha.cli.testcases.GenericTestCaseForUnsupportedRISCV64CPU;\n@@ -57,0 +58,2 @@\n+                new GenericTestCaseForUnsupportedRISCV64CPU(\n+                        SHAOptionsBase.USE_SHA1_INTRINSICS_OPTION),\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/TestUseSHA1IntrinsicsOptionOnUnsupportedCPU.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,1 @@\n+import compiler.intrinsics.sha.cli.testcases.GenericTestCaseForUnsupportedRISCV64CPU;\n@@ -57,0 +58,2 @@\n+                new GenericTestCaseForUnsupportedRISCV64CPU(\n+                        SHAOptionsBase.USE_SHA256_INTRINSICS_OPTION),\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/TestUseSHA256IntrinsicsOptionOnUnsupportedCPU.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,1 @@\n+import compiler.intrinsics.sha.cli.testcases.GenericTestCaseForUnsupportedRISCV64CPU;\n@@ -57,0 +58,2 @@\n+                new GenericTestCaseForUnsupportedRISCV64CPU(\n+                        SHAOptionsBase.USE_SHA512_INTRINSICS_OPTION),\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/TestUseSHA512IntrinsicsOptionOnUnsupportedCPU.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+import compiler.intrinsics.sha.cli.testcases.GenericTestCaseForUnsupportedRISCV64CPU;\n@@ -56,0 +57,2 @@\n+                new GenericTestCaseForUnsupportedRISCV64CPU(\n+                        SHAOptionsBase.USE_SHA_OPTION),\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/TestUseSHAOptionOnUnsupportedCPU.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n- * AArch64, PPC, S390x, SPARC and X86.\n+ * AArch64, RISCV64, PPC, S390x, SPARC and X86.\n@@ -40,1 +40,1 @@\n-        \/\/ Execute the test case on any CPU except AArch64, PPC, S390x, SPARC and X86.\n+        \/\/ Execute the test case on any CPU except AArch64, RISCV64, PPC, S390x, SPARC and X86.\n@@ -43,0 +43,1 @@\n+                              new OrPredicate(Platform::isRISCV64,\n@@ -47,1 +48,1 @@\n-                                              Platform::isX86)))))));\n+                                              Platform::isX86))))))));\n@@ -54,1 +55,1 @@\n-        \/\/ Verify that on non-x86, non-SPARC and non-AArch64 CPU usage of\n+        \/\/ Verify that on non-x86, non-RISCV64, non-SPARC and non-AArch64 CPU usage of\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/testcases\/GenericTestCaseForOtherCPU.java","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2014, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.intrinsics.sha.cli.testcases;\n+\n+import compiler.intrinsics.sha.cli.SHAOptionsBase;\n+import jdk.test.lib.process.ExitCode;\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.cli.CommandLineOptionTest;\n+import jdk.test.lib.cli.predicate.AndPredicate;\n+import jdk.test.lib.cli.predicate.NotPredicate;\n+\n+\/**\n+ * Generic test case for SHA-related options targeted to RISCV64 CPUs\n+ * which don't support instruction required by the tested option.\n+ *\/\n+public class GenericTestCaseForUnsupportedRISCV64CPU extends\n+        SHAOptionsBase.TestCase {\n+\n+    final private boolean checkUseSHA;\n+\n+    public GenericTestCaseForUnsupportedRISCV64CPU(String optionName) {\n+        this(optionName, true);\n+    }\n+\n+    public GenericTestCaseForUnsupportedRISCV64CPU(String optionName, boolean checkUseSHA) {\n+        super(optionName, new AndPredicate(Platform::isRISCV64,\n+                new NotPredicate(SHAOptionsBase.getPredicateForOption(\n+                        optionName))));\n+\n+        this.checkUseSHA = checkUseSHA;\n+    }\n+\n+    @Override\n+    protected void verifyWarnings() throws Throwable {\n+        String shouldPassMessage = String.format(\"JVM startup should pass with\"\n+                + \"option '-XX:-%s' without any warnings\", optionName);\n+        \/\/Verify that option could be disabled without any warnings.\n+        CommandLineOptionTest.verifySameJVMStartup(null, new String[] {\n+                        SHAOptionsBase.getWarningForUnsupportedCPU(optionName)\n+                }, shouldPassMessage, shouldPassMessage, ExitCode.OK,\n+                SHAOptionsBase.UNLOCK_DIAGNOSTIC_VM_OPTIONS,\n+                CommandLineOptionTest.prepareBooleanFlag(optionName, false));\n+\n+        if (checkUseSHA) {\n+            shouldPassMessage = String.format(\"If JVM is started with '-XX:-\"\n+                    + \"%s' '-XX:+%s', output should contain warning.\",\n+                    SHAOptionsBase.USE_SHA_OPTION, optionName);\n+\n+            \/\/ Verify that when the tested option is enabled, then\n+            \/\/ a warning will occur in VM output if UseSHA is disabled.\n+            if (!optionName.equals(SHAOptionsBase.USE_SHA_OPTION)) {\n+                CommandLineOptionTest.verifySameJVMStartup(\n+                        new String[] { SHAOptionsBase.getWarningForUnsupportedCPU(optionName) },\n+                        null,\n+                        shouldPassMessage,\n+                        shouldPassMessage,\n+                        ExitCode.OK,\n+                        SHAOptionsBase.UNLOCK_DIAGNOSTIC_VM_OPTIONS,\n+                        CommandLineOptionTest.prepareBooleanFlag(SHAOptionsBase.USE_SHA_OPTION, false),\n+                        CommandLineOptionTest.prepareBooleanFlag(optionName, true));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void verifyOptionValues() throws Throwable {\n+        \/\/ Verify that option is disabled by default.\n+        CommandLineOptionTest.verifyOptionValueForSameVM(optionName, \"false\",\n+                String.format(\"Option '%s' should be disabled by default\",\n+                        optionName),\n+                SHAOptionsBase.UNLOCK_DIAGNOSTIC_VM_OPTIONS);\n+\n+        if (checkUseSHA) {\n+            \/\/ Verify that option is disabled even if it was explicitly enabled\n+            \/\/ using CLI options.\n+            CommandLineOptionTest.verifyOptionValueForSameVM(optionName, \"false\",\n+                    String.format(\"Option '%s' should be off on unsupported \"\n+                            + \"RISCV64CPU even if set to true directly\", optionName),\n+                    SHAOptionsBase.UNLOCK_DIAGNOSTIC_VM_OPTIONS,\n+                    CommandLineOptionTest.prepareBooleanFlag(optionName, true));\n+\n+            \/\/ Verify that option is disabled when +UseSHA was passed to JVM.\n+            CommandLineOptionTest.verifyOptionValueForSameVM(optionName, \"false\",\n+                    String.format(\"Option '%s' should be off on unsupported \"\n+                            + \"RISCV64CPU even if %s flag set to JVM\",\n+                            optionName, CommandLineOptionTest.prepareBooleanFlag(\n+                                  SHAOptionsBase.USE_SHA_OPTION, true)),\n+                    SHAOptionsBase.UNLOCK_DIAGNOSTIC_VM_OPTIONS,\n+                    CommandLineOptionTest.prepareBooleanFlag(\n+                            SHAOptionsBase.USE_SHA_OPTION, true));\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/sha\/cli\/testcases\/GenericTestCaseForUnsupportedRISCV64CPU.java","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/ProdRed_Double.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/ProdRed_Float.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/ProdRed_Int.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/ReductionPerf.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"aarch64\"\n+ * @requires os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRedAbsNeg_Double.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"aarch64\"\n+ * @requires os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRedAbsNeg_Float.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRedSqrt_Double.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRed_Double.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRed_Float.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/SumRed_Int.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires (os.arch != \"aarch64\") & (os.arch != \"arm\")\n+ * @requires (os.arch != \"aarch64\") & (os.arch != \"riscv64\") & (os.arch != \"arm\")\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/argumentcorruption\/CheckLongArgs.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires (os.arch != \"aarch64\") & (os.arch != \"arm\")\n+ * @requires (os.arch != \"aarch64\") & (os.arch != \"riscv64\") & (os.arch != \"arm\")\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/lookup\/LookUp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,0 +64,1 @@\n+              new OrPredicate(new CPUSpecificPredicate(\"riscv64.*\", new String[] { \"sha1\" }, null),\n@@ -69,1 +70,1 @@\n-                              new CPUSpecificPredicate(\"x86.*\",     new String[] { \"sha\" },  null))))));\n+                              new CPUSpecificPredicate(\"x86.*\",     new String[] { \"sha\" },  null)))))));\n@@ -73,0 +74,1 @@\n+              new OrPredicate(new CPUSpecificPredicate(\"riscv64.*\", new String[] { \"sha256\"       }, null),\n@@ -82,1 +84,1 @@\n-                              new CPUSpecificPredicate(\"x86_64\",    new String[] { \"avx2\", \"bmi2\" }, null))))))))));\n+                              new CPUSpecificPredicate(\"x86_64\",    new String[] { \"avx2\", \"bmi2\" }, null)))))))))));\n@@ -86,0 +88,1 @@\n+              new OrPredicate(new CPUSpecificPredicate(\"riscv64.*\", new String[] { \"sha512\"       }, null),\n@@ -95,1 +98,1 @@\n-                              new CPUSpecificPredicate(\"x86_64\",    new String[] { \"avx2\", \"bmi2\" }, null))))))))));\n+                              new CPUSpecificPredicate(\"x86_64\",    new String[] { \"avx2\", \"bmi2\" }, null)))))))))));\n","filename":"test\/hotspot\/jtreg\/compiler\/testlibrary\/sha\/predicate\/IntrinsicPredicates.java","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -115,1 +115,1 @@\n-            if (Platform.isWindows() || Platform.isARM()) {\n+            if (Platform.isWindows() || Platform.isARM() || Platform.isRISCV64()) {\n","filename":"test\/hotspot\/jtreg\/runtime\/NMT\/CheckForProperDetailStackTrace.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -242,1 +242,1 @@\n-              Platform.isX86())) ||\n+              Platform.isX86() || Platform.isRISCV64())) ||\n","filename":"test\/hotspot\/jtreg\/runtime\/ReservedStack\/ReservedStackTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-        ARCH(\"isAArch64\", \"isARM\", \"isPPC\", \"isS390x\", \"isSparc\", \"isX64\", \"isX86\"),\n+        ARCH(\"isAArch64\", \"isARM\", \"isRISCV64\", \"isPPC\", \"isS390x\", \"isSparc\", \"isX64\", \"isX86\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/TestMutuallyExclusivePlatformPredicates.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,2 +57,2 @@\n-            Events.assertField(event, \"cpu\").containsAny(\"Intel\", \"AMD\", \"Unknown x86\", \"sparc\", \"ARM\", \"PPC\", \"PowerPC\", \"AArch64\", \"s390\");\n-            Events.assertField(event, \"description\").containsAny(\"Intel\", \"AMD\", \"Unknown x86\", \"SPARC\", \"ARM\", \"PPC\", \"PowerPC\", \"AArch64\", \"s390\");\n+            Events.assertField(event, \"cpu\").containsAny(\"Intel\", \"AMD\", \"Unknown x86\", \"sparc\", \"ARM\", \"PPC\", \"PowerPC\", \"AArch64\", \"RISCV64\", \"s390\");\n+            Events.assertField(event, \"description\").containsAny(\"Intel\", \"AMD\", \"Unknown x86\", \"SPARC\", \"ARM\", \"PPC\", \"PowerPC\", \"AArch64\", \"RISCV64\", \"s390\");\n","filename":"test\/jdk\/jdk\/jfr\/event\/os\/TestCPUInformation.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -205,0 +205,4 @@\n+    public static boolean isRISCV64() {\n+        return isArch(\"riscv64\");\n+    }\n+\n","filename":"test\/lib\/jdk\/test\/lib\/Platform.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}