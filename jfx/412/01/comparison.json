{"files":[{"patch":"@@ -406,0 +406,3 @@\n+\/\/ By default, target architecture = host architecture, but we allow different ones.\n+defineProperty(\"TARGET_ARCH\", OS_ARCH)\n+\n@@ -3298,1 +3301,1 @@\n-                            args(\"OUTPUT_DIR=${nativeOutputDir}\", \"BUILD_TYPE=${buildType}\", \"BASE_NAME=ffi\")\n+                            args(\"OUTPUT_DIR=${nativeOutputDir}\", \"BUILD_TYPE=${buildType}\", \"BASE_NAME=ffi\", \"ARCH=$TARGET_ARCH\")\n@@ -3304,1 +3307,1 @@\n-                            args(\"OUTPUT_DIR=${nativeOutputDir}\", \"BUILD_TYPE=${buildType}\", \"BASE_NAME=glib-lite\")\n+                            args(\"OUTPUT_DIR=${nativeOutputDir}\", \"BUILD_TYPE=${buildType}\", \"BASE_NAME=glib-lite\", \"ARCH=$TARGET_ARCH\")\n","filename":"build.gradle","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-    commonParams = [\"-target\", \"${TARGET_ARCH}-apple-macos-11\", commonParams]\n+    commonParams = [\"-target\", \"${TARGET_ARCH}-apple-macos-11\", commonParams].flatten()\n","filename":"buildSrc\/mac.gradle","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,2 +52,2 @@\n-#ifndef X86_64\n-#define X86_64\n+#ifndef ARM\n+#define ARM\n@@ -200,1 +200,1 @@\n-#if 1\n+#if 0\n@@ -209,1 +209,1 @@\n-#if 1\n+#if 0\n@@ -309,1 +309,1 @@\n-#if 0\n+#if 1\n@@ -356,1 +356,1 @@\n-#if 0\n+#if 1\n@@ -381,1 +381,1 @@\n-#if 0\n+#if 1\n@@ -491,1 +491,1 @@\n-#if 1\n+#if 0\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/include\/mac\/aarch64\/ffi.h","additions":8,"deletions":8,"binary":false,"changes":16,"previous_filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/include\/mac\/x64\/ffi.h","status":"copied"},{"patch":"@@ -22,1 +22,1 @@\n-\/* #undef FFI_EXEC_TRAMPOLINE_TABLE *\/\n+#define FFI_EXEC_TRAMPOLINE_TABLE 1\n@@ -28,1 +28,1 @@\n-#define FFI_MMAP_EXEC_WRIT 1\n+\/* #undef FFI_MMAP_EXEC_WRIT *\/\n@@ -60,1 +60,1 @@\n-#define HAVE_AS_X86_PCREL 1\n+\/* #undef HAVE_AS_X86_PCREL *\/\n@@ -72,1 +72,1 @@\n-#define HAVE_LONG_DOUBLE 1\n+\/* #undef HAVE_LONG_DOUBLE *\/\n@@ -156,1 +156,1 @@\n-#define SIZEOF_LONG_DOUBLE 16\n+#define SIZEOF_LONG_DOUBLE 8\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/include\/mac\/aarch64\/fficonfig.h","additions":5,"deletions":5,"binary":false,"changes":10,"previous_filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/include\/mac\/x64\/fficonfig.h","status":"copied"},{"patch":"@@ -0,0 +1,1009 @@\n+\/* Copyright (c) 2009, 2010, 2011, 2012 ARM Ltd.\n+\n+Permission is hereby granted, free of charge, to any person obtaining\n+a copy of this software and associated documentation files (the\n+``Software''), to deal in the Software without restriction, including\n+without limitation the rights to use, copy, modify, merge, publish,\n+distribute, sublicense, and\/or sell copies of the Software, and to\n+permit persons to whom the Software is furnished to do so, subject to\n+the following conditions:\n+\n+The above copyright notice and this permission notice shall be\n+included in all copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,\n+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  *\/\n+\n+#if defined(__aarch64__) || defined(__arm64__)|| defined (_M_ARM64)\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <stdint.h>\n+#include <fficonfig.h>\n+#include <ffi.h>\n+#include <ffi_common.h>\n+#include \"internal.h\"\n+#ifdef _M_ARM64\n+#include <windows.h> \/* FlushInstructionCache *\/\n+#endif\n+\n+\/* Force FFI_TYPE_LONGDOUBLE to be different than FFI_TYPE_DOUBLE;\n+   all further uses in this file will refer to the 128-bit type.  *\/\n+#if FFI_TYPE_DOUBLE != FFI_TYPE_LONGDOUBLE\n+# if FFI_TYPE_LONGDOUBLE != 4\n+#  error FFI_TYPE_LONGDOUBLE out of date\n+# endif\n+#else\n+# undef FFI_TYPE_LONGDOUBLE\n+# define FFI_TYPE_LONGDOUBLE 4\n+#endif\n+\n+union _d\n+{\n+  UINT64 d;\n+  UINT32 s[2];\n+};\n+\n+struct _v\n+{\n+  union _d d[2] __attribute__((aligned(16)));\n+};\n+\n+struct call_context\n+{\n+  struct _v v[N_V_ARG_REG];\n+  UINT64 x[N_X_ARG_REG];\n+};\n+\n+#if FFI_EXEC_TRAMPOLINE_TABLE\n+\n+#ifdef __MACH__\n+#include <mach\/vm_param.h>\n+#endif\n+\n+#else\n+\n+#if defined (__clang__) && defined (__APPLE__)\n+extern void sys_icache_invalidate (void *start, size_t len);\n+#endif\n+\n+static inline void\n+ffi_clear_cache (void *start, void *end)\n+{\n+#if defined (__clang__) && defined (__APPLE__)\n+  sys_icache_invalidate (start, (char *)end - (char *)start);\n+#elif defined (__GNUC__)\n+  __builtin___clear_cache (start, end);\n+#elif defined (_M_ARM64)\n+  FlushInstructionCache(GetCurrentProcess(), start, (char*)end - (char*)start);\n+#else\n+#error \"Missing builtin to flush instruction cache\"\n+#endif\n+}\n+\n+#endif\n+\n+\/* A subroutine of is_vfp_type.  Given a structure type, return the type code\n+   of the first non-structure element.  Recurse for structure elements.\n+   Return -1 if the structure is in fact empty, i.e. no nested elements.  *\/\n+\n+static int\n+is_hfa0 (const ffi_type *ty)\n+{\n+  ffi_type **elements = ty->elements;\n+  int i, ret = -1;\n+\n+  if (elements != NULL)\n+    for (i = 0; elements[i]; ++i)\n+      {\n+        ret = elements[i]->type;\n+        if (ret == FFI_TYPE_STRUCT || ret == FFI_TYPE_COMPLEX)\n+          {\n+            ret = is_hfa0 (elements[i]);\n+            if (ret < 0)\n+              continue;\n+          }\n+        break;\n+      }\n+\n+  return ret;\n+}\n+\n+\/* A subroutine of is_vfp_type.  Given a structure type, return true if all\n+   of the non-structure elements are the same as CANDIDATE.  *\/\n+\n+static int\n+is_hfa1 (const ffi_type *ty, int candidate)\n+{\n+  ffi_type **elements = ty->elements;\n+  int i;\n+\n+  if (elements != NULL)\n+    for (i = 0; elements[i]; ++i)\n+      {\n+        int t = elements[i]->type;\n+        if (t == FFI_TYPE_STRUCT || t == FFI_TYPE_COMPLEX)\n+          {\n+            if (!is_hfa1 (elements[i], candidate))\n+              return 0;\n+          }\n+        else if (t != candidate)\n+          return 0;\n+      }\n+\n+  return 1;\n+}\n+\n+\/* Determine if TY may be allocated to the FP registers.  This is both an\n+   fp scalar type as well as an homogenous floating point aggregate (HFA).\n+   That is, a structure consisting of 1 to 4 members of all the same type,\n+   where that type is an fp scalar.\n+\n+   Returns non-zero iff TY is an HFA.  The result is the AARCH64_RET_*\n+   constant for the type.  *\/\n+\n+static int\n+is_vfp_type (const ffi_type *ty)\n+{\n+  ffi_type **elements;\n+  int candidate, i;\n+  size_t size, ele_count;\n+\n+  \/* Quickest tests first.  *\/\n+  candidate = ty->type;\n+  switch (candidate)\n+    {\n+    default:\n+      return 0;\n+    case FFI_TYPE_FLOAT:\n+    case FFI_TYPE_DOUBLE:\n+    case FFI_TYPE_LONGDOUBLE:\n+      ele_count = 1;\n+      goto done;\n+    case FFI_TYPE_COMPLEX:\n+      candidate = ty->elements[0]->type;\n+      switch (candidate)\n+    {\n+    case FFI_TYPE_FLOAT:\n+    case FFI_TYPE_DOUBLE:\n+    case FFI_TYPE_LONGDOUBLE:\n+      ele_count = 2;\n+      goto done;\n+    }\n+      return 0;\n+    case FFI_TYPE_STRUCT:\n+      break;\n+    }\n+\n+  \/* No HFA types are smaller than 4 bytes, or larger than 64 bytes.  *\/\n+  size = ty->size;\n+  if (size < 4 || size > 64)\n+    return 0;\n+\n+  \/* Find the type of the first non-structure member.  *\/\n+  elements = ty->elements;\n+  candidate = elements[0]->type;\n+  if (candidate == FFI_TYPE_STRUCT || candidate == FFI_TYPE_COMPLEX)\n+    {\n+      for (i = 0; ; ++i)\n+        {\n+          candidate = is_hfa0 (elements[i]);\n+          if (candidate >= 0)\n+            break;\n+        }\n+    }\n+\n+  \/* If the first member is not a floating point type, it's not an HFA.\n+     Also quickly re-check the size of the structure.  *\/\n+  switch (candidate)\n+    {\n+    case FFI_TYPE_FLOAT:\n+      ele_count = size \/ sizeof(float);\n+      if (size != ele_count * sizeof(float))\n+        return 0;\n+      break;\n+    case FFI_TYPE_DOUBLE:\n+      ele_count = size \/ sizeof(double);\n+      if (size != ele_count * sizeof(double))\n+        return 0;\n+      break;\n+    case FFI_TYPE_LONGDOUBLE:\n+      ele_count = size \/ sizeof(long double);\n+      if (size != ele_count * sizeof(long double))\n+        return 0;\n+      break;\n+    default:\n+      return 0;\n+    }\n+  if (ele_count > 4)\n+    return 0;\n+\n+  \/* Finally, make sure that all scalar elements are the same type.  *\/\n+  for (i = 0; elements[i]; ++i)\n+    {\n+      int t = elements[i]->type;\n+      if (t == FFI_TYPE_STRUCT || t == FFI_TYPE_COMPLEX)\n+        {\n+          if (!is_hfa1 (elements[i], candidate))\n+            return 0;\n+        }\n+      else if (t != candidate)\n+        return 0;\n+    }\n+\n+  \/* All tests succeeded.  Encode the result.  *\/\n+ done:\n+  return candidate * 4 + (4 - (int)ele_count);\n+}\n+\n+\/* Representation of the procedure call argument marshalling\n+   state.\n+\n+   The terse state variable names match the names used in the AARCH64\n+   PCS. *\/\n+\n+struct arg_state\n+{\n+  unsigned ngrn;                \/* Next general-purpose register number. *\/\n+  unsigned nsrn;                \/* Next vector register number. *\/\n+  size_t nsaa;                  \/* Next stack offset. *\/\n+\n+#if defined (__APPLE__)\n+  unsigned allocating_variadic;\n+#endif\n+};\n+\n+\/* Initialize a procedure call argument marshalling state.  *\/\n+static void\n+arg_init (struct arg_state *state)\n+{\n+  state->ngrn = 0;\n+  state->nsrn = 0;\n+  state->nsaa = 0;\n+#if defined (__APPLE__)\n+  state->allocating_variadic = 0;\n+#endif\n+}\n+\n+\/* Allocate an aligned slot on the stack and return a pointer to it.  *\/\n+static void *\n+allocate_to_stack (struct arg_state *state, void *stack,\n+           size_t alignment, size_t size)\n+{\n+  size_t nsaa = state->nsaa;\n+\n+  \/* Round up the NSAA to the larger of 8 or the natural\n+     alignment of the argument's type.  *\/\n+#if defined (__APPLE__)\n+  if (state->allocating_variadic && alignment < 8)\n+    alignment = 8;\n+#else\n+  if (alignment < 8)\n+    alignment = 8;\n+#endif\n+\n+  nsaa = FFI_ALIGN (nsaa, alignment);\n+  state->nsaa = nsaa + size;\n+\n+  return (char *)stack + nsaa;\n+}\n+\n+static ffi_arg\n+extend_integer_type (void *source, int type)\n+{\n+  switch (type)\n+    {\n+    case FFI_TYPE_UINT8:\n+      return *(UINT8 *) source;\n+    case FFI_TYPE_SINT8:\n+      return *(SINT8 *) source;\n+    case FFI_TYPE_UINT16:\n+      return *(UINT16 *) source;\n+    case FFI_TYPE_SINT16:\n+      return *(SINT16 *) source;\n+    case FFI_TYPE_UINT32:\n+      return *(UINT32 *) source;\n+    case FFI_TYPE_INT:\n+    case FFI_TYPE_SINT32:\n+      return *(SINT32 *) source;\n+    case FFI_TYPE_UINT64:\n+    case FFI_TYPE_SINT64:\n+      return *(UINT64 *) source;\n+      break;\n+    case FFI_TYPE_POINTER:\n+      return *(uintptr_t *) source;\n+    default:\n+      abort();\n+    }\n+}\n+\n+#if defined(_MSC_VER)\n+void extend_hfa_type (void *dest, void *src, int h);\n+#else\n+static void\n+extend_hfa_type (void *dest, void *src, int h)\n+{\n+  ssize_t f = h - AARCH64_RET_S4;\n+  void *x0;\n+\n+  asm volatile (\n+    \"adr    %0, 0f\\n\"\n+\"   add %0, %0, %1\\n\"\n+\"   br  %0\\n\"\n+\"0: ldp s16, s17, [%3]\\n\"   \/* S4 *\/\n+\"   ldp s18, s19, [%3, #8]\\n\"\n+\"   b   4f\\n\"\n+\"   ldp s16, s17, [%3]\\n\"   \/* S3 *\/\n+\"   ldr s18, [%3, #8]\\n\"\n+\"   b   3f\\n\"\n+\"   ldp s16, s17, [%3]\\n\"   \/* S2 *\/\n+\"   b   2f\\n\"\n+\"   nop\\n\"\n+\"   ldr s16, [%3]\\n\"        \/* S1 *\/\n+\"   b   1f\\n\"\n+\"   nop\\n\"\n+\"   ldp d16, d17, [%3]\\n\"   \/* D4 *\/\n+\"   ldp d18, d19, [%3, #16]\\n\"\n+\"   b   4f\\n\"\n+\"   ldp d16, d17, [%3]\\n\"   \/* D3 *\/\n+\"   ldr d18, [%3, #16]\\n\"\n+\"   b   3f\\n\"\n+\"   ldp d16, d17, [%3]\\n\"   \/* D2 *\/\n+\"   b   2f\\n\"\n+\"   nop\\n\"\n+\"   ldr d16, [%3]\\n\"        \/* D1 *\/\n+\"   b   1f\\n\"\n+\"   nop\\n\"\n+\"   ldp q16, q17, [%3]\\n\"   \/* Q4 *\/\n+\"   ldp q18, q19, [%3, #32]\\n\"\n+\"   b   4f\\n\"\n+\"   ldp q16, q17, [%3]\\n\"   \/* Q3 *\/\n+\"   ldr q18, [%3, #32]\\n\"\n+\"   b   3f\\n\"\n+\"   ldp q16, q17, [%3]\\n\"   \/* Q2 *\/\n+\"   b   2f\\n\"\n+\"   nop\\n\"\n+\"   ldr q16, [%3]\\n\"        \/* Q1 *\/\n+\"   b   1f\\n\"\n+\"4: str q19, [%2, #48]\\n\"\n+\"3: str q18, [%2, #32]\\n\"\n+\"2: str q17, [%2, #16]\\n\"\n+\"1: str q16, [%2]\"\n+    : \"=&r\"(x0)\n+    : \"r\"(f * 12), \"r\"(dest), \"r\"(src)\n+    : \"memory\", \"v16\", \"v17\", \"v18\", \"v19\");\n+}\n+#endif\n+\n+#if defined(_MSC_VER)\n+void* compress_hfa_type (void *dest, void *src, int h);\n+#else\n+static void *\n+compress_hfa_type (void *dest, void *reg, int h)\n+{\n+  switch (h)\n+    {\n+    case AARCH64_RET_S1:\n+      if (dest == reg)\n+    {\n+#ifdef __AARCH64EB__\n+      dest += 12;\n+#endif\n+    }\n+      else\n+    *(float *)dest = *(float *)reg;\n+      break;\n+    case AARCH64_RET_S2:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"st2 { v16.s, v17.s }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\");\n+      break;\n+    case AARCH64_RET_S3:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"ldr q18, [%1, #32]\\n\\t\"\n+       \"st3 { v16.s, v17.s, v18.s }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\", \"v18\");\n+      break;\n+    case AARCH64_RET_S4:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"ldp q18, q19, [%1, #32]\\n\\t\"\n+       \"st4 { v16.s, v17.s, v18.s, v19.s }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\", \"v18\", \"v19\");\n+      break;\n+\n+    case AARCH64_RET_D1:\n+      if (dest == reg)\n+    {\n+#ifdef __AARCH64EB__\n+      dest += 8;\n+#endif\n+    }\n+      else\n+    *(double *)dest = *(double *)reg;\n+      break;\n+    case AARCH64_RET_D2:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"st2 { v16.d, v17.d }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\");\n+      break;\n+    case AARCH64_RET_D3:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"ldr q18, [%1, #32]\\n\\t\"\n+       \"st3 { v16.d, v17.d, v18.d }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\", \"v18\");\n+      break;\n+    case AARCH64_RET_D4:\n+      asm (\"ldp q16, q17, [%1]\\n\\t\"\n+       \"ldp q18, q19, [%1, #32]\\n\\t\"\n+       \"st4 { v16.d, v17.d, v18.d, v19.d }[0], [%0]\"\n+       : : \"r\"(dest), \"r\"(reg) : \"memory\", \"v16\", \"v17\", \"v18\", \"v19\");\n+      break;\n+\n+    default:\n+      if (dest != reg)\n+    return memcpy (dest, reg, 16 * (4 - (h & 3)));\n+      break;\n+    }\n+  return dest;\n+}\n+#endif\n+\n+\/* Either allocate an appropriate register for the argument type, or if\n+   none are available, allocate a stack slot and return a pointer\n+   to the allocated space.  *\/\n+\n+static void *\n+allocate_int_to_reg_or_stack (struct call_context *context,\n+                  struct arg_state *state,\n+                  void *stack, size_t size)\n+{\n+  if (state->ngrn < N_X_ARG_REG)\n+    return &context->x[state->ngrn++];\n+\n+  state->ngrn = N_X_ARG_REG;\n+  return allocate_to_stack (state, stack, size, size);\n+}\n+\n+ffi_status FFI_HIDDEN\n+ffi_prep_cif_machdep (ffi_cif *cif)\n+{\n+  ffi_type *rtype = cif->rtype;\n+  size_t bytes = cif->bytes;\n+  int flags, i, n;\n+\n+  switch (rtype->type)\n+    {\n+    case FFI_TYPE_VOID:\n+      flags = AARCH64_RET_VOID;\n+      break;\n+    case FFI_TYPE_UINT8:\n+      flags = AARCH64_RET_UINT8;\n+      break;\n+    case FFI_TYPE_UINT16:\n+      flags = AARCH64_RET_UINT16;\n+      break;\n+    case FFI_TYPE_UINT32:\n+      flags = AARCH64_RET_UINT32;\n+      break;\n+    case FFI_TYPE_SINT8:\n+      flags = AARCH64_RET_SINT8;\n+      break;\n+    case FFI_TYPE_SINT16:\n+      flags = AARCH64_RET_SINT16;\n+      break;\n+    case FFI_TYPE_INT:\n+    case FFI_TYPE_SINT32:\n+      flags = AARCH64_RET_SINT32;\n+      break;\n+    case FFI_TYPE_SINT64:\n+    case FFI_TYPE_UINT64:\n+      flags = AARCH64_RET_INT64;\n+      break;\n+    case FFI_TYPE_POINTER:\n+      flags = (sizeof(void *) == 4 ? AARCH64_RET_UINT32 : AARCH64_RET_INT64);\n+      break;\n+\n+    case FFI_TYPE_FLOAT:\n+    case FFI_TYPE_DOUBLE:\n+    case FFI_TYPE_LONGDOUBLE:\n+    case FFI_TYPE_STRUCT:\n+    case FFI_TYPE_COMPLEX:\n+      flags = is_vfp_type (rtype);\n+      if (flags == 0)\n+    {\n+      size_t s = rtype->size;\n+      if (s > 16)\n+        {\n+          flags = AARCH64_RET_VOID | AARCH64_RET_IN_MEM;\n+          bytes += 8;\n+        }\n+      else if (s == 16)\n+        flags = AARCH64_RET_INT128;\n+      else if (s == 8)\n+        flags = AARCH64_RET_INT64;\n+      else\n+        flags = AARCH64_RET_INT128 | AARCH64_RET_NEED_COPY;\n+    }\n+      break;\n+\n+    default:\n+      abort();\n+    }\n+\n+  for (i = 0, n = cif->nargs; i < n; i++)\n+    if (is_vfp_type (cif->arg_types[i]))\n+      {\n+    flags |= AARCH64_FLAG_ARG_V;\n+    break;\n+      }\n+\n+  \/* Round the stack up to a multiple of the stack alignment requirement. *\/\n+  cif->bytes = (unsigned) FFI_ALIGN(bytes, 16);\n+  cif->flags = flags;\n+#if defined (__APPLE__)\n+  cif->aarch64_nfixedargs = 0;\n+#endif\n+\n+  return FFI_OK;\n+}\n+\n+#if defined (__APPLE__)\n+\/* Perform Apple-specific cif processing for variadic calls *\/\n+ffi_status FFI_HIDDEN\n+ffi_prep_cif_machdep_var(ffi_cif *cif, unsigned int nfixedargs,\n+             unsigned int ntotalargs)\n+{\n+  ffi_status status = ffi_prep_cif_machdep (cif);\n+  cif->aarch64_nfixedargs = nfixedargs;\n+  return status;\n+}\n+#endif \/* __APPLE__ *\/\n+\n+extern void ffi_call_SYSV (struct call_context *context, void *frame,\n+               void (*fn)(void), void *rvalue, int flags,\n+               void *closure) FFI_HIDDEN;\n+\n+\/* Call a function with the provided arguments and capture the return\n+   value.  *\/\n+static void\n+ffi_call_int (ffi_cif *cif, void (*fn)(void), void *orig_rvalue,\n+          void **avalue, void *closure)\n+{\n+  struct call_context *context;\n+  void *stack, *frame, *rvalue;\n+  struct arg_state state;\n+  size_t stack_bytes, rtype_size, rsize;\n+  int i, nargs, flags;\n+  ffi_type *rtype;\n+\n+  flags = cif->flags;\n+  rtype = cif->rtype;\n+  rtype_size = rtype->size;\n+  stack_bytes = cif->bytes;\n+\n+  \/* If the target function returns a structure via hidden pointer,\n+     then we cannot allow a null rvalue.  Otherwise, mash a null\n+     rvalue to void return type.  *\/\n+  rsize = 0;\n+  if (flags & AARCH64_RET_IN_MEM)\n+    {\n+      if (orig_rvalue == NULL)\n+    rsize = rtype_size;\n+    }\n+  else if (orig_rvalue == NULL)\n+    flags &= AARCH64_FLAG_ARG_V;\n+  else if (flags & AARCH64_RET_NEED_COPY)\n+    rsize = 16;\n+\n+  \/* Allocate consectutive stack for everything we'll need.  *\/\n+  context = alloca (sizeof(struct call_context) + stack_bytes + 32 + rsize);\n+  stack = context + 1;\n+  frame = (void*)((uintptr_t)stack + (uintptr_t)stack_bytes);\n+  rvalue = (rsize ? (void*)((uintptr_t)frame + 32) : orig_rvalue);\n+\n+  arg_init (&state);\n+  for (i = 0, nargs = cif->nargs; i < nargs; i++)\n+    {\n+      ffi_type *ty = cif->arg_types[i];\n+      size_t s = ty->size;\n+      void *a = avalue[i];\n+      int h, t;\n+\n+      t = ty->type;\n+      switch (t)\n+    {\n+    case FFI_TYPE_VOID:\n+      FFI_ASSERT (0);\n+      break;\n+\n+    \/* If the argument is a basic type the argument is allocated to an\n+       appropriate register, or if none are available, to the stack.  *\/\n+    case FFI_TYPE_INT:\n+    case FFI_TYPE_UINT8:\n+    case FFI_TYPE_SINT8:\n+    case FFI_TYPE_UINT16:\n+    case FFI_TYPE_SINT16:\n+    case FFI_TYPE_UINT32:\n+    case FFI_TYPE_SINT32:\n+    case FFI_TYPE_UINT64:\n+    case FFI_TYPE_SINT64:\n+    case FFI_TYPE_POINTER:\n+    do_pointer:\n+      {\n+        ffi_arg ext = extend_integer_type (a, t);\n+        if (state.ngrn < N_X_ARG_REG)\n+          context->x[state.ngrn++] = ext;\n+        else\n+          {\n+        void *d = allocate_to_stack (&state, stack, ty->alignment, s);\n+        state.ngrn = N_X_ARG_REG;\n+        \/* Note that the default abi extends each argument\n+           to a full 64-bit slot, while the iOS abi allocates\n+           only enough space. *\/\n+#ifdef __APPLE__\n+        memcpy(d, a, s);\n+#else\n+        *(ffi_arg *)d = ext;\n+#endif\n+          }\n+      }\n+      break;\n+\n+    case FFI_TYPE_FLOAT:\n+    case FFI_TYPE_DOUBLE:\n+    case FFI_TYPE_LONGDOUBLE:\n+    case FFI_TYPE_STRUCT:\n+    case FFI_TYPE_COMPLEX:\n+      {\n+        void *dest;\n+\n+        h = is_vfp_type (ty);\n+        if (h)\n+          {\n+              int elems = 4 - (h & 3);\n+#ifdef _M_ARM64 \/* for handling armasm calling convention *\/\n+                if (cif->is_variadic)\n+              {\n+                if (state.ngrn + elems <= N_X_ARG_REG)\n+                {\n+                  dest = &context->x[state.ngrn];\n+                  state.ngrn += elems;\n+                  extend_hfa_type(dest, a, h);\n+                  break;\n+                }\n+                state.nsrn = N_X_ARG_REG;\n+                dest = allocate_to_stack(&state, stack, ty->alignment, s);\n+              }\n+              else\n+              {\n+#endif \/* for handling armasm calling convention *\/\n+                if (state.nsrn + elems <= N_V_ARG_REG)\n+                {\n+                  dest = &context->v[state.nsrn];\n+                  state.nsrn += elems;\n+                  extend_hfa_type (dest, a, h);\n+                  break;\n+                }\n+                state.nsrn = N_V_ARG_REG;\n+                dest = allocate_to_stack (&state, stack, ty->alignment, s);\n+#ifdef _M_ARM64 \/* for handling armasm calling convention *\/\n+              }\n+#endif \/* for handling armasm calling convention *\/\n+          }\n+        else if (s > 16)\n+          {\n+        \/* If the argument is a composite type that is larger than 16\n+           bytes, then the argument has been copied to memory, and\n+           the argument is replaced by a pointer to the copy.  *\/\n+        a = &avalue[i];\n+        t = FFI_TYPE_POINTER;\n+        s = sizeof (void *);\n+        goto do_pointer;\n+          }\n+        else\n+          {\n+        size_t n = (s + 7) \/ 8;\n+        if (state.ngrn + n <= N_X_ARG_REG)\n+          {\n+            \/* If the argument is a composite type and the size in\n+               double-words is not more than the number of available\n+               X registers, then the argument is copied into\n+               consecutive X registers.  *\/\n+            dest = &context->x[state.ngrn];\n+                    state.ngrn += (unsigned int)n;\n+          }\n+        else\n+          {\n+            \/* Otherwise, there are insufficient X registers. Further\n+               X register allocations are prevented, the NSAA is\n+               adjusted and the argument is copied to memory at the\n+               adjusted NSAA.  *\/\n+            state.ngrn = N_X_ARG_REG;\n+            dest = allocate_to_stack (&state, stack, ty->alignment, s);\n+          }\n+        }\n+          memcpy (dest, a, s);\n+        }\n+      break;\n+\n+    default:\n+      abort();\n+    }\n+\n+#if defined (__APPLE__)\n+      if (i + 1 == cif->aarch64_nfixedargs)\n+    {\n+      state.ngrn = N_X_ARG_REG;\n+      state.nsrn = N_V_ARG_REG;\n+      state.allocating_variadic = 1;\n+    }\n+#endif\n+    }\n+\n+  ffi_call_SYSV (context, frame, fn, rvalue, flags, closure);\n+\n+  if (flags & AARCH64_RET_NEED_COPY)\n+    memcpy (orig_rvalue, rvalue, rtype_size);\n+}\n+\n+void\n+ffi_call (ffi_cif *cif, void (*fn) (void), void *rvalue, void **avalue)\n+{\n+  ffi_call_int (cif, fn, rvalue, avalue, NULL);\n+}\n+\n+#ifdef FFI_GO_CLOSURES\n+void\n+ffi_call_go (ffi_cif *cif, void (*fn) (void), void *rvalue,\n+         void **avalue, void *closure)\n+{\n+  ffi_call_int (cif, fn, rvalue, avalue, closure);\n+}\n+#endif \/* FFI_GO_CLOSURES *\/\n+\n+\/* Build a trampoline.  *\/\n+\n+extern void ffi_closure_SYSV (void) FFI_HIDDEN;\n+extern void ffi_closure_SYSV_V (void) FFI_HIDDEN;\n+\n+ffi_status\n+ffi_prep_closure_loc (ffi_closure *closure,\n+                      ffi_cif* cif,\n+                      void (*fun)(ffi_cif*,void*,void**,void*),\n+                      void *user_data,\n+                      void *codeloc)\n+{\n+  if (cif->abi != FFI_SYSV)\n+    return FFI_BAD_ABI;\n+\n+  void (*start)(void);\n+\n+  if (cif->flags & AARCH64_FLAG_ARG_V)\n+    start = ffi_closure_SYSV_V;\n+  else\n+    start = ffi_closure_SYSV;\n+\n+#if FFI_EXEC_TRAMPOLINE_TABLE\n+#ifdef __MACH__\n+  void **config = (void **)((uint8_t *)codeloc - PAGE_MAX_SIZE);\n+  config[0] = closure;\n+  config[1] = start;\n+#endif\n+#else\n+  static const unsigned char trampoline[16] = {\n+    0x90, 0x00, 0x00, 0x58, \/* ldr  x16, tramp+16   *\/\n+    0xf1, 0xff, 0xff, 0x10, \/* adr  x17, tramp+0    *\/\n+    0x00, 0x02, 0x1f, 0xd6  \/* br   x16     *\/\n+  };\n+  char *tramp = closure->tramp;\n+\n+  memcpy (tramp, trampoline, sizeof(trampoline));\n+\n+  *(UINT64 *)(tramp + 16) = (uintptr_t)start;\n+\n+  ffi_clear_cache(tramp, tramp + FFI_TRAMPOLINE_SIZE);\n+\n+  \/* Also flush the cache for code mapping.  *\/\n+#ifdef _M_ARM64\n+  \/\/ Not using dlmalloc.c for Windows ARM64 builds\n+  \/\/ so calling ffi_data_to_code_pointer() isn't necessary\n+  unsigned char *tramp_code = tramp;\n+  #else\n+  unsigned char *tramp_code = ffi_data_to_code_pointer (tramp);\n+  #endif\n+  ffi_clear_cache (tramp_code, tramp_code + FFI_TRAMPOLINE_SIZE);\n+#endif\n+\n+  closure->cif = cif;\n+  closure->fun = fun;\n+  closure->user_data = user_data;\n+\n+  return FFI_OK;\n+}\n+\n+#ifdef FFI_GO_CLOSURES\n+extern void ffi_go_closure_SYSV (void) FFI_HIDDEN;\n+extern void ffi_go_closure_SYSV_V (void) FFI_HIDDEN;\n+\n+ffi_status\n+ffi_prep_go_closure (ffi_go_closure *closure, ffi_cif* cif,\n+                     void (*fun)(ffi_cif*,void*,void**,void*))\n+{\n+  void (*start)(void);\n+\n+  if (cif->abi != FFI_SYSV)\n+    return FFI_BAD_ABI;\n+\n+  if (cif->flags & AARCH64_FLAG_ARG_V)\n+    start = ffi_go_closure_SYSV_V;\n+  else\n+    start = ffi_go_closure_SYSV;\n+\n+  closure->tramp = start;\n+  closure->cif = cif;\n+  closure->fun = fun;\n+\n+  return FFI_OK;\n+}\n+#endif \/* FFI_GO_CLOSURES *\/\n+\n+\/* Primary handler to setup and invoke a function within a closure.\n+\n+   A closure when invoked enters via the assembler wrapper\n+   ffi_closure_SYSV(). The wrapper allocates a call context on the\n+   stack, saves the interesting registers (from the perspective of\n+   the calling convention) into the context then passes control to\n+   ffi_closure_SYSV_inner() passing the saved context and a pointer to\n+   the stack at the point ffi_closure_SYSV() was invoked.\n+\n+   On the return path the assembler wrapper will reload call context\n+   registers.\n+\n+   ffi_closure_SYSV_inner() marshalls the call context into ffi value\n+   descriptors, invokes the wrapped function, then marshalls the return\n+   value back into the call context.  *\/\n+\n+int FFI_HIDDEN\n+ffi_closure_SYSV_inner (ffi_cif *cif,\n+            void (*fun)(ffi_cif*,void*,void**,void*),\n+            void *user_data,\n+            struct call_context *context,\n+            void *stack, void *rvalue, void *struct_rvalue)\n+{\n+  void **avalue = (void**) alloca (cif->nargs * sizeof (void*));\n+  int i, h, nargs, flags;\n+  struct arg_state state;\n+\n+  arg_init (&state);\n+\n+  for (i = 0, nargs = cif->nargs; i < nargs; i++)\n+    {\n+      ffi_type *ty = cif->arg_types[i];\n+      int t = ty->type;\n+      size_t n, s = ty->size;\n+\n+      switch (t)\n+    {\n+    case FFI_TYPE_VOID:\n+      FFI_ASSERT (0);\n+      break;\n+\n+    case FFI_TYPE_INT:\n+    case FFI_TYPE_UINT8:\n+    case FFI_TYPE_SINT8:\n+    case FFI_TYPE_UINT16:\n+    case FFI_TYPE_SINT16:\n+    case FFI_TYPE_UINT32:\n+    case FFI_TYPE_SINT32:\n+    case FFI_TYPE_UINT64:\n+    case FFI_TYPE_SINT64:\n+    case FFI_TYPE_POINTER:\n+      avalue[i] = allocate_int_to_reg_or_stack (context, &state, stack, s);\n+      break;\n+\n+    case FFI_TYPE_FLOAT:\n+    case FFI_TYPE_DOUBLE:\n+    case FFI_TYPE_LONGDOUBLE:\n+    case FFI_TYPE_STRUCT:\n+    case FFI_TYPE_COMPLEX:\n+      h = is_vfp_type (ty);\n+      if (h)\n+        {\n+          n = 4 - (h & 3);\n+#ifdef _M_ARM64  \/* for handling armasm calling convention *\/\n+              if (cif->is_variadic)\n+                {\n+                  if (state.ngrn + n <= N_X_ARG_REG)\n+                    {\n+                      void *reg = &context->x[state.ngrn];\n+                      state.ngrn += (unsigned int)n;\n+\n+                      \/* Eeek! We need a pointer to the structure, however the\n+                       homogeneous float elements are being passed in individual\n+                       registers, therefore for float and double the structure\n+                       is not represented as a contiguous sequence of bytes in\n+                       our saved register context.  We don't need the original\n+                       contents of the register storage, so we reformat the\n+                       structure into the same memory.  *\/\n+                      avalue[i] = compress_hfa_type(reg, reg, h);\n+                    }\n+                  else\n+                    {\n+                      state.ngrn = N_X_ARG_REG;\n+                      state.nsrn = N_V_ARG_REG;\n+                      avalue[i] = allocate_to_stack(&state, stack,\n+                             ty->alignment, s);\n+                    }\n+                }\n+              else\n+                {\n+#endif  \/* for handling armasm calling convention *\/\n+                  if (state.nsrn + n <= N_V_ARG_REG)\n+                    {\n+                      void *reg = &context->v[state.nsrn];\n+                      state.nsrn += (unsigned int)n;\n+                      avalue[i] = compress_hfa_type(reg, reg, h);\n+                    }\n+                  else\n+                    {\n+                      state.nsrn = N_V_ARG_REG;\n+                      avalue[i] = allocate_to_stack(&state, stack,\n+                                                   ty->alignment, s);\n+                    }\n+#ifdef _M_ARM64  \/* for handling armasm calling convention *\/\n+                }\n+#endif  \/* for handling armasm calling convention *\/\n+            }\n+          else if (s > 16)\n+            {\n+              \/* Replace Composite type of size greater than 16 with a\n+                  pointer.  *\/\n+              avalue[i] = *(void **)\n+              allocate_int_to_reg_or_stack (context, &state, stack,\n+                                         sizeof (void *));\n+            }\n+          else\n+            {\n+              n = (s + 7) \/ 8;\n+              if (state.ngrn + n <= N_X_ARG_REG)\n+                {\n+                  avalue[i] = &context->x[state.ngrn];\n+                  state.ngrn += (unsigned int)n;\n+                }\n+              else\n+                {\n+                  state.ngrn = N_X_ARG_REG;\n+                  avalue[i] = allocate_to_stack(&state, stack,\n+                                           ty->alignment, s);\n+                }\n+            }\n+          break;\n+\n+        default:\n+          abort();\n+      }\n+\n+#if defined (__APPLE__)\n+      if (i + 1 == cif->aarch64_nfixedargs)\n+    {\n+      state.ngrn = N_X_ARG_REG;\n+      state.nsrn = N_V_ARG_REG;\n+      state.allocating_variadic = 1;\n+    }\n+#endif\n+    }\n+\n+  flags = cif->flags;\n+  if (flags & AARCH64_RET_IN_MEM)\n+    rvalue = struct_rvalue;\n+\n+  fun (cif, rvalue, avalue, user_data);\n+\n+  return flags;\n+}\n+\n+#endif \/* (__aarch64__) || defined(__arm64__)|| defined (_M_ARM64)*\/\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/aarch64\/ffi.c","additions":1009,"deletions":0,"binary":false,"changes":1009,"status":"added"},{"patch":"@@ -0,0 +1,104 @@\n+\/* -----------------------------------------------------------------*-C-*-\n+   ffitarget.h - Copyright (c) 2012  Anthony Green\n+                 Copyright (c) 2010  CodeSourcery\n+                 Copyright (c) 1996-2003  Red Hat, Inc.\n+\n+   Target configuration macros for ARM.\n+\n+   Permission is hereby granted, free of charge, to any person obtaining\n+   a copy of this software and associated documentation files (the\n+   ``Software''), to deal in the Software without restriction, including\n+   without limitation the rights to use, copy, modify, merge, publish,\n+   distribute, sublicense, and\/or sell copies of the Software, and to\n+   permit persons to whom the Software is furnished to do so, subject to\n+   the following conditions:\n+\n+   The above copyright notice and this permission notice shall be included\n+   in all copies or substantial portions of the Software.\n+\n+   THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,\n+   EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+   MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n+   NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n+   HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n+   WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+   DEALINGS IN THE SOFTWARE.\n+\n+   ----------------------------------------------------------------------- *\/\n+\n+#ifndef LIBFFI_TARGET_H\n+#define LIBFFI_TARGET_H\n+\n+#ifndef LIBFFI_H\n+#error \"Please do not include ffitarget.h directly into your source.  Use ffi.h instead.\"\n+#endif\n+\n+#ifndef LIBFFI_ASM\n+typedef unsigned long          ffi_arg;\n+typedef signed long            ffi_sarg;\n+\n+typedef enum ffi_abi {\n+  FFI_FIRST_ABI = 0,\n+  FFI_SYSV,\n+  FFI_VFP,\n+  FFI_LAST_ABI,\n+#if defined(__ARM_PCS_VFP) || defined(_M_ARM)\n+  FFI_DEFAULT_ABI = FFI_VFP,\n+#else\n+  FFI_DEFAULT_ABI = FFI_SYSV,\n+#endif\n+} ffi_abi;\n+#endif\n+\n+#define FFI_EXTRA_CIF_FIELDS            \\\n+  int vfp_used;                 \\\n+  unsigned short vfp_reg_free, vfp_nargs;   \\\n+  signed char vfp_args[16]          \\\n+\n+#define FFI_TARGET_SPECIFIC_VARIADIC\n+#ifndef _M_ARM\n+#define FFI_TARGET_HAS_COMPLEX_TYPE\n+#endif\n+\n+\/* ---- Definitions for closures ----------------------------------------- *\/\n+\n+#define FFI_CLOSURES 1\n+#define FFI_GO_CLOSURES 1\n+#define FFI_NATIVE_RAW_API 0\n+\n+#if defined (FFI_EXEC_TRAMPOLINE_TABLE) && FFI_EXEC_TRAMPOLINE_TABLE\n+\n+#ifdef __MACH__\n+#define FFI_TRAMPOLINE_SIZE 12\n+#define FFI_TRAMPOLINE_CLOSURE_OFFSET 8\n+#else\n+#error \"No trampoline table implementation\"\n+#endif\n+\n+#else\n+#ifdef _MSC_VER\n+#define FFI_TRAMPOLINE_SIZE 16\n+#define FFI_TRAMPOLINE_CLOSURE_FUNCTION 12\n+#else\n+#define FFI_TRAMPOLINE_SIZE 12\n+#endif\n+#define FFI_TRAMPOLINE_CLOSURE_OFFSET FFI_TRAMPOLINE_SIZE\n+#endif\n+\n+#endif\n+\n+\/* ---- Internal ---- *\/\n+\n+#if defined (__APPLE__)\n+#define FFI_EXTRA_CIF_FIELDS unsigned aarch64_nfixedargs\n+#elif !defined(_WIN32)\n+\/* iOS and Windows reserve x18 for the system.  Disable Go closures until\n+   a new static chain is chosen.  *\/\n+#define FFI_GO_CLOSURES 1\n+#endif\n+\n+#ifndef _WIN32\n+\/* No complex type on Windows *\/\n+#define FFI_TARGET_HAS_COMPLEX_TYPE\n+#endif\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/aarch64\/ffitarget.h","additions":104,"deletions":0,"binary":false,"changes":104,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+Permission is hereby granted, free of charge, to any person obtaining\n+a copy of this software and associated documentation files (the\n+``Software''), to deal in the Software without restriction, including\n+without limitation the rights to use, copy, modify, merge, publish,\n+distribute, sublicense, and\/or sell copies of the Software, and to\n+permit persons to whom the Software is furnished to do so, subject to\n+the following conditions:\n+\n+The above copyright notice and this permission notice shall be\n+included in all copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,\n+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  *\/\n+\n+#define AARCH64_RET_VOID    0\n+#define AARCH64_RET_INT64   1\n+#define AARCH64_RET_INT128  2\n+\n+#define AARCH64_RET_UNUSED3 3\n+#define AARCH64_RET_UNUSED4 4\n+#define AARCH64_RET_UNUSED5 5\n+#define AARCH64_RET_UNUSED6 6\n+#define AARCH64_RET_UNUSED7 7\n+\n+\/* Note that FFI_TYPE_FLOAT == 2, _DOUBLE == 3, _LONGDOUBLE == 4,\n+   so _S4 through _Q1 are layed out as (TYPE * 4) + (4 - COUNT).  *\/\n+#define AARCH64_RET_S4      8\n+#define AARCH64_RET_S3      9\n+#define AARCH64_RET_S2      10\n+#define AARCH64_RET_S1      11\n+\n+#define AARCH64_RET_D4      12\n+#define AARCH64_RET_D3      13\n+#define AARCH64_RET_D2      14\n+#define AARCH64_RET_D1      15\n+\n+#define AARCH64_RET_Q4      16\n+#define AARCH64_RET_Q3      17\n+#define AARCH64_RET_Q2      18\n+#define AARCH64_RET_Q1      19\n+\n+\/* Note that each of the sub-64-bit integers gets two entries.  *\/\n+#define AARCH64_RET_UINT8   20\n+#define AARCH64_RET_UINT16  22\n+#define AARCH64_RET_UINT32  24\n+\n+#define AARCH64_RET_SINT8   26\n+#define AARCH64_RET_SINT16  28\n+#define AARCH64_RET_SINT32  30\n+\n+#define AARCH64_RET_MASK    31\n+\n+#define AARCH64_RET_IN_MEM  (1 << 5)\n+#define AARCH64_RET_NEED_COPY   (1 << 6)\n+\n+#define AARCH64_FLAG_ARG_V_BIT  7\n+#define AARCH64_FLAG_ARG_V  (1 << AARCH64_FLAG_ARG_V_BIT)\n+\n+#define N_X_ARG_REG     8\n+#define N_V_ARG_REG     8\n+#define CALL_CONTEXT_SIZE   (N_V_ARG_REG * 16 + N_X_ARG_REG * 8)\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/aarch64\/internal.h","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,440 @@\n+\/* Copyright (c) 2009, 2010, 2011, 2012 ARM Ltd.\n+\n+Permission is hereby granted, free of charge, to any person obtaining\n+a copy of this software and associated documentation files (the\n+``Software''), to deal in the Software without restriction, including\n+without limitation the rights to use, copy, modify, merge, publish,\n+distribute, sublicense, and\/or sell copies of the Software, and to\n+permit persons to whom the Software is furnished to do so, subject to\n+the following conditions:\n+\n+The above copyright notice and this permission notice shall be\n+included in all copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,\n+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  *\/\n+\n+#if defined(__aarch64__) || defined(__arm64__)\n+#define LIBFFI_ASM\n+#include <fficonfig.h>\n+#include <ffi.h>\n+#include <ffi_cfi.h>\n+#include \"internal.h\"\n+\n+#ifdef HAVE_MACHINE_ASM_H\n+#include <machine\/asm.h>\n+#else\n+#ifdef __USER_LABEL_PREFIX__\n+#define CONCAT1(a, b) CONCAT2(a, b)\n+#define CONCAT2(a, b) a ## b\n+\n+\/* Use the right prefix for global labels.  *\/\n+#define CNAME(x) CONCAT1 (__USER_LABEL_PREFIX__, x)\n+#else\n+#define CNAME(x) x\n+#endif\n+#endif\n+\n+#ifdef __AARCH64EB__\n+# define BE(X)\tX\n+#else\n+# define BE(X)\t0\n+#endif\n+\n+#ifdef __ILP32__\n+#define PTR_REG(n)      w##n\n+#else\n+#define PTR_REG(n)      x##n\n+#endif\n+\n+#ifdef __ILP32__\n+#define PTR_SIZE\t4\n+#else\n+#define PTR_SIZE\t8\n+#endif\n+\n+\t.text\n+\t.align 4\n+\n+\/* ffi_call_SYSV\n+   extern void ffi_call_SYSV (void *stack, void *frame,\n+\t\t\t      void (*fn)(void), void *rvalue,\n+\t\t\t      int flags, void *closure);\n+\n+   Therefore on entry we have:\n+\n+   x0 stack\n+   x1 frame\n+   x2 fn\n+   x3 rvalue\n+   x4 flags\n+   x5 closure\n+*\/\n+\n+\tcfi_startproc\n+CNAME(ffi_call_SYSV):\n+\t\/* Use a stack frame allocated by our caller.  *\/\n+\tcfi_def_cfa(x1, 32);\n+\tstp\tx29, x30, [x1]\n+\tmov\tx29, x1\n+\tmov\tsp, x0\n+\tcfi_def_cfa_register(x29)\n+\tcfi_rel_offset (x29, 0)\n+\tcfi_rel_offset (x30, 8)\n+\n+\tmov\tx9, x2\t\t\t\/* save fn *\/\n+\tmov\tx8, x3\t\t\t\/* install structure return *\/\n+#ifdef FFI_GO_CLOSURES\n+\tmov\tx18, x5\t\t\t\/* install static chain *\/\n+#endif\n+\tstp\tx3, x4, [x29, #16]\t\/* save rvalue and flags *\/\n+\n+\t\/* Load the vector argument passing registers, if necessary.  *\/\n+\ttbz\tw4, #AARCH64_FLAG_ARG_V_BIT, 1f\n+\tldp     q0, q1, [sp, #0]\n+\tldp     q2, q3, [sp, #32]\n+\tldp     q4, q5, [sp, #64]\n+\tldp     q6, q7, [sp, #96]\n+1:\n+\t\/* Load the core argument passing registers, including\n+\t   the structure return pointer.  *\/\n+\tldp     x0, x1, [sp, #16*N_V_ARG_REG + 0]\n+\tldp     x2, x3, [sp, #16*N_V_ARG_REG + 16]\n+\tldp     x4, x5, [sp, #16*N_V_ARG_REG + 32]\n+\tldp     x6, x7, [sp, #16*N_V_ARG_REG + 48]\n+\n+\t\/* Deallocate the context, leaving the stacked arguments.  *\/\n+\tadd\tsp, sp, #CALL_CONTEXT_SIZE\n+\n+\tblr     x9\t\t\t\/* call fn *\/\n+\n+\tldp\tx3, x4, [x29, #16]\t\/* reload rvalue and flags *\/\n+\n+\t\/* Partially deconstruct the stack frame.  *\/\n+\tmov     sp, x29\n+\tcfi_def_cfa_register (sp)\n+\tldp     x29, x30, [x29]\n+\n+\t\/* Save the return value as directed.  *\/\n+\tadr\tx5, 0f\n+\tand\tw4, w4, #AARCH64_RET_MASK\n+\tadd\tx5, x5, x4, lsl #3\n+\tbr\tx5\n+\n+\t\/* Note that each table entry is 2 insns, and thus 8 bytes.\n+\t   For integer data, note that we're storing into ffi_arg\n+\t   and therefore we want to extend to 64 bits; these types\n+\t   have two consecutive entries allocated for them.  *\/\n+\t.align\t4\n+0:\tret\t\t\t\t\/* VOID *\/\n+\tnop\n+1:\tstr\tx0, [x3]\t\t\/* INT64 *\/\n+\tret\n+2:\tstp\tx0, x1, [x3]\t\t\/* INT128 *\/\n+\tret\n+3:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tret\n+4:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tret\n+5:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tret\n+6:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tret\n+7:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tret\n+8:\tst4\t{ v0.s, v1.s, v2.s, v3.s }[0], [x3]\t\/* S4 *\/\n+\tret\n+9:\tst3\t{ v0.s, v1.s, v2.s }[0], [x3]\t\/* S3 *\/\n+\tret\n+10:\tstp\ts0, s1, [x3]\t\t\/* S2 *\/\n+\tret\n+11:\tstr\ts0, [x3]\t\t\/* S1 *\/\n+\tret\n+12:\tst4\t{ v0.d, v1.d, v2.d, v3.d }[0], [x3]\t\/* D4 *\/\n+\tret\n+13:\tst3\t{ v0.d, v1.d, v2.d }[0], [x3]\t\/* D3 *\/\n+\tret\n+14:\tstp\td0, d1, [x3]\t\t\/* D2 *\/\n+\tret\n+15:\tstr\td0, [x3]\t\t\/* D1 *\/\n+\tret\n+16:\tstr\tq3, [x3, #48]\t\t\/* Q4 *\/\n+\tnop\n+17:\tstr\tq2, [x3, #32]\t\t\/* Q3 *\/\n+\tnop\n+18:\tstp\tq0, q1, [x3]\t\t\/* Q2 *\/\n+\tret\n+19:\tstr\tq0, [x3]\t\t\/* Q1 *\/\n+\tret\n+20:\tuxtb\tw0, w0\t\t\t\/* UINT8 *\/\n+\tstr\tx0, [x3]\n+21:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+22:\tuxth\tw0, w0\t\t\t\/* UINT16 *\/\n+\tstr\tx0, [x3]\n+23:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+24:\tmov\tw0, w0\t\t\t\/* UINT32 *\/\n+\tstr\tx0, [x3]\n+25:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+26:\tsxtb\tx0, w0\t\t\t\/* SINT8 *\/\n+\tstr\tx0, [x3]\n+27:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+28:\tsxth\tx0, w0\t\t\t\/* SINT16 *\/\n+\tstr\tx0, [x3]\n+29:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+30:\tsxtw\tx0, w0\t\t\t\/* SINT32 *\/\n+\tstr\tx0, [x3]\n+31:\tret\t\t\t\t\/* reserved *\/\n+\tnop\n+\n+\tcfi_endproc\n+\n+\t.globl\tCNAME(ffi_call_SYSV)\n+\tFFI_HIDDEN(CNAME(ffi_call_SYSV))\n+#ifdef __ELF__\n+\t.type\tCNAME(ffi_call_SYSV), #function\n+\t.size CNAME(ffi_call_SYSV), .-CNAME(ffi_call_SYSV)\n+#endif\n+\n+\/* ffi_closure_SYSV\n+\n+   Closure invocation glue. This is the low level code invoked directly by\n+   the closure trampoline to setup and call a closure.\n+\n+   On entry x17 points to a struct ffi_closure, x16 has been clobbered\n+   all other registers are preserved.\n+\n+   We allocate a call context and save the argument passing registers,\n+   then invoked the generic C ffi_closure_SYSV_inner() function to do all\n+   the real work, on return we load the result passing registers back from\n+   the call context.\n+*\/\n+\n+#define ffi_closure_SYSV_FS (8*2 + CALL_CONTEXT_SIZE + 64)\n+\n+\t.align 4\n+CNAME(ffi_closure_SYSV_V):\n+\tcfi_startproc\n+\tstp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!\n+\tcfi_adjust_cfa_offset (ffi_closure_SYSV_FS)\n+\tcfi_rel_offset (x29, 0)\n+\tcfi_rel_offset (x30, 8)\n+\n+\t\/* Save the argument passing vector registers.  *\/\n+\tstp     q0, q1, [sp, #16 + 0]\n+\tstp     q2, q3, [sp, #16 + 32]\n+\tstp     q4, q5, [sp, #16 + 64]\n+\tstp     q6, q7, [sp, #16 + 96]\n+\tb\t0f\n+\tcfi_endproc\n+\n+\t.globl\tCNAME(ffi_closure_SYSV_V)\n+\tFFI_HIDDEN(CNAME(ffi_closure_SYSV_V))\n+#ifdef __ELF__\n+\t.type\tCNAME(ffi_closure_SYSV_V), #function\n+\t.size\tCNAME(ffi_closure_SYSV_V), . - CNAME(ffi_closure_SYSV_V)\n+#endif\n+\n+\t.align\t4\n+\tcfi_startproc\n+CNAME(ffi_closure_SYSV):\n+\tstp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!\n+\tcfi_adjust_cfa_offset (ffi_closure_SYSV_FS)\n+\tcfi_rel_offset (x29, 0)\n+\tcfi_rel_offset (x30, 8)\n+0:\n+\tmov     x29, sp\n+\n+\t\/* Save the argument passing core registers.  *\/\n+\tstp     x0, x1, [sp, #16 + 16*N_V_ARG_REG + 0]\n+\tstp     x2, x3, [sp, #16 + 16*N_V_ARG_REG + 16]\n+\tstp     x4, x5, [sp, #16 + 16*N_V_ARG_REG + 32]\n+\tstp     x6, x7, [sp, #16 + 16*N_V_ARG_REG + 48]\n+\n+\t\/* Load ffi_closure_inner arguments.  *\/\n+\tldp\tPTR_REG(0), PTR_REG(1), [x17, #FFI_TRAMPOLINE_CLOSURE_OFFSET]\t\/* load cif, fn *\/\n+\tldr\tPTR_REG(2), [x17, #FFI_TRAMPOLINE_CLOSURE_OFFSET+PTR_SIZE*2]\t\/* load user_data *\/\n+.Ldo_closure:\n+\tadd\tx3, sp, #16\t\t\t\t\/* load context *\/\n+\tadd\tx4, sp, #ffi_closure_SYSV_FS\t\t\/* load stack *\/\n+\tadd\tx5, sp, #16+CALL_CONTEXT_SIZE\t\t\/* load rvalue *\/\n+\tmov\tx6, x8\t\t\t\t\t\/* load struct_rval *\/\n+\tbl      CNAME(ffi_closure_SYSV_inner)\n+\n+\t\/* Load the return value as directed.  *\/\n+\tadr\tx1, 0f\n+\tand\tw0, w0, #AARCH64_RET_MASK\n+\tadd\tx1, x1, x0, lsl #3\n+\tadd\tx3, sp, #16+CALL_CONTEXT_SIZE\n+\tbr\tx1\n+\n+\t\/* Note that each table entry is 2 insns, and thus 8 bytes.  *\/\n+\t.align\t4\n+0:\tb\t99f\t\t\t\/* VOID *\/\n+\tnop\n+1:\tldr\tx0, [x3]\t\t\/* INT64 *\/\n+\tb\t99f\n+2:\tldp\tx0, x1, [x3]\t\t\/* INT128 *\/\n+\tb\t99f\n+3:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tnop\n+4:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tnop\n+5:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tnop\n+6:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tnop\n+7:\tbrk\t#1000\t\t\t\/* UNUSED *\/\n+\tnop\n+8:\tldr\ts3, [x3, #12]\t\t\/* S4 *\/\n+\tnop\n+9:\tldr\ts2, [x3, #8]\t\t\/* S3 *\/\n+\tnop\n+10:\tldp\ts0, s1, [x3]\t\t\/* S2 *\/\n+\tb\t99f\n+11:\tldr\ts0, [x3]\t\t\/* S1 *\/\n+\tb\t99f\n+12:\tldr\td3, [x3, #24]\t\t\/* D4 *\/\n+\tnop\n+13:\tldr\td2, [x3, #16]\t\t\/* D3 *\/\n+\tnop\n+14:\tldp\td0, d1, [x3]\t\t\/* D2 *\/\n+\tb\t99f\n+15:\tldr\td0, [x3]\t\t\/* D1 *\/\n+\tb\t99f\n+16:\tldr\tq3, [x3, #48]\t\t\/* Q4 *\/\n+\tnop\n+17:\tldr\tq2, [x3, #32]\t\t\/* Q3 *\/\n+\tnop\n+18:\tldp\tq0, q1, [x3]\t\t\/* Q2 *\/\n+\tb\t99f\n+19:\tldr\tq0, [x3]\t\t\/* Q1 *\/\n+\tb\t99f\n+20:\tldrb\tw0, [x3, #BE(7)]\t\/* UINT8 *\/\n+\tb\t99f\n+21:\tbrk\t#1000\t\t\t\/* reserved *\/\n+\tnop\n+22:\tldrh\tw0, [x3, #BE(6)]\t\/* UINT16 *\/\n+\tb\t99f\n+23:\tbrk\t#1000\t\t\t\/* reserved *\/\n+\tnop\n+24:\tldr\tw0, [x3, #BE(4)]\t\/* UINT32 *\/\n+\tb\t99f\n+25:\tbrk\t#1000\t\t\t\/* reserved *\/\n+\tnop\n+26:\tldrsb\tx0, [x3, #BE(7)]\t\/* SINT8 *\/\n+\tb\t99f\n+27:\tbrk\t#1000\t\t\t\/* reserved *\/\n+\tnop\n+28:\tldrsh\tx0, [x3, #BE(6)]\t\/* SINT16 *\/\n+\tb\t99f\n+29:\tbrk\t#1000\t\t\t\/* reserved *\/\n+\tnop\n+30:\tldrsw\tx0, [x3, #BE(4)]\t\/* SINT32 *\/\n+\tnop\n+31:\t\t\t\t\t\/* reserved *\/\n+99:\tldp     x29, x30, [sp], #ffi_closure_SYSV_FS\n+\tcfi_adjust_cfa_offset (-ffi_closure_SYSV_FS)\n+\tcfi_restore (x29)\n+\tcfi_restore (x30)\n+\tret\n+\tcfi_endproc\n+\n+\t.globl\tCNAME(ffi_closure_SYSV)\n+\tFFI_HIDDEN(CNAME(ffi_closure_SYSV))\n+#ifdef __ELF__\n+\t.type\tCNAME(ffi_closure_SYSV), #function\n+\t.size\tCNAME(ffi_closure_SYSV), . - CNAME(ffi_closure_SYSV)\n+#endif\n+\n+#if FFI_EXEC_TRAMPOLINE_TABLE\n+\n+#ifdef __MACH__\n+#include <mach\/machine\/vm_param.h>\n+    .align PAGE_MAX_SHIFT\n+CNAME(ffi_closure_trampoline_table_page):\n+    .rept PAGE_MAX_SIZE \/ FFI_TRAMPOLINE_SIZE\n+    adr x16, -PAGE_MAX_SIZE\n+    ldp x17, x16, [x16]\n+    br x16\n+\tnop\t\t\/* each entry in the trampoline config page is 2*sizeof(void*) so the trampoline itself cannot be smaller that 16 bytes *\/\n+    .endr\n+\n+    .globl CNAME(ffi_closure_trampoline_table_page)\n+    FFI_HIDDEN(CNAME(ffi_closure_trampoline_table_page))\n+    #ifdef __ELF__\n+    \t.type\tCNAME(ffi_closure_trampoline_table_page), #function\n+    \t.size\tCNAME(ffi_closure_trampoline_table_page), . - CNAME(ffi_closure_trampoline_table_page)\n+    #endif\n+#endif\n+\n+#endif \/* FFI_EXEC_TRAMPOLINE_TABLE *\/\n+\n+#ifdef FFI_GO_CLOSURES\n+\t.align 4\n+CNAME(ffi_go_closure_SYSV_V):\n+\tcfi_startproc\n+\tstp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!\n+\tcfi_adjust_cfa_offset (ffi_closure_SYSV_FS)\n+\tcfi_rel_offset (x29, 0)\n+\tcfi_rel_offset (x30, 8)\n+\n+\t\/* Save the argument passing vector registers.  *\/\n+\tstp     q0, q1, [sp, #16 + 0]\n+\tstp     q2, q3, [sp, #16 + 32]\n+\tstp     q4, q5, [sp, #16 + 64]\n+\tstp     q6, q7, [sp, #16 + 96]\n+\tb\t0f\n+\tcfi_endproc\n+\n+\t.globl\tCNAME(ffi_go_closure_SYSV_V)\n+\tFFI_HIDDEN(CNAME(ffi_go_closure_SYSV_V))\n+#ifdef __ELF__\n+\t.type\tCNAME(ffi_go_closure_SYSV_V), #function\n+\t.size\tCNAME(ffi_go_closure_SYSV_V), . - CNAME(ffi_go_closure_SYSV_V)\n+#endif\n+\n+\t.align\t4\n+\tcfi_startproc\n+CNAME(ffi_go_closure_SYSV):\n+\tstp     x29, x30, [sp, #-ffi_closure_SYSV_FS]!\n+\tcfi_adjust_cfa_offset (ffi_closure_SYSV_FS)\n+\tcfi_rel_offset (x29, 0)\n+\tcfi_rel_offset (x30, 8)\n+0:\n+\tmov     x29, sp\n+\n+\t\/* Save the argument passing core registers.  *\/\n+\tstp     x0, x1, [sp, #16 + 16*N_V_ARG_REG + 0]\n+\tstp     x2, x3, [sp, #16 + 16*N_V_ARG_REG + 16]\n+\tstp     x4, x5, [sp, #16 + 16*N_V_ARG_REG + 32]\n+\tstp     x6, x7, [sp, #16 + 16*N_V_ARG_REG + 48]\n+\n+\t\/* Load ffi_closure_inner arguments.  *\/\n+\tldp\tPTR_REG(0), PTR_REG(1), [x18, #PTR_SIZE]\/* load cif, fn *\/\n+\tmov\tx2, x18\t\t\t\t\t\/* load user_data *\/\n+\tb\t.Ldo_closure\n+\tcfi_endproc\n+\n+\t.globl\tCNAME(ffi_go_closure_SYSV)\n+\tFFI_HIDDEN(CNAME(ffi_go_closure_SYSV))\n+#ifdef __ELF__\n+\t.type\tCNAME(ffi_go_closure_SYSV), #function\n+\t.size\tCNAME(ffi_go_closure_SYSV), . - CNAME(ffi_go_closure_SYSV)\n+#endif\n+#endif \/* FFI_GO_CLOSURES *\/\n+#endif \/* __arm64__ *\/\n+\n+#if defined __ELF__ && defined __linux__\n+\t.section .note.GNU-stack,\"\",%progbits\n+#endif\n+\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/aarch64\/sysv.S","additions":440,"deletions":0,"binary":false,"changes":440,"status":"added"},{"patch":"","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/x86\/ffitarget.h","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/include\/ffitarget.h","status":"renamed"},{"patch":"@@ -0,0 +1,1138 @@\n+\/* -----------------------------------------------------------------------\n+   sysv.S - Copyright (c) 2017  Anthony Green\n+          - Copyright (c) 2013  The Written Word, Inc.\n+          - Copyright (c) 1996,1998,2001-2003,2005,2008,2010  Red Hat, Inc.\n+   \n+   X86 Foreign Function Interface \n+\n+   Permission is hereby granted, free of charge, to any person obtaining\n+   a copy of this software and associated documentation files (the\n+   ``Software''), to deal in the Software without restriction, including\n+   without limitation the rights to use, copy, modify, merge, publish,\n+   distribute, sublicense, and\/or sell copies of the Software, and to\n+   permit persons to whom the Software is furnished to do so, subject to\n+   the following conditions:\n+\n+   The above copyright notice and this permission notice shall be included\n+   in all copies or substantial portions of the Software.\n+\n+   THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,\n+   EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+   MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n+   NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n+   HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n+   WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+   DEALINGS IN THE SOFTWARE.\n+   ----------------------------------------------------------------------- *\/\n+\n+#ifdef __i386__\n+#ifndef _MSC_VER\n+\n+#define LIBFFI_ASM\t\n+#include <fficonfig.h>\n+#include <ffi.h>\n+#include \"internal.h\"\n+\n+#define C2(X, Y)  X ## Y\n+#define C1(X, Y)  C2(X, Y)\n+#ifdef __USER_LABEL_PREFIX__\n+# define C(X)     C1(__USER_LABEL_PREFIX__, X)\n+#else\n+# define C(X)     X\n+#endif\n+\n+#ifdef X86_DARWIN\n+# define L(X)     C1(L, X)\n+#else\n+# define L(X)     C1(.L, X)\n+#endif\n+\n+#ifdef __ELF__\n+# define ENDF(X)  .type\tX,@function; .size X, . - X\n+#else\n+# define ENDF(X)\n+#endif\n+\n+\/* Handle win32 fastcall name mangling.  *\/\n+#ifdef X86_WIN32\n+# define ffi_call_i386\t\t\"@ffi_call_i386@8\"\n+# define ffi_closure_inner\t\"@ffi_closure_inner@8\"\n+#else\n+# define ffi_call_i386\t\tC(ffi_call_i386)\n+# define ffi_closure_inner\tC(ffi_closure_inner)\n+#endif\n+\n+\/* This macro allows the safe creation of jump tables without an\n+   actual table.  The entry points into the table are all 8 bytes.\n+   The use of ORG asserts that we're at the correct location.  *\/\n+\/* ??? The clang assembler doesn't handle .org with symbolic expressions.  *\/\n+#if defined(__clang__) || defined(__APPLE__) || (defined (__sun__) && defined(__svr4__))\n+# define E(BASE, X)\t.balign 8\n+#else\n+# define E(BASE, X)\t.balign 8; .org BASE + X * 8\n+#endif\n+\n+\t.text\n+\t.balign\t16\n+\t.globl\tffi_call_i386\n+\tFFI_HIDDEN(ffi_call_i386)\n+\n+\/* This is declared as\n+\n+   void ffi_call_i386(struct call_frame *frame, char *argp)\n+        __attribute__((fastcall));\n+\n+   Thus the arguments are present in\n+\n+        ecx: frame\n+        edx: argp\n+*\/\n+\n+ffi_call_i386:\n+L(UW0):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+#if !HAVE_FASTCALL\n+\tmovl\t4(%esp), %ecx\n+\tmovl\t8(%esp), %edx\n+#endif\n+\tmovl\t(%esp), %eax\t\t\/* move the return address *\/\n+\tmovl\t%ebp, (%ecx)\t\t\/* store %ebp into local frame *\/\n+\tmovl\t%eax, 4(%ecx)\t\t\/* store retaddr into local frame *\/\n+\n+\t\/* New stack frame based off ebp.  This is a itty bit of unwind\n+\t   trickery in that the CFA *has* changed.  There is no easy way\n+\t   to describe it correctly on entry to the function.  Fortunately,\n+\t   it doesn't matter too much since at all points we can correctly\n+\t   unwind back to ffi_call.  Note that the location to which we\n+\t   moved the return address is (the new) CFA-4, so from the\n+\t   perspective of the unwind info, it hasn't moved.  *\/\n+\tmovl\t%ecx, %ebp\n+L(UW1):\n+\t# cfi_def_cfa(%ebp, 8)\n+\t# cfi_rel_offset(%ebp, 0)\n+\n+\tmovl\t%edx, %esp\t\t\/* set outgoing argument stack *\/\n+\tmovl\t20+R_EAX*4(%ebp), %eax\t\/* set register arguments *\/\n+\tmovl\t20+R_EDX*4(%ebp), %edx\n+\tmovl\t20+R_ECX*4(%ebp), %ecx\n+\n+\tcall\t*8(%ebp)\n+\n+\tmovl\t12(%ebp), %ecx\t\t\/* load return type code *\/\n+\tmovl\t%ebx, 8(%ebp)\t\t\/* preserve %ebx *\/\n+L(UW2):\n+\t# cfi_rel_offset(%ebx, 8)\n+\n+\tandl\t$X86_RET_TYPE_MASK, %ecx\n+#ifdef __PIC__\n+\tcall\tC(__x86.get_pc_thunk.bx)\n+L(pc1):\n+\tleal\tL(store_table)-L(pc1)(%ebx, %ecx, 8), %ebx\n+#else\n+\tleal\tL(store_table)(,%ecx, 8), %ebx\n+#endif\n+\tmovl\t16(%ebp), %ecx\t\t\/* load result address *\/\n+\t_CET_NOTRACK jmp *%ebx\n+\n+\t.balign\t8\n+L(store_table):\n+E(L(store_table), X86_RET_FLOAT)\n+\tfstps\t(%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_DOUBLE)\n+\tfstpl\t(%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_LDOUBLE)\n+\tfstpt\t(%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_SINT8)\n+\tmovsbl\t%al, %eax\n+\tmov\t%eax, (%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_SINT16)\n+\tmovswl\t%ax, %eax\n+\tmov\t%eax, (%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_UINT8)\n+\tmovzbl\t%al, %eax\n+\tmov\t%eax, (%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_UINT16)\n+\tmovzwl\t%ax, %eax\n+\tmov\t%eax, (%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_INT64)\n+\tmovl\t%edx, 4(%ecx)\n+\t\/* fallthru *\/\n+E(L(store_table), X86_RET_INT32)\n+\tmovl\t%eax, (%ecx)\n+\t\/* fallthru *\/\n+E(L(store_table), X86_RET_VOID)\n+L(e1):\n+\tmovl\t8(%ebp), %ebx\n+\tmovl\t%ebp, %esp\n+\tpopl\t%ebp\n+L(UW3):\n+\t# cfi_remember_state\n+\t# cfi_def_cfa(%esp, 4)\n+\t# cfi_restore(%ebx)\n+\t# cfi_restore(%ebp)\n+\tret\n+L(UW4):\n+\t# cfi_restore_state\n+\n+E(L(store_table), X86_RET_STRUCTPOP)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_STRUCTARG)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_STRUCT_1B)\n+\tmovb\t%al, (%ecx)\n+\tjmp\tL(e1)\n+E(L(store_table), X86_RET_STRUCT_2B)\n+\tmovw\t%ax, (%ecx)\n+\tjmp\tL(e1)\n+\n+\t\/* Fill out the table so that bad values are predictable.  *\/\n+E(L(store_table), X86_RET_UNUSED14)\n+\tud2\n+E(L(store_table), X86_RET_UNUSED15)\n+\tud2\n+\n+L(UW5):\n+\t# cfi_endproc\n+ENDF(ffi_call_i386)\n+\n+\/* The inner helper is declared as\n+\n+   void ffi_closure_inner(struct closure_frame *frame, char *argp)\n+\t__attribute_((fastcall))\n+\n+   Thus the arguments are placed in\n+\n+\tecx:\tframe\n+\tedx:\targp\n+*\/\n+\n+\/* Macros to help setting up the closure_data structure.  *\/\n+\n+#if HAVE_FASTCALL\n+# define closure_FS\t(40 + 4)\n+# define closure_CF\t0\n+#else\n+# define closure_FS\t(8 + 40 + 12)\n+# define closure_CF\t8\n+#endif\n+\n+#define FFI_CLOSURE_SAVE_REGS\t\t\\\n+\tmovl\t%eax, closure_CF+16+R_EAX*4(%esp);\t\\\n+\tmovl\t%edx, closure_CF+16+R_EDX*4(%esp);\t\\\n+\tmovl\t%ecx, closure_CF+16+R_ECX*4(%esp)\n+\n+#define FFI_CLOSURE_COPY_TRAMP_DATA\t\t\t\t\t\\\n+\tmovl\tFFI_TRAMPOLINE_SIZE(%eax), %edx;\t\/* copy cif *\/\t\\\n+\tmovl\tFFI_TRAMPOLINE_SIZE+4(%eax), %ecx;\t\/* copy fun *\/\t\\\n+\tmovl\tFFI_TRAMPOLINE_SIZE+8(%eax), %eax;\t\/* copy user_data *\/ \\\n+\tmovl\t%edx, closure_CF+28(%esp);\t\t\t\t\\\n+\tmovl\t%ecx, closure_CF+32(%esp);\t\t\t\t\\\n+\tmovl\t%eax, closure_CF+36(%esp)\n+\n+#if HAVE_FASTCALL\n+# define FFI_CLOSURE_PREP_CALL\t\t\t\t\t\t\\\n+\tmovl\t%esp, %ecx;\t\t\t\/* load closure_data *\/\t\\\n+\tleal\tclosure_FS+4(%esp), %edx;\t\/* load incoming stack *\/\n+#else\n+# define FFI_CLOSURE_PREP_CALL\t\t\t\t\t\t\\\n+\tleal\tclosure_CF(%esp), %ecx;\t\t\/* load closure_data *\/\t\\\n+\tleal\tclosure_FS+4(%esp), %edx;\t\/* load incoming stack *\/ \\\n+\tmovl\t%ecx, (%esp);\t\t\t\t\t\t\\\n+\tmovl\t%edx, 4(%esp)\n+#endif\n+\n+#define FFI_CLOSURE_CALL_INNER(UWN) \\\n+\tcall\tffi_closure_inner\n+\n+#define FFI_CLOSURE_MASK_AND_JUMP(N, UW)\t\t\t\t\\\n+\tandl\t$X86_RET_TYPE_MASK, %eax;\t\t\t\t\\\n+\tleal\tL(C1(load_table,N))(, %eax, 8), %edx;\t\t\t\\\n+\tmovl\tclosure_CF(%esp), %eax;\t\t\/* optimiztic load *\/\t\\\n+\t_CET_NOTRACK jmp *%edx\n+\n+#ifdef __PIC__\n+# if defined X86_DARWIN || defined HAVE_HIDDEN_VISIBILITY_ATTRIBUTE\n+#  undef FFI_CLOSURE_MASK_AND_JUMP\n+#  define FFI_CLOSURE_MASK_AND_JUMP(N, UW)\t\t\t\t\\\n+\tandl\t$X86_RET_TYPE_MASK, %eax;\t\t\t\t\\\n+\tcall\tC(__x86.get_pc_thunk.dx);\t\t\t\t\\\n+L(C1(pc,N)):\t\t\t\t\t\t\t\t\\\n+\tleal\tL(C1(load_table,N))-L(C1(pc,N))(%edx, %eax, 8), %edx;\t\\\n+\tmovl\tclosure_CF(%esp), %eax;\t\t\/* optimiztic load *\/\t\\\n+\t_CET_NOTRACK jmp *%edx\n+# else\n+#  define FFI_CLOSURE_CALL_INNER_SAVE_EBX\n+#  undef FFI_CLOSURE_CALL_INNER\n+#  define FFI_CLOSURE_CALL_INNER(UWN)\t\t\t\t\t\\\n+\tmovl\t%ebx, 40(%esp);\t\t\t\/* save ebx *\/\t\t\\\n+L(C1(UW,UWN)):\t\t\t\t\t\t\t\t\\\n+\t\/* cfi_rel_offset(%ebx, 40); *\/\t\t\t\t\t\\\n+\tcall\tC(__x86.get_pc_thunk.bx);\t\/* load got register *\/\t\\\n+\taddl\t$C(_GLOBAL_OFFSET_TABLE_), %ebx;\t\t\t\\\n+\tcall\tffi_closure_inner@PLT\n+#  undef FFI_CLOSURE_MASK_AND_JUMP\n+#  define FFI_CLOSURE_MASK_AND_JUMP(N, UWN)\t\t\t\t\\\n+\tandl\t$X86_RET_TYPE_MASK, %eax;\t\t\t\t\\\n+\tleal\tL(C1(load_table,N))@GOTOFF(%ebx, %eax, 8), %edx;\t\\\n+\tmovl\t40(%esp), %ebx;\t\t\t\/* restore ebx *\/\t\\\n+L(C1(UW,UWN)):\t\t\t\t\t\t\t\t\\\n+\t\/* cfi_restore(%ebx); *\/\t\t\t\t\t\\\n+\tmovl\tclosure_CF(%esp), %eax;\t\t\/* optimiztic load *\/\t\\\n+\t_CET_NOTRACK jmp *%edx\n+# endif \/* DARWIN || HIDDEN *\/\n+#endif \/* __PIC__ *\/\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_go_closure_EAX)\n+\tFFI_HIDDEN(C(ffi_go_closure_EAX))\n+C(ffi_go_closure_EAX):\n+L(UW6):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS, %esp\n+L(UW7):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\tFFI_CLOSURE_SAVE_REGS\n+\tmovl\t4(%eax), %edx\t\t\t\/* copy cif *\/\n+\tmovl\t8(%eax), %ecx\t\t\t\/* copy fun *\/\n+\tmovl\t%edx, closure_CF+28(%esp)\n+\tmovl\t%ecx, closure_CF+32(%esp)\n+\tmovl\t%eax, closure_CF+36(%esp)\t\/* closure is user_data *\/\n+\tjmp\tL(do_closure_i386)\n+L(UW8):\n+\t# cfi_endproc\n+ENDF(C(ffi_go_closure_EAX))\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_go_closure_ECX)\n+\tFFI_HIDDEN(C(ffi_go_closure_ECX))\n+C(ffi_go_closure_ECX):\n+L(UW9):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS, %esp\n+L(UW10):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\tFFI_CLOSURE_SAVE_REGS\n+\tmovl\t4(%ecx), %edx\t\t\t\/* copy cif *\/\n+\tmovl\t8(%ecx), %eax\t\t\t\/* copy fun *\/\n+\tmovl\t%edx, closure_CF+28(%esp)\n+\tmovl\t%eax, closure_CF+32(%esp)\n+\tmovl\t%ecx, closure_CF+36(%esp)\t\/* closure is user_data *\/\n+\tjmp\tL(do_closure_i386)\n+L(UW11):\n+\t# cfi_endproc\n+ENDF(C(ffi_go_closure_ECX))\n+\n+\/* The closure entry points are reached from the ffi_closure trampoline.\n+   On entry, %eax contains the address of the ffi_closure.  *\/\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_closure_i386)\n+\tFFI_HIDDEN(C(ffi_closure_i386))\n+\n+C(ffi_closure_i386):\n+L(UW12):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS, %esp\n+L(UW13):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\n+\tFFI_CLOSURE_SAVE_REGS\n+\tFFI_CLOSURE_COPY_TRAMP_DATA\n+\n+\t\/* Entry point from preceeding Go closures.  *\/\n+L(do_closure_i386):\n+\n+\tFFI_CLOSURE_PREP_CALL\n+\tFFI_CLOSURE_CALL_INNER(14)\n+\tFFI_CLOSURE_MASK_AND_JUMP(2, 15)\n+\n+\t.balign\t8\n+L(load_table2):\n+E(L(load_table2), X86_RET_FLOAT)\n+\tflds\tclosure_CF(%esp)\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_DOUBLE)\n+\tfldl\tclosure_CF(%esp)\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_LDOUBLE)\n+\tfldt\tclosure_CF(%esp)\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_SINT8)\n+\tmovsbl\t%al, %eax\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_SINT16)\n+\tmovswl\t%ax, %eax\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_UINT8)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_UINT16)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_INT64)\n+\tmovl\tclosure_CF+4(%esp), %edx\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_INT32)\n+\tnop\n+\t\/* fallthru *\/\n+E(L(load_table2), X86_RET_VOID)\n+L(e2):\n+\taddl\t$closure_FS, %esp\n+L(UW16):\n+\t# cfi_adjust_cfa_offset(-closure_FS)\n+\tret\n+L(UW17):\n+\t# cfi_adjust_cfa_offset(closure_FS)\n+E(L(load_table2), X86_RET_STRUCTPOP)\n+\taddl\t$closure_FS, %esp\n+L(UW18):\n+\t# cfi_adjust_cfa_offset(-closure_FS)\n+\tret\t$4\n+L(UW19):\n+\t# cfi_adjust_cfa_offset(closure_FS)\n+E(L(load_table2), X86_RET_STRUCTARG)\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_STRUCT_1B)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e2)\n+E(L(load_table2), X86_RET_STRUCT_2B)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e2)\n+\n+\t\/* Fill out the table so that bad values are predictable.  *\/\n+E(L(load_table2), X86_RET_UNUSED14)\n+\tud2\n+E(L(load_table2), X86_RET_UNUSED15)\n+\tud2\n+\n+L(UW20):\n+\t# cfi_endproc\n+ENDF(C(ffi_closure_i386))\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_go_closure_STDCALL)\n+\tFFI_HIDDEN(C(ffi_go_closure_STDCALL))\n+C(ffi_go_closure_STDCALL):\n+L(UW21):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS, %esp\n+L(UW22):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\tFFI_CLOSURE_SAVE_REGS\n+\tmovl\t4(%ecx), %edx\t\t\t\/* copy cif *\/\n+\tmovl\t8(%ecx), %eax\t\t\t\/* copy fun *\/\n+\tmovl\t%edx, closure_CF+28(%esp)\n+\tmovl\t%eax, closure_CF+32(%esp)\n+\tmovl\t%ecx, closure_CF+36(%esp)\t\/* closure is user_data *\/\n+\tjmp\tL(do_closure_STDCALL)\n+L(UW23):\n+\t# cfi_endproc\n+ENDF(C(ffi_go_closure_STDCALL))\n+\n+\/* For REGISTER, we have no available parameter registers, and so we\n+   enter here having pushed the closure onto the stack.  *\/\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_closure_REGISTER)\n+\tFFI_HIDDEN(C(ffi_closure_REGISTER))\n+C(ffi_closure_REGISTER):\n+L(UW24):\n+\t# cfi_startproc\n+\t# cfi_def_cfa(%esp, 8)\n+\t# cfi_offset(%eip, -8)\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS-4, %esp\n+L(UW25):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\tFFI_CLOSURE_SAVE_REGS\n+\tmovl\tclosure_FS-4(%esp), %ecx\t\/* load retaddr *\/\n+\tmovl\tclosure_FS(%esp), %eax\t\t\/* load closure *\/\n+\tmovl\t%ecx, closure_FS(%esp)\t\t\/* move retaddr *\/\n+\tjmp\tL(do_closure_REGISTER)\n+L(UW26):\n+\t# cfi_endproc\n+ENDF(C(ffi_closure_REGISTER))\n+\n+\/* For STDCALL (and others), we need to pop N bytes of arguments off\n+   the stack following the closure.  The amount needing to be popped\n+   is returned to us from ffi_closure_inner.  *\/\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_closure_STDCALL)\n+\tFFI_HIDDEN(C(ffi_closure_STDCALL))\n+C(ffi_closure_STDCALL):\n+L(UW27):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$closure_FS, %esp\n+L(UW28):\n+\t# cfi_def_cfa_offset(closure_FS + 4)\n+\n+\tFFI_CLOSURE_SAVE_REGS\n+\n+\t\/* Entry point from ffi_closure_REGISTER.  *\/\n+L(do_closure_REGISTER):\n+\n+\tFFI_CLOSURE_COPY_TRAMP_DATA\n+\n+\t\/* Entry point from preceeding Go closure.  *\/\n+L(do_closure_STDCALL):\n+\n+\tFFI_CLOSURE_PREP_CALL\n+\tFFI_CLOSURE_CALL_INNER(29)\n+\n+\tmovl\t%eax, %ecx\n+\tshrl\t$X86_RET_POP_SHIFT, %ecx\t\/* isolate pop count *\/\n+\tleal\tclosure_FS(%esp, %ecx), %ecx\t\/* compute popped esp *\/\n+\tmovl\tclosure_FS(%esp), %edx\t\t\/* move return address *\/\n+\tmovl\t%edx, (%ecx)\n+\n+\t\/* From this point on, the value of %esp upon return is %ecx+4,\n+\t   and we've copied the return address to %ecx to make return easy.\n+\t   There's no point in representing this in the unwind info, as\n+\t   there is always a window between the mov and the ret which\n+\t   will be wrong from one point of view or another.  *\/\n+\n+\tFFI_CLOSURE_MASK_AND_JUMP(3, 30)\n+\n+\t.balign\t8\n+L(load_table3):\n+E(L(load_table3), X86_RET_FLOAT)\n+\tflds    closure_CF(%esp)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_DOUBLE)\n+\tfldl    closure_CF(%esp)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_LDOUBLE)\n+\tfldt    closure_CF(%esp)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_SINT8)\n+\tmovsbl  %al, %eax\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_SINT16)\n+\tmovswl  %ax, %eax\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_UINT8)\n+\tmovzbl  %al, %eax\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_UINT16)\n+\tmovzwl  %ax, %eax\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_INT64)\n+\tmovl\tclosure_CF+4(%esp), %edx\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_INT32)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_VOID)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_STRUCTPOP)\n+\tmovl    %ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_STRUCTARG)\n+\tmovl\t%ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_STRUCT_1B)\n+\tmovzbl\t%al, %eax\n+\tmovl\t%ecx, %esp\n+\tret\n+E(L(load_table3), X86_RET_STRUCT_2B)\n+\tmovzwl\t%ax, %eax\n+\tmovl\t%ecx, %esp\n+\tret\n+\n+\t\/* Fill out the table so that bad values are predictable.  *\/\n+E(L(load_table3), X86_RET_UNUSED14)\n+\tud2\n+E(L(load_table3), X86_RET_UNUSED15)\n+\tud2\n+\n+L(UW31):\n+\t# cfi_endproc\n+ENDF(C(ffi_closure_STDCALL))\n+\n+#if !FFI_NO_RAW_API\n+\n+#define raw_closure_S_FS\t(16+16+12)\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_closure_raw_SYSV)\n+\tFFI_HIDDEN(C(ffi_closure_raw_SYSV))\n+C(ffi_closure_raw_SYSV):\n+L(UW32):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\tsubl\t$raw_closure_S_FS, %esp\n+L(UW33):\n+\t# cfi_def_cfa_offset(raw_closure_S_FS + 4)\n+\tmovl\t%ebx, raw_closure_S_FS-4(%esp)\n+L(UW34):\n+\t# cfi_rel_offset(%ebx, raw_closure_S_FS-4)\n+\n+\tmovl\tFFI_TRAMPOLINE_SIZE+8(%eax), %edx\t\/* load cl->user_data *\/\n+\tmovl\t%edx, 12(%esp)\n+\tleal\traw_closure_S_FS+4(%esp), %edx\t\t\/* load raw_args *\/\n+\tmovl\t%edx, 8(%esp)\n+\tleal\t16(%esp), %edx\t\t\t\t\/* load &res *\/\n+\tmovl\t%edx, 4(%esp)\n+\tmovl\tFFI_TRAMPOLINE_SIZE(%eax), %ebx\t\t\/* load cl->cif *\/\n+\tmovl\t%ebx, (%esp)\n+\tcall\t*FFI_TRAMPOLINE_SIZE+4(%eax)\t\t\/* call cl->fun *\/\n+\n+\tmovl\t20(%ebx), %eax\t\t\t\t\/* load cif->flags *\/\n+\tandl\t$X86_RET_TYPE_MASK, %eax\n+#ifdef __PIC__\n+\tcall\tC(__x86.get_pc_thunk.bx)\n+L(pc4):\n+\tleal\tL(load_table4)-L(pc4)(%ebx, %eax, 8), %ecx\n+#else\n+\tleal\tL(load_table4)(,%eax, 8), %ecx\n+#endif\n+\tmovl\traw_closure_S_FS-4(%esp), %ebx\n+L(UW35):\n+\t# cfi_restore(%ebx)\n+\tmovl\t16(%esp), %eax\t\t\t\t\/* Optimistic load *\/\n+\tjmp\t*%ecx\n+\n+\t.balign\t8\n+L(load_table4):\n+E(L(load_table4), X86_RET_FLOAT)\n+\tflds\t16(%esp)\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_DOUBLE)\n+\tfldl\t16(%esp)\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_LDOUBLE)\n+\tfldt\t16(%esp)\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_SINT8)\n+\tmovsbl\t%al, %eax\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_SINT16)\n+\tmovswl\t%ax, %eax\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_UINT8)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_UINT16)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_INT64)\n+\tmovl\t16+4(%esp), %edx\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_INT32)\n+\tnop\n+\t\/* fallthru *\/\n+E(L(load_table4), X86_RET_VOID)\n+L(e4):\n+\taddl\t$raw_closure_S_FS, %esp\n+L(UW36):\n+\t# cfi_adjust_cfa_offset(-raw_closure_S_FS)\n+\tret\n+L(UW37):\n+\t# cfi_adjust_cfa_offset(raw_closure_S_FS)\n+E(L(load_table4), X86_RET_STRUCTPOP)\n+\taddl\t$raw_closure_S_FS, %esp\n+L(UW38):\n+\t# cfi_adjust_cfa_offset(-raw_closure_S_FS)\n+\tret\t$4\n+L(UW39):\n+\t# cfi_adjust_cfa_offset(raw_closure_S_FS)\n+E(L(load_table4), X86_RET_STRUCTARG)\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_STRUCT_1B)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e4)\n+E(L(load_table4), X86_RET_STRUCT_2B)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e4)\n+\n+\t\/* Fill out the table so that bad values are predictable.  *\/\n+E(L(load_table4), X86_RET_UNUSED14)\n+\tud2\n+E(L(load_table4), X86_RET_UNUSED15)\n+\tud2\n+\n+L(UW40):\n+\t# cfi_endproc\n+ENDF(C(ffi_closure_raw_SYSV))\n+\n+#define raw_closure_T_FS\t(16+16+8)\n+\n+\t.balign\t16\n+\t.globl\tC(ffi_closure_raw_THISCALL)\n+\tFFI_HIDDEN(C(ffi_closure_raw_THISCALL))\n+C(ffi_closure_raw_THISCALL):\n+L(UW41):\n+\t# cfi_startproc\n+\t_CET_ENDBR\n+\t\/* Rearrange the stack such that %ecx is the first argument.\n+\t   This means moving the return address.  *\/\n+\tpopl\t%edx\n+L(UW42):\n+\t# cfi_def_cfa_offset(0)\n+\t# cfi_register(%eip, %edx)\n+\tpushl\t%ecx\n+L(UW43):\n+\t# cfi_adjust_cfa_offset(4)\n+\tpushl\t%edx\n+L(UW44):\n+\t# cfi_adjust_cfa_offset(4)\n+\t# cfi_rel_offset(%eip, 0)\n+\tsubl\t$raw_closure_T_FS, %esp\n+L(UW45):\n+\t# cfi_adjust_cfa_offset(raw_closure_T_FS)\n+\tmovl\t%ebx, raw_closure_T_FS-4(%esp)\n+L(UW46):\n+\t# cfi_rel_offset(%ebx, raw_closure_T_FS-4)\n+\n+\tmovl\tFFI_TRAMPOLINE_SIZE+8(%eax), %edx\t\/* load cl->user_data *\/\n+\tmovl\t%edx, 12(%esp)\n+\tleal\traw_closure_T_FS+4(%esp), %edx\t\t\/* load raw_args *\/\n+\tmovl\t%edx, 8(%esp)\n+\tleal\t16(%esp), %edx\t\t\t\t\/* load &res *\/\n+\tmovl\t%edx, 4(%esp)\n+\tmovl\tFFI_TRAMPOLINE_SIZE(%eax), %ebx\t\t\/* load cl->cif *\/\n+\tmovl\t%ebx, (%esp)\n+\tcall\t*FFI_TRAMPOLINE_SIZE+4(%eax)\t\t\/* call cl->fun *\/\n+\n+\tmovl\t20(%ebx), %eax\t\t\t\t\/* load cif->flags *\/\n+\tandl\t$X86_RET_TYPE_MASK, %eax\n+#ifdef __PIC__\n+\tcall\tC(__x86.get_pc_thunk.bx)\n+L(pc5):\n+\tleal\tL(load_table5)-L(pc5)(%ebx, %eax, 8), %ecx\n+#else\n+\tleal\tL(load_table5)(,%eax, 8), %ecx\n+#endif\n+\tmovl\traw_closure_T_FS-4(%esp), %ebx\n+L(UW47):\n+\t# cfi_restore(%ebx)\n+\tmovl\t16(%esp), %eax\t\t\t\t\/* Optimistic load *\/\n+\tjmp\t*%ecx\n+\n+\t.balign\t8\n+L(load_table5):\n+E(L(load_table5), X86_RET_FLOAT)\n+\tflds\t16(%esp)\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_DOUBLE)\n+\tfldl\t16(%esp)\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_LDOUBLE)\n+\tfldt\t16(%esp)\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_SINT8)\n+\tmovsbl\t%al, %eax\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_SINT16)\n+\tmovswl\t%ax, %eax\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_UINT8)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_UINT16)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_INT64)\n+\tmovl\t16+4(%esp), %edx\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_INT32)\n+\tnop\n+\t\/* fallthru *\/\n+E(L(load_table5), X86_RET_VOID)\n+L(e5):\n+\taddl\t$raw_closure_T_FS, %esp\n+L(UW48):\n+\t# cfi_adjust_cfa_offset(-raw_closure_T_FS)\n+\t\/* Remove the extra %ecx argument we pushed.  *\/\n+\tret\t$4\n+L(UW49):\n+\t# cfi_adjust_cfa_offset(raw_closure_T_FS)\n+E(L(load_table5), X86_RET_STRUCTPOP)\n+\taddl\t$raw_closure_T_FS, %esp\n+L(UW50):\n+\t# cfi_adjust_cfa_offset(-raw_closure_T_FS)\n+\tret\t$8\n+L(UW51):\n+\t# cfi_adjust_cfa_offset(raw_closure_T_FS)\n+E(L(load_table5), X86_RET_STRUCTARG)\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_STRUCT_1B)\n+\tmovzbl\t%al, %eax\n+\tjmp\tL(e5)\n+E(L(load_table5), X86_RET_STRUCT_2B)\n+\tmovzwl\t%ax, %eax\n+\tjmp\tL(e5)\n+\n+\t\/* Fill out the table so that bad values are predictable.  *\/\n+E(L(load_table5), X86_RET_UNUSED14)\n+\tud2\n+E(L(load_table5), X86_RET_UNUSED15)\n+\tud2\n+\n+L(UW52):\n+\t# cfi_endproc\n+ENDF(C(ffi_closure_raw_THISCALL))\n+\n+#endif \/* !FFI_NO_RAW_API *\/\n+\n+#ifdef X86_DARWIN\n+# define COMDAT(X)\t\t\t\t\t\t\t\\\n+        .section __TEXT,__text,coalesced,pure_instructions;\t\t\\\n+        .weak_definition X;\t\t\t\t\t\t\\\n+        FFI_HIDDEN(X)\n+#elif defined __ELF__ && !(defined(__sun__) && defined(__svr4__))\n+# define COMDAT(X)\t\t\t\t\t\t\t\\\n+\t.section .text.X,\"axG\",@progbits,X,comdat;\t\t\t\\\n+\t.globl\tX;\t\t\t\t\t\t\t\\\n+\tFFI_HIDDEN(X)\n+#else\n+# define COMDAT(X)\n+#endif\n+\n+#if defined(__PIC__)\n+\tCOMDAT(C(__x86.get_pc_thunk.bx))\n+C(__x86.get_pc_thunk.bx):\n+\tmovl\t(%esp), %ebx\n+\tret\n+ENDF(C(__x86.get_pc_thunk.bx))\n+# if defined X86_DARWIN || defined HAVE_HIDDEN_VISIBILITY_ATTRIBUTE\n+\tCOMDAT(C(__x86.get_pc_thunk.dx))\n+C(__x86.get_pc_thunk.dx):\n+\tmovl\t(%esp), %edx\n+\tret\n+ENDF(C(__x86.get_pc_thunk.dx))\n+#endif \/* DARWIN || HIDDEN *\/\n+#endif \/* __PIC__ *\/\n+\n+\/* Sadly, OSX cctools-as doesn't understand .cfi directives at all.  *\/\n+\n+#ifdef __APPLE__\n+.section __TEXT,__eh_frame,coalesced,no_toc+strip_static_syms+live_support\n+EHFrame0:\n+#elif defined(X86_WIN32)\n+.section .eh_frame,\"r\"\n+#elif defined(HAVE_AS_X86_64_UNWIND_SECTION_TYPE)\n+.section .eh_frame,EH_FRAME_FLAGS,@unwind\n+#else\n+.section .eh_frame,EH_FRAME_FLAGS,@progbits\n+#endif\n+\n+#ifdef HAVE_AS_X86_PCREL\n+# define PCREL(X)\tX - .\n+#else\n+# define PCREL(X)\tX@rel\n+#endif\n+\n+\/* Simplify advancing between labels.  Assume DW_CFA_advance_loc1 fits.  *\/\n+#define ADV(N, P)\t.byte 2, L(N)-L(P)\n+\n+\t.balign 4\n+L(CIE):\n+\t.set\tL(set0),L(ECIE)-L(SCIE)\n+\t.long\tL(set0)\t\t\t\/* CIE Length *\/\n+L(SCIE):\n+\t.long\t0\t\t\t\/* CIE Identifier Tag *\/\n+\t.byte\t1\t\t\t\/* CIE Version *\/\n+\t.ascii\t\"zR\\0\"\t\t\t\/* CIE Augmentation *\/\n+\t.byte\t1\t\t\t\/* CIE Code Alignment Factor *\/\n+\t.byte\t0x7c\t\t\t\/* CIE Data Alignment Factor *\/\n+\t.byte\t0x8\t\t\t\/* CIE RA Column *\/\n+\t.byte\t1\t\t\t\/* Augmentation size *\/\n+\t.byte\t0x1b\t\t\t\/* FDE Encoding (pcrel sdata4) *\/\n+\t.byte\t0xc, 4, 4\t\t\/* DW_CFA_def_cfa, %esp offset 4 *\/\n+\t.byte\t0x80+8, 1\t\t\/* DW_CFA_offset, %eip offset 1*-4 *\/\n+\t.balign 4\n+L(ECIE):\n+\n+\t.set\tL(set1),L(EFDE1)-L(SFDE1)\n+\t.long\tL(set1)\t\t\t\/* FDE Length *\/\n+L(SFDE1):\n+\t.long\tL(SFDE1)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW0))\t\t\/* Initial location *\/\n+\t.long\tL(UW5)-L(UW0)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW1, UW0)\n+\t.byte\t0xc, 5, 8\t\t\/* DW_CFA_def_cfa, %ebp 8 *\/\n+\t.byte\t0x80+5, 2\t\t\/* DW_CFA_offset, %ebp 2*-4 *\/\n+\tADV(UW2, UW1)\n+\t.byte\t0x80+3, 0\t\t\/* DW_CFA_offset, %ebx 0*-4 *\/\n+\tADV(UW3, UW2)\n+\t.byte\t0xa\t\t\t\/* DW_CFA_remember_state *\/\n+\t.byte\t0xc, 4, 4\t\t\/* DW_CFA_def_cfa, %esp 4 *\/\n+\t.byte\t0xc0+3\t\t\t\/* DW_CFA_restore, %ebx *\/\n+\t.byte\t0xc0+5\t\t\t\/* DW_CFA_restore, %ebp *\/\n+\tADV(UW4, UW3)\n+\t.byte\t0xb\t\t\t\/* DW_CFA_restore_state *\/\n+\t.balign\t4\n+L(EFDE1):\n+\n+\t.set\tL(set2),L(EFDE2)-L(SFDE2)\n+\t.long\tL(set2)\t\t\t\/* FDE Length *\/\n+L(SFDE2):\n+\t.long\tL(SFDE2)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW6))\t\t\/* Initial location *\/\n+\t.long\tL(UW8)-L(UW6)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW7, UW6)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE2):\n+\n+\t.set\tL(set3),L(EFDE3)-L(SFDE3)\n+\t.long\tL(set3)\t\t\t\/* FDE Length *\/\n+L(SFDE3):\n+\t.long\tL(SFDE3)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW9))\t\t\/* Initial location *\/\n+\t.long\tL(UW11)-L(UW9)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW10, UW9)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE3):\n+\n+\t.set\tL(set4),L(EFDE4)-L(SFDE4)\n+\t.long\tL(set4)\t\t\t\/* FDE Length *\/\n+L(SFDE4):\n+\t.long\tL(SFDE4)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW12))\t\t\/* Initial location *\/\n+\t.long\tL(UW20)-L(UW12)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW13, UW12)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+#ifdef FFI_CLOSURE_CALL_INNER_SAVE_EBX\n+\tADV(UW14, UW13)\n+\t.byte\t0x80+3, (40-(closure_FS+4))\/-4  \/* DW_CFA_offset %ebx *\/\n+\tADV(UW15, UW14)\n+\t.byte\t0xc0+3\t\t\t\/* DW_CFA_restore %ebx *\/\n+\tADV(UW16, UW15)\n+#else\n+\tADV(UW16, UW13)\n+#endif\n+\t.byte\t0xe, 4\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW17, UW16)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW18, UW17)\n+\t.byte\t0xe, 4\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW19, UW18)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE4):\n+\n+\t.set\tL(set5),L(EFDE5)-L(SFDE5)\n+\t.long\tL(set5)\t\t\t\/* FDE Length *\/\n+L(SFDE5):\n+\t.long\tL(SFDE5)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW21))\t\t\/* Initial location *\/\n+\t.long\tL(UW23)-L(UW21)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW22, UW21)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE5):\n+\n+\t.set\tL(set6),L(EFDE6)-L(SFDE6)\n+\t.long\tL(set6)\t\t\t\/* FDE Length *\/\n+L(SFDE6):\n+\t.long\tL(SFDE6)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW24))\t\t\/* Initial location *\/\n+\t.long\tL(UW26)-L(UW24)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\t.byte\t0xe, 8\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\t.byte\t0x80+8, 2\t\t\/* DW_CFA_offset %eip, 2*-4 *\/\n+\tADV(UW25, UW24)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE6):\n+\n+\t.set\tL(set7),L(EFDE7)-L(SFDE7)\n+\t.long\tL(set7)\t\t\t\/* FDE Length *\/\n+L(SFDE7):\n+\t.long\tL(SFDE7)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW27))\t\t\/* Initial location *\/\n+\t.long\tL(UW31)-L(UW27)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW28, UW27)\n+\t.byte\t0xe, closure_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+#ifdef FFI_CLOSURE_CALL_INNER_SAVE_EBX\n+\tADV(UW29, UW28)\n+\t.byte\t0x80+3, (40-(closure_FS+4))\/-4  \/* DW_CFA_offset %ebx *\/\n+\tADV(UW30, UW29)\n+\t.byte\t0xc0+3\t\t\t\/* DW_CFA_restore %ebx *\/\n+#endif\n+\t.balign\t4\n+L(EFDE7):\n+\n+#if !FFI_NO_RAW_API\n+\t.set\tL(set8),L(EFDE8)-L(SFDE8)\n+\t.long\tL(set8)\t\t\t\/* FDE Length *\/\n+L(SFDE8):\n+\t.long\tL(SFDE8)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW32))\t\t\/* Initial location *\/\n+\t.long\tL(UW40)-L(UW32)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW33, UW32)\n+\t.byte\t0xe, raw_closure_S_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW34, UW33)\n+\t.byte\t0x80+3, 2\t\t\/* DW_CFA_offset %ebx 2*-4 *\/\n+\tADV(UW35, UW34)\n+\t.byte\t0xc0+3\t\t\t\/* DW_CFA_restore %ebx *\/\n+\tADV(UW36, UW35)\n+\t.byte\t0xe, 4\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW37, UW36)\n+\t.byte\t0xe, raw_closure_S_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW38, UW37)\n+\t.byte\t0xe, 4\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW39, UW38)\n+\t.byte\t0xe, raw_closure_S_FS+4\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE8):\n+\n+\t.set\tL(set9),L(EFDE9)-L(SFDE9)\n+\t.long\tL(set9)\t\t\t\/* FDE Length *\/\n+L(SFDE9):\n+\t.long\tL(SFDE9)-L(CIE)\t\t\/* FDE CIE offset *\/\n+\t.long\tPCREL(L(UW41))\t\t\/* Initial location *\/\n+\t.long\tL(UW52)-L(UW41)\t\t\/* Address range *\/\n+\t.byte\t0\t\t\t\/* Augmentation size *\/\n+\tADV(UW42, UW41)\n+\t.byte\t0xe, 0\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\t.byte\t0x9, 8, 2\t\t\/* DW_CFA_register %eip, %edx *\/\n+\tADV(UW43, UW42)\n+\t.byte\t0xe, 4\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW44, UW43)\n+\t.byte\t0xe, 8\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\t.byte\t0x80+8, 2\t\t\/* DW_CFA_offset %eip 2*-4 *\/\n+\tADV(UW45, UW44)\n+\t.byte\t0xe, raw_closure_T_FS+8\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW46, UW45)\n+\t.byte\t0x80+3, 3\t\t\/* DW_CFA_offset %ebx 3*-4 *\/\n+\tADV(UW47, UW46)\n+\t.byte\t0xc0+3\t\t\t\/* DW_CFA_restore %ebx *\/\n+\tADV(UW48, UW47)\n+\t.byte\t0xe, 8\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW49, UW48)\n+\t.byte\t0xe, raw_closure_T_FS+8\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW50, UW49)\n+\t.byte\t0xe, 8\t\t\t\/* DW_CFA_def_cfa_offset *\/\n+\tADV(UW51, UW50)\n+\t.byte\t0xe, raw_closure_T_FS+8\t\/* DW_CFA_def_cfa_offset *\/\n+\t.balign\t4\n+L(EFDE9):\n+#endif \/* !FFI_NO_RAW_API *\/\n+\n+#ifdef _WIN32\n+\t.def\t @feat.00;\n+\t.scl\t3;\n+\t.type\t0;\n+\t.endef\n+\t.globl\t@feat.00\n+@feat.00 = 1\n+#endif\n+\n+#ifdef __APPLE__\n+    .subsections_via_symbols\n+    .section __LD,__compact_unwind,regular,debug\n+\n+    \/* compact unwind for ffi_call_i386 *\/\n+    .long    C(ffi_call_i386)\n+    .set     L1,L(UW5)-L(UW0)\n+    .long    L1\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_go_closure_EAX *\/\n+    .long    C(ffi_go_closure_EAX)\n+    .set     L2,L(UW8)-L(UW6)\n+    .long    L2\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_go_closure_ECX *\/\n+    .long    C(ffi_go_closure_ECX)\n+    .set     L3,L(UW11)-L(UW9)\n+    .long    L3\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_closure_i386 *\/\n+    .long    C(ffi_closure_i386)\n+    .set     L4,L(UW20)-L(UW12)\n+    .long    L4\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_go_closure_STDCALL *\/\n+    .long    C(ffi_go_closure_STDCALL)\n+    .set     L5,L(UW23)-L(UW21)\n+    .long    L5\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_closure_REGISTER *\/\n+    .long    C(ffi_closure_REGISTER)\n+    .set     L6,L(UW26)-L(UW24)\n+    .long    L6\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_closure_STDCALL *\/\n+    .long    C(ffi_closure_STDCALL)\n+    .set     L7,L(UW31)-L(UW27)\n+    .long    L7\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_closure_raw_SYSV *\/\n+    .long    C(ffi_closure_raw_SYSV)\n+    .set     L8,L(UW40)-L(UW32)\n+    .long    L8\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+\n+    \/* compact unwind for ffi_closure_raw_THISCALL *\/\n+    .long    C(ffi_closure_raw_THISCALL)\n+    .set     L9,L(UW52)-L(UW41)\n+    .long    L9\n+    .long    0x04000000 \/* use dwarf unwind info *\/\n+    .long    0\n+    .long    0\n+#endif \/* __APPLE__ *\/\n+\n+#endif \/* ifndef _MSC_VER *\/\n+#endif \/* ifdef __i386__ *\/\n+\n+#if defined __ELF__ && defined __linux__\n+\t.section\t.note.GNU-stack,\"\",@progbits\n+#endif\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/3rd_party\/libffi\/src\/x86\/sysv.S","additions":1138,"deletions":0,"binary":false,"changes":1138,"status":"added"},{"patch":"@@ -33,2 +33,1 @@\n-         -DOSX                     \\\n-         -msse2\n+         -DOSX\n@@ -42,0 +41,4 @@\n+ifneq ($(ARCH), arm64)\n+    CFLAGS += -msse2\n+endif\n+\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/mac\/fxplugins\/Makefile","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -49,2 +49,10 @@\n-\t   -I$(LIBFFI_DIR)\/include \\\n-\t   -I$(LIBFFI_DIR)\/include\/mac\/x64\n+\t   -I$(LIBFFI_DIR)\/src \\\n+\t   -I$(LIBFFI_DIR)\/include\n+\n+ifeq ($(ARCH), arm64)\n+INCLUDES += -I$(LIBFFI_DIR)\/src\/aarch64 \\\n+\t    -I$(LIBFFI_DIR)\/include\/mac\/aarch64\n+else\n+INCLUDES += -I$(LIBFFI_DIR)\/src\/x86 \\\n+\t    -I$(LIBFFI_DIR)\/include\/mac\/x64\n+endif\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/mac\/glib-lite\/Makefile","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -54,2 +54,1 @@\n-         -DOSX \\\n-         -msse2\n+         -DOSX\n@@ -63,0 +62,4 @@\n+ifneq ($(ARCH), arm64)\n+    CFLAGS += -msse2\n+endif\n+\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/mac\/gstreamer-lite\/Makefile","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -5,0 +5,6 @@\n+ifeq ($(ARCH), arm64)\n+    MARCH = aarch64\n+else\n+    MARCH = x86\n+endif\n+\n@@ -11,1 +17,1 @@\n-DIRLIST = src src\/x86\n+DIRLIST = src src\/$(MARCH)\n@@ -19,2 +25,7 @@\n-         -DGSTREAMER_LITE \\\n-         -DX86_64\n+         -DGSTREAMER_LITE\n+\n+ifeq ($(ARCH), arm64)\n+    CFLAGS += -DAARCH64\n+else\n+    CFLAGS += -DX86_64\n+endif\n@@ -28,1 +39,9 @@\n-INCLUDES = -I$(SRCBASE_DIR)\/include -I$(SRCBASE_DIR)\/include\/mac\/x64\n+INCLUDES = -I$(SRCBASE_DIR)\/include\n+\n+ifeq ($(ARCH), arm64)\n+INCLUDES += -I$(SRCBASE_DIR)\/src\/aarch64 \\\n+\t    -I$(SRCBASE_DIR)\/include\/mac\/aarch64\n+else\n+INCLUDES += -I$(SRCBASE_DIR)\/src\/x86 \\\n+\t    -I$(SRCBASE_DIR)\/include\/mac\/x64\n+endif\n@@ -34,3 +53,15 @@\n-\t   src\/types.c \\\n-\t   src\/x86\/ffi64.c \\\n-            src\/x86\/ffiw64.c\n+\t   src\/types.c\n+\n+ifeq ($(ARCH), arm64)\n+    C_SOURCES += src\/$(MARCH)\/ffi.c\n+else\n+    C_SOURCES += src\/$(MARCH)\/ffi64.c \\\n+\t         src\/$(MARCH)\/ffiw64.c\n+endif\n+\n+ifeq ($(ARCH), arm64)\n+    ASM_SOURCES = src\/$(MARCH)\/sysv.S\n+else\n+    ASM_SOURCES = src\/$(MARCH)\/unix64.S \\\n+                  src\/$(MARCH)\/win64.S\n+endif\n@@ -38,2 +69,0 @@\n-ASM_SOURCES = src\/x86\/unix64.S \\\n-              src\/x86\/win64.S\n@@ -64,1 +93,1 @@\n-\t$(AR) -static $(OBJECTS) -arch_only x86_64 -o $@\n+\t$(AR) -static $(OBJECTS) -arch_only $(ARCH) -o $@\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/mac\/libffi\/Makefile","additions":39,"deletions":10,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -23,1 +23,2 @@\n-INCLUDES = -I$(SRCBASE_DIR)\/include\n+INCLUDES = -I$(SRCBASE_DIR)\/include \\\n+           -I$(SRCBASE_DIR)\/src\/x86\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/win\/glib-lite\/Makefile.ffi","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,1 +42,2 @@\n-\t  -I$(LIBFFI_DIR)\/include\n+\t  -I$(LIBFFI_DIR)\/include \\\n+\t  -I$(LIBFFI_DIR)\/src\/x86\n","filename":"modules\/javafx.media\/src\/main\/native\/gstreamer\/projects\/win\/glib-lite\/Makefile.gobject","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,5 @@\n+#if defined(__arm64__)\n+#ifndef TARGET_OS_MAC_ARM64\n+#define TARGET_OS_MAC_ARM64   1\n+#endif\n+#endif \/\/ __arm64__\n","filename":"modules\/javafx.media\/src\/main\/native\/jfxmedia\/Common\/ProductFlags.h","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2013, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include <Common\/ProductFlags.h>\n@@ -30,0 +31,3 @@\n+#if defined(TARGET_OS_MAC_ARM64)\n+#define ENABLE_SIMD_SSE2 0\n+#else\n@@ -31,0 +35,1 @@\n+#endif\n","filename":"modules\/javafx.media\/src\/main\/native\/jfxmedia\/Utils\/ColorConverter.c","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,2 +33,1 @@\n-         -D_GNU_SOURCE             \\\n-         -msse2\n+         -D_GNU_SOURCE\n@@ -42,0 +41,3 @@\n+ifneq ($(ARCH), arm64)\n+    CFLAGS += -msse2\n+endif\n","filename":"modules\/javafx.media\/src\/main\/native\/jfxmedia\/projects\/mac\/Makefile","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"}]}